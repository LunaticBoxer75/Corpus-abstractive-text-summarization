<url id = '0'>https://nature.com/articles/d41586-020-01451-0</url>
<title id = '0'>Restricting movement on the basis of biology threatens freedom, fairness and public health.</title>
<body id = '0'>Imagine a world where your ability to get a job, housing or a loan depends on passing a blood test. You are confined to your home and locked out of society if you lack certain antibodies. It has happened before. For most of the nineteenth century, immunity to yellow fever divided people in New Orleans, Louisiana, between the ‘acclimated’ who had survived yellow fever and the ‘unacclimated’, who had not had the disease1. Lack of immunity dictated whom people could marry, where they could work, and, for those forced into slavery, how much they were worth. Presumed immunity concentrated political and economic power in the hands of the wealthy elite, and was weaponized to justify white supremacy. Something similar could be our dystopian future if governments introduce ‘immunity passports’ in efforts to reverse the economic catastrophe of the COVID-19 pandemic. The idea is that such certificates would be issued to those who have recovered and tested positive for antibodies to SARS-CoV-2 — the coronavirus that causes the disease. Authorities would lift restrictions on those who are presumed to have immunity, allowing them to return to work, to socialize and to travel. This idea has so many flaws that it is hard to know where to begin.   On 24 April, the World Health Organization (WHO) cautioned against issuing immunity passports because their accuracy could not be guaranteed. It stated that: “There is currently no evidence that people who have recovered from COVID-19 and have antibodies are protected from a second infection”(see go.nature.com/3cutjqz). Nonetheless, the idea is being floated in the United States, Germany, the United Kingdom and other nations. China has already introduced virtual health checks, contact tracing and digital QR codes to limit the movement of people. Antibody test results could easily be integrated into this system. And Chile, in a game of semantics, says that it intends to issue ‘medical release certificates’ with three months’ validity to people who have recovered from the disease2. In our view, any documentation that limits individual freedoms on the basis of biology risks becoming a platform for restricting human rights, increasing discrimination and threatening — rather than protecting — public health. Here we present ten reasons why immunity passports won’t, can’t and shouldn’t be allowed to work. Four huge practical problems and six ethical objections add up to one very bad idea. COVID-19 immunity is a mystery. Recent data3 suggest that a majority of recovered patients produce some antibodies against SARS-CoV-2. But scientists don’t know whether everyone produces enough antibodies to guarantee future protection, what a safe level might be or how long immunity might last. Current estimates, based on immune responses to closely related viruses such as those that cause severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), suggest that recovered individuals could be protected from re-infection for one to two years. But if SARS-CoV-2 immunity instead mimics that seen with the common cold, the protection period could be shorter. Serological tests are unreliable. Tests to measure SARS-CoV-2 antibodies in the blood can be a valuable tool to assess the prevalence and spread of the virus. But they vary widely in quality and efficacy. This has led the WHO and former US Food and Drug Administration commissioner Scott Gottlieb to caution against their use in assessing individual health or immune status. Several available tests are sufficiently accurate, meaning they are validated to have at least 99% specificity and sensitivity. But preliminary data suggest that the vast majority aren’t reliable4. Low specificity means the test measures antibodies other than those that are specific to SARS-CoV-2. This causes false positives, leading people to think they are immune when they aren’t. Low sensitivity means that the test requires a person to have a high concentration of SARS-CoV-2 antibodies for them to be measured effectively. This causes false negatives in people who have few antibodies, leading to potentially immune individuals being incorrectly labelled as not immune. The volume of testing needed is unfeasible. Tens to hundreds of millions of serological tests would be needed for a national immunity certification programme. For example, Germany has a population of nearly 84 million people, so would require at least 168 million serological tests to validate every resident’s COVID-19 immune status at least twice. Two tests per person are the minimum, because anyone who tested negative might later become infected and would need to be retested to be immune certified. Repeat testing, on no less than an annual basis, would be necessary to ensure ongoing immunity. From June, the German government will receive 5 million serological tests a month from the Swiss firm Roche Pharmaceuticals — a leading supplier of one SARS-CoV-2 serological test that has been approved by regulators. This will allow only 6% of the German population to be tested each month. Even if immunity passports were limited to health-care workers, the number of tests required could still be unfeasible. The United States, for example, would need more than 16 million such tests. At the time of writing, the US Centers for Disease Control and Prevention and US public-health laboratories have performed more than 12 million diagnostic tests for SARS-CoV-2 (3% of the total US population; see go.nature.com/2wemdd2). Even South Korea, a country with high testing rates, had managed to test only 1.5% of its population by 20 May (see go.nature.com/2aztfvp). Health-care workers in Munich, Germany, take blood to test for antibodies to SARS-CoV-2.Credit: Laetitia Vancon/NYT/Redux/eyevine Too few survivors to boost the economy. The proportion of individuals known to have recovered from COVID-19 varies widely in different populations. Reports from hot spots in Germany and the United States suggest some locations could have recovery rates between 14% and 30%. In New York state, for example, where 3,000 people were tested at random in grocery shops and other public locations, 14.9% had antibodies against COVID-19 (see go.nature.com/2waaku9). But these seem to be the exception. In an April press conference, the WHO estimated that only 2–3% of the global population had recovered from the virus. Low disease prevalence combined with limited testing capacity, not to mention highly unreliable tests, means that only a small fraction of any population would be certified as free to work. Based on current numbers of confirmed US cases, for example, only 0.43% of the population would be certified. Such percentages are inconsequential for the economy and for safety. A cafe can’t open and serve customers without risk if only a fraction of its staff are certified as immune. A shop can’t turn a profit if only a minuscule proportion of customers are allowed to enter. Monitoring erodes privacy. The whole point of immunity passports is to control movement. Thus, any strategy for immunity certification must include a system for identification and monitoring. Paper documentation could be vulnerable to forgery. Electronic documentation integrated into a smartphone app would be more resistant to fraud and more effective for contact tracing, retesting and updates of immune status. But electronic documents present a more serious risk to privacy5. In some Chinese provinces, QR codes on smartphones control entrance into public places on the basis of the individual’s COVID-19 health status. However, these apps report more than COVID-19 information — including people’s locations, travel history, who they’ve come into contact with and other health information, ranging from their body temperature to whether they’ve recently had a cold. Taiwan is also using smartphone apps with alert systems that are directly linked to police departments. The United Kingdom, United States and many other countries are testing various app options. Yet there’s no guarantee that the apps will recede when COVID-19 does. China has announced that elements of its QR-code tracking system are likely to remain in place after the pandemic ends. Marginalized groups will face more scrutiny. With increased monitoring comes increased policing, and with it higher risks of profiling and potential harms to racial, sexual, religious or other minority groups. During the pandemic, China has been accused of racially profiling residents by forcing all African nationals to be tested for the virus. In other parts of the world, people from Asia have faced spikes in racialized prejudice. Before this pandemic, stop-and-frisk laws in the United States already disproportionately affected people of colour. In 2019, 88% of people who were stopped and searched in New York City were African American or Latin American (go.nature.com/2jntjym). And during the pandemic, policing continues to target people from minority groups. Between mid-March and the start of May in Brooklyn, New York, 35 of the 40 people arrested for violating physical distancing laws were black6.   These numbers are deeply concerning, but would be even more so if monitoring and policing for COVID-19 immunity were to be used for ulterior motives. For example, ‘digital incarceration’ has already increased in countries such as the United States, Brazil and Iran, where individuals have been released from prison to minimize the spread of COVID-19 and then monitored using digital ankle bracelets. In the United States, where people of colour are racially segregated by neighbourhood and disproportionately incarcerated, digital incarceration could be used to monitor large segments of certain communities. The risk would be even higher if digital monitoring were to be linked to immigration status. Unfair access. With a shortage of testing, many will not have access. Experience so far suggests that the wealthy and powerful are more likely to obtain a test than the poor and vulnerable. In tiered health-care systems, these inequities are felt even more acutely. In early March, for example, when professional sports teams, technology executives and film celebrities were getting tested, dozens of US states were conducting fewer than 20 tests per day (see https://covidtracking.com/data). The very people who need to get back to work most urgently — workers who need to keep a roof over their head and food on the table — are likely to struggle to get an antibody test. Testing children before they return to school could be a low priority, as would testing retired older people and those who face physical, mental-health or cognitive challenges. Societal stratification. Labelling people on the basis of their COVID-19 status would create a new measure by which to divide the ‘haves’ and the ‘have-nots’ — the immunoprivileged and the immunodeprived. Such labelling is particularly concerning in the absence of a free, universally available vaccine. If a vaccine becomes available, then people could choose to opt in and gain immune certification. Without one, stratification would depend on luck, money and personal circumstances. Restricting work, concerts, museums, religious services, restaurants, political polling sites and even health-care centres to COVID-19 survivors would harm and disenfranchise a majority of the population.   Social and financial inequities would be amplified. For example, employers wanting to avoid workers who are at risk of becoming unwell might privilege current employees who have had the disease, and preferentially hire those with ‘confirmed’ immunity. Immunity passports could also fuel divisions between nations. Individuals from countries that are unable or unwilling to implement immunity passport programmes could be barred from travelling to countries that stipulate them. Already people with HIV are subjected to restrictions on entering, living and working in countries with laws that impinge on the rights of those from sexual and gender minorities — such as Russia, Egypt and Singapore. New forms of discrimination. Platforms for SARS-CoV-2 immune certification could easily be expanded to include other forms of personal health data, such as mental-health records and genetic-test results. The immunity passports of today could become the all-encompassing biological passports of tomorrow. These would introduce a new risk for discrimination if employers, insurance companies, law-enforcement officers and others could access private health information for their own benefit. Such concerns have been catalogued over the past few years in debates about who should have access to genetic information, as demand rises from clinicians, researchers, insurers, employers and law enforcers, for example7. Threats to public health. Immunity passports could create perverse incentives. If access to certain social and economic liberties is given only to people who have recovered from COVID-19, then immunity passports could incentivize healthy, non-immune individuals to wilfully seek out infection — putting themselves and others at risk8. Economic hardship could amplify the incentive if an immunity passport is the only way to a pay cheque. Individuals might obtain documents illicitly, through bribery, transfer between individuals or forgery. These could create further health threats, because people claiming immunity could continue to spread the virus. Crises tend to foster nefarious trade, as happened during the Second World War when food rations in Britain caused the emergence of a robust underground exchange system. Strategies that focus on the individual — using conceptions of ethics rooted in libertarianism — contradict the mission of public health9. They distract attention from actions that benefit all, such as funding international collaborations, practising effective public-health measures and redressing income inequity. In North America (and elsewhere), because of structural inequities, people of colour are dying from COVID-19 at much higher rates than are white people, and the virus is disproportionately affecting those who live in First Nations territories. Success depends on solidarity, a genuine appreciation that we are all in this together. An ethic premised on individual autonomy is grossly inappropriate during a public-health crisis; the overarching aim must be to promote the common good. Instead of immunity passports, we contend that governments and businesses should invest available time, talent and money in two things. First is the tried and true formula of pandemic damage limitation — test, trace and isolate — that has worked well from Singapore and New Zealand to Guernsey and Hanoi. Health status, personal data and location must be anonymized. Apps that empower individuals to make safe choices about their own movements should be prioritized. Second is the development, production and global distribution of a vaccine for SARS-CoV-2. If universal, timely, free access to a vaccination becomes possible, then it could be ethically permissible to require vaccine certification for participation in certain activities. But if access to a vaccine is limited in any way, then some of the inequities we highlight could still apply, as the literature on uptake of other vaccines attests10. Threats to freedom, fairness and public health are inherent to any platform that is designed to segregate society on the basis of biological data. All policies and practices must be guided by a commitment to social justice. </body>
<date id = '0'>21 May 2020</date>
<url id = '1'>https://nature.com/articles/d41586-020-01538-8</url>
<title id = '1'>Reported rates of influenza and other infections have fallen sharply, but some communicable diseases may see a rise.</title>
<body id = '1'>Measures aimed at slowing coronavirus spread are affecting other communicable diseases.Credit: Leon Neal/Getty Lockdowns and social-distancing measures aimed at slowing the spread of coronavirus seem to have shortened the influenza season in the northern hemisphere by about six weeks. Globally, an estimated 290,000–650,000 people typically die from seasonal flu, so a shorter flu season could mean tens of thousands of lives are spared. But the net impacts on global health will be hard to unpick against the large number of deaths from COVID-19 as well as other causes in 2020 and beyond. Tracking influenza and other infectious diseases can help to reveal the effectiveness of public-health policies aimed at stopping the coronavirus pandemic. Seasonal flu cases in the northern hemisphere usually peak in February and tail off by the end of May. This year, unusually, lab-confirmed cases of influenza dropped precipitously in early April, a few weeks after the coronavirus pandemic was declared on 11 March (see ‘Flu season cut short by COVID-19 measures’). The data comes from tests of more than 150,000 samples from national influenza laboratories in 71 countries that report data to FluNet, a global surveillance system. Source: FluNet Global Influenza Surveillance and Response System. The early end to the flu season comes despite the fact that it started with a bang; in January, before the coronavirus pandemic, the influenza season was on track to be the most severe in decades. There are other possible contributors to the decline: people with flu symptoms might have avoided clinics altogether, for example, isolating at home and so not showing up in the statistics. But the response to the pandemic is likely to be an important factor: “Public-health measures such as movement restrictions, social distancing and increased personal hygiene likely had an effect on decreasing influenza and other respiratory virus transmission,” said the World Health Organization in a statement to Nature.   Local data from the state of New York show a similar pattern. Although the flu season started a few weeks earlier than usual there, the rate of cases fell sharply and the season ended five weeks early. In Hong Kong, the 2019–20 influenza season was 63% shorter than those of the previous five years, and the number of deaths from lab-confirmed flu was 62% lower1. A similar decline was seen during the 2003 epidemic of the related coronavirus that causes SARS (severe acute respiratory syndrome). Other infectious diseases might also have been affected this year, says study co-author, infectious-disease researcher Pak-leung Ho at the University of Hong Kong. In Hong Kong, compared with previous years, the number of chickenpox cases dropped by about half to three-quarters. In April, cases of measles and rubella were their lowest, globally, since at least 2016, according to provisional data available so far — only 36 cases of rubella were reported in April worldwide. Ho notes that typically these are diseases that affect children. “Closure of schools may have had the biggest impact,” he says.   Sexually transmitted infections (STIs) might also be affected, says Amanda Simanek, an epidemiologist at the University of Wisconsin–Milwaukee. Cases may decline in the absence of close contact, she says, but there may also be a decline in detection and treatment leading to a later surge. Other communicable diseases, such as tuberculosis, are more likely to see an upswing, because programmes to fight the disease have been derailed by the pandemic. The international organization the Stop TB Partnership released a report in May estimating that a 3-month lockdown and a 10-month period of recovery would cause an additional 1.37 million deaths globally during the next 5 years. The flu season in the southern hemisphere is just starting (it typically peaks in July or August); it is unclear whether a similar flu trend will be seen there. Chan, K. H. et al. Br. Med. J. 369, m1628 (2020). Download references Latest on: Diseases News 25 MAY 20 Article 20 MAY 20 Correspondence 19 MAY 20 Epidemiology News 25 MAY 20 News 22 MAY 20 Editorial 19 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '1'>21 May 2020</date>
<url id = '2'>https://nature.com/articles/d41586-020-01561-9</url>
<title id = '2'>Bumblebees bite into leaves to induce flowering up to one month earlier. Plus: Social distancing cut the flu season short and some US coronavirus testing stats are “uninterpretable”.</title>
<body id = '2'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Bumble bee (Bombus terrestris) worker damages a plant leaf. Bee-inflicted leaf damage leads to accelerated flowering and might have implications for the phenological synchrony of plants and pollinators.Hannier Pulido, De Moraes and Mescher Laboratories When pollen is scarce, bumblebees have their way of extorting more from plants. The insects bite into leaves with their mandibles and proboscises to induce flowering up to one month earlier than normal. Chemical ecologists spotted the unusual behaviour in Bombus terrestris during an unrelated laboratory experiment. Artificial cuts were not as effective, suggesting that some chemicals in the insects’ saliva could play a part. “This is one of those really rare studies that observes a natural phenomenon that hadn’t been documented before,” says ecologist John Mola. Scientific American | 4 min read Source: Science paper An April meeting planned to be held in Addis Ababa, Ethiopia, would have been the first major artificial-intelligence (AI) conference in an African country — until coronavirus forced it to go virtual. The location of the annual International Conference on Learning Representations would have made it more accessible to researchers who cannot readily get visas to Western countries, and given Ethiopia a powerful opportunity to boost its research environment. “Africans and the institutions in Africa missed a chance to make connections and secure important collaborations,” says computer scientist Meareg Hailemariam. Nature | 5 min read Measures aimed at slowing coronavirus spread are affecting other communicable diseases.Leon Neal/Getty Lockdowns stopped flu in its tracks Social-distancing measures aimed at slowing the spread of coronavirus seem to have shortened the influenza season in the northern hemisphere by about six weeks. A shorter flu season could spare tens of thousands of lives But the net impacts on global health will be hard to tease apart from the large number of deaths from COVID-19 as well as other causes in 2020 and beyond. Nature | 4 min read CDC muddies US testing numbers The US Centers for Disease Control and Prevention (CDC)’s new dashboard was meant to offer the public insight into the progress of the country’s outbreak — now the biggest in the world. Instead, the agency has caused confusion by lumping together antibody tests along with tests for active COVID-19 infections. Several states are making the same mistake, reports The Atlantic. Testing is a crucial pillar of outbreak control, and mixing the two numbers — people who are not infected now, and people who probably haven’t been infected in the past — renders the statistic “uninterpretable”, says public-health researcher Ashish Jha. The Atlantic | 8 min read First book to tackle COVID-19 economics Economist Joshua Gans has taken a brave shot at an impossible task: publishing the first book explaining the economic thinking that should guide policy in the time of COVID-19. With the situation and knowledge changing daily, the book is useful, but inevitably limited, says reviewer Philip Ball. Nature | 6 min read The proportion of 200 million tweets about the coronavirus that were probably from fake accounts designed to sow disinformation, according to an as-yet-unpublished analysis. (NPR | 4 min read) Virologist Peter Piot, who co-discovered Ebola and spent years leading the fight against HIV, opens up about getting COVID-19 on his institution’s podcast. (London School of Hygiene & Tropical Medicine podcast | 30 min listen) Features & opinion Deep learning lets computers work out statistical patterns in massive amounts of data. It is a more brute-force approach than symbolic AI, in which programmers encode explicit rules in their algorithms. Now, two computer scientists have shown that deep learning can handle mathematical symbols, too. Borrowing techniques from automated translation, they taught their neural networks to solve mathematical problems, such as integration. “Mathematicians will in general be very impressed if these techniques allow them to solve problems that people could not solve before,” says mathematician Anders Hansen. Some hope that a similar strategy could enable computers to find their own mathematical proofs. Quanta | 8 min read Source: arXiv preprint Electronic engineer Zhiyong Fan and his colleagues have built a biomimetic eye with a hemispherical retina made of perovskite nanowires. The shape gives the eye better image-sensing characteristics than a flat light sensor, and makes it appear more human. Electronic engineer Zhiyong Fan tells the Nature Podcast how he went from admiring robots on Star Trek to taking a step closer to making them a reality. Nature Podcast | 22 min listen Get the expert analysis from electronic engineer Hongrui Jiang in the Nature News & Views article. Reference: Nature paper Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Where I work Anne-Marie Coriat is Wellcome’s head of UK and Europe Research Landscape in London.Credit: Leonora Saunders for Nature The awesome migrations of seemingly frail birds inspire wildlife conservationist Corina Newsome. (The Philadelphia Inquirer | 5 min read) This week, our avian explorer Leif Pengiunson is hiding among the slightly suggestive rock towers in Göreme National Park in Turkey. Can you spot the penguin? The answer will be in Monday’s e-mail, all thanks to Briefing photo editor and penguin wrangler Tom Houghton. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '2'>22 May 2020</date>
<url id = '3'>https://nature.com/articles/d41586-020-00502-w</url>
<title id = '3'>Nature wades through the literature on the new coronavirus — and summarizes key papers as they appear.</title>
<body id = '3'>Family members of a man who died of COVID-19 mourn at a cemetery in Rio de Janeiro, Brazil.Credit: Buda Mendes/Getty 25 May — Trump’s favoured drug shows no benefit — but another drug does World leaders such as US President Donald Trump have touted the antimalarial drug hydroxychloroquine as a treatment for COVID-19, but a study of nearly 100,000 people has linked the drug to an elevated risk of death and abnormal heart rhythms. Mandeep Mehra at the Brigham and Women’s Hospital Heart and Vascular Center in Boston, Massachusetts, and his colleagues analysed the health records of more than 96,000 people treated for COVID-19. The study drew from patients at 671 hospitals on six continents (M. R. Mehra et al. The Lancet http://doi.org/ggwzsb; 2020). Roughly 15% of these patients received hydroxychloroquine, the related drug chloroquine, or one of those two paired with an antibiotic.   Compared to the people who did not take the drugs, people in all four treatment arms were more likely to die in the hospital and more likely to develop a disordered heartbeat, or arrhythmia. The authors say that although their results do not show cause and effect, only people enrolled in clinical trials should take the drugs. A separate trial of the drug remdesivir showed that it shortens the recovery of people hospitalized for COVID-19. John Beigel at the National Institutes of Allergy and Infectious Diseases in Rockville, Maryland, and his colleagues studied more than 1,000 people enrolled in a randomized, double- blind trial and found that those who took remdesivir had a median recovery time of 11 days, compared to 15 days for those who took a placebo (J. H. Beigel et al. N. Engl. J. Med. http://doi.org/dwkd; 2020). 22 May — DNA vaccines protect monkeys from coronavirus Monkeys were protected from the new coronavirus after receiving a DNA vaccine against the virus.   Seeking a strategy to stave off SARS-CoV-2 infection, Dan Barouch at Harvard Medical School in Boston, Massachusetts, and his colleagues explored vaccines composed of DNA (J. Yu et al. Science http://doi.org/dwfb; 2020). This type of vaccine prompts the recipient’s cells to make a pathogen or its components. That, in turn, stimulates the immune system. The researchers developed six DNA vaccines based on a coronavirus protein called spike and tested them in rhesus macaques (Macaca mulatta). The animals mounted an antibody response similar to the response seen in macaques and humans who’d recovered from SARS-CoV-2 infection. The team then gave doses of coronavirus to the vaccinated monkeys, which developed only mild illness. Viral multiplication in the animals was generally lower than in unvaccinated controls, probably because the vaccinated animals’ immune systems kept the virus in check. Particles (yellow) of SARS-CoV-2 infect a human cell.Credit: NIAID/NIH/SPL 21 May — Potent human antibodies could inspire a vaccine A vaccine typically works by triggering the body’s immune response, which generates antibodies that fend off a particular virus. But some viruses do not stimulate a protective antibody response, which means there’s no guarantee that a vaccine can be developed for every disease.   Davide Robbiani at Rockefeller University in New York City and his colleagues studied 68 people who had recovered from SARS-CoV-2 infection and found that they all had generated varying amounts of antibodies against the virus. A fraction of these antibodies strongly blocked the coronavirus from invading human cells (D. F. Robbiani et al. Preprint at bioRxiv http://doi.org/ggwfcm; 2020). The work has not yet been peer reviewed. People who’d recovered from severe disease had higher levels of these potent antibodies, on average, than people whose illness was milder. But every participant appeared to be capable of making them. The authors suggest that a vaccine designed to elicit these potent antibodies might be universally effective. 21 May — Monkeys resist re-infection after recovering from the virus Monkeys that had recovered from infection with the new coronavirus were protected from re-infection, although how long the protection lasts is unclear.   Public-health officials need to know whether people who have been infected with SARS-CoV-2 can be infected again. To address this issue, Dan Barouch at Harvard Medical School in Boston, Massachusetts, and his colleagues gave doses of the coronavirus to nine rhesus macaques (Macaca mulatta). The monkeys developed mild symptoms, such as appetite loss, as well as antibodies against the virus (A. Chandrashekar et al. Science http://doi.org/dwck; 2020). Roughly one month later, the researches gave the monkeys another dose of virus. Over the following two weeks, the team detected low, rapidly declining levels of viral RNA in the animals’ noses and almost none in the monkeys’ lungs. All of the monkeys mounted an antibody response to the second dose of SARS-CoV-2, suggesting that their immune systems had fought off the virus. Tape blocks the seat next to a child in school in Hannut, Belgium.Credit: John Thys/AFP/Getty 20 May — The virus ravages organs from heart to brain Autopsies have found the new coronavirus not only in the lungs, but also in the kidneys, heart, brain and other organs.   COVID-19 is principally considered a respiratory disease, but some infected people experience non-respiratory symptoms, such as stroke. Tobias Huber at the University Medical Center Hamburg-Eppendorf in Germany and his colleagues conducted autopsies on 27 people with COVID-19. They found that the virus was most abundant in the lungs, but was also present at lower levels in the kidneys, liver, heart, brain and blood (V. G. Puelles et al. N. Engl. J. Med. http://doi.org/dv56; 2020). By scrutinizing databases of genetic activity, the team found that three genes known to encourage SARS-CoV-2 infection are highly active in kidney cells. Additional analysis of 6 people detected virus in all examined kidney compartments, which helps to explain the kidney damage seen in some people with the illness. 19 May — An antibody blocks the new coronavirus and an older relative An antibody discovered in the blood of a person who survived SARS could help others to fight COVID-19.   The coronavirus that caused the 2003 SARS outbreak is a distant relative of SARS-CoV-2, the virus responsible for the current pandemic. The newfound antibody, dubbed S309, recognizes and blocks both viruses, report David Veesler at the University of Washington in Seattle, Davide Corti at Vir Biotechnology in Bellinzona, Switzerland, and their colleagues (D. Pinto et al. Nature https://doi.org/dv4x; 2020). The antibody is an immune signalling molecule that attaches to a viral protein called spike, which both viruses use to enter human cells. The team’s structural analysis shows that S309 binds to a location on spike that is distinct from the attachment site of some of the person’s other coronavirus-targeted antibodies. Two cocktails, each combining one of these two antibodies with S309, were better at blocking the virus than was each antibody alone. Friends can once again sip beer together in Prague, where officials have allowed restaurants and pubs to reopen outdoor seating areas.Credit: Gabriel Kuchta/Getty 18 May — Dogs can catch coronavirus from their owners The first two dogs reported to have coronavirus probably caught the infection from their owners, say researchers who studied the animals and members of the infected households in Hong Kong. An analysis showed that the viral genetic sequences from the dogs were identical to those from the infected people.   The researchers studied 15 dogs who lived with people with COVID-19 (T. H. C. Sit et al. Nature http://doi.org/dvt4; 2020). Only two — a Pomeranian and a German shepherd — caught the disease. The team detected viral RNA and antibodies in both dogs, and live virus in one. Neither dog became noticeably sick. The study showed no evidence that dogs can pass the infection to other dogs or to people. 15 May — Promising vaccine shields monkeys from lung damage An experimental COVID-19 vaccine protected monkeys from pneumonia and prompted a strong immune response in the animals. Vincent Munster at the National Institute of Allergy and Infectious Diseases in Hamilton, Montana, Sarah Gilbert at the University of Oxford, UK, and their colleagues designed a vaccine that encodes the new coronavirus’s spike protein, which it uses to invade host cells (N. van Doremalen et al. Preprint at bioRxiv http://doi.org/dvvd; 2020). The researchers injected 6 rhesus macaques (Macaca mulatta) with the vaccine before giving the animals high doses of virus.   The vaccinated monkeys all developed neutralizing antibodies — which can prevent a virus from entering cells — against SARS-CoV-2. Vaccinated animals had much lower levels of viral RNA in their lung tissue than unvaccinated animals, suggesting that the vaccine stopped the virus from multiplying in the monkeys’ lungs. Two of the three control monkeys developed pneumonia; none of the vaccinated monkeys did. The research has not yet been peer-reviewed. A clinical trial of the vaccine is now underway. 15 May — Lifting lockdown could spell surge of infections for France More than 20,000 people in France have died of COVID-19, but the nation’s infection rate in mid-May stood at roughly 5% — well short of the 65% needed for herd immunity.   Simon Cauchemez at the Pasteur Institute in Paris and his colleagues modelled France’s coronavirus outbreak. (H. Salje et al. Science, http://doi.org/dvt3; 2020). They found that France’s lockdown, which began 17 March, reduced viral spread by 77%. The team projected that by the time the lockdown was relaxed on 11 May, an estimated 4.4% of the population would have been infected. Some two-thirds of the population would need to be immune for immunity alone to control the epidemic. As a result, herd immunity cannot prevent “a second wave at the end of the lockdown”, the authors write. Medical workers board a bus in New York City.Credit: Alexi Rosenfeld/Getty 14 May — ‘Superspread’ at a choir practice infects dozens A single ill person who attended a choir practice in Washington State led to the probable infection of more than 50 choir members, including 2 who died. Lea Hamner and her colleagues at Skagit County Public Health in Mount Vernon, Washington, analysed numerous local cases of COVID-like illness and traced them to an evening choir practice on 10 March (L. Hamner et al. Morb. Mortal Wkly. Rep. 69, 606–610; 2020). One symptomatic person attended the 2.5-hour practice and later tested positive for SARS-CoV-2. Of the other 60 people in attendance, 32 became ill with confirmed COVID-19 and an additional 20 became ill with probable infections.   Choir members sat in close-packed rows and sang for long periods, which might have contributed to viral transmission. This superspreading event emphasizes the importance of avoiding crowds and close interactions to keep the virus at bay, the authors say. 14 May — The very youngest children are most likely to enter hospital Children with COVID-19 are at a lower risk of death than are adults with the disease, according to the largest study of infected children in Europe. Silvia Garazzino at the University of Turin, Italy, and her colleagues analysed data from children under the age of 18 who turned up at hospitals and clinics with COVID-19 symptoms. All 168 who tested positive for the coronavirus recovered fully (S. Garazzino et al. Preprint at Eurosurveillance http://doi.org/dvk8; 2020). The study has not yet been peer-reviewed.   Nearly 80% of infants under the age of one were hospitalized, compared with 53% of those between the ages of 11 and 17. A national survey estimates that the overall hospitalization rate for infected children in Italy is much lower — around 4%. Two-thirds of the children had at least one infected parent, whose symptoms often appeared before the child’s did. 13 May — New York City’s infection hotspots have high numbers of commuters New York City neighbourhoods that were COVID-19 hotspots between March and May correlate with those that were home to the highest number of commuters over the past three months. To understand why deaths and hospitalizations from COVID-19 varied so substantially between the city’s neighbourhoods, Stephen Kissler at the Harvard T.H. Chan School of Public Health in Boston, Massachusetts, and his colleagues compiled coronavirus test results from about 1,700 women who came to 6 city hospitals to give birth (S. M. Kissler et al. Preprint at https://bit.ly/2Aq7dpb; 2020).   The team analysed the postal codes of infected women to estimate disease prevalence in city neighbourhoods. The researchers then compared this information with location data from Facebook that revealed the number of daily trips that people take into and out of each neighbourhood, and found a link between a neighbourhood’s infection rate and the number of trips taken by its residents. Many of the commuters are probably ‘essential workers’, who should be protected to prevent the virus’s spread, the authors say. A cell (green; artificially coloured) is heavily infected with the virus (pink) that causes COVID-19.Credit: NIAID/NIH/Reuters 12 May — The body launches a sweeping antibody response to coronavirus People infected by the new coronavirus make antibodies against several of the virus’s proteins — a finding that could lead to more effective vaccines and more sensitive tests to determine who has already been infected and might now be immune. Niloufar Kavian and Sophie Valkenburg at the University of Hong Kong and their colleagues wanted to determine which SARS-CoV-2 proteins are targeted by immune molecules called antibodies, which help to fight infection.  Much of the effort to develop vaccines and diagnostic tests has focused on a viral protein called Spike. But these results, which have not yet been peer-reviewed, suggest that other proteins might also be important determinants of immunity against SARS-CoV-2. 11 May — High risk of COVID-19 death for minority ethnic groups is a troubling mystery People who are not white face a substantially higher risk of dying from COVID-19 than do white people — and pre-existing health conditions and socioeconomic factors explain only a small part of the higher risk.   In the most sweeping study of its kind, Ben Goldacre at the University of Oxford, UK, and his colleagues examined the medical records of more than 17 million residents of England (E. Williamson et al. Preprint at medRxiv http://doi.org/dt9z; 2020). The analysis, which has not yet been peer reviewed, showed that medical conditions such as diabetes are linked to a higher risk of death from the new coronavirus. But the prevalence of such conditions in people who belong to minority ethnic groups plays only a small part in the heightened risk, as does the prevalence of social disadvantages such as low income. The researchers say that there is an urgent need for better measures to protect people in minority ethnic groups from the disease. A person with COVID-19 is taken off a train that carried patients from Paris to cities with less-crowded hospitals.Credit: Fred Tanneau/AFP/Getty 8 May — A strong antibody response is common in people who’ve recovered Nearly everyone who recovers from COVID-19 makes antibodies against the new coronavirus, according to a study of more than 1,300 people who had symptoms of the disease.   Ania Wajnberg, Carlos Cordon-Cardo and their colleagues at the Icahn School of Medicine at Mount Sinai in New York City found that more than 99% of study participants who had been infected eventually developed antibodies — suggesting that they are immune from reinfection for an unknown length of time (A. Wajnberg et al. Preprint at medRxiv http://doi.org/dt5t; 2020). The immune response could be slow: some study volunteers didn’t produce detectable antibodies until one month after they first started feeling ill. The team found that a person’s age and sex didn’t affect their chance of developing antibodies. Almost 20% of study volunteers tested positive for viral RNA two or more weeks after their symptoms ended. This might mean that the presence of viral RNA is not a good indicator of whether the body has cleared the virus. The study has not yet been peer reviewed. 7 May — Even laypeople could use this new test to detect the coronavirus A test that uses a CRISPR gene-editing system can detect the new coronavirus in an hour, without the need for specialized equipment or trained personnel.   Feng Zhang at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, and his colleagues sought to develop a test for SARS-CoV-2 that would be quicker and simpler than the current procedure, which requires expensive lab equipment and scarce reagents. The team’s CRISPR-based protocol can be performed by a layperson with access to a sous vide cooker, a piece of kitchen equipment that is commonly available for less than US$40 (J. Joung et al. Preprint at https://go.nature.com/35csgqk; 2020). The test makes results available on paper strips similar to those used in pregnancy tests. The results have not yet been peer reviewed. The team says the test could be used in doctors’ offices, workplaces and other settings where fast diagnosis is necessary. 6 May — Speedy technique churns out synthetic viruses Researchers have used yeast cells to create a synthetic version of the SARS-CoV-2 genome much more quickly than other methods can achieve.   The SARS-CoV-2 genome is composed of RNA, but the protocol developed by Joerg Jores and Volker Thiel at the University of Bern in Switzerland and their colleagues uses a dozen overlapping stretches of the SARS-CoV-2 genome converted into DNA (T. T. N. Thao et al. Nature http://doi.org/ggttcr; 2020). The team inserted these DNA fragments into cells of the yeast Saccharomyces cerevisiae, which stitched them into a complete viral genome. The team then built live viruses by converting the synthetic genome back into RNA and inserting these strands into human cells. The synthetic coronaviruses took a week to make. The technique could be used to assemble viruses rapidly to study the biological effects of new mutations, the researchers say. Students wear face masks to return to school in Shanghai. Credit: Tang Yanjun/China News Service/Getty 5 May — What stopped the epidemic in China? Two teams show there is no easy answer New evidence shows the value of school closures, travel bans and other painful measures in curbing the coronavirus epidemic in China. Marco Ajelli at the Bruno Kessler Foundation in Trento, Italy, Hongjie Yu at Fudan University in Shanghai and their colleagues found that after authorities mandated a stringent lockdown, people in Shanghai and Wuhan cut their encounters with others from 15–20 per day to roughly 2 per day (J. Zhang et al. Science http://doi.org/ggthtr; 2020). This drastic social distancing was enough to bring the epidemic under control in the two cities. The team’s modelling work suggests that, in Shanghai, school closures alone would not have stopped the epidemic — but did lower the number of new infections per day at the epidemic’s peak, which relieved stress on hospitals.   Another study, by Shengjie Lai at the University of Southampton, UK, and his colleagues, shows that quick detection of infections and isolation of infected people were the most effective steps for containing COVID-19 cases in China (S. Lai et al. Nature http://doi.org/dtr4; 2020). But even with those efforts in place, the number of cases would have soared if officials hadn’t restricted travel and social interactions, as well. If epidemic-control actions had been delayed by only three weeks, the number of infected people in China might have been 18 times higher, the authors found. 4 May — Portraits of a viral enzyme could aid hunt for drugs Molecular snapshots of a key SARS-CoV-2 enzyme in action provide clues to how drugs, including the experimental therapy remdesivir, attack the virus. Remdesivir has been shown in an early trial to speed up the recovery of people with COVID-19. The compound blocks the action of a viral enzyme called an RNA-dependent RNA polymerase. A team led by Patrick Cramer at the Max Planck Institute for Biophysical Chemistry in Göttingen, Germany, used an imaging technique called cryo-electron microscopy to map the 3D shape of the enzyme as it copied the virus’s genetic material (H. S. Hillen et al. Preprint at bioRxiv http://doi.org/dtgw; 2020; not peer reviewed before posting). The researchers say that further studies of the polymerase could lead to the identification of new antiviral compounds.   A separate team led by Eric Xu at the Shanghai Institute of Materia Medica in China solved the structure of the polymerase while it was linked to an RNA snippet incorporating a molecule of remdesivir (W. Yin et al. Science http://doi.org/dtnb; 2020). The results could help researchers to design powerful drugs that block the polymerase’s activity, the authors say. People queue for a train in Yichang, China, after the lockdowns ease.Credit: STR/AFP/Getty 1 May — Immune system shows abnormal response to COVID-19 The immune response to SARS-CoV-2 differs from the response prompted by other respiratory viruses, according to an analysis of infected cells, ferrets and people. The finding supports the idea that treatments targeting the immune system could help people with COVID-19. Benjamin tenOever at the Icahn School of Medicine at Mount Sinai in New York City and his colleagues found that cells infected with SARS-CoV-2 produce unusually low levels of antiviral proteins called interferons compared with cells infected with other respiratory viruses (D. Blanco-Melo et al. Cell https://go.nature.com/3bWE82b, 2020). But levels of some proteins, such as IL-6, that activate more general immune responses are higher in infected ferrets and people than in uninfected controls. The results suggest an immune imbalance: low levels of interferons reduce a cell’s ability to limit viral replication, and the activation of less-specific immune responses promotes inflammation. 30 April — Young children are not immune to COVID-19 Children are as likely as adults to become infected with SARS-CoV-2 after close contact with an infected person, according to a study of people in Shenzhen, China.   Justin Lessler at Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland, Tiejian Feng at the Shenzhen Center for Disease Control and Prevention and their colleagues analysed nearly 400 cases of COVID-19 and 1,300 people who were ‘close contacts’ of the infected people (Q. Bi et al. Lancet Inf. Dis. http://doi.org/dtd7; 2020). The team found that 7% of close contacts younger than age 10 became infected — roughly the same as in the population overall. The work was first posted online as a preprint 27 March (http://doi.org/dpf9). The researchers also found that just 9% of original cases were responsible for 80% of infections detected in close contacts. Such ‘superspreading’ events could lead to “large COVID-19 clusters”, the authors write. SARS-CoV-2 binds to a target cell by deploying viral proteins (red) to connect to ACE2 proteins (blue) on the target’s surface.Credit: Juan Gaertner/SPL 29 April — SARS-CoV-2 might invade by hijacking its host’s immune defences The new coronavirus invades human cells after one of its proteins binds with ACE2, a protein found in cells in many human organs. But little has been known about that crucial interaction. To learn more, Alex Shalek at Harvard Medical School and the Massachusetts Institute of Technology (MIT) in Boston, Jose Ordovas-Montanes at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, and their colleagues studied airway cells from people with influenza (C. G. K. Ziegler et al. Cell http://doi.org/ds9j ; 2020). Both influenza virus and SARS-CoV-2 invade the respiratory tract.   The team found that in people with flu, signalling molecules called interferons — which normally help to fend off viruses — switch on the host genes encoding the ACE2 protein. The result suggests that the body’s defences against viral attack drive the activation of the gene for ACE2. 28 April — ‘Dry swabbing’ offers a workaround to test-chemical scarcity Wide-scale genetic testing for SARS-CoV-2 has been hampered, in part, by shortages of the solutions used to store sampling swabs and extract viral RNA from them. To overcome this difficulty, a team led by Lea Starita and Jay Shendure at the University of Washington in Seattle developed a procedure for detecting viral RNA in swabs without the highly sought solutions (S. Srivatsan et al. Preprint at bioRxiv http://doi.org/ds6k; 2020; not peer reviewed before posting). The ‘dry swab, extraction-free’ procedure correctly detected viral RNA in 9 out of 11 samples from people known to have SARS-CoV-2 infections. Conventional extraction methods yielded positive results in only 8 of the 11. The researchers say that their protocol could enable a massive scale-up in the use of self-collected samples for genetic testing at centralized laboratories. A health-care worker preparing to test people for SARS-CoV-2 holds throat swabs, now a scarce and sought-after resource. Credit: Mohd Rasfan/AFP/Getty 27 April —  Hospital toilets can be a hotspot for airborne viral RNA The new coronavirus’s RNA can travel through the air, and might spread by way of small particles exhaled by infected people. Ke Lan at Wuhan University in China and his colleagues tested the concentration of SARS-CoV-2 RNA in aerosols — fine airborne particles — at two hospitals treating people with COVID-19 (Y. Liu et al. Nature https://doi.org/10.1038/s41586-020-2271-3; 2020). The team detected elevated levels of viral RNA in locations such as a small toilet used by patients, and staff changing rooms. No viral RNA was detected in staff rooms after they had been disinfected. Low to undetectable levels were found in the hospitals’ well-ventilated patient wards. The presence of airborne viral RNA suggests that SARS-CoV-2 has the potential to spread by way of aerosols, the researchers say. They suggest that measures such as routine disinfection and better ventilation could help to control the virus’s spread. 24 April — Spit could be the solution to testing shortages A person’s saliva accurately reveals whether they are infected with SARS-CoV-2, a finding that could make tests for the virus safer and more widely available. The gold-standard test for coronavirus infection requires a long swab to be rubbed against the back of the throat. But such swabs are in short supply, and swabbing can prompt people to cough or sneeze, potentially launching a barrage of viral particles. Anne Wyllie at the Yale School of Public Health in New Haven, Connecticut, and her colleagues collected both saliva and throat samples from people hospitalized with COVID-19 (A. Wyllie et al. Preprint at medRxiv, http://doi.org/ggssqf, 2020; not peer reviewed before posting). The team’s testing did not detect the virus in some patients’ throat-swab samples — but did detect it in the same patients’ saliva samples. Saliva testing also showed that two health-care workers who felt fine and had negative throat tests were actually infected. A human cell (blue; artificially coloured) infected with SARS-CoV-2 (yellow).Credit: NIAID/NATIONAL INSTITUTES OF HEALTH/SPL 23 April — Intensive testing finds a small town’s many silent infections A large proportion of people with COVID-19 have no symptoms, according to research in a small Italian town. On 21 February, the town of Vo’ reported Italy’s first COVID-19 death, leading authorities to ban movement in the town and end public services and commercial activities there for two weeks. Andrea Crisanti at Imperial College London and his colleagues swabbed almost every resident of Vo’ for viral RNA at the beginning and end of the lockdown. The team found that some 43% of the people infected with SARS-CoV-2 in the town reported no fever or other symptoms (E. Lavezzo et al. Preprint at medRxiv http://doi.org/ggsmcj; 2020; not peer reviewed before posting). The researchers observed no statistically significant difference in potential infectiousness between those who reported symptoms and those who did not. Asymptomatic and pre-symptomatic individuals have a key role in COVID-19 transmission, which makes it difficult to control the disease without strict social distancing, the authors say. 22 April — A vaccine candidate shows early success in an animal trial An experimental vaccine protects monkeys from infection with the virus that causes COVID-19. A team led by Chuan Qin at the Peking Union Medical College in Beijing injected rhesus macaques (Macaca mulatta) with three doses of a vaccine comprised of chemically inactivated particles of SARS-CoV-2 (Q. Gao et al. Preprint at bioRxiv http://doi.org/dskt; 2020; not peer reviewed before posting). Eight monkeys were then intentionally exposed to the virus. All four monkeys given a high dose of the vaccine had no detectable virus in their throat or lungs seven days after exposure. Monkeys that received a lower dose of vaccine showed some signs of coronavirus infection — but their levels of virus were much lower than in exposed animals that received no vaccine. This month, the company developing the vaccine received approval to start human safety trials on it. A man with COVID-19 is treated at an intensive care unit in Rome.Credit: Antonio Masiello/Getty 20 April — How Hong Kong stemmed viral spread without harsh restrictions Hong Kong slowed the spread of SARS-CoV-2 through a combination of intensive surveillance, quarantining and social distancing without relying on severe measures used elsewhere. In January, the authorities in Wuhan, where the coronavirus outbreak began, halted travel out of the city in an attempt to control the spread of the virus that causes COVID-19. But Hong Kong relied on a programme that included widespread testing, quarantining of those who had been in contact with infected people, and distancing measures such as school closures. When Peng Wu at the University of Hong Kong and her colleagues surveyed residents in early March, 99% said they wore a mask in public and 85% said they avoided crowds (B. J. Cowling et al. Lancet Public Health http://doi.org/dsfw; 2020). The combination of public behavioural changes and government measures kept the virus’s spread relatively low in Hong Kong during the period to the end of March, the team found. 17 April — Vaccine from viral spikes holds promise A key portion of a coronavirus protein could form the basis of a safe and effective vaccine. Coronavirus particles bristle with spiny ‘spike proteins’. A portion of the spike called the receptor-binding domain recognizes and attaches to a molecule found on the surface of many human cells, allowing the viral particle to gain entry into those cells. Hyeryun Choe and Michael Farzan at the Scripps Research Institute in Jupiter, Florida, and their colleagues immunized rats with fragments of the spike’s binding domain (B. D. Quinlan et al. Preprint at bioRxiv, http://doi.org/ggrs5t; 2020; not peer reviewed before posting). In response, the rodents’ immune systems made antibodies that can recognize coronavirus and prevent it from infecting cells. One of the spike proteins (red) on a SARS-CoV-2 particle grabs a receptor on a cell.Credit: SPL Further experiments suggested that these antibodies are unlikely to make host cells more susceptible to coronavirus infection — one of the main safety concerns for vaccines. 16 April — Ski buffs helped to seed coronavirus in Iceland Holidaymakers returning from ski trips to the Alps helped to bring the coronavirus to Iceland. In late January, Kari Stefansson at deCODE Genetics-Amgen in Reykjavik and his colleagues began testing for SARS-CoV-2 among Iceland residents at high risk of exposure to the virus, such as travellers to China (D. F. Gudbjartsson et al. N. Engl. J. Med. http://doi.org/ggr6wx; 2020). Some 13% of the 9,199 people tested by early April were infected. The team sequenced viral RNA from people who tested positive and found that some of the strains had probably originated in Austria or Italy, which both have Alpine ski resorts. Tests in the second half of March on more than 2,000 randomly selected individuals found that only 0.6% were infected. The researchers say their analysis suggests that measures to contain the virus through testing, contact tracing and quarantining have been successful in Iceland. A healthcare worker dressed for a shift in the COVID-19 intensive care unit in Lisbon.Credit: Patricia De Melo Moreira/AFP/Getty 15 April — Relief from social distancing could unleash the virus anew Cases of COVID-19 are likely to surge after current social-distancing measures are eased, according to models. Yonatan Grad, Marc Lipsitch and their colleagues at the Harvard T.H. Chan School of Public Health in Boston, Massachusetts, modelled the spread of coronaviruses in places that have temperate climates, such as the United States. The results helped the team to predict the spread of SARS-CoV-2, the coronavirus that causes COVID-19 (S.M. Kissler et al. Science http://doi.org/drz3; 2020). The researchers found that if SARS-CoV-2 spreads more efficiently in some seasons than in others — as influenza virus does, for example — the peak number of COVID-19 cases after social distancing ends could be larger than the peak number without any social distancing at all. That’s because distancing measures leave a high proportion of people susceptible to infection, leading to a spike of disease if viral transmission ramps up late in the year. If human immunity to SARS-CoV-2 wanes over the course of a few years, the virus is likely to cause repeated outbreaks in wintertime, the authors say. 15 April — Common sequencing technique could speed large-scale diagnosis A standard genomic-analysis method that can sequence tens of thousands of DNA samples in a day has been adapted to detect the virus that causes COVID-19. In a testing protocol proposed by Jonathan Schmid-Burgk at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, and his team, every sample being tested for SARS-CoV-2 would be tagged with a unique DNA sequence that would serve as a biological barcode (J. L. Schmid-Burgk et al. Preprint at bioRxiv, http://doi.org/drzc; 2020; not peer reviewed before posting). High-speed sequencing instruments common in research laboratories around the world could then be used to analyse as many as 100,000 DNA samples at one time. The authors anticipate that if clinical testing validates the method, then millions of samples could be analysed per day at each sequencing site — a far more efficient output than that of current testing techniques. Correction: An earlier version did not include the name of the paper’s corresponding author, Jonathan Schmid-Burgk. 10 April — A viral enzyme’s structure points to possible drugs Scientists have detailed the crystal structure of one of SARS-CoV-2’s key proteins, an enzyme called a protease that the virus needs to replicate within our cells. Hualiang Jiang, Zihe Rao and Haitao Yang at ShanghaiTech University in China and their colleagues deposited the structure in a protein data bank two months ago, and have since used it to help them identify compounds that inhibit the protease (Z. Jin et al. Nature https://doi.org/10.1038/s41586-020-2223-y; 2020). The team’s screening revealed several powerful viral inhibitors, including ebselen, whose safety has already been tested in people. These inhibitors work by infiltrating a hollow in the protease. Proteases found in other coronaviruses have a similar hollow, raising hopes that a single compound might help to treat a wide variety of diseases caused by coronaviruses. One of SARS-CoV-2’s key enzymes consists of two units (blue and red; artist’s impression) and includes a hollow where candidate drugs (green) can bind the enzyme.Credit: Z. Jin et al./Nature 9 April — Absent antibodies suggest mystery immune response After recovering from infection with SARS-Cov-2, many people have high levels of antibodies against the virus. But a recent study finds that in some recovered patients, such antibodies are present at very low levels — and in some cases are undetectable. When a foreign microbe intrudes on the body, the immune system usually makes proteins called antibodies that help to fight off the invader. A team led by Jinghe Huang and Fan Wu at Fudan University in Shanghai, China measured antibodies to the novel coronavirus in 175 volunteers who had recovered from mild infections (F. Wu et al. https://www.medrxiv.org/content/10.1101/2020.03.30.20047365v1, 2020; not peer reviewed before posting). About 30% of the volunteers — and especially those under the age of 40 — never developed high levels of SARS-CoV-2 antibodies, suggesting that other immune responses helped rid them of their infections. 8 April — Viral load soars as infected people start feeling ill Viral RNA levels are highest in people with COVID-19 soon after their symptoms appear, according to two separate research teams. Kwok-Yung Yuen at The University of Hong Kong–Shenzhen Hospital, China, and his colleagues analysed saliva samples coughed up by 23 people infected with SARS-CoV-2. The team found that study participants’ viral concentrations peaked shortly after they started feeling ill, and began declining about one week after the peak. The more viral RNA detected in a person’s body, the more they excrete when coughing or sneezing. The authors say that the high levels of SARS-CoV-2 particles detected at the onset of symptoms suggest that the virus can be transmitted easily between people, even when symptoms are relatively mild (K. K.-W. To et al. Lancet Infect. Dis. http://doi.org/ggp4qx; 2020). The results are consistent with another study of nose and throat swabs from 18 people with COVID-19. The concentrations of viral RNA in the 17 symptomatic patients were similar to that in the one asymptomatic patient (L. Zou et al. N. Engl. J. Med. http://doi.org/ggmzsp; 2020). However, another study found that people with milder COVID-19 symptoms on admission to hospital had much lower concentrations of viral RNA than did those with more severe symptoms (Y. Liu et al. Lancet Infect. Dis. http://doi.org/dqrr; 2020). Wei Zhang at The First Affiliated Hospital of Nanchang University, China, Leo Poon at the University of Hong Kong, and their colleagues say the findings suggest that viral RNA concentrations could predict whether infected people will develop more severe symptoms. Particles (blue) of the virus that causes COVID-19.Credit: NATIONAL INFECTION SERVICE/SPL 7 April — A comparison finds subtle differences between tests for the COVID-19 virus Doctors rely on a test called quantitative reverse-transcription polymerase chain reaction (qRT-PCR) to determine whether a person is infected with SARS-CoV-2. A team led by Nathan Grubaugh at Yale School of Public Health in New Haven, Connecticut, compared nine widely used versions of the test and found that all of them reliably detect the virus (C. B. F. Vogels et al. Preprint at medRxiv https://www.medrxiv.org/content/10.1101/2020.03.30.20048108v1; 2020; not peer reviewed before posting). But the researchers also found that some tests — including one made by the US Centers for Disease Control and Prevention, another developed at Hong Kong University, and a third from Charité–Universitätsmedizin Berlin — performed best when it came to detecting low levels of the virus in samples. 5 April — Bats harbour a pool of coronaviruses related to pandemic culprit Viruses closely related to SARS-CoV-2, the virus causing the COVID-19 pandemic, have been circulating in horseshoe bats, ready to jump to humans, for decades — and maybe even longer. David Robertson at the University of Glasgow, UK, and his colleagues analysed the RNA of 68 coronaviruses, including SARS-CoV-2 and the virus that causes severe acute respiratory syndrome, or SARS (M. F. Boni et al. Preprint at bioRxiv https://doi.org/10.1101/2020.03.30.015008; 2020; not peer reviewed before posting). This analysis shows that horseshoe bats (Rhinolophus spp.) host an expanding lineage of viruses that, like SARS-CoV-2, can infect humans. The team estimates that the ancestor of SARS-CoV-2 split 40 to 70 years ago from the closely related bat virus RaTG13. Though the two viruses are highly similar genetically, RaTG13 doesn’t infect humans.   The analysis also suggests that viruses in the lineage are ready to jump to humans directly from bats. But SARS-CoV-2 might have first hopped to another species that humans are more exposed to, rather than spreading straight from bat to human. 3 April — Masks could cut spread of COVID-19 virus Surgical face masks effectively block the spread of seasonal coronaviruses in respiratory droplets, suggesting that masks could prevent transmission of SARS-CoV-2. Seasonal coronaviruses are one cause of the common cold. Benjamin Cowling at the University of Hong Kong and his colleagues had ill volunteers who were infected with seasonal coronaviruses sit in an enclosed booth and place their faces in a sampling device, called the Gesundheit-II, that captures airborne particles (N. H. L. Leung et al. Nat. Med. https://doi.org/10.1038/s41591-020-0843-2; 2020). The scientists detected coronavirus RNA in both coarse droplets and finer ‘aerosol’ droplets emitted by volunteers who were not wearing masks. Mask reduced detection of viral RNA in both types of droplet. Larger particles are carried by sneezes and coughs, whereas exhaled breath can spread aerosol droplets, which have a diameter of five micrometres or less. The authors say that surgical masks reduce transmission of not only seasonal coronaviruses, but also influenza. Correction: An earlier version of this article said masks reduced detection of viral DNA. 1 April — Antibodies from llamas help to foil the COVID-19 virus Antibodies from llamas (Lama glama) could help in the fight against several coronaviruses that infect humans. A team led by Bert Schepens and Xavier Saelens of the VIB life-sciences institute in Ghent, Belgium, and Jason McLellan of the University of Texas at Austin has isolated two llama antibodies that bind the ‘spike’ proteins that coronaviruses use to enter cells (D. Wrapp et al. Preprint at bioRxiv https://doi.org/10.1101/2020.03.26.010165; 2020; not peer reviewed before posting). One antibody neutralized the coronavirus responsible for Middle East respiratory syndrome (MERS); the second mopped up the severe acute respiratory syndrome (SARS) coronavirus. Fusing the SARS antibody from a llama with an antibody from a human yielded a hybrid that neutralized the virus responsible for COVID-19. The data suggest that such antibodies could be useful in combating coronavirus epidemics 30 March — Debilitated patients rally after dose of survivors’ blood People seriously ill with COVID-19 experienced striking improvement after receiving infusions of blood from disease survivors, according to two separate research teams. Both teams extracted antibody-laden plasma — a component of blood — from people who’d recovered from COVID-19.   Xiaoming Yang at the National Engineering Technology Research Center for Combined Vaccines in Wuhan, China, and his colleagues gave the plasma to ten severely ill people. By the sixth day after the treatment, the virus that causes COVID-19 was undetectable in seven of the ten. The recipients experienced no significant side effects (K. Duan et al. Preprint at medRxiv http://doi.org/dqrs; 2020; not peer reviewed before posting). A group led by Lei Liu at Shenzhen Third People’s Hospital in China gave survivors’ plasma to five “critically ill” people (C. Shen et al. J. Am. Med. Assoc. http://doi.org/dqn7; 2020). Symptoms dwindled in all five; within ten days of receiving the plasma, three recipients no longer needed ventilators. Other researchers would like to try such transfusions to treat health workers who have been directly exposed. 27 March — Viral proteins point to potential treatments A list of the human proteins affected by the SARS-CoV-2 virus offers a guide to potential treatments for infected people. A team led by Nevan Krogan at the University of California, San Francisco, engineered human cells to produce one of 26 proteins made by the coronavirus (D. E. Gordon et al. Preprint at bioRxiv https://doi.org/10.1101/2020.03.22.002386; 2020; not peer reviewed before posting). This allowed the researchers to identify human proteins that physically interact with coronavirus proteins. Out of 332 interactions between human and viral proteins, the authors identified 67 that existing or candidate drugs could potentially disrupt. The researchers and their collaborators are now testing some of these compounds for antiviral activity — and urge others to do the same. Latest on: Diseases News 21 MAY 20 Article 20 MAY 20 Correspondence 19 MAY 20 Epidemiology News 22 MAY 20 News 21 MAY 20 Editorial 19 MAY 20 Medical research News 22 MAY 20 News 21 MAY 20 World View 19 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '3'>25 May 2020</date>
<url id = '4'>https://nature.com/articles/d41586-020-01540-0</url>
<title id = '4'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '4'>   Hanoch Gutfreund & Jürgen Renn Princeton Univ. Press (2020) Albert Einstein admitted in his final essay, ‘Autobiographical sketch’, that fellow physicists opposed his quest to unify the general theory of relativity with quantum mechanics. But he took comfort from philosopher Gotthold Lessing’s dictum: “The search for truth is more precious than its possession.” The 1955 work appears in English for the first time in this outstanding study of another essay, which Einstein called his “obituary”: 1949’s ‘Autobiographical notes’. Physicist Hanoch Gutfreund and historian Jürgen Renn provide a sparky commentary.    Philip Matyszak Thames & Hudson (2020) Western ideas on antiquity are dominated by Egyptians, Babylonians, Assyrians, Hebrews, Greeks and Romans, with other cultures often reduced to stereotypes. Historian Philip Matyszak asks: were the Philistines philistines and the Vandals vandals? His stimulating encyclopaedia of 40 “forgotten peoples” begins with the Akkadians around 2330 bc and ends with the Hephthalites (‘White Huns’) in the fifth century ad. Illustrations include a Roman-style Vandal mosaic; far from vilifying Roman culture, the Vandals respected it, say current historians.    Jason Brennan Johns Hopkins Univ. Press (2020) Economist and strategist Jason Brennan delivers a data-driven, punchily practical guide to succeeding in academia, aimed at PhD students. He knows how success requires narrow professionalism, but he also networks widely. And if a PhD does not yield an academic job, all is far from lost. “Faculty jobs are the nail for which the PhD is the hammer,” he concludes in the chapter ‘Exit options’. Yet the hammer can be repurposed for diverse non-academic jobs: the US unemployment rate for PhD holders is just 1.7%.    Eds Lexi Jamieson Marsh & Ellen Currano Columbia Univ. Press (2020) “Many of our Bearded Ladies became professional palaeontologists because they did not want to spend every workday inside, in an office, behind a desk,” writes palaeobotanist Ellen Currano, co-founder of the Bearded Lady Project with film-maker Lexi Jamieson Marsh, in their photo-biography of a weirdly compelling collaboration. It began six years ago, out of despair at male dominance of their professions. “Maybe I should sport a beard,” Currano joked. Dozens of female geoscientists have now posed, artificially hirsute.    Eds Saru Jayaraman & Kathryn De Master Univ. California Press (2020) In this cleverly titled collection, attorney Saru Jayaraman and rural sociologist Kathryn De Master conclude that corporations control much of our food because of “their unbridled, unregulated power over our democracy”. Articles on seeds, labour, hunger and more describe calls to action and collective response, such as mobilization of New York state residents to force a ban on fracking, because of its potential to harm farms. Only direct public confrontation with corporate food elites will succeed, the editors argue. </body>
<date id = '4'>25 May 2020</date>
<url id = '5'>https://nature.com/articles/d41586-020-01543-x</url>
<title id = '5'>Researchers looking to make tests widely available worry as regulators freeze the team that first identified US community spread.</title>
<body id = '5'>Coronavirus tests where specimens are collected at home face high barriers to approval.Credit: Grant Hindsley/The New York Times/Redux/eyevine The Seattle research team that first uncovered COVID-19 spreading in US communities has been asked to stop testing for the disease. The decision by the US Food and Drug Administration (FDA) to prevent the SCAN project from analysing nose swabs sent from people’s homes—and reporting the results— is likely to be temporary. But it deflates local and national public-health initiatives. “SCAN provided valuable data to help us more completely understand the epidemiology of the outbreak,” says Jeff Duchin, a health officer at the public-health department for Seattle and King County in Washington state. The programme processed 20,000 tests during the past 10 weeks and helped to reveal which communities in Seattle were being hit hardest by COVID-19. Duchin says the city has begun to integrate the programme into its contact tracing, so that people who have been exposed to the virus can find out whether they’ve been infected and might pass it on to others.   The project, which is backed by philanthropist Bill Gates, has already navigated a regulatory thicket and won key approvals from state authorities. The FDA’s request to pause testing on 12 May frustrates many researchers developing diagnostics that can be conducted partially or fully outside hospitals. The SCAN group was the first to roll out such tests in the United States and — as importantly — to partner with local health authorities to deploy the diagnostics strategically. As businesses begin to reopen in the United States this month and next, many people argue that the kind of testing that SCAN provided is needed more than ever. “The Seattle group was literally the only group that has really figured out these logistics, and was trying to scale this, and now you want to shut them down?” asks Sri Kosuri, co-founder of Octant, a biotech start-up in Emeryville, California, that is developing tools to diagnose COVID-19. “It blows my mind,” he says. SCAN — which stands for the Seattle Coronavirus Assessment Network — has its roots in the Seattle Flu Study, in which people around the city swabbed their own noses and sent the samples to a lab at the University of Washington, where researchers analysed them for the presence of influenza viruses. To understand how the flu spreads and hopefully stop it, researchers needed to test people outside hospitals. The same idea holds for the coronavirus. And as COVID-19 was spreading around the world in February, the team retooled its assay to identify the new virus.   Robin Patel, president of the American Society for Microbiology, who is based at the Mayo Clinic in Rochester, Minnesota , says that researchers have long called for diagnostic tests to be deployed outside of healthcare settings. “This push is being accelerated now by COVID-19, because the reasons to do this are so apparent in an outbreak.” A model that requires health workers to draw samples can’t easily be scaled up. It’s labour intensive, and is risky for the workers required to stick long swabs up people’s noses. Moreover, by coming into clinics, people could either pick up an infection or spread one. In late February, the SCAN team cleared a first set of regulatory hurdles with the US Centers for Disease Control and Prevention. Next, the group analysed about 30 specimens with its own test alongside other assays in clinical labs, to vet its accuracy. With the data from these experiments, Washington’s department of health authorized the researchers to screen people and return the results to them. Seattle’s health department discusses SCAN on its official blog. It says the effort helps to show whether social distancing is working, and it slows the spread of the coronavirus because more people know their status and can isolate if necessary. Duchin says: “I have no reason to question the accuracy of the test.” SCAN had developed key partnerships with the Seattle and King County public-health department.Credit: Lindsey Wasson/Getty The researchers assumed that they were in the clear because the FDA had granted Washington state the ability to authorize emergency use of COVID-19 tests. In February, FDA guidelines precluded at-home testing from this type of provision, but made no mention of at-home specimen collection, which is how SCAN operates. As of 11 May, however, updated FDA guidelines do specify that tests with at-home collection need to be assessed by the agency. On 12 May, Bill Gates posted a blog entry on his personal website, saying that the project was testing 300 people per day, with plans to do more. Later that day, the FDA asked SCAN to stop testing until it receives agency authorization. An FDA spokesperson explains that anything that requires a person to take samples themselves, at home, raises concerns. For example, the agency wants to ensure that samples remain stable if they end up spending a long time in a hot vehicle on the way to a lab. Since 21 April, the FDA has green-lit three home-based tests, developed by the companies LabCorp and Everlywell and by Rutgers University in New Jersey.   The FDA has requested data from the SCAN team’s first 17,000 samples, as well as information about the programme and its experimental results. The group has scrambled to provide this: for example, it reports that RNA from the coronavirus can still be detected in its swabs after incubating for 9 days at 28 °C — suggesting that samples can survive sitting in a truck. While the researchers wait for the FDA to review their data, a spokesperson says, the team is considering alternatives such as restarting the programme under the auspices of a study. Researchers tend to agree that quality control is required, and they understand that the FDA is struggling to catch up with a flood of applications from dozens of test developers. Still, Fyodor Urnov, a bioengineer at the Innovative Genomics Institute at the University of California, Berkeley, says the need for testing outside hospitals is so urgent that something has to bend. Homeless shelters, nursing homes and other shared living facilities should screen residents regularly regardless of their symptoms, to stop outbreaks before they explode, he says. Restaurants and other businesses might want to screen their employees when they reopen and, ideally, hundreds of thousands of contacts of infected people would be tested.   “I see no way forward except self-administration of tests,” Urnov says. Although the FDA has adjusted its policies to facilitate at-home testing, Urnov worries the process remains too slow. One option, he suggests, is for the FDA to work more closely with public-health departments and scientists, to share data and find out what they need to control the coronavirus. Kosuri agrees. He argues that a degree of uncertainty should be acceptable when it comes to routine testing of people who aren’t in need of clinical care. Tests that many research groups are developing across the United States will have less data to back them than well-established tests. Inevitably, some of these will turn out to be less reliable than expected, but Kosuri says that’s okay. “The relevant question is not, ‘Is the test as good as one conducted in a hospital by a trained nurse practitioner?’” Kosuri says. “The question is whether it is better than not being tested at all.” Latest on: Epidemiology News 25 MAY 20 News 21 MAY 20 Editorial 19 MAY 20 Infection News 19 MAY 20 News 18 MAY 20 News 18 MAY 20 Policy Comment 21 MAY 20 Obituary 21 MAY 20 News 20 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '5'>22 May 2020</date>
<url id = '6'>https://nature.com/articles/d41586-020-01530-2</url>
<title id = '6'>A meeting planned for Addis Ababa in April was meant to diversify artificial-intelligence conferences, but the push for greater participation continues.</title>
<body id = '6'>The Ethiopian capital Addis Ababa was meant to host the International Conference on Learning Representations in April.Credit: Yannick Tylle/Getty Computer scientists from around the world had planned to converge in Addis Ababa in late April for the first major artificial-intelligence (AI) conference to be held in an African country. But like most other scientific gatherings, the conference was virtual, depriving Ethiopia of a powerful opportunity to boost its research environment. Organizers had had high hopes for this year’s International Conference on Learning Representations (ICLR), an annual event focusing on the AI technique of deep learning. Holding it in Ethiopia was meant to make it more accessible to researchers who cannot readily get visas to Western countries, where these meetings are often held. “It was sad for everybody,” says Esube Bekele, an Ethiopian-born computer-vision architect at In-Q-Tel, Arlington, Virginia. After two years of preparations, the ICLR board was “super eager” to travel to Ethiopia, he says. “Africans and the institutions in Africa missed a chance to make connections and secure important collaborations,” says computer scientist Meareg Hailemariam at Addis Ababa University. The meeting would also have had enormous symbolic meaning for those who have been pushing for AI to become more diverse in terms of geography, race, gender, sexual orientation and physical accessibility — while inspiring the region’s youth to pursue studies in machine learning and data science. “Over the last few years, we’ve seen this great growth in awareness, both in how representation is lacking in AI and also in terms of AI bias itself,” says Animashree Anandkumar, head of machine-learning research at the technology company NVIDIA in Los Angeles, California. The virtual venue that took Addis Ababa’s place did have its advantages. At least in places with a fast-enough internet connection, online workshops can be easier to take part in for people who cannot afford travel, or for those with children or disabilities, Anandkumar points out. “It’s a broader kind of inclusion,” says Shakir Mohamed, a South African-born computer scientist at the AI powerhouse DeepMind in London who was the meeting’s senior programme chair. Part of the ICLR’s mission, he says, is “to break that barrier, to engage different communities”. And participants in the virtual conference generally praised the organizers for running a productive meeting. The first ICLR took place seven years ago as the deep-learning revolution was beginning to take off. The technique, which trains artificial neural networks with vast amounts of data, now powers technologies ranging from surveillance cameras to automated translation. But in recent years, visa issues have blocked many participants, often from developing countries, from attending AI meetings. The Canadian government was criticized in 2018 for turning down visa applications for NeurIPS, another major AI meeting, which was held in Toronto. That spurred a push to hold a meeting in an African city, spearheaded by a group of researchers called Black in AI. Addis Ababa seemed like a natural candidate. The city has experience in welcoming large international delegations: it is the seat of the continental organization, the African Union. And it has hosted important scientific conferences, including one on HIV/AIDS in 2011 and an International Astronomical Union symposium in 2019. Bekele, who is part of Black in AI, says that the idea of hosting ICLR there encountered some resistance at first. “People were pushing back and saying, ‘They don’t have the infrastructure or Internet connection.’” Others were looking forward to attending. “Addis Ababa is one of the best places to organize a conference in Africa,” says El Mahdi El Mhamdi, a computer scientist at the Swiss Federal Institute of Technology in Lausanne, who has travelled to meetings in the city and several others on the continent. El Mhamdi, who is a Moroccan national, says he has long had to endure the hassle and the uncertainty of visa-application procedures — and has often given up altogether trying to fly to conferences in countries such as Canada, the United Kingdom and the United States. Ethiopia is one of the countries where he could have received a visa with relative ease, he says. “Had the conference happened physically in Addis Ababa, it would have demonstrated the capacity of the city to host such events and also would encourage such conferences to come to Africa as frequently as possible,” Hailemariam says. But holding the conference in the city could have had significant local impact as well, Hailemariam adds. Many Ethiopian students are enthusiastic about working in machine learning, he says. And the technology start-up scene in the capital, although growing, is still too small to make use of that potential. Talented Ethiopian students are often forced to emigrate because of a lack of local opportunities, says Kommy Weldemariam, an Ethiopian national who is chief scientist for IBM’s Africa Labs in Nairobi. But having a meeting in town — and being face to face with leading researchers — could help in establishing connections and finding external thesis advisers, for example, says Weldemariam. And a high-profile conference can attract the attention of political leaders, he says. “The hope I had was that local political leaders, including the ministry of education and all the research-based ministries, could have seen how science works in the world,” Weldemariam says. The dream of hosting an ICLR meeting in Addis Ababa lives on. “Once big conferences are in-person again, we’ll push for 2022,” Bekele says. Latest on: Machine learning News 18 MAY 20 World View 14 MAY 20 News 02 APR 20 Technology Comment 21 MAY 20 Career Feature 18 MAY 20 Correspondence 05 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '6'>21 May 2020</date>
<url id = '7'>https://nature.com/articles/d41586-020-01418-1</url>
<title id = '7'>A powerful radio telescope has peered back through time to observe a galaxy that contained a cold, rotating disk of gas not long after the Big Bang — fuelling the debate about when and how disks first formed in galaxies.</title>
<body id = '7'>Galaxies are immense, gravitationally bound systems composed of stars, dust, gas and invisible ‘dark matter’. Understanding how galaxies have formed and grown over time is essential for a more general view of how matter assembles into large structures — a key piece of the puzzle in our efforts to comprehend the Universe. A crucial step towards this goal is to obtain a clear picture of when disk structures first appeared in galaxies. Writing in Nature, Neeleman et al.1 present observations that reveal a massive, rotating disk of cold gas inside a star-forming galaxy only 1.5 billion years after the Big Bang. This is considerably earlier in cosmic history than the times when previously detected gas disks were found to have existed2.   According to our current understanding of cosmology, the earliest large-scale structures in the Universe were spherical dark-matter ‘haloes’ that collapsed under their own gravity3. Surrounding gas fell into these haloes, subsequently forming stars and, ultimately, galaxies4. Haloes and galaxies are thought to have continued to grow together by hierarchical assembly (merging), and through the further accretion of gas and its conversion to stars5. Hierarchical assembly is simple, and is thought to be well understood. However, there is still much debate surrounding the exact pathways by which gas accretion and its assembly into stars occurs, and how it relates to the formation of physical and dynamical structures in galaxies over time. A key component of this mystery is why some galaxies, such as our own star-forming Milky Way, have physical structures dominated by disks of stars and gas (Fig. 1), whereas other, generally older and more quiescent galaxies do not. The answer is probably intimately linked to each galaxy’s history of assembly — specifically, to the relative importance of hierarchical merging (which can either promote or destroy disk growth, depending on the circumstances6,7) and of growth through gas accretion (among other processes). Figure 1 | The dusty spiral galaxy NGC 4414. Many star-forming galaxies contain disks of dust and gas — here, the dust is visible as dark patches and streaks silhouetted against the starlight. Neeleman et al.1 report the observation of another galaxy disk that existed just 1.5 billion years after the Big Bang, considerably earlier than previously reported disks.Credit: The Hubble Heritage Team (STScI/AURA/NASA) Gas accretion is thought to occur through either a hot or cold mode. As the names suggest, the main difference in these modes is whether the gas is hot or cold as it falls towards the centre of a dark-matter halo onto a galaxy. The hot mode of accretion results in galaxy disks forming late, because a considerable amount of time is needed for the accreted gas to cool and eventually settle into a disk. In the cold mode of accretion, the gas instead remains cool as it falls into the halo centre, thus allowing more-rapid disk formation8. Determining when disks first emerged in galaxies, and how frequently, should thus provide important insights into how the early assembly of galaxies took place. To do this, disks must be found in progressively more-distant galaxies, so that researchers can probe ever further back in time towards the Big Bang. (The light from more-distant galaxies takes longer to arrive at our Earth-bound telescopes and detectors than does light from closer galaxies, and therefore provides information about the Universe from further back in time.) This requires extremely sensitive instruments that produce high-resolution data. Modern advances in detector and telescope technology, and in instrument design, have enabled the detection of gas disks in massive galaxies that existed around 3 billion years after the Big Bang2. To extend observations of gas in galaxies to even earlier periods of cosmic history, Neeleman et al. used the Atacama Large Millimeter/submillimeter Array (ALMA), one of the most powerful radio telescopes in the world, situated in the Atacama Desert in northern Chile. The researchers detected light emitted from cold gas in a galaxy from around 12.5 billion years ago. By resolving the light to a scale of 1.3 kiloparsecs (about one-sixth of the distance from our Sun to the centre of the Milky Way9), they were able to examine the structure and kinematics of the emitting gas in impressive detail. They then used simple but robust analytical models to show that their observations are consistent with the presence of a rapidly rotating gas disk, spatially coincident with the galaxy’s stars and dust.   Neeleman and colleagues’ results constitute some of the first observational evidence for the existence of cold gas disks in massive galaxies very soon after the Big Bang, directly establishing that massive gas disks could form 1.5 billion years earlier than previous observations had indicated2. The authors’ work considerably shifts the observational frontier for the detailed study of spatially resolved gas properties in galaxies to when the Universe was only about one-tenth of its current age. Their discovery is intriguing when viewed alongside the results of some numerical simulations of galaxy formation, which suggest that disks did not begin to dominate in galaxies of similar mass until the Universe was between 4 billion and 6 billion years old10,11. However, it is consistent with the theoretical expectation that cold-mode accretion should be dominant early in the Universe’s history8. It also ties in with recent, higher-resolution simulations that have seen disks emerge at earlier cosmic epochs12. One limitation of the work, when it comes to constraining our theoretical understanding of galaxy formation or testing the differing predictions of numerical simulations, is that the authors consider only one galaxy. Similar observations of many more galaxies from the same epoch are needed before we can determine whether the galaxy studied is representative of the whole population at that time, or whether it is an outlier. Moreover, although the authors’ results seem to speak against hot-mode accretion scenarios for early galaxy growth, their data do not explicitly rule out other ways, besides cold-mode accretion, in which cool gas could be efficiently transported to the centres of haloes — for example, through the merging of galaxies and their haloes7. Further observational data are required to resolve this issue. Nevertheless, Neeleman and colleagues’ findings will excite astronomers, and open up a new epoch of the Universe’s history for the study of early galaxy formation. </body>
<date id = '7'>20 May 2020</date>
<url id = '8'>https://nature.com/articles/d41586-020-01420-7</url>
<title id = '8'>An artificial eye has been reported that incorporates densely packed, nanometre-scale light sensors into a hemispherical retina-like component. Some of its sensory capabilities are comparable to that of its biological counterpart.</title>
<body id = '8'>Science fiction frequently features robots that have artificial eyes, as well as bionic eyes that interface with the human brain to restore the vision of people who are blind. Much effort has been made to develop such devices, but fabricating the spherical shape of a human eye — particularly a hemispherical retina — is an enormous challenge that severely limits the function of artificial and bionic eyes. In a paper in Nature, Gu et al.1 report an innovative, concavely hemispherical retina consisting of an array of nanometre-scale light sensors (photosensors) that mimic the photoreceptor cells in human retinas. The authors use this retina in an electrochemical eye that has several capabilities comparable to those of the human eye, and that performs the basic function of acquiring image patterns. The human eye, with its hemispherical retina, has a more ingenious optical layout than, say, that of the flat image sensors in cameras: the dome shape of the retina naturally reduces spreading of light that has passed through the lens, thus sharpening the focus. The core component of Gu and colleagues’ biomimetic electrochemical eye is the high-density array of photosensors that serves as the retina (Fig. 1). The photosensors were formed directly inside the pores of a hemispherical membrane of aluminium oxide (Al2O3). Figure 1 | A biomimetic artificial eye. Gu et al.1 report an artificial visual system that mimics the human eye. A lens is fixed over an aperture in an ‘eyeball’, which consists of a metal shell at the front, an artificial retina at the back and an ionic liquid in the middle. The key advance is the hemispherical retina: a dense array of light-sensitive nanowires held in the pores of an aluminium oxide membrane. The nanowires mimic the photoreceptor cells in biological retinas. A polymeric socket holds the retina, ensuring electrical contact between the nanowires and liquid-metal wires at the back. The liquid-metal wires mimic the nerve fibres by transmitting signals from the nanowires to external circuitry for signal processing. Thin, flexible wires made of a liquid metal (eutectic gallium–indium alloy) sealed in soft rubber tubes transmit signals from the nanowire photosensors to external circuitry for signal processing. These wires mimic the nerve fibres that connect the human eye to the brain. A layer of indium between the liquid-metal wires and nanowires improves electrical contact between the two. The artificial retina is held in place by a socket made from a silicone polymer, to ensure proper alignment between the wires and nanowires. A lens combined with an artificial iris is placed at the front of the device, just as in the human eye. The retina at the back combines with a hemispherical shell at the front to form a spherical chamber (the ‘eyeball’); the frontal hemispherical shell is made from aluminium lined with a tungsten film. The chamber is filled with an ionic liquid that mimics the vitreous humour — the gel that fills the space between the lens and the retina in the human eye. This arrangement is necessary for the electrochemical operation of the nanowires. The overall structural similarity between the artificial eye and the human eye confers on Gu and colleagues’ device a wide field of view of 100°. This compares with roughly 130° for the vertical field of view of a static human eye.   The structural mimicry of Gu and colleagues’ artificial eye is certainly impressive, but what makes it truly stand out from previously reported devices is that many of its sensory capabilities compare favourably with those of its natural counterpart. For example, the artificial retina can detect a large range of light intensities, from 0.3 microwatts to 50 milliwatts per square centimetre. At the lowest intensity measured, each nanowire in the artificial retina detects an average of 86 photons per second, on a par with the sensitivity of photoreceptors in human retinas. This sensitivity derives from the perovskite material used to make the nanowires. Perovskite compounds are extremely promising materials for various optoelectronic and photonic applications2. The perovskite used by Gu et al. is formamidinium lead iodide, and was chosen for its excellent optoelectronic properties and good stability. The responsivity of the nanowires, which measures the current produced per watt of incident light, is almost the same for all frequencies of the visible spectrum. Moreover, when the nanowire array is stimulated by regular, rapid pulses of light, it can produce a current in response to a pulse in just 19.2 milliseconds, and can then take as little as 23.9 ms to recover (return to its inactive state) when the pulse has ended. The response and recovery times are important parameters, because they ultimately determine how quickly the artificial eye can respond to a light signal. For comparison, the response and recovery times of photoreceptors in human retinas range from 40 to 150 ms. Perhaps most impressive is the high resolution of the imaging achieved by Gu and colleagues’ artificial retina, which results from the high density of the nanowire array. In previous artificial retinas, the photosensors were first fabricated on flat, rigid substrates; after that, either they were transferred onto curved supporting surfaces3 or the substrate was folded into a curve4. This limited the density of the imager units, because space had to be left between them to allow for the transfer or folding. By contrast, the nanowires in Gu and co-workers’ device are formed directly on a curved surface, which allows them to be packed together more closely. Indeed, the nanowire density is as high as 4.6 × 108 cm–2, much greater than that of photoreceptors in the human retina (about 107 cm–2). The signal from each nanowire can be acquired individually, but the pixels in the current device were formed from groups of three or four nanowires.   The overall performance of Gu and colleagues’ artificial eye represents a leap forwards for such devices, but much still needs to be done. First, the photosensor array is currently only 10 × 10 pixels, with roughly 200-µm gaps between the pixels; this means that the light-detecting region is only about 2 mm wide. Moreover, the fabrication process involves some costly and low-throughput steps — for example, an expensive process known as focused-ion-beam etching is used to prepare each pore for nanowire formation. High-throughput fabrication methods must be developed in the future to produce larger photosensor arrays, at drastically reduced cost. Second, to improve the resolution and scale of the retina, the size of the liquid-metal wires will need to be reduced. The outer diameter of the wires is about 700 µm, but this should ideally be comparable to the nanowire diameter (a few micrometres). It is currently challenging to reduce the diameter of the liquid-metal wires to that size. Third, more testing is needed to establish the operational lifetime of the artificial retina. Gu et al. report that there is no obvious reduction in its performance after nine hours of operation, but the performance of other electrochemical devices can deteriorate over time. Lastly, the authors note that the response and recovery times of their device are reduced at higher concentrations of the ionic liquid, but at the expense of light transmission through the liquid. Further optimization of the ionic-liquid composition is needed to address this problem. Nevertheless, Gu and colleagues’ work adds to the breakthroughs that have been made in the past few decades3–9, which have been achieved by mimicking not only camera-like eyes (such as those of humans), but also compound eyes similar to those of insects. Given these advances, it seems feasible that we might witness the wide use of artificial and bionic eyes in daily life within the next decade. </body>
<date id = '8'>20 May 2020</date>
<url id = '9'>https://nature.com/articles/d41586-020-01282-z</url>
<title id = '9'>Seventy laboratories that analysed the same neuroimaging data each produced different results. This finding highlights the potential consequences of a lack of standardized pipelines for processing complex data.</title>
<body id = '9'>For most types of big data, from genome sequences to medical images, there is no single ‘best’ way to process the data. This issue is exemplified by the substantial differences in how individual laboratories preprocess and analyse data from functional magnetic resonance imaging (fMRI) experiments, which generate information about brain activity. Indeed, a survey of fMRI studies found that nearly every study used a different analysis pipeline1. Writing in Nature, Botvinik-Nezer et al.2 provide further evidence of this variability, highlighting how analytical choices made by individual researchers can greatly influence the findings gleaned from an fMRI data set. The work is bound to spark lively discussion.   Functional MRI experiments produce a series of images of the brain at work. These images go through several stages of processing and analysis to determine which brain regions show significant activity. However, the choice of pipeline can alter the outcome of an fMRI study. In 2012, for instance, 6,912 unique processing and analysis pipelines were applied to the same fMRI data set3. Many of the pipelines indicated that the same general brain regions were active, but the locations at which neural activity was deemed to be highest varied widely depending on the pipeline used. Botvinik-Nezer et al. explored this phenomenon further. The authors gave 70 independent research teams the same fMRI data set, generated from 108 people performing decision-related tasks. The teams were asked to use the data set to test nine hypotheses, each of which posited that activity in a specific brain region related to a particular feature of the tasks. This allowed Botvinik-Nezer et al. to evaluate the impact of analytical flexibility on fMRI results ‘in the wild’ (rather than performing the analyses themselves, as was done in the 2012 study). Notably, no two teams chose identical workflows to analyse the data, resulting in substantial variation in the results (Fig. 1). Figure 1 | Implications of choosing a neuroimaging pipeline. Botvinik-Nezer et al.2 report that researchers process neuroimaging data using a wide variety of pipelines, which can produce varying results. In this simplified example, the pipeline has three steps: spatial smoothing of the images to reduce noise, which in this example is done to three different degrees; statistical modelling, which in this example can be performed in one of two ways; and ‘thresholding’ of statistical tests associated with these models to determine the level at which neuronal activity in each brain region is deemed to be significant, which in this example is set to two different values. Making different choices for each step leads to a different end point — the red dots represent how activation moves throughout the brain depending on which pipeline is used. It is standard in neuroimaging to test types of hypothesis such as the nine put forward by Botvinik-Nezer et al. by generating a statistical map. The map comprises a compendium of statistical tests performed on different parts (voxels) of the image. The results of these tests are subjected to a process called thresholding to set the level, for each voxel, at which activity picked up by the experiment is deemed to indicate real neuronal activity rather than noise. The authors found considerable differences between each research team’s results even when the underlying statistical maps they had used were highly correlated. The strongest factor in explaining the differences between each team’s results was the spatial smoothness of the data being analysed. Spatial smoothing is a preprocessing step in which the activity of each voxel is averaged with that of its neighbours — a process designed to reduce noise. Higher estimated smoothness was associated with a greater likelihood of reaching a significant outcome for each hypothesis. The fact that each team’s results were so pipeline-dependent is highly problematic, particularly because the exact configuration of analytical pipelines is often poorly described in research articles. Moreover, sensitivity analyses — which assess how different pipeline choices might affect an experiment’s outcome — are rarely performed in neuroimaging. However, Botvinik-Nezer and colleagues offer several reasonable suggestions for addressing the concerns that their work will raise.   The first is to share unthresholded activity maps, because this will allow image-based meta-analysis. The authors found that such an analysis, which aggregated information across teams, yielded consensus results, no doubt aided by the fact that the spatial patterns in the activity maps were highly correlated across groups. Second is a call to publicly share both data and code, making it easier for others to attempt to reproduce a paper’s findings. In this regard, the authors model good behaviour by making all their data and processing pipelines publicly available. Third is the use of pre-registration — in which a hypothesis and analysis plan is made public before the experiment is performed. It is unfortunately common for researchers to explore various pipelines to find the version that yields the ‘best’ results, ultimately reporting only that pipeline and ignoring the others. This practice can lead to errors and make it difficult to replicate findings4. Pre-registration would make it easy to detect cases in which researchers had explored various pipelines. Fourth is to analyse all data through multiple pipelines and use the results to obtain consensus findings. This could be achieved by implementing the type of meta-analysis used by Botvinik-Nezer and co-workers. Another goal of the study was to evaluate how accurately researchers could predict the number of teams that would report significant results for each hypothesis. To study this, the authors ran separate ‘prediction markets’, one for the analysis teams and one for researchers who did not participate in the analysis. In them, researchers attempted to predict the outcomes of the scientific analyses and received monetary payouts on the basis of how well they predicted performance. Participants — even researchers who had direct knowledge of the data set — consistently overestimated the likelihood of significant findings. Botvinik-Nezer et al. do not explicitly explore how the analysts’ prior beliefs affected their findings and pipeline choices. For example, if a research finding does not initially coincide with expectations, will groups seek to alter pipelines until expectations and results align? The prevalence of pipeline exploration implies that this is likely. What other improvements could be made for the future? One approach is to use pipeline-optimization tools5,6 to reduce analysis flexibility. These tools automatically identify pipelines that maximize reproducibility, which can reduce the risk of excessive pipeline exploration and selective reporting. In addition, increased use of sensitivity analyses to evaluate the effects of pipeline decisions would provide a better understanding of the link between analysis choices and research findings. The fact that activity maps were highly correlated across groups implies that multivariate statistical approaches, which identify spatial patterns in the data, might provide more-consistent results across pipelines than would a series of tests performed at individual voxels. Ultimately, neuroimaging results should be carefully verified using independent data sets to demonstrate generalizability across samples, research contexts and populations. A positive example of this already being done comes from an approach for developing predictive models on the basis of brain activation, which can be shared, tested in multiple contexts and used in applied settings7. It seems unlikely that the fMRI field will ever coalesce on a standard workflow that is applicable to all types of study, because studies tend to be too varied for one pipeline to always be appropriate. But Botvinik-Nezer et al. conclude their paper by calling for an increased awareness of the situation, and a drive to improve the quality of method reporting. This is wise and prudent advice that researchers in any field analysing high-dimensional data would be well advised to heed. Carp, J. NeuroImage 63, 289–300 (2012). Botvinik-Nezer, R. et al. Nature https://doi.org/10.1038/s41586-020-2314-9 (2020). Carp, J. Front. Neurosci. 6, 149 (2012). Simmons, J. P., Nelson, L. D. & Simonsohn, U. Psychol. Sci. 22, 1359–1366 (2011). Strother, S. C. et al. NeuroImage 15, 747–771 (2002). Churchill, N. W. et al. Hum. Brain Mapp. 33, 609–627 (2012). Woo, C. W., Chang, L. J., Lindquist, M. A. & Wager, T. D. Nature Neurosci. 20, 365 (2017). Download references Latest on: Human behaviour Article 20 MAY 20 Matters Arising 04 MAR 20 News & Views Forum 10 FEB 20 Imaging Outlook 13 MAY 20 News 06 MAY 20 Article 01 APR 20 Neuroscience Research Highlight 21 MAY 20 Article 20 MAY 20 Article 20 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '9'>20 May 2020</date>
<url id = '10'>https://nature.com/articles/d41586-020-01287-8</url>
<title id = '10'>The multi-subunit protein haemoglobin relies on complex interactions between its components to function properly. Analysis of ancient precursors suggests that its evolution from a simple monomer involved only a few steps.</title>
<body id = '10'>The oxygen-transporting protein haemoglobin has under gone repeated adaptations as animals evolved to conquer new environments — from the depths of the oceans1 to high mountain ranges2. These adaptations relied on changes in the long-range interactions between oxygen-binding sites buried in the protein’s subunits, and between these regions and binding sites for a multitude of small effector molecules on the protein’s surface3. How did this complex molecular machine, which can respond so exquisitely to available levels of both oxygen and several other effector molecules, come into being? Writing in Nature, Pillai et al.4 reconstruct the stepwise evolution of haemoglobin from precursors that existed more than 400 million years ago.   Almost nothing was previously known about how the four-subunit (tetrameric) form of haemoglobin that is found in modern-day jawed vertebrates evolved from ancient monomers. Tetrameric haemoglobin consists of two α- and two β-subunits. Pillai et al. computationally reconstructed an evolutionary tree to chart the protein’s ancient history, using the amino-acid sequences of a large collection of the closely related vertebrate globin proteins, which exist as either monomers or tetramers. The authors’ tree was constructed taking into account that amino-acid substitutions a given protein shares with close relatives tend to have originated in more-recent common ancestors than have those it shares with more-distant relatives. The reconstructed evolutionary tree indicates that multiple rounds of gene duplication and subsequent divergence gave rise to the globin family and, by way of several ancestral proteins, to tetrameric haemoglobin (Fig. 1). Figure 1 | Key steps in the evolution of the tetrameric haemoglobin protein. In jawed vertebrates, haemoglobin exists as a tetramer, formed from two α- and two β-subunits. Pillai et al.4 resurrected extinct ancestors of haemoglobin using predicted amino-acid sequences to reconstruct the protein’s evolution. The authors showed that the protein AncMH, the last common ancestor of haemoglobin and the related protein myoglobin, existed as a monomer. Duplication of the gene that encoded AncMH, and subsequent divergence into two genes, produced monmeric myoglobin and the ancestor of haemoglobin, Ancα/β, which forms a homodimer. Further gene duplication of Ancα/β and subsequent divergence yielded the ancestors of the α- and β-subunits, dubbed Ancα and Ancβ. These two subunits evolved an interface that allowed the formation of heterodimers. A few further changes in amino-acid residues generated a second interface that allowed the assembly of modern-day α- and β-subunits into haemoglobin heterotetramers. What is special about the study is that Pillai and colleagues went on to resurrect several of these extinct ancestral proteins, generating them from the amino-acid sequences predicted by the tree. The group then tested these proteins’ functions. First, Pillai and colleagues analysed whether each ancestral protein could form dimers and tetramers of like or unlike subunits. The earliest protein — a common ancestor of haemoglobin and the monomeric globin protein myoglobin, named AncMH by the authors — exists only as a monomer. A later protein, named Ancα/β, which is the ancestor of all existing haemoglobin subunits, forms homodimers when expressed at high levels. The authors’ tree indicates that Ancα/β underwent gene duplication to produce two proteins: the ancestors of all existing α- or β-subunits, which the group respectively named Ancα and Ancβ. These proteins also form homodimers, or even homotetramers, when expressed alone. However, when the two are expressed together in equal proportions, they can form heterodimers, which then further align to yield haemoglobin tetramers. The group next investigated the oxygen-binding affinity of the ancestral proteins, along with their oxygen cooperativity (the ability of oxygen-binding subunits to interact with one another) and their ‘allosteric’ regulation by a potent, artificial effector molecule, inositol hexaphosphate (IHP). They found that only Ancα and Ancβ — when expressed together at high concentrations — show similar oxygen-binding affinity, cooperativity and allosteric regulation to today’s haemoglobin protein. These features are shared by all living jawed vertebrates, but are absent or achieved in a different way in jawless vertebrates, whose haemoglobin proteins are of more ancient origin. This indicates that the basic functions of jawed-vertebrate haemoglobin had already evolved in a common ancestor of these animals but at some time after the split with jawless vertebrates.   Next, Pillai et al. modelled the stepwise changes in α- and β-subunit interfaces that might have allowed Ancα and Ancβ first to form heterodimers with one another, and later heterotetramers from pairs of such dimers. The modelling indicated that strikingly few amino-acid substitutions might have been needed to transform a simple monomeric oxygen-binding protein similar to myoglobin (whose oxygen binding is non-cooperative and almost totally unaffected by allosteric effector molecules5) into haemoglobin. Importantly, the researchers verified the results of their model by generating proteins that carried mutations of the amino-acid residues identified, and showing that heterotetramer formation was disrupted. The authors’ work shows how natural selection, acting on pre-existing biophysical protein properties, can, in just a few evolutionary steps, create multimeric structures that have complex functions. Most cellular processes involve the action of protein multimers, and Pillai and colleagues’ work serves as one of the clearest examples so far of how such complexity can arise during protein evolution. There are inevitable uncertainties in these kinds of reconstruction of the deep past of life, because the accuracy of such reconstructions relies on the proteins under consideration having several specific properties6. The proteins should, ideally, show small overall rates of amino-acid sequence divergence from one another, have thoroughly known and well-supported evolutionary relationships to each other, and exhibit a dense evolutionary branching pattern over the time period(s) of interest. Finally, a detailed knowledge of structure–function relationships is essential. It would be difficult to reconstruct with any confidence ancestral sequences and functions for proteins that do not fulfil all or any of these conditions. However, haemoglobin is well suited for this type of study for several reasons. For instance, a wealth of comparative data on globin function across vertebrates has accumulated over the past 100 years7. We have intimate knowledge of haemoglobin structure–function relationships3,8,9. In addition, there is an ever-expanding pool of globin sequence information, thanks to genome-sequencing projects in diverse organisms (these efforts will also benefit similar studies on other proteins).   Pillai and colleagues’ study is sure to raise several follow-up questions and to spark further research. For instance, the authors used the artificial effector IHP in their experiments, but the binding sites for physiologically relevant effectors of haemoglobin oxygen affinity, such as hydrogen ions, only partly overlap with — and in some cases are quite different from — the IHP binding site3,8,9. Some evidence1 suggests that mechanisms by which hydrogen ions modify haemoglobin oxygen affinity have evolved independently multiple times in vertebrates. This would make the picture much more complicated than can be assessed using IHP. It will be interesting to probe the evolutionary origins of the regulation of haemoglobin oxygen binding by other effectors, such as carbon dioxide or physiologically relevant organic phosphates including ATP and 2,3-bisphosphoglycerate. In doing so, we could examine, for instance, how haemoglobin regulation changed as demands on the body’s oxygen-transport system rose during the evolution of warm-blooded, active vertebrates, or how it was affected by changes over geological time in atmospheric oxygen levels, which have been proposed by some to have shaped vertebrate evolution10. The assembly of multimeric proteins depends on specific concentrations and thus expression levels of the protein’s subunits. Natural selection presumably prevents imbalanced subunit production, both to limit the costly energy expenditure involved in protein synthesis and to prevent accumulation of potentially harmful spare globin subunits, as occurs in some hereditary human blood disorders11. But, at some point, an increase and balancing of expression levels of haemoglobin’s subunits would have been needed to enable tetramer formation. When did this occur? It has been shown12 that the net surface charge of myoglobin acts as a molecular ‘signature’ that can be used to assess the expression levels of ancestral myoglobin. However, such markers are largely unknown from other globin proteins, and we do not know the ancestral expression levels of any of the reconstructed haemoglobin precursors in Pillai and colleagues’ study. Finally, as previously noted13, one of the most fascinating frontiers in this research field might be uncovering the evolutionary history of gene regulation. This remains an open question in the evolution of the genes that encode haemoglobin’s subunits. </body>
<date id = '10'>20 May 2020</date>
<url id = '11'>https://nature.com/articles/d41586-020-01417-2</url>
<title id = '11'>It emerges that strings of nucleotides are added to messenger RNAs that are undergoing silencing in nematode worms. The composition of these nucleotide tails promotes the formation of small RNAs that drive heritable gene regulation.</title>
<body id = '11'>Discovered in the minuscule nematode worm Caenorhabditis elegans nearly 30 years ago, small RNAs have been implicated in a surprising range of biological processes, from antiviral defence in plants to cancer in humans1. In C. elegans, these RNAs can be transmitted from one generation to the next, providing a non-DNA-based mechanism for heritable gene silencing, in which messenger RNAs are inhibited or degraded2. But the molecular details underlying the phenomenon have remained elusive. Writing in Nature, Shukla et al.3 describe an enzyme that converts even seemingly innocuous mRNAs into templates for the formation of small RNAs and mediators of transgenerational gene silencing.   In 1987, a genetic mutation was identified in C. elegans that activates transposons4 — abundant but normally inactive genes that can replicate and reinsert themselves at new locations in the genome, causing mutations. Twelve years later, a mutation was found5 that deactivates a gene-silencing phenomenon called RNA interference (RNAi). The two mutations had the same physical effects on the worm, revealing a crucial role for RNAi in transposon silencing. The mutations were later mapped6 to a single gene, RNAi-defective-3 (rde-3). The protein encoded by this gene, RDE-3, belongs to a family of enzymes that extend the ends of DNA and RNA by adding strings of untemplated nucleotides (that is, those not copied from existing DNA or RNA). But the specific role of RDE-3 remained a mystery. More recently, RDE-3 was shown to add strings of alternating uridine (U) and guanosine (G) nucleotides to RNA ends, forming poly(UG) tails7. Could this RNA-tailing activity underpin the molecular mechanisms of transposon silencing and RNAi? Typically, RNAi is initiated by double-stranded RNA. When introduced into a cell, either experimentally or naturally, double-stranded RNA is chopped up by enzymes into small interfering RNAs (siRNAs). These, like other classes of small RNA, associate with Argonaute proteins, either to guide sequence-specific degradation of matching mRNAs or to repress their translation into protein. This effectively silences the genes that encode those mRNAs. Shukla and colleagues found that, when they injected double-stranded RNA into the C. elegans germ line (the tissue that produces reproductive cells), poly(UG) tails were appended to the matching cellular mRNA. Importantly, the addition of poly(UG) tails — a process aptly named pUGylation — depended on rde-3.   But are poly(UG) tails simply markers of RNA degradation, or do they have a direct role in RNAi? In a key experiment, the authors attached RNA tails of various nucleotide compositions to single-stranded mRNA fragments produced in vitro, and then introduced them into C. elegans. The RNA fragments appended with poly(UG) tails, but not other compositions, were potent triggers for gene silencing. In C. elegans, primary siRNAs produced during the initial stage of RNAi trigger a second phase, in which secondary small RNAs called 22G-RNAs are synthesized from the target mRNA by enzymes called RNA-dependent RNA polymerases2. The 22G-RNAs probably act in a feedback loop to maintain small-RNA production and mRNA silencing (Fig. 1). Earlier work showed that rde-3 is required for the formation of 22G-RNAs, but its specific role was unclear6. How an RNA is transformed into a substrate for 22G-RNA synthesis was also not understood. Could poly(UG) tails serve this function? Perhaps: Shukla et al. report that poly(UG)-tailed RNAs synthesized in vitro are bound by RNA-dependent RNA polymerases and function as templates for 22G-RNA production in vivo. So, poly(UG) tails might act as landing pads for RNA-dependent RNA polymerases. Figure 1 | RNA interference in nematode worms. Messenger RNAs can be silenced (degraded or their translation inhibited) by small RNA molecules. The small RNA that initiates mRNA silencing is anchored to an Argonaute/Piwi protein, and acts as a sequence-specific guide to direct mRNA cleavage or trimming, perhaps removing the mRNA’s poly(A) tail. Shukla et al.3 report that, in nematodes, the truncated RNA produced is bound by an RDE-3 enzyme, which adds a tail that consists of alternating uridine (U) and guanosine (G) nucleotides in a process called pUGylation. This pUGylated RNA acts as a ‘template’ for an RNA-dependent RNA polymerase (RdRP) enzyme. RdRP synthesizes secondary small RNAs, which are 22 nucleotides long and begin with a guanosine (22G-RNAs). The 22G-RNAs probably act to maintain mRNA silencing by having the same role as the initiating small RNA. Cycles of mRNA truncation, pUGylation and 22G-RNA synthesis drive transgenerational gene silencing. Shukla et al. found evidence that mRNA was cleaved or trimmed before poly(UG) tails were added (Fig. 1). Cleavage might therefore prime an mRNA for pUGylation, possibly because the process of cleavage would remove another tail consisting of adenosine (A) molecules, which is added to most mRNAs to promote their stability and translation. Taken together, the authors’ work suggests a model in which mRNAs are bound by siRNAs and associated proteins, leading to their cleavage. RDE-3 adds poly(UG) tails to the cleaved mRNA end, enabling RNA-dependent RNA polymerases to bind and synthesize 22G-RNAs templated from the mRNA. These 22G-RNAs act in the same way as the initial siRNA molecules, maintaining gene silencing. Earlier work8 implicated RDE-3 in the addition of poly(U) tails, rather than poly(UG) tails. The reason for the discrepancy between this work and the new findings is unclear, but it might reflect tissue-specific effects, such as those of germline compared with non-germline tissue. If so, it would suggest that RDE-3 can switch tailing modes, from poly(UG) to poly(U), depending on the cell type. A shortcoming of the new study is that the authors did not test whether poly(U) tails can also trigger RNAi, although they did show that other tail varieties cannot do so.   In C. elegans, RNAi underlies transgenerational epigenetic inheritance — a phenomenon in which changes in gene expression can be transmitted across three or more generations without changes in DNA sequence2. Strikingly, Shukla et al. show that a single dose of poly(UG)-tailed RNA injected into the worm germ line can trigger silencing of a matching gene for several generations. Through a series of simple genetic experiments, the authors found that cycles of pUGylation and 22G-RNA synthesis drive gene silencing from one generation to the next. This is a key breakthrough in our understanding of experimentally induced RNAi. But does pUGylation occur naturally? Hundreds of C. elegans genes, including transposons, are naturally regulated by an RNAi pathway involving RDE-3 — hence the original link between RNAi and transposon silencing. Naturally occurring RNAi commonly involves a distinct class of small RNA, called Piwi-interacting RNAs (piRNAs). These piRNAs, like siRNAs, trigger the production of 22G-RNAs and heritable gene silencing. Shukla et al. identified poly(UG) tails on several natural RNAi targets, including transposons. This is enticing evidence that pUGylation is not restricted to experimental RNAi, and might have a broad role in regulating gene expression. But the phenomenon will have to be explored on a wider scale to uncover how central it is to the various pathways involving small RNAs. Is pUGylation unique to nematodes? In ciliates — a group of unicellular, nucleus-bearing organisms — poly(U) tails promote RNA-dependent RNA polymerase activity during RNAi9. But whether pUGylation occurs in ciliates, and what function poly(UG) tails might serve in organisms that lack RNA-dependent RNA polymerases (such as mammals), is unclear. RDE-3 has potential counterparts in species ranging from yeast to humans6, and artificial forms of it can add poly(UG) tails to RNAs, even in distantly related organisms, such as yeast and frogs7. Shukla and colleagues’ study paves the way for the identification of poly(UG)-tailed RNAs in other species, and the exploration of their roles in the production of small RNAs and other biological processes. Zamore, P. D. & Haley, B. Science 309, 1519–1524 (2005). Rechavi, O. & Lev, I. Curr. Biol. 27, R720–R730 (2017). Shukla, A. et al. Nature https://doi.org/10.1038/s41586-020-2323-8 (2020). Collins, J., Saari, B. & Anderson, P. Nature 328, 726–728 (1987). Tabara, H. et al. Cell 99, 123–132 (1999). Chen, C.-C. G. et al. Curr. Biol. 15, 378–383 (2005). Preston, M. A. et al. Nature Methods 16, 437–445 (2019). Tsai, H.-Y. et al. Cell 160, 407–419 (2015). Talsky, K. B. & Collins, K. J. Biol. Chem. 285, 27614–27623 (2010). Download references Latest on: Epigenetics Article 20 MAY 20 News & Views 11 MAR 20 News & Views 26 FEB 20 Genetics Article 20 MAY 20 News 14 MAY 20 Article 13 MAY 20 Molecular biology Article 21 MAY 20 News & Views 20 MAY 20 Article 20 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '11'>20 May 2020</date>
<url id = '12'>https://nature.com/articles/d41586-020-01447-w</url>
<title id = '12'>Rapid, reliable identification of an unknown viral infection is challenging. Use of CRISPR technology can simultaneously detect nucleic acids of many viruses and pinpoint specific ones, such as the virus that causes COVID-19.</title>
<body id = '12'>The current severe effects of the global pandemic of COVID-19 reveals our vulnerability to emerging infectious diseases1,2. It also highlights the need for tools to detect a broad range of disease-causing agents, both known and recently emerged, that could threaten public health. However, the genetic diversity of the potential perpetrators, which include viruses, bacteria, fungi and protozoa, presents a practical difficulty. Molecular methods that detect nucleic acids are uniquely suited to this task because such infectious agents contain DNA, RNA, or both, that enables their recognition and identification. Feasible surveillance methods for tracking emerging global infections must have broad detection capability, be suited to high-throughput use and have low cost per test. Writing in Nature, Ackerman et al.3 describe an attempt to meet these requirements using a diagnostic detection platform they have created, called CARMEN (combinatorial arrayed reactions for multiplexed evaluation of nucleic acids).   CARMEN is an extension of SHERLOCK, a diagnostic platform previously developed by some members of the same team4 that was built around the biotechnology tool CRISPR, which can be used to selectively edit nucleic acids. CRISPR is based on a bacterial defence system. Its use as a laboratory tool depends on a ‘guide’ RNA (also termed a CRISPR RNA) present in a complex with a Cas enzyme. If the guide RNA binds to a nucleic-acid target that is complementary to it in sequence, Cas is activated and cleaves the target. Some Cas proteins cut target nucleic acids only at a specific site related to the guide sequence. However, Cas13 is different from other Cas proteins in that it digests only RNA and not DNA5, and exerts its RNA-cleaving activity on any nearby RNA that it encounters. This property can be used to generate a signal that indicates the presence of a sequence of interest. This principle underlies SHERLOCK and CARMEN. A reporter RNA is cleaved in a non-sequence-specific manner by Cas13 if it is activated through recognition of a specific sequence. This cleavage generates a fluorescent signal by separating two components attached to the reporter RNA: a fluorescence quencher and a fluorescent molecule (Fig. 1). CARMEN retains the sensitive and specific detection achieved by SHERLOCK, and adds the capability for simultaneous detection of multiple nucleic-acid targets. This makes the workflow compatible with a high-throughput, miniaturized setting that enables rapid turnaround at a low cost per test. Figure 1 | A method to detect viral infection. Ackerman et al.3 present a platform called CARMEN that can identify a range of infections in a given sample, including infection by the virus that causes COVID-19. Two categories of emulsified droplet are needed for this platform — sample droplets and droplets that enable the detection of a specific nucleic acid. Each different type of sample or detection droplet contains a unique dye-based colour identifier. Mixed droplets are loaded onto a chip containing microwells that hold two random droplets, and the colour codes are recorded. If a sample has viral sequences of interest, these nucleic acids are amplified when droplets are prepared. The detection-mix droplets contain the enzyme Cas13 in complex with a guide RNA that enables the recognition of a particular target RNA sequence. They also contain a reporter RNA, which is tagged by a fluorescent molecule that does not fluoresce owing to a quencher molecule attached to the RNA. Each pair of droplets in a well merges when an electric field is applied. If the Cas13 complex recognizes its target RNA in amplified nucleic acid, Cas13 is activated and cleaves the reporter RNA. This removes the quencher and generates a fluorescent signal, which reveals a particular viral infection in the person from whom the sample came. To detect specific nucleic acids using CARMEN, the process begins with the amplification of target viral nucleic acids (if present) in a specimen by methods such as the polymerase chain reaction (PCR) or recombinase polymerase reaction (RPA). The amplified nucleic acid can be the product of a specific amplification reaction targeting a single viral sequence, or can be the product of pooled reactions used to potentially amplify a range of different viral sequences. The sample of amplified RNA is given a unique colour code by the use of four fluorescent dyes mixed in a ratio that provides one of 1,050 possible colour combinations. Oil is then added to generate emulsified one-nanolitre droplets. The authors prepared such droplets for all the different amplification reactions carried out. They also generated a series of emulsified droplets with unique colour codes containing the components needed to detect the presence of specific viral sequences. Each detection mixture comprised a quenched fluorescently labelled reporter RNA and Cas13 bound to a guide RNA needed to detect a viral target. All the emulsified droplets are mixed in a single tube, and its contents loaded onto a chip containing microwells that each accommodate only two droplets. The droplets distribute randomly into the microwells, to constitute what the authors refer to as a self-assembling array, such that each amplified nucleic-acid target is expected to be exposed to each detection mix, in multiple replicates in different locations on the chip. Exactly where this happens is revealed by recording the two colour codes present in each well. The detection reactions are then initiated simultaneously in each well by merging the droplet pairs by exposure to an electric field. If an amplified viral sequence is in a well that contains Cas13 in complex with a guide RNA that can recognize this sequence, Cas13 is activated and its nonspecific RNA-cleavage activity generates a fluorescent signal from the reporter RNA. This platform is admirably innovative, marrying the desirable characteristics of the ability of the guide RNA–Cas13 complex to recognize a specific sequence with a labour-saving platform that is inherently flexible because the user can select the PCR reactions and the guide-RNA sequences used.   To illustrate the potential application of CARMEN for broad testing of virus samples, the authors show that the technique could simultaneously detect all 169 human viruses for which at least 10 genome sequences were available at the time. The authors also demonstrate that CARMEN enables comprehensive identification of different influenza strains from samples obtained from infected people. This is important, because it could allow detection of a newly emerging type of influenza. CARMEN can be adapted to detect a viral variant resulting from a mutation after the variant sequence is determined. Ackerman and colleagues report that, when CARMEN was used on samples from people infected with HIV, it could detect six known viral mutations associated with drug resistance. Finally, the authors illustrate CARMEN’s flexibility by rapidly adapting the system to detect SARS-CoV-2, the coronavirus that causes COVID-19. The authors report that CARMEN distinguishes SARS-CoV-2 from the other human coronaviruses, including four seasonal coronaviruses and the coronaviruses responsible, respectively, for severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS). Rapid, sensitive and specific detection of SARS-CoV-2 by a method using CRISPR and Cas12 has recently been reported6, and that technique has similarities to CARMEN but only one type of virus can be detected. How does CARMEN compare with other emerging diagnostic platforms? In the breadth of its detection, CARMEN is most similar to the microarray methods used for the simultaneous detection of amplified nucleic acid from multiple viruses7. But CARMEN has the advantage of avoiding the need to manufacture in advance microarrays that contain the specific nucleic-acid sequences needed. Commercially available multiplex PCR panels, now widely used in diagnostics in clinical laboratories, provide another possible platform8. These kits, described as ‘sample-in, answer-out’ systems, are admirably simple to use and can detect 20 or more targets in just over an hour. However, they are not modifiable by users in the way that CARMEN is — the kits come preloaded with the components needed to amplify nucleic acids and have been optimized for a specific combination of targets.   Another option is metagenomic sequencing9, which is a next-generation sequencing approach that directly determines the sequences of any nucleic acids present without needing a PCR-based amplification step or a specifically tailored reporter probe to detect particular sequences. However, compared with CARMEN, this method requires more-complex equipment and data processing, and takes longer to generate results. Although CARMEN incorporates numerous desirable features for the surveillance of emerging infectious disease or the identification of a viral infection, there are some concerns. First is that the CARMEN workflow includes the manipulation of amplified nucleic acid, and so there is the risk of contamination. Perhaps appropriate automated instrumentation could reduce this key vulnerability. Second, will off-target effects of Cas13, possibly resulting from binding of guide RNAs to incorrect targets, lead to nonspecific detection reactions? Third, will the generation and image analysis of the nanodrops in these chips be sufficiently robust under ‘field conditions’ in a range of different types of laboratory, considering the need for sophisticated fluorescent-microscopy analysis, and given that users will have different levels of experience and expertise? Finally, the sequences used to amplify RNA and the guide RNA sequences used might need to be changed to achieve optimal sensitivity and specificity and to account for virus variation over time. These issues need to be taken into account, but they do not lessen the authors’ achievement in developing a new diagnostic platform designed around the need for surveillance of global emerging infectious diseases. Smolinski, M. S., Hamburg, M. A. & Lederberg, J. (eds) Microbial Threats to Health in the 21st Century (National Academies Press, 2003). Morens, D. M. & Fauci, A. S. PLoS Pathog. 9, e1003467 (2013). Ackerman, C. M. et al. Nature https://doi.org/10.1038/s41586-020-2279-8 (2020). Myhrvold, C. et al. Science 360, 444–448 (2014). Terns, M. P. Mol. Cell 72, 404–412 (2018). Broughton, J. P. et al. Nature Biotechnol. https://doi.org/10.1038/s41587-020-0513-4 (2020). Wang D. et al. Proc. Natl Acad. Sci. USA 99, 15687–15692 (2002). Hanson, K. E. & Couturier, M. R. Clin. Infect. Dis. 63, 1361–1367 (2016). Wu, G., Miller, S. & Chiu, C. Y. Annu. Rev. Pathol. 14, 319–338 (2019). Download references Latest on: Biotechnology Article 20 MAY 20 News 19 MAY 20 Article 18 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 Virology News 25 MAY 20 News 22 MAY 20 Career Column 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '12'>18 May 2020</date>
<url id = '13'>https://nature.com/articles/d41586-020-01336-2</url>
<title id = '13'>Toll-like receptors can initiate an immune response when they detect signs of a viral or microbial threat. New insight into how such receptor activation drives defence programs should aid our efforts to understand autoimmune diseases.</title>
<body id = '13'>Cells of the innate branch of the immune system can detect infectious agents outside the cell or in various intracellular compartments. This process depends on the sensing of ‘non-self’ molecular signatures by proteins known as pattern-recognition receptors (PRRs)1. In the endolysosome, an organelle into which extracellular material can be taken up by the cell, PRRs called Toll-like receptor 7 (TLR7), TLR8 and TLR9 can be activated by the presence of viral or microbial nucleic acids2. However, these same receptors are often linked to the erroneous detection of ‘self’ nucleic acids in autoimmune diseases3. Writing in Nature, Heinz et al.4 report that a protein that they named TASL links the activation of TLR7, TLR8 and TLR9 to the production of molecules called type I interferons, which mediate antiviral defence. The gene that encodes TASL has been associated with the autoimmune disease systemic lupus erythematosus, and this finding might shed light on factors that contribute to the disease. To distinguish between different disease-causing viruses and microbes and to tailor a suitable response, the innate immune system uses PRRs in various parts of the cell1. Each of these sensors recognizes a distinct hallmark of infectious agents termed a pathogen-associated molecular pattern (PAMP). One such family of receptors, the TLRs (Fig. 1), are transmembrane proteins that are found on the cell surface or in the endolysosome2. Most cell-surface TLRs detect bacterial components, such as lipopeptides found in bacterial cell walls. By contrast, the endolysosomal TLRs — TLR3, TLR7, TLR8 and TLR9 — recognize nucleic acids or their degradation products, which are typically associated with viral infection but are also a signature of living microbes. Figure 1 | A crucial role for the TASL protein in the activation of interferon-mediated antiviral defences. a, Nucleic acids from viruses or bacteria can be taken up by human immune cells into an organelle called the endolysosome and are recognized by Toll-like receptor proteins (TLR7, TLR8 and TLR9). This recognition activates the protein MyD88, which, in turn, activates the transcription-factor protein NF-κB, a key player in the immune response to infection. Heinz et al.4 investigated how a protein called SCL15A4, which is associated with the autoimmune disease systemic lupus erythematosus, aids this defence response. They found that it binds to a protein named TASL, which contains an evolutionarily conserved amino-acid sequence called the pLxIS motif. When endolysosomal TLRs recognize foreign nucleic acids, a phosphate group (P) is added to TASL by a kinase enzyme (possibly IKKβ from a pathway downstream of MyD88). This phosphorylation recruits the transcription factor IRF5 to TASL. b, TASL then acts as a scaffold to facilitate the phosphorylation and activation of IRF5 by a kinase (possibly IKKβ). It is the first pLxIS-containing protein known to mediate IRF5 activation. Phosphorylated IRF5 enters the nucleus and drives the expression of genes that encode antiviral interferon molecules. NF-κB drives the expression of pro-inflammatory defence molecules called cytokines. On activation in response to binding a PAMP, TLRs engage another protein, termed an adaptor protein, which provides a crucial control point that sets off distinct signalling cascades culminating in defence responses2. Together with other defence mechanisms, two major gene-expression programs can be distinguished that are broadly tailored to the particular threat sensed. Downstream of most TLRs, an adaptor protein called MyD88 activates the transcription-factor protein NF-κB, which drives expression of pro-inflammatory genes as part of the immune response. A subgroup of TLRs (TLR3 and TLR4) can engage the protein TRIF, which acts as a scaffold enabling a kinase enzyme to add a phosphate group to the transcription factor IRF3. This phosphorylation activates IRF3, a member of a family of transcription factors termed interferon regulatory factors (IRFs), which activate broad gene-expression programs. A hallmark of these programs is the production of type I interferon molecules5.   Interferons are potent drivers of a branch of the immune system termed the adaptive immune response, and their presence therefore runs the risk of contributing to autoimmunity. To prevent such an attack by the host’s own immune system, an interferon response must be tightly regulated. As a safeguard, a particular sequence of amino-acid residues in TRIF, the pLxIS motif, must be phosphorylated before IRF3 can be activated. This control mechanism provides a ‘licensing step’ that is not specific just for TRIF as an adaptor protein for TLR signalling, but is a general hallmark of sensing pathways that engage IRF3, or the related protein IRF7, to drive interferon expression. Every identified innate sensing pathway connecting the recognition of nucleic acids to the production of type I interferons, with one exception, had been shown previously to signal through one of the three adaptor proteins known so far to contain a pLxIS motif: TRIF, MAVS and STING. Thus, pLxIS-motif-containing adaptor proteins specifically hardwire nucleic-acid recognition to antiviral defences. The only exception to this rule had been the endolysosomal TLRs — TLR7, TLR8 and TLR9. Although these detect nucleic acids and drive type I interferon gene expression, they do not use TRIF2. Instead, they use MyD88 to activate the interferon regulatory factor IRF56, which is related in structure and function to IRF3 and IRF7. A licensing step involving a pLxIS motif had not been found previously in the signalling cascades of TLR7, TLR8 and TLR9. Immune cells called plasmacytoid dendritic cells express high levels of TLR7 and TLR9, and are crucial to antiviral defences through their production of large amounts of type I interferon. But these cells are also central players in systemic lupus erythematosus7. Previous studies of the molecular mechanisms underlying this disease7,8 have identified a protein called SLC15A4, located on endolysosomal membranes, which transports polypeptides and the amino acid histidine. SLC15A4 has been linked to the activation of TLRs.   To investigate the role of SLC15A4 further, Heinz and colleagues used mass spectrometry to probe for proteins that interact with it. This approach identified the protein TASL, which had previously been little researched. TASL is highly abundant in cells of the innate immune system, and Heinz and colleagues found that it is tethered to endolysosomes through interactions with SLC15A4. The authors’ further experiments confirmed that this interaction is specific: SLC15A4 bound TASL in immunoprecipitation tests; however, neither the related protein SLC15A3 nor a mutant version of SLC15A4 interacted with TASL in such assays. When the authors engineered plasmacytoid dendritic cells and immune cells called monocytes to lack expression of the gene encoding TASL, they found that signalling mediated by TLR7, TLR8 and TLR9 was abolished, and a similar effect was seen when SLC15A4 was absent. Heinz et al. went on to demonstrate that TASL acts specifically through IRF5 by finding that the response to TLR7 and TLR9 activation remained intact in immune cells lacking IRF3 or IRF7, but was blocked in cells deficient in TASL or IRF5. However, NF-κB-mediated signalling was unaffected when the pathway acting through TASL was disrupted. Intriguingly, the authors identified a pLxIS motif in TASL, and found evidence that phosphorylation of this motif — by kinases downstream of MyD88 that are associated with NF-κB activation — mediates IRF5 activation. This discovery elevates TASL to membership of an exclusive circle of IRF-activating adaptor proteins containing pLxIS motifs, of which the other members are TRIF, MAVS and STING5. These four proteins together control the type I interferon response induced by nucleic-acid sensing, a picture that has now been completed with the discovery of TASL as the missing pLxIS adaptor of TLR7, TLR8 and TLR9 signalling.   Given that TASL signals to IRF5, but not to IRF3 or IRF7, it will be interesting to determine the structural features required for the differential recruitment of IRF-family members to pLxIS-motif-containing proteins. Although the authors performed preliminary experiments to investigate phosphorylation events in this system, phosphorylation of the pLxIS motif in TASL should be investigated in detail to identify the kinase(s) responsible. Moreover, it will be interesting to sort out how this newly identified signalling pathway operates in relation to activation of the pathway involving MyD88, which is the key adaptor of TLR7, TLR8 and TLR9 signalling9. As has been shown for other signalling cascades triggered by TLRs and involving multiple adaptors, it is possible that MyD88-mediated signalling and the pLxIS licensing step involving TASL emanate sequentially from distinct endolysosomal vesicles at different stages of maturation2. Although TASL is not involved in NF-κB activation, the authors found that the expression of certain pro-inflammatory genes was still blocked in TASL-deficient cells, probably because of the associated defect in IRF5 activation. Nonetheless, by offering a way to dampen interferon-mediated autoimmunity in a way that doesn’t block the ability to launch an inflammatory defence response, TASL might prove to be a drug target for treating autoimmune diseases that are fuelled by the engagement of TLR7, TLR8 and TLR9. </body>
<date id = '13'>13 May 2020</date>
<url id = '14'>https://nature.com/articles/d41586-020-01169-z</url>
<title id = '14'>Patterns in the vibrations of stars produce a sort of natural music that offers clues to the stars’ internal structure. Astronomers have identified such patterns for some δ Scuti stars, a group for which this music had been elusive.</title>
<body id = '14'>Our knowledge of the stars is based almost exclusively on the study of their light. But the light that reaches us originates from their upper layers — we can’t see inside. There is, however, a tool we can use to look into the interior of the stars: asteroseismology. Writing in Nature, Bedding et al.1 report that a subgroup of the enigmatic δ Scuti stars exhibits regular pulsations that will finally enable the stars to be probed using this tool.   Inside a star, gravity and gas pressure compete with each other. If the two are in balance, the star is in equilibrium, but if one increases more than the other, the star contracts or expands. Hot gas spheres such as stars can show characteristic periodic oscillations in which the star pulsates in this way. These characteristic oscillations, called eigenmodes, are standing waves, like the standing sound waves responsible for the sounds of musical instruments such as violins and oboes. The eigenmodes are determined by the physics of the oscillating system. Asteroseismology is the study of these stellar oscillations2. The idea is similar to that of seismology, in which the interior structure of our planet is inferred from earthquakes. Each star can have different — and often very large numbers of — eigenmodes, depending on its internal structure. Oscillations that have different periods or frequencies are sensitive to physical conditions in different regions inside the stars. The more eigenmodes that can be determined from observations, the more detailed will be our map of the internal structure. Oscillations produce brighter and darker areas (corresponding to higher and lower pressures and temperatures; Fig. 1) on the star’s surface2. However, we cannot resolve the surfaces of stars, apart from that of our Sun and a few other special cases (see ref. 3 and references therein, for example). Only their total brightness can be measured. The complicated distribution and variation of surface brightness results in an equally complex temporal variation in the total brightness. By measuring the brightness of a star, a photometric time series known as the light curve is obtained. Figure 1 | Simple modes of stellar oscillations. Pressure oscillations in stars occur in many different combinations of characteristic patterns and frequencies, such as the simple examples shown here. These oscillations change the brightness of stars, and offer clues about the physical conditions inside them. The oscillatory modes of an important family of stars known as δ Scuti stars have been difficult to identify. Bedding et al.1 have identified a group of δ Scuti stars that pulsate at a high rate and with regular patterns of frequency that agree with theoretical predictions. This allowed the oscillation modes to be identified. For asteroseismology, then, we need the following steps. To obtain the surface-brightness distribution from the measured light curves, the frequency of the brightness variations must be determined. Next, we must work out how these frequencies correspond to the eigenmodes expected from theoretical models, a process called mode identification. If the mode identification is successful, actual asteroseismology can begin by determining key physical parameters such as stellar mass and age. Then the ultimate goal of asteroseismology can follow: obtaining the total seismic inversion, which means the detailed determination of stratifications of the pressure, temperature and chemical composition inside the star. For decades, researchers have made tremendous efforts to obtain valuable asteroseismic data sets. Extensive observation campaigns were carried out using ground-based telescopes, but inevitable variations in detectors and weather conditions affected the data. Space missions (such as CoRoT, Kepler and TESS) delivered the real breakthrough. Thanks to the missions’ long, homogeneous and evenly sampled light curves, and the precision of the collected data, asteroseismology has now been successfully applied to thousands of stars across several stellar types that have different internal structures4–6.   But the abundant star type known as δ Scuti7, named after a star in the constellation Scutum, has remained one of the exceptions. Stars of this type have a slightly larger mass than that of our Sun, but their inner structure is very different. They have long been known to have complicated, low-amplitude light curves7. One might think that the large number of detected frequencies would make these stars ideal targets for asteroseismology. The theoretical models of δ Scuti stars predict many possible excited eigenmodes and corresponding frequencies. In fact, there are many more such frequencies in models than have been observed8, and usually we do not know which of the possible modes are seen. If there were some regular structure to the frequencies (such as frequencies with comb-like regular differences), we would have a better chance of identifying them. But the theoretical models generally do not predict regular frequency structure for these stars. Bedding et al. have identified a special subgroup of δ Scuti stars that pulsate at higher frequencies than do most such stars. For this subgroup, both theory and observations suggest the existence of regular frequency structures. Other researchers have previously found regular structures in observed data for some δ Scuti stars (see refs 9–14, for example), but did not identify the oscillating modes conclusively, if at all. Bedding et al. provide unambiguous mode identification for a uniform and relatively large sample of these stars.   A key factor in the authors’ success is that many of the stars in the subgroup rotate more slowly than do other δ Scuti stars. (Alternatively, it could be that some of the stars are observed almost pole-on, resulting in apparently small rotation velocities.) Theoretical models predict that the frequency spectra of stars that have low rotation velocities are less complicated than those with higher rotation speeds14, which makes it easier to recognize their regular frequency structures. Bedding et al. not only identified these structures, but also associated the frequencies with the corresponding eigenmodes. Sky surveys now and in the near future will target many thousands of δ Scuti stars, including many that are similar to those described by Bedding and co-workers. This is not merely an opportunity to understand the physics of a special group of δ Scuti stars better. The authors show that these are young stars, which means that they can be used as tracers to estimate the age of open star clusters or of young stellar associations in our Galaxy. In this way, we might learn more about the evolution of the Milky Way. Bedding and colleagues’ study is therefore not the last word on δ Scuti stars. Rather, it opens up avenues of investigation for this important stellar group. </body>
<date id = '14'>13 May 2020</date>
<url id = '15'>https://nature.com/articles/d41586-020-01334-4</url>
<title id = '15'>Nanoscale particles have been observed to form and grow in the atmospheres of many cities, contradicting our understanding of particle-formation processes. Experiments now reveal a possible explanation for this mystery.</title>
<body id = '15'>In a paper in Nature, Wang et al.1 report observations of the rapid growth of newly formed atmospheric particles through the condensation of ammonium nitrate under conditions typical of many urban environments in wintertime. The observations were made in a chamber in a laboratory, but the authors convincingly argue that similar conditions can occur transiently in megacities. The findings fill a major gap in our knowledge of particle growth rates in cities.   Particulate matter is a key factor in the air quality of many of the world’s megacities because it has been directly linked to multiple non-communicable diseases (see go.nature.com/2w49q1t). It also substantially affects regional climate through its interactions with solar radiation and clouds2. Particle-formation processes are important in the air above large cities because they replenish the particle population, determine the total particle-number concentration and can act as ‘seeds’ for cloud formation. We therefore need to know how particles form and grow in order to predict the effects of particulate matter on health and regional climate. Although our knowledge of particle formation has improved over the past few years3,4, our understanding of the early stages of particle growth — particularly the crucial step in which an initial cluster of molecules grows large enough to become an actual particle — cannot explain why new particles form in megacity environments5. The persistence of newly formed clusters depends on the ratio of the condensation sink (the rate at which vapour and clusters are scavenged by pre-existing particles) to the growth rate of the clusters3. In the real world, both of these quantities depend on the particle-size distribution. The condensation sink can be derived directly from the particle-size distribution. However, the growth rate is commonly determined by monitoring how clusters grow over time, typically in the size range between 1 and 10 nanometres. This method assumes that the environmental factors that affect cluster growth are uniform throughout a given region, and it has worked well in describing particle-growth behaviour in rural environments. However, it has failed to explain particle growth in cities5. The particle loading of air in urban environments can be greater than 500 micrograms per cubic metre6, whereas that of rural or remote environments is usually less than 5 μg m–3 (ref. 7). Newly formed clusters in cities must therefore rapidly scavenge vapour or combine with other clusters so that they can grow large enough for the rates at which they are themselves scavenged to be reduced (Fig. 1a), and therefore survive to become more-persistent, larger particles. Given that observed growth rates in urban areas are only a few times greater than those in remote environments, it is hard to understand how newly formed particles can reach diameters of 10 nm or more in urban areas — but such growth seems to be widespread in megacities in wintertime. Figure 1 | The growth and formation of atmospheric particles. a, Small clusters of atmospheric molecules can gradually accumulate more molecules until they form stable particles. However, other particles in the atmosphere can scavenge the available vapour, limiting cluster growth, or even scavenge whole clusters. The concentration of particles in urban environments is high, which means that any clusters or vapour would be expected to be scavenged by existing particles before they form stable particles themselves. Yet the observed rate of new-particle formation is surprisingly high in megacities. b, Wang et al.1 report that clusters can grow rapidly by accumulating ammonium nitrate (which forms from ammonia and nitric acid molecules) under conditions known to occur in megacities in winter. This allows the clusters to reach stable particle sizes before they are scavenged by other particles — and might explain the high particle-formation rates observed in urban areas. Wang et al. investigated this issue by carrying out a set of chamber experiments that reproduced atmospheric conditions typical of a megacity, focusing on the behaviour of ammonium nitrate. This compound is a crucial component of urban winter- and springtime particulate matter8, but has not been thought to have a major role in particle formation. Ammonium nitrate exists in a temperature-dependent equilibrium with gaseous ammonia and nitric acid, and this equilibrium favours the gas phase when it is warm. However, the authors observed that ammonium nitrate rapidly condenses onto newly formed clusters at temperatures below 5 °C (Fig. 1b). This is because the atmospheric concentrations of ammonia and nitric acid vapours at these temperatures can exceed their equilibrium values. To put it another way, when the ratio of the concentration of these gases to their concentration at equilibrium (the saturation ratio) under the same environmental conditions is greater than 1, rapid condensation takes place. Crucially, the observed rapid condensation accelerates particle growth. The particle growth rates at –10 °C were 200 times faster than those at +5 °C, for the same gas concentrations of ammonia and nitric acid. The growth rates at cold temperatures are much higher than those previously derived from field observations in urban areas.   By measuring the composition of vapours and particles using an array of advanced mass spectrometers, Wang and colleagues showed that ammonium nitrate does not participate in particle formation at temperatures above –15 °C. Particle formation instead proceeds through a well-recognized pathway involving ammonia and sulfuric acid9; rapid growth through ammonium nitrate condensation begins to occur once a threshold cluster size has been reached. However, the authors report that new particles can form directly from ammonium nitrate at temperatures below –15 °C. The authors speculate that this process could occur in the humid air outflows at the top of convective tropical clouds. The authors show that the critical size at which ammonium nitrate starts to induce rapid growth depends on the saturation ratio of the ammonia–nitric acid system. Furthermore, once a particle has reached that size, it continues to grow rapidly because the equilibrium concentration of ammonia and nitric acid above ammonium nitrate is much lower for larger particles. This growth occurs in much the same way that a liquid cloud forms on particles called condensation nuclei, which grow rapidly as soon as the saturation ratio of water exceeds 1. So, how representative of the real world are these experimental observations, and what do they tell us about real urban environments? The mixing ratios of ammonia and nitric acid in the experiments are typical of those of many urban environments and are often greatly exceeded in some megacities. Moreover, in many places, such as Beijing or Delhi (Fig. 2), severe air-pollution events involving high concentrations of ammonia and nitric acid occur often, mostly in wintertime when the daytime temperatures are at, or below, 5 °C (see ref. 10, for example). Figure 2 | Heavy smog in Delhi during winter 2019. Severe wintertime pollution events in megacities can produce atmospheric conditions similar to those reported by Wang et al.1 to cause rapid growth of atmospheric particles.Credit: Sajjad Hussain/AFP/Getty However, air must become supersaturated with ammonia and nitric acid before clusters can grow through ammonium nitrate condensation. Wang et al. convincingly argue that localized supersaturation of these gases is likely in many cities because the environment is heterogeneous. For example, the emission sources vary widely, and the flow of emissions around buildings, in street canyons and as a result of traffic movement combine to generate substantial gradients of concentration. The temperature in cities also often varies by several degrees over distances of a few metres to a few tens of metres, because of direct heating or shadowing from buildings, and because different surfaces absorb and reflect heat differently. These temperature variations can alter the saturation ratio of ammonia and nitric acid sufficiently for rapid condensation to occur. Wang and colleagues calculate that the rapid condensation of ammonia and nitric acid occurs on timescales of several minutes in their experiments. The temperature heterogeneities observed in cities are sustained for similar timescales across various distances, potentially allowing clusters to grow to more-stable sizes at which further mass can be added to grow the particles. In other words, the new findings might explain why the initial stages of particle growth can be so fast in cities. Previously calculated cluster-growth rates in cities were averaged over space and time, and therefore did not capture this heterogeneity. It will be extremely challenging to demonstrate that rapid ammonium nitrate condensation occurs in the real atmosphere, but the concept is very persuasive. Numerous semi-volatile organic compounds in the atmosphere might well have a similar role in particle growth. More broadly, Wang and colleagues’ work provides key knowledge that will inform air-quality policy as the chemical composition of urban atmospheres changes in the future. Most notably, sulfur dioxide emissions are being reduced across many cities. This makes it increasingly likely that urban pollution will be dominated by emissions of nitrogen oxide (a precursor of nitric acid) from road traffic and by ammonia from agriculture for the coming decade or more. </body>
<date id = '15'>13 May 2020</date>
<url id = '16'>https://nature.com/articles/d41586-020-01335-3</url>
<title id = '16'>There is growing evidence that gut microbes can influence disease. Analysis of a mouse model of the neurodegenerative condition amyotrophic lateral sclerosis offers insight into how gut bacteria might contribute to this illness.</title>
<body id = '16'>Animals have co-evolved with diverse communities of microorganisms that are integral to the development and activity of their immune and nervous systems1. Alterations in the composition and function of the community of gut microorganisms (termed the microbiota) are increasingly being implicated in neurological disorders that involve neuroinflammation, including multiple sclerosis2, autism spectrum disorder3 and Parkinson’s disease4. Studies are also emerging that link the gut microbiota to amyotrophic lateral sclerosis (ALS), a neurodegenerative disorder characterized by the progressive loss of motor neurons crucial for movement, speech and cognition. This devastating disease is usually fatal within a few years of diagnosis. Writing in Nature, Burberry et al.5 fill some gaps in our knowledge of how gut microbes might contribute to ALS, from studies of the condition in a mouse model. Their findings might help to shed light on how a gene linked to ALS called C9orf72 affects this disease.   Initial studies6,7 have shown that the gut microbiota of people who have ALS differ from those of unaffected individuals. A study of a mouse model of the disease, based on an ALS-associated mutation in the Sod1 gene6, has provided strong evidence that alterations in the microbiota can exacerbate neurodegeneration and drive early mortality. That study also identified microbes and microbial molecules that promote improved motor function and longer lifespan in the mice. It showed that the particular positive or negative effects observed might depend on differences in the microbes encountered in the animals’ housing facility (termed a vivarium). Mouse models of inflammatory diseases have also revealed that the animals’ environment has such an effect8. Burberry et al. used a mouse model of ALS (Fig. 1) in which the animals have a mutant version of the gene C9orf72, resulting in a deficiency in the encoded C9orf72 protein (these mice also model a neurodegenerative condition called frontotemporal dementia). The authors observed that if the animals were reared in the Harvard University animal facility, they had a shorter lifespan, exacerbated movement problems and an elevated immune response (as indicated by the presence in their blood of pro-inflammatory molecules called cytokines and autoimmune antibodies), compared with mice reared at the Broad Institute of Harvard and MIT. After excluding diet, light cycles and other environmental factors as being responsible for this difference, the authors compared the microbial profiles for the animals in the two facilities, and found that a virus called murine norovirus and the bacteria Helicobacter, Pasteurella pneumotropica and Tritrichomonas muris were more common at the Harvard facility than at the Broad facility. Figure 1 | Gut microbes modulate inflammation and lifespan in a mouse model of neurodegeneration. a, b, Burberry et al.5 report that mice with a mutation in a gene called C9orf72, which is often mutated in the disease amyotrophic lateral sclerosis, had a longer lifespan and less-severe movement problems if they were housed in facilities at the Broad Institute (a) than if housed at Harvard University (b). The animals housed at the Broad had more-diverse species of gut microbes (different colours indicate different species) than did those kept at Harvard. The mice at the Broad also showed fewer signs of inflammation in their bloodstream (fewer immune signalling molecules called cytokines or self-reactive antibodies associated with autoimmunity) and in their spinal cord (fewer immune cells such as T cells and neutrophils, with immune cells called microglia not in an activated state). Gut microbes contributed to these differences between the animals, and antibiotic treatment (not shown) reduced the inflammation and lessened the movement problems of the animals kept at Harvard. The authors investigated the diversity of the gut microbial species further by analysing faecal samples and sequencing a bacterial gene needed for synthesis of the ribosome, the cell’s protein-production machinery. This revealed that the ALS mice housed at Harvard or Johns Hopkins University (where mice had a short lifespan) had less microbiota diversity than did mice housed at the Broad or at the Jackson Laboratory research institute (where mice had a longer lifespan). Microbiota profiling of bacterial species revealed alterations in the relative abundances of 62 of 301 bacterial taxa assessed when the ALS mice reared at Harvard and Johns Hopkins were compared with the mice reared at the Jackson Labs and the Broad. The authors next investigated whether the gut microbiota contributed to the vivarium-dependent differences in the severity of symptoms observed for these animals. They tested the effects of antibiotic treatment and of transplants of faecal microbiota on the inflammation and autoimmune responses of the mice reared at Harvard. Antibiotic treatment of young mice as they aged prevented the induction of pro-inflammatory cytokines and reduced other hallmarks of inflammation — such as a rise in the number of immune cells called neutrophils, the presence of autoimmune antibodies and enlargement of the spleen. Antibiotic treatment of aged mice had a similar effect. Inflammation in the Harvard mice was also reduced if they received transplants of faecal microbiota from animals housed at the Broad.   To explore whether the microbiota contributed to neuroinflammation in the spinal cord in these mice, animals reared at Harvard were continuously treated with antibiotics. The authors assessed whether inflammation had developed by checking for infiltration of immune cells into the spinal cord and whether the spinal cord contained immune cells called microglia in an activated state. They found that infiltration by neutrophils and by subsets of immune cells called CD3e+ T cells was reduced, as was microglial activation, compared with the situation in untreated animals. These findings are consistent with the results of studies of other disease models, indicating a role for the gut microbiota in modulating inflammation in the central nervous system and the development and function of microglia9,10. Burberry and colleagues have shown that alterations in the gut microbiota variously modulate how ALS-related symptoms manifest in mice deficient in C9orf72. Their results suggest that microbial modulation of inflammation outside the brain might be responsible. More research will be needed to find the particular microbes and microbial functions involved in regulating the different effects on inflammation outside the central nervous system, and to assess whether this peripheral inflammation influences the degeneration of motor neurons in the central nervous system and the associated movement impairments that such neuronal losses cause. It will also be interesting to determine whether particular immune pathways in the central nervous system and/or outside it are responsible for the lifespan changes that occur through C9orf72 deficiency. Unravelling the molecular and cellular mechanisms involved in the neuroinflammation and neurodegeneration observed would advance our understanding of the interactions between environmental factors and genetic risk factors in ALS, and might lead to new targets for clinical intervention. Fung, T. C., Olson, C. A. & Hsiao, E. Y. Nature Neurosci. 20, 145–155 (2017). Berer, K. et al. Proc. Natl Acad. Sci. USA 114, 10719–10724 (2017). Hsiao, E. Y. et al. Cell 155, 1451–1463 (2013). Sampson, T. R. et al. Cell 167, 1469–1480 (2016). Burberry, A. et al. Nature https://doi.org/10.1038/s41586-020-2288-7 (2020). Blacher, E. et al. Nature 572, 474–480 (2019). Fang, X. et al. Front. Microbiol. 7, 1479 (2016). Stappenbeck, T. S. & Virgin, H. W. Nature 534, 191–199 (2016). Erny, D. et al. Nature Neurosci. 18, 965–977 (2015). Thion, M. S. et al. Cell 172, 500–516 (2018). Download references Latest on: Immunology News 22 MAY 20 Comment 21 MAY 20 Article 20 MAY 20 Microbiology News 25 MAY 20 News 22 MAY 20 Career Column 22 MAY 20 Neuroscience Research Highlight 21 MAY 20 News & Views 20 MAY 20 Article 20 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '16'>13 May 2020</date>
<url id = '17'>https://nature.com/articles/d41586-020-01281-0</url>
<title id = '17'>An analysis of faecal samples reveals that obese people who take cholesterol-lowering statin drugs have a ‘healthier’ community of gut microorganisms than would be expected. What are the implications of this surprising finding?</title>
<body id = '17'>Our digestive systems harbour more bacterial cells than there are human cells in our bodies. Although the often-mentioned estimate of a tenfold excess of microorganisms over human cells might exaggerate the ratio1, even conservative estimates2 accord the microbes numerical dominance at a ratio of about 1.3:1. These close gut microbial neighbours of ours comprise around 0.3% of a person’s mass2, and there are more than 100 times more bacterial genes in the gut3 than there are genes in their human host. Interest has burgeoned in the potential effects of these normal gut residents (sometimes termed commensal bacteria) on our well-being. Writing in Nature, Vieira-Silva et al.4 report an unexpected discovery, regarding patterns of gut microbes, that might have clinical consequences. When trying to assess the daunting complexity of the many thousands of bacterial species in our gut, one option available is a categorization method5 that assigns an individual’s microbial profile to one of four groupings called enterotypes (Fig. 1), depending on the abundance of signature species. Those of us not immersed in the world of bacterial binomials owe a debt of gratitude to the colonically oriented colleagues who came up with this and other possible classification approaches. Figure 1 | Gut-microbe changes associated with the use of statin drugs. A person’s gut microorganisms can be classified5, by the analysis of faecal samples, into one of four groups called enterotypes, depending on the abundance of particular microbial species. These groupings are termed Bacteroides 1 (Bact1), Bacteroides 2 (Bact2), Ruminococcaceae (Rum) and Prevotella (Prev). The Bact2 enterotype is associated with health problems and inflammation6. Vieira-Silva et al.4 assessed enterotype data for individuals who were recruited as part of a project to understand factors influencing cardiovascular health. The authors made the unexpected discovery that the prevalence of the Bact2 enterotype was lower than expected in obese individuals who were taking cholesterol-lowering drugs called statins. Whether this decrease in prevalence of the Bact2 enterotype in obese individuals is directly caused by statins or is due to another factor associated with statin use (if individuals taking statins have better access to health care, for example) will require further study. The ‘dysbiotic’ enterotype called Bacteroides 2 (Bact2) is associated with inflammation6. People who have this enterotype tend to have a lower load of gut microbes than do those with other enterotypes, and more Bacteroides bacteria than Faecalibacterium microbes. These individuals also have a higher blood concentration of C-reactive protein — a hallmark of inflammation — than do individuals who have other enterotypes7. A cascade of data from the colonic cognoscenti links the composition of gut microbes to aspects of health. For example, more than 75% of individuals who have inflammatory bowel disease have the Bact2 enterotype, whereas fewer than 15% of people who do not have the disease harbour this enterotype6. Beyond the gut, many researchers have implicated gut microbes in obesity8 and the cluster of conditions referred to as metabolic syndrome. However, the nature of the relationship between microbes and these conditions remains under debate.   Studies have also linked gut bacteria to cardiovascular disease. Molecules such as trimethylamine oxide, which are made by gut bacteria, might accelerate atherosclerosis, and their presence is associated with adverse cardiovascular outcomes, including death9. Vieira-Silva et al. report that the Bact2 mix of intestinal bacteria is characterized by a paucity of bacterial producers of another microbial molecule, butyrate. This short-chain fatty acid might help to preserve the barrier function of the epithelial cells that line the gut, perhaps preventing leakage of harmful bacterial endotoxin molecules from the bowel and thereby dampening systemic inflammation of the body10. In their quest for a potential connection between the bacterial population of the gut and obesity, Vieira-Silva and colleagues made a striking discovery when they mined data collected in a European Union study called the MetaCardis project (http://www.metacardis.net). This project has gathered data on the composition of human gut microbes using state-of-the-art technology, to assess the microbes’ role in cardiovascular disease. More than 2,000 individuals recruited from European countries took part in an exhaustive survey that collected data for around 1,400 variables, such as medication taken and body-mass index (a measure used to assess a person’s weight that takes height into account). Vieira-Silva et al. report that in a subset of nearly 900 participants whose data they analysed, a higher prevalence of the Bact2 enterotype correlated with a higher body-mass index and obesity. However, the authors made the striking discovery that the pattern of enterotypes found in the population of obese individuals differed significantly depending on whether people were taking cholesterol-lowering drugs called statins (comprising about 12% of those studied). This result raised a surprising possible connection between statin intake and gut microbes. The obese participants taking statins had a significantly lower prevalence of the Bact2 enterotype (5.9% of the obese population) than did their obese counterparts not taking statins (17.7% of the obese population). Vieira-Silva and colleagues confirmed this phenomenon in an independent data set from the Flemish Gut Flora Project11.   The use of statins is one of the great success stories of modern cardiovascular therapeutics. Originally derived from natural products of microbial denizens of the soil, these agents inhibit a rate-limiting enzyme in the pathway that makes cholesterol. By lowering cholesterol production, the treatment coaxes cells to boost the expression of receptors for low-density lipoprotein (LDL) that capture cholesterol-rich LDL particles, and this results in a robust decrease in cholesterol in the bloodstream. This LDL reduction substantially lowers the risk of cardiovascular events such as heart attack and stroke in a large swathe of the population at risk of such conditions, and many people use drugs of the statin class. Large meta-analyses of the effects of statin treatment reveal that it prolongs lifespan and that, on balance, the benefits outweigh any unwanted effects12. Independently of their effects on LDL, statins have anti-inflammatory actions that probably contribute to their clinical benefit through well-established molecular mechanisms13. However, no statin study has singled out obese individuals as targets for therapy, and no current guideline recommends considering obesity when making decisions about using statins for treatment. Vieira-Silva and co-workers’ unexpected findings therefore raise intriguing questions relating to the clinical use of statins. Yet interpretation of these findings warrants caution, in particular with regard to the risk of confusing correlation with causation. As the authors of this large and carefully executed study rightfully acknowledge, we should consider whether statin takers have had better access to health care or been more engaged in other health-promoting behaviours than have the individuals who were not taking statins. A large-scale clinical trial to determine whether statins lead to a reduced prevalence of the Bact2 enterotype in obese participants who would not otherwise receive statins could address this possibility, which is known as confounding by indication. Moreover, whether these findings apply across ethnic groups will require further study. In any case, following up on these provocative observations promises to provide new mechanistic insight into the complex relationships between obesity, metabolic status, gut microbes and cardiovascular disease. </body>
<date id = '17'>06 May 2020</date>
<url id = '18'>https://nature.com/articles/d41586-020-01250-7</url>
<title id = '18'>An elusive type of atom known as pionic helium has been directly excited by laser light for the first time. The work establishes a promising experimental platform for probing fundamental physics.</title>
<body id = '18'>Exotic atoms are those in which one or more of the constituents of normal atoms have been replaced by an exotic particle, such as an antimatter particle. These atoms can then be probed to search for any tiny discrepancies in their properties from those predicted by models using techniques that underpin the world’s most accurate timekeepers, atomic clocks — and thereby opening a window on the foundations of physics. Writing in Nature, Hori et al.1 are the first to report laser excitation of helium atoms in which one electron has been replaced by a subatomic particle called a pion.   The interest in exotic atoms arises from the fact that they often facilitate the most basic experimental strategy used in physics: changing a single parameter or component in an otherwise complex system, to observe the effect. In practice, this is not as simple as it might seem. Different particles can have different masses or charges, and might interact with their surroundings differently in other subtle ways. However, such subtleties often add to the value of exotic atoms. As the techniques needed to study exotic atoms improve, increasing numbers of scientists are working with these atoms to investigate the fundamental properties of nature. A good example of this is the ‘proton radius puzzle’, which arose from a study of muonic hydrogen2 — a hydrogen atom in which the electron has been replaced by a subatomic muon particle (muons have similar properties to electrons, but have about 200 times greater mass). Muonic hydrogen was used to determine a key property of the proton known as the charge radius, but the value obtained was about seven standard deviations away from the expected value at the time. The value obtained using muonic hydrogen has since been independently confirmed in a study of ordinary hydrogen3, and also in experiments in which electrons are scattered from protons4, potentially clarifying the true value of the proton radius and thus solving the puzzle. Nevertheless, muonic hydrogen aptly illustrates how exotic, sometimes short-lived, atomic systems can be used to poke holes in seemingly well-established results. An important feature of exotic atoms that adds to their utility as probes for fundamental physics is that they are bound systems (energy is needed to pull their components apart), with multiple internal energy states. Transitions between these states are therefore amenable to study by laser spectroscopy, the most precise measurement tool in the physics toolkit. The study of transitions in atoms — and particularly in the hydrogen atom — is an ongoing effort that has spanned more than two centuries. It inspired Niels Bohr’s groundbreaking model of the atom in the early twentieth century, for example, and has driven much of the development of quantum mechanics.   Today, atomic transitions are the foundation on which all time measurements are built: a transition in the caesium-133 atom provides a reference value that underpins the International System of Units definition of the second. Therefore, spectroscopic techniques for measuring transitions are constantly being refined, and the best measurements can now reach staggering precisions — to almost 18 significant figures5. The most precise measurements of exotic atoms still lag some way behind, but a measurement on antihydrogen (the bound state of an antiproton and an antielectron) achieved a milestone precision of almost 12 significant figures6, thus paving the way for extremely sensitive tests of the fundamental properties of antimatter. In their seminal work, Hori et al. record the first observation of a transition in a ‘pionic’ helium atom. In such an atom, one of the two electrons of a helium atom has been replaced by a subatomic particle called a pion. Pions were discovered7 by Cecil Powell and co-workers in 1947, but their existence was first predicted8 in 1935 by Hideki Yukawa. They belong to the family of subatomic particles known as mesons, which are made up of a quark and an antiquark; quarks are the particles that make up protons and neutrons. Pions are short-lived particles that come in positively charged, negatively charged and neutral varieties. The negatively charged pions used by Hori et al. have a lifetime of only 26 nanoseconds when isolated. It is thus no small feat that the experimenters not only succeeded in replacing an electron in helium atoms with a pion, but also observed the resulting exotic atom undergo a quantum transition. A further difficulty is that the lifetime of the pion in the exotic atom can be reduced to picoseconds because of its vicinity to the atom’s nucleus. The authors prepared pionic helium atoms by firing a beam of pions at a liquid-helium target. In Hori and colleagues’ experiment, the helium target was cooled to a cryogenic temperature of about 2 kelvin. This allowed some pions to be captured in a weakly bound state of pionic helium, in which the pion was sufficiently far from the nucleus to be shielded from it by the remaining electron (Fig. 1). The resulting exotic atom therefore retained a lifetime of nanoseconds, which is long enough for a laser pulse to excite the nascent exotic atom. Figure 1 | How to make and excite pionic helium. Hori et al.1 fired a beam of negatively charged subatomic particles, called pions, at liquid helium. When a pion struck a helium atom, it could knock out one of the electrons and replace it in a high-energy orbit around the helium nucleus (which consists of two protons and two neutrons), thus forming an exotic atom known as pionic helium. The authors fired laser light at the exotic atoms and thus observed the transition of the pion to a lower-energy orbit — a process that triggers the ejection of the remaining electron. This ejection speeds up the absorption of the pion by the nucleus (not shown), which finally breaks apart. Using a short pulse (0.8 nanoseconds) of infrared laser light, Hori and colleagues provoked a transition of the pion. This resulted in the ejection of the remaining electron, leaving a short-lived system consisting of just a pion bound to a helium nucleus. The pion was then absorbed by the nucleus, leading to the latter’s break-up (fission). The pion transition was detected by carefully removing large ‘background’ signals from the experimental data; this background was associated with fission products from short-lived states of pionic helium, or was generated by the pion beam itself. That left a signal from just three transitioning pionic helium atoms per hour, or an estimated three per billion exotic atoms produced. Despite the challengingly low numbers, the laser-induced signal for the transition was clearly detected, and the laser frequency at which it occurred (and which corresponds to the energy change of the transition) could be determined with an absolute precision of about five significant figures. This result builds on Hori and colleagues’ extensive experience of studying another exotic helium-type atom, in which one electron was replaced by an antiproton9. That work resulted in, among other things, the most precise determination so far of the ratio of the mass of the antiproton to that of the electron. However, the authors had to overcome further challenges to study pionic helium. For example, the lifetimes of pionic helium atoms are shorter than those of antiprotonic helium and the widths of lines in its spectrum are broadened, because the excited exotic atoms undergo many collisions in the relatively dense liquid-helium target. It might be possible to reduce the density of the liquid helium to resolve these somewhat incalculable effects, although this would also lower the signal rate. It remains to be seen whether extrapolation to zero density, where collisional effects are minimized, is practicable. Nevertheless, Hori and colleagues’ work opens up a whole new experimental system for further exploration. If the above challenges can be overcome, such exploration might enable the accuracy of the mass of the negative pion to be improved by a factor of 10–100, for instance; currently, this mass is known to only six decimal places. The experiment thus paves the way to fresh insights into the fundamental constituents of nature. </body>
<date id = '18'>06 May 2020</date>
<url id = '19'>https://nature.com/articles/d41586-020-01251-6</url>
<title id = '19'>Highly charged ions could form the basis of the next generation of ultra-precise clocks, using electronic transitions in the ions as the ‘pendulum’. An ingenious method for characterizing such transitions has been reported.</title>
<body id = '19'>Atomic clocks, which use transitions between the energy levels of electrons in atoms as a reference for their timekeeping mechanism, are the world’s most accurate clocks — they will not lose one second during the lifetime of the Universe1. This means that they can be used in ultra-precise measurements to probe some of the fundamental postulates of modern physics. Clocks based on highly charged ions (HCIs; atoms from which many electrons have been removed) are predicted to have even more sensitivity in these investigations2. However, the development of such clocks is hampered by the difficulty of detecting suitable transitions in HCIs.   Writing in Nature, Schüssler et al.3 report that they have measured a long-lived, excited electronic state in a highly charged rhenium ion using the mass difference of the ion in its ground and excited states. This non-destructive, direct determination of an electronic excitation in an HCI will aid the discovery of HCI transitions that would be suitable for use in a clock. To build a clock, one needs a periodic event whose frequency acts as a reference for timekeeping. Electronic transitions in atoms are perfect natural oscillators for this purpose. An ultra-stable laser must be tuned to the exact frequency of the atomic transition to drive the oscillation, much as a musical instrument must be tuned to produce the right tone. Can just any atomic transition be used? No — suitable transitions are hard to come by. The best transitions start from the lowest energy state of an atom (the ground state) and must end up in a long-lived (metastable) excited state. The energy needed to stimulate the transition must also be within the range of tabletop-laser technologies. Moreover, the atoms must be held in traps, so that their motion is almost completely frozen — in other words, the operation of atomic clocks requires precision manipulation of quantum systems. For this reason, currently available clocks use transitions either in electrically neutral atoms or in ions produced by removing one electron from an atom, because these systems are the most amenable to precision quantum control. Substantial advances have been made in studies of HCIs, and all the technologies required to make a clock using such an ion were demonstrated only this year4. However, progress is hindered by the difficulty in using conventional atomic spectroscopy to identify and measure transitions suitable for use in clocks — the characteristics of such transitions mean that they are, by definition, very weak (the probability of the transition occurring is small). Schüssler et al. therefore used a completely different and ingenious method to measure the energy change that occurs during a weak transition in a highly charged rhenium ion (Re29+): they used Einstein’s famous principle of energy–mass equivalence (E = mc2) to convert a mass measurement into an energy measurement.   The basic idea is to trap a single ion in a Penning trap, a device that confines charged particles using magnetic and electric fields. The mass of an ion in a Penning trap can be determined by measuring the frequency of the ion’s motion in a magnetic field (the cyclotron frequency). The binding energy of an atom or ion — the energy required to break the atom into its free electrons and a nucleus — is different in an excited metastable state from that in the ground state. The mass therefore also changes, which, in turn, alters the cyclotron frequency. In their experiments, Schüssler et al. measured the ratio (R) of the cyclotron frequency of Re29+ in the ground state and the metastable state. Because the difference in energy of the two states in Re29+ is extremely small compared with the total energy of the ion, the precision of the measurement needs to be extraordinarily high. The authors measured R to a precision of 10–11, using a device known as PENTATRAP. PENTATRAP consists of a stack of five Penning traps cooled to a temperature of 4 kelvin (Fig. 1). Traps 2 and 3 are used to measure cyclotron frequencies, whereas traps 1 and 4 are used to store ions. Trap 5 was not used in the current experiments, but will allow monitoring of fluctuations in the magnetic field and in other experimental variables in the future. Figure 1 | Mass measurements of a highly charged ion. Schüssler et al.3 used an instrument called PENTATRAP, which consists of five stacked ion traps, to determine how the mass of the rhenium ion Re29+ differs in its ground state (blue ions) and in a metastable excited state (red ions). The authors captured three ions in their first position (traps 2–4) at 4 kelvin, and simultaneously measured the cyclotron frequency — the frequency of an ion’s motion in a magnetic field — of the ions in traps 2 and 3. All three ions were then shifted to their second position, so that the ions in traps 2 and 3 had different states from the ones that were trapped in the first position. The cyclotron frequencies of the ions in those traps were measured, and the whole sequence was repeated many times. From the ratio of cyclotron frequencies of ions in the two different states, the authors determined the associated difference in ion mass, and thus the change of energy that occurs when a Re29+ ion transitions between states. Such a measurement would be difficult using conventional atomic spectroscopy. The authors loaded three ions into the innermost traps, so that the ions in traps 2 and 4 were in the same state (either the metastable state or the ground state), and the ion in trap 3 was in the alternative state. First, they measured the cyclotron frequencies of the ions in traps 2 and 3 simultaneously. They then moved the three ions up by one trap, effectively swapping the states of the ions in traps 2 and 3 (the states of the ions did not change, only their positions; Fig. 1), and simultaneously measured the cyclotron frequencies of those ions. The three ions were moved back down by one trap, and the sequence began again. Overall, the electronic states in traps 2 and 3 were repeatedly swapped, and simultaneous measurements were taken after each swap. This experimental procedure, combined with the design of the PENTATRAP device, suppresses the effect of magnetic-field variations on R, thus allowing R to be determined with high accuracy. The energy difference between the ground and excited states can then be calculated using R and the ion mass in a variant of Einstein’s equation; the actual mass of the ion needs to be known only to a precision of 10–4. This first demonstration of the method opens up exciting possibilities for measuring the transition energies in HCIs that are difficult to measure using conventional approaches. Moreover, the energy change measured by Schüssler and colleagues is in excellent agreement with that predicted from the authors’ advanced theoretical calculations. This agreement demonstrates that theory can be used to predict the transition energies in HCIs, thereby facilitating the discovery of more transitions. The transition energy measured in the current work corresponds to a frequency that lies outside the range of lasers that can be used in a clock. However, the authors note that it should be possible to use their method to measure transitions that have lower frequencies suitable for clock development in the near future. Clocks based on HCI transitions are particularly attractive because they could be used in stringent tests that are sensitive enough to detect physics beyond the standard model of particles and interactions — such as variations of fundamental physical constants and violations of Lorentz invariance2 (a cornerstone of physics that acts as the mathematical foundation for Einstein’s special theory of relativity). Such clocks would also be particularly sensitive to the effects of ultralight dark matter2,5, one of the candidates for the ‘missing’ matter in the Universe. Tremendous progress in the control of HCIs has been made in the past few years2,4,6, paving the way towards these applications. The precision mass spectrometry enabled by PENTATRAP will also have other valuable applications7, such as in tests of the energy–mass equivalence principle, experimental determinations of the upper limits of the mass of neutrino particles, and tests of quantum electrodynamics, the theory that describes the interactions between particles and light. </body>
<date id = '19'>06 May 2020</date>
<url id = '20'>https://nature.com/articles/d41586-020-01280-1</url>
<title id = '20'>Liquid-like organelles in cells form when key constituents reach a certain concentration and then condense. Evidence now indicates that the concentration at which condensation occurs can vary, contrary to previous assumptions.</title>
<body id = '20'>Water transitions from a liquid to a gas phase as it reaches its boiling point. Similarly, proteins in cells can transition from freely mixing in the cytoplasm or its nuclear equivalent, the nucleoplasm, to condensing into a concentrated liquid-drop phase once they reach a threshold concentration1. This saturation concentration has been assumed to be an invariant quantity, but, writing in Nature, Riback et al.2 demonstrate that this assumption is invalid. Much as the boiling point of water varies depending on pressure, the saturation concentration depends on the concentrations of the proteins involved.   Condensation of molecules into a liquid-like droplet — a process called phase separation — is a well-studied physical phenomenon, which can be caused by mutual attractions between proteins or other molecules. But many biological studies of phase separation so far have used simple model systems, rather than complex living cells. Riback and colleagues reasoned that the idea of a single fixed saturation concentration might have arisen because of the use of simple systems. In cells, phase separation produces liquid-like organelles called biomolecular condensates3. One such condensate is the nucleolus, in which the ribosome machinery involved in protein synthesis is made. Riback et al. set out to examine saturation concentration in cells by studying the protein nucleophosmin 1 (NPM1), which is a key driver of nucleolus formation4,5. The group found that increasing the overall concentration of NPM1 in cells increased the corresponding saturation concentration at which the nucleolus forms in the nucleoplasm. Likewise, increasing the concentration of key proteins altered the saturation concentration of stress granules — another type of liquid-like organelle.   Next, the authors showed that the variability of saturation concentration is caused by distinct interactions between a condensate’s components. Rather than molecules of the same protein interacting during condensation, which might produce a fixed saturation concentration (NPM1 binding to other molecules of NPM1, for instance), the group found that phase separation depends on heterotypic interactions between different proteins in the condensate. As the concentrations of different proteins alter, the free energy of the nucleoplasmic mixture — the thermodynamic quantity that dictates how the components in the cell system are partitioned by phase separation — can change in a complicated manner, leading to changes in saturation concentration. Biomolecular condensates are often intricately linked to cell functions6. Riback and colleagues went on to show how heterotypic interactions are exploited by nucleoli to facilitate the processing of ribosomal RNA, which makes up part of the ribosome. They found that phase-separating proteins such as NPM1 and another protein, SURF6, interact freely with immature forms of ribosomal RNA, but not as well as with more mature forms of the molecule. This leads to the mature RNA being expelled from the liquid-like nucleolus (Fig. 1). This finding highlights that nucleoli might not only act to concentrate key molecules and facilitate biochemical reactions, but also possess an underlying conveyor-belt mechanism to ensure a continuous and smooth production process. Hence, the reputation of the nucleolus as the ribosome factory might be even more pertinent than people thought7. Figure 1 | Protein–RNA interactions control biological processes in the nucleolus. Riback et al.2 report that complex interactions between different molecules govern the formation of liquid-like organelles such as the nucleolus, and can also regulate organelle function. The ribosome is a protein-synthesizing machine that is assembled from protein and RNA subunits in the nucleolus. The authors demonstrate that the proteins nucleophosmin 1 (NPM1) and SURF6 (not shown), which are key for formation of the nucleolus, interact freely with immature ribosomal RNA (rRNA). But as the rRNA becomes properly folded and incorporated into the ribosome, these interactions cease, and so the mature ribosomal RNA is expelled from the organelle into the surrounding nucleoplasm. Riback and colleagues complemented each of their experimental findings theoretically, using methodology borrowed from equilibrium physics — the premise that there is no flow of energy into or out of a system. However, the environment of the cell interior, with its many processes driven by energy-carrying ATP molecules, is far from existing in equilibrium. As such, it is remarkable that the authors’ close-to-equilibrium theory matches their real-world observations. I think that, although the picture laid out by Riback and colleagues is a valuable starting point, the reality will inevitably be more complex. Establishing a quantitative connection between experiments and theory will require further development of our theoretical understanding of non-equilibrium phase separation, which is still in its infancy8,9. The fact that physicists do not know much about phase separation in non-equilibrium regimes should not be viewed as a drawback in the study of biomolecular condensates, however. Instead, it signposts a golden opportunity for life scientists, bioengineers and physicists to work closely together to expand our understanding of this complex phenomenon. </body>
<date id = '20'>06 May 2020</date>
<url id = '21'>https://nature.com/articles/d41586-020-01152-8</url>
<title id = '21'>People who carry the gene variant APOE4 are at higher-than-average risk of developing Alzheimer’s disease. It emerges that this variant is linked to defects in the blood–brain barrier and subsequent cognitive decline.</title>
<body id = '21'>The best-known hallmarks of Alzheimer’s disease are clumps of misfolded amyloid-β (Aβ) and tau proteins, which aggregate in the brain. However, there is increasing awareness that Aβ and tau might not be the whole story — alterations in the blood–brain barrier (BBB) have also emerged as early markers of this neurodegenerative disorder1. The degree of disruption to the BBB correlates with the degree of cognitive dysfunction that a person experiences2, but what causes BBB breakdown has been unknown. Writing in Nature, Montagne et al.3 present evidence that the leading genetic risk factor for Alzheimer’s disease, apolipoprotein E4, is linked to BBB breakdown.   The gene apolipoprotein E (APOE) encodes a major lipid-carrier protein, ApoE, in the brain4. There are three predominant variants of APOE: APOE2, APOE3 and APOE4. As with almost all genes, people carry two copies of APOE, which can be either the same or different variants. Compared with the more-common APOE3 variant, APOE4 markedly increases the risk of Alzheimer’s disease — up to 4-fold in people with one copy of this variant, and 15-fold in people who have two copies4. People carrying APOE4 who do contract Alzheimer’s disease also tend to develop symptoms of the disorder earlier than those who develop the disease but do not carry the variant4. Proteins from blood plasma have been found in the cerebrospinal fluid (the liquid that surrounds the brain and spinal cord) of cognitively healthy people who carry APOE4 and who subsequently go on to develop Alzheimer’s disease. These proteins have presumably leaked through the BBB, indicating that the integrity of the barrier is lost before cognition declines5. Evidence from mouse models, and from the brains of people who have died with Alzheimer’s disease, suggests that BBB breakdown is caused by the degeneration of pericytes — cells nestled in the wall of cerebral capillaries. These cells normally safeguard the BBB5 by preventing the breakdown of junctions between endothelial cells, which make up the capillary walls. Whether ApoE4 is responsible for early BBB dysfunction in Alzheimer’s disease, by itself or in concert with Aβ and tau, was unknown. Montagne and colleagues set out to address this knowledge gap. The authors used a technique called dynamic contrast-enhanced magnetic resonance imaging to investigate the permeability of the BBB in people who had either healthy cognition or mild cognitive impairment (a prelude to Alzheimer’s disease), grouped according to their APOE status. They found that people who were cognitively healthy and carried either one or two copies of APOE4 had a leaky BBB in two brain regions important for memory and cognition — the hippocampus and the parahippocampal gyrus. This leakage was worse in APOE4 carriers who exhibited mild cognitive decline. Remarkably, these effects preceded any signs of tissue loss in the hippocampus and parahippocampal gyrus, attesting to the idea that BBB disruption is an early event in the onset of neurodegeneration. BBB leakage was independent of Aβ and tau accumulation, which the authors assessed both by studying samples of cerebrospinal fluid and through another brain-imaging technique, positron emission tomography. Montagne and co-workers found that, unlike in APOE4 carriers, the BBB was intact in cognitively healthy APOE3 carriers. It was, however, leaky in APOE3 carriers who showed cognitive impairment — although less so than in APOE4 carriers at an equivalent stage of impairment.   Next, Montagne et al. examined whether BBB breakdown in APOE4 carriers was linked to pericyte degeneration. In support of this idea, they found that a biomarker of pericyte injury — a soluble form of a protein known as platelet-derived growth factor-receptor-β (sPDGFRβ) — was elevated in the cerebrospinal fluid of APOE4 carriers compared with APOE3 carriers. High levels of the protein in people who carried APOE4 were associated with a leaky BBB and cognitive impairment. sPDGFRβ elevation was independent of Aβ and tau. The authors then looked for insight into the mechanisms by which pericytes might become injured. They focused on cyclophilin A (CypA) and matrix metalloproteinase-9 (MMP9), two proteins that are part of an inflammatory pathway implicated in APOE4-driven pericyte damage and BBB breakdown6. Levels of CypA and MMP9 in the cerebrospinal fluid were higher in APOE4 carriers who had mild cognitive impairment than in cognitively healthy APOE4 carriers or APOE3 carriers who had comparable cognitive dysfunction. Again, this change was not related to increases in Aβ or tau. Finally, the researchers generated pericytes in vitro from human induced pluripotent stem cells that expressed APOE3 or APOE4. They found that APOE4-expressing pericytes secreted substantially more CypA and MMP9 than did APOE3 pericytes. ApoE4 (but not ApoE3) secreted by pericytes activates the CypA–MMP9 pathway on nearby pericytes — the cells therefore cause their own demise. ApoE4 could also activate the CypA–MMP9 pathway in endothelial cells, which are susceptible to the harmful effects of APOE47. Therefore, injury to pericytes and endothelial cells might both cause BBB leakage (Fig. 1). Figure 1 | The gene variant APOE4 and Alzheimer’s disease. People who carry APOE4 are at heightened risk of Alzheimer’s disease. Montagne et al.3 provide evidence that ApoE4 protein is secreted by cells called pericytes, which abut endothelial cells that line cerebral capillaries at the blood–brain barrier (BBB). Secreted ApoE4 activates the protein cyclophilin A (CypA) in the pericytes. This triggers a downstream signalling pathway involving activation of the inflammatory protein matrix metalloproteinase-9 (MMP9) in pericytes, and possibly also in endothelial cells. This causes disruption of junctions between adjoining endothelial cells, opening the BBB in brain regions involved in learning and memory. Disruption of the BBB is associated with impaired cognition, although the mechanisms that link the two are unclear. These observations cast new light on APOE4 that runs contrary to the widely held idea that this gene variant contributes to Alzheimer’s disease solely by promoting Aβ and tau accumulation4. Instead, it seems that BBB dysfunction might explain why APOE4 carriers are susceptible to Alzheimer’s disease. The authors’ findings might also explain why APOE4 carriers have worse outcomes following stroke or traumatic brain injury8 than do people who carry other APOE variants. However, as Alzheimer’s disease progresses, APOE4 could also slow Aβ and tau clearance, exacerbating declines in cognition. Even more striking is the finding that early drivers of cognitive impairment differ between APOE4 and APOE3 carriers. Montagne and colleagues’ findings indicate that activation of the CypA pathway and pericyte damage might not be involved in cognitive impairment in people who carry the most common APOE variant, APOE3. But whether a leaky BBB caused by factors that are independent of pericytes (for example, damage to endothelial cells caused by Aβ1) contributes to cognitive impairment in APOE3 carriers remains unclear. The role of the BBB in APOE2 carriers, which was not assessed in the current study, also remains unknown. Although APOE2 is associated with a reduced risk of Alzheimer’s disease compared with other APOE variants, this is unlikely to result from a more resilient BBB, because APOE2 carriers have an increased risk of microhaemorrhages, suggesting vascular frailty4. Whether and how BBB breakdown leads to cognitive impairment also remains to be determined. Is it a cause or a consequence of the disease process? Evidence from mice indicates that some proteins in the blood, such as fibrinogen, damage the synaptic connections between neurons9. But a pathogenic role for these proteins in the human brain has not yet been demonstrated. Irrespective of these questions, Montagne et al. have broadened our understanding of how APOE4 promotes cognitive impairment. They have also demonstrated that different APOE statuses can promote disease through different mechanisms. A deeper appreciation of how gene variants shape Alzheimer’s disease might prove crucial for more-personalized approaches to treating this prevalent and incurable disease. </body>
<date id = '21'>29 April 2020</date>
<url id = '22'>https://nature.com/articles/d41586-020-01167-1</url>
<title id = '22'>The addition of a methyl group to a drug molecule can greatly alter the drug’s pharmacological properties. A catalyst has been developed that enables this ‘magic methyl effect’ to be rapidly explored for drug discovery.</title>
<body id = '22'>Developing a small-molecule drug requires iterations of building and testing new compounds to find one that strikes the right balance of pharmacological properties. The process typically takes more than 10 years and costs billions of dollars, because, for every 5,000 compounds made and tested, only one will become an approved drug1,2. Indeed, a high-school basketball player is twice as likely to end up playing in the US professional league as any single compound tested in a drug-discovery programme is to become a marketed drug (see go.nature.com/2v8pnfm). One approach to accelerating drug discovery is late-stage functionalization, in which previously prepared test compounds are decorated with new atoms in the hope of favourably adjusting their pharmacological properties. Writing in Nature, Feng et al.3 report an outstanding advance towards this long-standing and historically challenging strategy.   Introducing just one cluster of atoms (a functional group) into a drug molecule can drastically alter the molecule’s properties. For instance, adding a methyl group (CH3, one of the smallest functional groups) can enhance a compound’s binding affinity for its biological target more than 1,000-fold, a phenomenon termed4 the ‘magic methyl effect’. This is because the installation of a methyl group alters the shape of the molecule such that it can readily nestle inside a targeted protein’s active site, akin to how an ergonomic computer mouse fits snugly in the palm of your hand. However, making even small adjustments to molecules is frequently a major undertaking, one that effectively requires chemists to break apart the entire structure and reassemble a dozen or more smaller pieces for each change. Imagine how much time and money it would cost if adding a new window to your home required the entire house to be taken apart and rebuilt from scratch. Chemists working in drug discovery regularly have to do this with their molecules. Late-stage functionalization has therefore emerged as a desirable approach to accelerate drug discovery5,6: much as a construction crew saws through existing walls to insert new windows, chemists aspire to cut through existing chemical bonds to insert new functional groups into molecules. C–H functionalization, a type of reaction that converts ubiquitous carbon–hydrogen (C–H) bonds in complex molecules into alternative functional groups, has garnered much attention for this purpose. Feng et al. report a substantial advance in this area with the design of a metal catalyst that cuts through specific C–H bonds to insert methyl groups, thus allowing the magic methyl effect to be explored in myriad complex and drug-like compounds.   Selective late-stage C–H functionalization is constantly used in nature. For example, iron-based metalloenzymes known as cytochrome P450s (CYP450s) are omnipresent throughout the animal kingdom because of their crucial role in regulating metabolism7,8. The iron atom of a CYP450 binds to biologically active molecules and triggers their metabolism by inserting oxygen into C–H bonds to form double carbon–oxygen (C=O) bonds, a type of C–H functionalization. The elaborate enzyme architecture around the iron centre tames the metal’s otherwise rampant catalytic activity, thus allowing these reactions to proceed precisely and specifically, such that only those substrates that fit in the enzyme’s pocket are oxidized. Feng and colleagues are part of a research group that has long been interested in making ligand molecules that mimic the CYP450-enzyme architecture, in the hope of broadening the ability of iron complexes to transform C–H bonds into C=O bonds in diverse substrates, using hydrogen peroxide as the source of oxygen9. Scientists from that group had previously made great strides in taming the reactivity of iron complexes for C–H functionalization, but even the best catalysts proved promiscuous (they reacted at many different C–H bonds, rather than at just one) and could not be used in the presence of many functional groups commonly found in drug-like molecules. The same research group had therefore also investigated manganese — iron’s less-oxidizing neighbour in the periodic table — as an alternative metal centre for catalysts that oxidize specific C–H bonds in complex molecules10. Feng et al. hypothesized that a less-oxidizing manganese catalyst would target the C–H bonds that are most easily metabolized on drug-like molecules. Moreover, they thought that the oxidation reaction could be halted midway to produce a hemi-oxidized intermediate, into which a methyl group could be inserted (Fig. 1). This group would essentially block the molecule’s metabolic degradation, invoking the magic methyl effect. Figure 1 | Late-stage methylation of biologically relevant targets. Feng et al.3 report that a highly tuned manganese (Mn) catalyst enables methyl (CH3) groups to be incorporated at specific sites into complex molecules, particularly those that have structures typical of drugs. The manganese catalyst inserts an oxygen atom from hydrogen peroxide into the carbon–hydrogen bond that the human body can most easily metabolize, yielding a reactive hemi-oxidized intermediate (square brackets indicate that the intermediate is formed in situ and is not isolated) that is poised for reaction with a methyl-group source. The reaction was used successfully on 38 biologically active molecules, including the antidepressant citalopram. It could therefore be used to rapidly explore the magic methyl effect — a phenomenon in which the addition of a methyl group to a drug molecule greatly enhances the molecule’s pharmacological properties. The challenge with this approach is that the hemi-oxidized intermediate is more readily oxidized than is the starting material — so, if the oxidation reaction were a train, it would be a non-stop service to a C=O bond. To circumvent this complication, Feng et al. tuned the reaction conditions to contain the precise amount of catalyst and hydrogen peroxide needed to deliver the hemi-oxidized intermediate, effectively pulling the train into a station en route to the C=O terminus. The resulting hemi-oxidized species can then be seamlessly transformed into a methyl group under a variety of conditions, depending on the functional groups present in the rest of the molecule. Feng and colleagues’ work is a superb example of a symbiotic collaboration between academia and the pharmaceutical industry, with cutting-edge chemistry being used to solve real-world problems. The industrial influence is evident throughout the work: the molecules selected to demonstrate this methodology accurately reflect the types frequently encountered in drug development. More specifically, the authors report that 38 biologically relevant targets (drugs, natural products, peptides and steroids) and their building blocks undergo the new reactions with excellent selectivity and functional-group tolerance. For more than a century, drug discovery focused mainly on small molecules. However, the field is now turning to more-elaborate molecules, such as peptides, which can potentially target complex biological targets with high specificity. Peptides are usually stitched together from amino acids in a linear sequence of reactions. The functional groups that provide the structural diversity of peptides are built into the amino acids, and are therefore introduced at each step of the sequence. Some of the groups in a target peptide are inevitably installed in the first step, and can be changed only by running the whole sequence again, but using a different amino acid at the start. Feng et al. upend this norm by demonstrating that methyl groups can be installed on a tetrapeptide (a peptide built from four amino acids) at the end of the synthetic sequence. Further extension of this chemistry to more-complex linear and macrocyclic (ring-forming) peptides would be game-changing for drug discovery. Continued breakthroughs on complex catalytic processes in the spirit of Feng and colleagues’ work might finally enable medicinal chemistry to cruise at the same speed as biological research. </body>
<date id = '22'>29 April 2020</date>
<url id = '23'>https://nature.com/articles/d41586-020-01168-0</url>
<title id = '23'>Elucidating how the brain controls peripheral organs in the fight against infection is crucial for our understanding of brain–body interactions. A study in mice reveals one such pathway worthy of further investigation.</title>
<body id = '23'>Interactions between the mind and the body have sparked the interest of scientists and philosophers for centuries. In ancient Greece, the physician Galen described the spleen as being the source of black bile, which was thought to cause melancholy when secreted in excess. Today, research is uncovering complex ways in which the brain and body interact to affect diverse aspects of health, from mood to immune function. The spleen aids immune defences by functioning as part of the lymphatic system; the organ is a major hub of activities needed to initiate responses in the adaptive branch of the immune system, which handles defences that are tailored to a specific disease-causing agent. The spleen is a target of top-down control from the brain1. Writing in Nature, Zhang et al.2 have taken our understanding of brain–spleen connections to the next level by revealing an aspect of top-down control that regulates the adaptive immune system. The spleen’s contribution to immune responses occurs mainly in its white-pulp region, where immune cells that have arrived from elsewhere in the body present peptide fragments called antigens to immune cells called T cells. If a T cell binds to and recognizes such an antigen, which might indicate the presence of an abnormal cell or a foreign invader, this activates the T cell, which in turn activates immune cells called B cells. B cells differentiate to form plasma cells (Fig. 1) that secrete antibodies specific for the antigen presented, and these antibodies are released into the bloodstream to fight infection3. Figure 1 | Brain control of antibody production. Zhang et al.2 describe a circuit between the brain and the spleen that aids immune defences. The authors injected animals with an antigen (a peptide fragment) that can be recognized by immune cells. Placing the animal on a high platform activated neurons that produce the molecule corticotropin. These neurons are located in brain regions that respond to stress, called the central amygdala (CeA) and the paraventricular nucleus (PVN) of the hypothalamus. A cellular circuit connects these activated neurons to the splenic nerve and drives it to release the molecule noradrenaline. An immune cell termed a CD4+ T cell is activated when its T-cell receptor (TCR) binds to antigen. When such a cell encounters the noradrenaline released in the spleen (which binds to what is termed an adrenergic receptor), this leads the T cell to secrete the molecule acetylcholine6. This molecule binds to a nicotinic receptor on an immune cell called a B cell, causing it to differentiate into a plasma cell. The plasma cell boosts immune defences by making antibodies that recognize the specific antigen that activated the T cell. Spleen activity is controlled by the autonomic nervous system — a part of the nervous system that regulates organs. More specifically, the spleen is controlled mainly by the sympathetic branch of the autonomic nervous system, which is associated with the ‘fight-or-flight’ response4. However, little was known previously about possible upstream brain regions that might connect to the autonomic nervous system in the spleen to control it and, by extension, adaptive immunity. An earlier study in mice5 revealed that stimulation of a brain region called the ventral tegmental area, a part of the brain’s reward circuit, boosts immune responses and protection against harmful bacteria.   Zhang and colleagues developed a surgical technique to remove nerves from the spleen in mice. This mainly removed inputs from the autonomic nervous system and prevented top-down control from the brain to the spleen. After surgery, the animals were injected with an antigen. Plasma cells that made antibodies targeting that antigen arose in abundance in control mice that had undergone a ‘sham’ operation that did not remove nerves. Such an increase did not occur in the denervated mice, indicating that splenic-nerve activity regulates the formation of plasma cells and thus adaptive immunity. The authors investigated which molecular mechanisms might be needed for plasma-cell formation in this context. They studied the expression of various types of receptor that can bind the neurotransmitter molecule acetylcholine, which is a key signalling component of the autonomic nervous system. Zhang et al. report that B cells express a type of acetylcholine receptor called a nicotinic receptor, and the authors pinpointed protein subunits of this receptor, including one called Chrna9. To test the role of nicotinic receptors containing Chrna9 in plasma-cell formation, Zhang et al. transplanted haematopoietic stem cells, which can generate immune cells, into mice that had undergone a treatment to remove their own haematopoietic stem cells. When the transplanted stem cells came from mice engineered to lack the gene encoding Chrna9, these animals generated fewer plasma cells after an injection of antigens than did animals that received antigen injections and transplants of stem cells with the gene intact. This result indicates that plasma-cell formation requires the presence of nicotinic receptors. When a type of T cell called a CD4+ T cell is activated by antigen recognition, it secretes acetylcholine in response to the hormone noradrenaline6. The authors reveal that such T cells serve as a ‘relay’ between the release of noradrenaline from the splenic nerve and the subsequent acetylcholine-dependent6 formation of plasma cells (Fig. 1). To map the neural circuit that connects the spleen and brain, the authors used a method termed retrograde tracing, which relies on monitoring the expression of a fluorescent protein encoded by a virus that can ‘jump’ across the synapses that connect neurons. This enabled Zhang and colleagues to track all upstream inputs to a given nerve cell in the spleen. The authors thereby identified two key brain regions (the central nucleus of the amygdala and the paraventricular nucleus of the hypothalamus) that contain neurons that connect to splenic nerves. These regions are major centres involved in the response to psychological stressors such as fear or threatening situations7, and they have essential roles in regulating the production of neuroendocrine hormones, for example, by a pathway called the hypothalamic-pituitary-adrenal axis8.   One population of nerve cells in these two regions releases the hormone corticotropin, which is thought to have a key role in initiating the body’s response to stress9. To determine whether corticotropin-producing neurons affect the spleen, Zhang et al. stimulated these neurons using a technique called optogenetics, and assessed whether this affected the activation of splenic nerves by monitoring their firing using electrophysiological recording. This provided crucial functional evidence for a brain–spleen connection, because such stimulation increased the firing of splenic-nerve cells. The authors also report that the inhibition or ablation of corticotropin-producing neurons in either of the two brain regions impaired the formation of plasma cells after antigen injection. Conversely, activation of the neurons stimulated such plasma-cell formation. Although these circuit-based experimental approaches provide key proof for the existence of the brain–spleen axis, the authors also needed to test their model using suitable interventions that activate the ‘stress centres’ in the brain. However, neurons in the central nucleus of the amygdala and the paraventricular nucleus function in a pathway that causes the adrenal gland to secrete the hormone glucocorticoid in response to stress, and glucocorticoids are potentially immunosuppressive10. The authors therefore considered whether the concentration of glucocorticoids secreted by the adrenal gland might depend on the severity of the stress. To avoid possible glucocorticoid-driven immunosuppression that might interfere with their analysis of antibody production, Zhang et al. studied mice that had been placed on an elevated, transparent platform; this provided a behavioural situation that induced only moderate stress. Following antigen injection, this scenario, but not another set-up that caused more-severe stress, led to the generation of antigen-specific antibodies. The authors showed that this antibody production depends on corticotropin-producing neurons in the brain circuit that they had described. There is growing evidence that dysregulation of the immune system has a bottom-up role in promoting several behaviours relevant to neuropsychiatric disorders11. Zhang and colleagues’ study provides insights in the other direction — how the brain exerts top-down control of immune-system function. Future research will be needed to investigate whether this particular brain–spleen circuit exists in humans. The authors’ work opens up the exciting possibility that activating certain brain regions (through behavioural interventions or by selective stimulation using neuromodulatory techniques such as transcranial magnetic stimulation) could modulate the immune system. To return to Galen, he was right that the spleen is a key site of connection between the brain and the body, but his ideas about how the spleen induces melancholy now give way to this new perspective on how the mind might modulate resilience-promoting antibodies. </body>
<date id = '23'>29 April 2020</date>
<url id = '24'>https://nature.com/articles/d41586-020-01081-6</url>
<title id = '24'>A study of cancer-associated mutations in normal endometrial glands of the uterus has now been performed using whole-genome sequencing. The analysis sheds light on the early changes that lead to invasive disease.</title>
<body id = '24'>Understanding how normal tissues give rise to cancer is crucial for improving prevention and early detection of this deadly disease. Over the past two decades, the genomic profiles of most types of invasive cancer have been catalogued; however, similar profiling of normal tissues presents a unique set of challenges. Cancer tissues are often abundantly available from biopsies or surgery, but samples from normal tissues tend to be much smaller, and specimen-collection practices are less well established, making it hard to gather high-quality material. Writing in Nature, Moore et al.1 overcome these challenges and successfully catalogue cancer-driving mutations in normal endometrial glands.   Endometrial glands are abundant in the lining of the uterus, where they secrete hormones and other substances that are essential for normal menstruation and embryonic development. Endometrial cancer is the sixth most common cancer in women worldwide, with more than 382,000 cases annually2. The mortality rate has increased over the past decade3, heightening the need for prevention and early detection of this disease. Moore et al. obtained 257 normal endometrial glands from 28 women of various ages. In each case, the authors meticulously isolated the glands using a technique called laser-capture microdissection to separate the epithelial tissue, which lines the gland, from the surrounding stromal cells that make up the gland’s connective tissue. They then performed whole-genome sequencing of the epithelial samples and various other normal tissues from the same women, using a protocol they had developed that is tailored to the analysis of small amounts of DNA. The group analysed these sequences to identify mutations that are unique to the normal endometrial glands, as well as endometrium-specific changes in the number of copies of any genetic region (caused by duplication or deletion of DNA). They found that, in almost 90% of individuals, the normal endometrial tissue contained driver mutations — which give cells a selective advantage over non-mutated counterparts, and so are thought to promote cancer development. Nearly 60% of the endometrial glands in these women contained one or more drivers. The authors found 12 genes that contained driver mutations with statistically increased prevalence in normal endometrial tissue compared with that in other tissues. These genes are all known to be frequently mutated in cancer, and, collectively, these mutations have the potential to affect many cellular processes. However, isolated mutations in the individual genes, as was typically the case in Moore and colleagues’ samples, are probably insufficient to make a tissue become cancerous4.   A remarkable finding is that each endometrial gland seems to be clonal — that is, all the cells in the gland are derived from a single epithelial progenitor cell. It might be expected that each gland could develop multiple independent mutations, but the authors’ discovery of clonality indicates that there is instead a uniformity to the mutational process. As would be expected, the number of mutations increased with age, at the rate of about 29 nucleotide substitutions per gland per year during adult life. Moore et al. reconstructed the phylogeny (the evolutionary development and diversification) of individual glands to document the initial presentation and spread of driver mutations through the tissue over time. They report that many glands that were located in close physical proximity in the uterine wall displayed distant phylogeny. This suggests that the cellular populations in each gland remain genetically isolated, providing many separate opportunities for cancer to develop. The researchers also provide evidence that driver mutations can arise at any time, occurring in some women in their first decade of life and in others at various stages of adulthood. This insight is important because the typical timeline between developing driver mutations and cancer is not yet well defined. The group’s rigorous methods for sample isolation and sequencing, coupled with their well-developed bioinformatics algorithms, mean that the results of this study should be highly reliable and reproducible. But one caveat is that the authors isolated endometrial glands from a select population of women: most samples were obtained from people undergoing evaluation for infertility, from organ donors, or from women who had died of non-gynaecological causes. Both infertility and nulliparity (having never given birth) are known independent risk factors for endometrial cancer5. And samples collected from women who had died of non-gynaecological causes might be more likely than the average endometrial gland to contain low-risk driver mutations that have less potential to trigger cancer, given that these women died without having developed endometrial cancer.   Future studies would benefit from a more-representative cross-sectional population. The inclusion of women who have conditions that are well-known precursors to cancer, such as atypical endometrial hyperplasia (in which the lining of the uterus becomes abnormally thick) could help in this regard. Researchers might then be able to define a robust landscape of changes that occur during the progression from normal to precancerous tissue to invasive disease. This approach might also help to define the pathogenicity of, and possible necessity for, individual driver mutations that lead to the development of cancer. Another caveat is the discrepancy between driver mutations identified by Moore et al. and those from other cancer-genome projects, including The Cancer Genome Atlas6. Although the most frequently mutated genes identified in the current study have been previously reported in endometrial cancers, several of the most commonly mutated genes in this cancer are notably not mutated in Moore and colleagues’ samples. The group found mutations in these well-known drivers in less than 2% of the normal endometrial glands that they studied — a surprisingly low frequency, because one would expect that the drivers present in all cancer cells would be the first to arise in normal tissue. This discrepancy probably hints at unknown aspects of the multistep process of tumour initiation, in which certain mutations must arise before others. Determining when and how gatekeeper mutations occur and permit the next step in tumour development will require further analyses of benign, premalignant and invasive tissues. Knowing that the compilation of driver mutations in normal endometrial glands is different from those found in established endometrial cancers might change the approach for further research into the prevention and early detection of this disease. Determining the role of these mutations in concert with other known risk factors, such as nulliparity, obesity, race and genetic predisposition, will help to better identify women who are at risk of endometrial cancer. Even before we obtain this information, Moore and colleagues’ findings should be useful for ongoing research to detect endometrial cancer at early stages, which includes analyses of cell-free DNA circulating in blood, tampon-based collection of vaginal secretions and liquid-based examination of cervical tissues7–9. More broadly, a better overall understanding of the normal mutational spectra in tissues will refine our knowledge of the consequences of specific cancer drivers for many solid tumours. </body>
<date id = '24'>22 April 2020</date>
<url id = '25'>https://nature.com/articles/d41586-020-01091-4</url>
<title id = '25'>Early 2018 saw unusually heavy rainfall in Hawaii. Modelling now suggests that groundwater pressure increased owing to rainfall: this might have triggered changes in the eruption of the island’s Kīlauea volcano.</title>
<body id = '25'>Figure 1 | Lava from the lower east rift zone of KĪlauea volcano. Farquharson and Amelung2 propose that exceptionally heavy rainfall led to the eruption of magma from this part of the volcano in 2018.Credit: Alamy The most recent eruption of Kīlauea volcano on the island of Hawaii began in 1983. For 35 years, most of its magma emerged from a set of fissures in the volcano called the upper east rift zone. But on 3 May 2018, Kīlauea’s lower east rift zone opened up, giving way to a massive outpouring of lava that devastated the southeastern part of the island1 (Fig. 1). An important question is why this change occurred in May 2018, rather than earlier or later in the course of the eruption. Writing in Nature, Farquharson and Amelung2 propose that record-breaking levels of rainfall in early 2018 increased groundwater pressures which, in turn, made it easier for rock to break and hence magma to rise to the surface at new locations. The creation of a pathway that brings magma to Earth’s surface begins with the mechanical failure of rocks. This failure can occur in two ways: new cracks can open, or existing faults can slip. Both processes can be promoted by pressure changes in groundwater. For the former, increases in fluid pressure decrease the amount of stress needed to open new cracks. For the latter, faults can slip when the stresses acting parallel to the fault (shear stresses) overcome those perpendicular to the fault (normal stresses). These normal stresses act to clamp the fault shut. Increasing fluid pressure in rocks lowers normal stresses without changing shear stresses, thus promoting fault failure.   Heavy rainfall increases water levels underground and thus pressure in groundwater. The volcanic rocks in Hawaii are very permeable, which allows water to infiltrate and pressure changes to propagate to a depth of several kilometres, close to where magma is stored. Fluid-pressure changes take time to propagate from the surface to those depths. Thus, downward migration of rock failure over time, along with a time lag between the accumulation of water at the surface and failure at depth3, would be key indicators that rainfall was the cause of rock failure at Kīlauea. Farquharson and Amelung modelled pressure changes at Kīlauea caused by rainfall in the months leading up to the eruption on 3 May 2018. Their model showed an increase in pressure of tens to hundreds of pascals at depths of several kilometres. On the basis of these changes, along with four sets of observations indicating that eruptions at Kīlauea are associated with patterns of substantial rainfall, the authors propose that heavy rainfall promoted the rock failure that enabled magma to flow into the lower east rift zone. Is their hypothesis plausible? The pressure changes computed by their models are small — smaller than stresses from tides. However, if rocks are already close to breaking, such changes might be sufficient to initiate failure. The 2018 eruption was accompanied by a magnitude-6.9 earthquake, and examples of earthquakes caused by pressure changes on this scale are abundant4. For example, the widespread increase in earthquake frequency in the central and eastern United States in the past decade results from wastewater injection into permeable rocks that increases water pressure and changes stresses5.   The geological record also confirms that changes in stresses at Earth’s surface can modulate volcanic activity. On land, volcanism is promoted by the retreat of glaciers6. Sea-level changes between glacial and interglacial periods can modulate eruption rates at mid-ocean ridges7. Stresses from large earthquakes increase the probability of volcanic eruptions8 and can change activity at volcanoes that are already active9. Although it is well established that changes in water pressure promote earthquakes, they are not necessarily a direct cause of magma eruption. To begin moving through Earth’s crust, magma must create large enough stresses in the surrounding rocks to open a pathway. Earthquakes triggered in the crust around that stored magma, however, can actually relieve stress — as such, they might make it more difficult for magma to erupt10. Ultimately, whether fault failure from water-pressure changes can occur close to stored magma, as hypothesized by Farquharson and Amelung, remains uncertain. The first magma to erupt from the lower east rift zone in 2018 was old, perhaps left over from an earlier, 1955 eruption11, implying that the rift zone was already hot. As a result, groundwater in the rift zone might have been vapour at shallow depths12, and at greater depths it could have been a supercritical fluid (a substance that is not in a distinct liquid or gas phase, but has properties of both). The high compressibility of both vapours and supercritical fluids would dampen the magnitude of pressure changes in the authors’ model, making failure less probable.   How, then, can we test the hypothesis that rainfall initiated the lower east rift zone eruption? Unfortunately, subsurface pressure measurements — and hydrogeological data more generally — are rarely part of volcano monitoring. Instead, as with many geoscience and Earth-history questions, we have to look back in time using the geological and historical record of eruptions. In support of their hypothesis, Farquharson and Amelung analysed all reported eruptions at Kīlauea since 1790, and showed that the volcano tends to erupt at the wettest time of year. Should we increase alert levels at volcanoes after heavy rainfall? We could ask the same question about other stress changes, such as those from regional earthquakes. This is an open question. These stress changes are small, and hence, if anything, modulate the exact timing of the surface eruption. At Kīlauea, there were other sources of stress — in fact, a change in eruption behaviour had been anticipated on the basis of ground-deformation measurements and inferred magma movement. The Hawaiian Volcano Observatory issued a warning on 17 April that a new vent might open1. The possibility that external processes initiate volcanic eruptions is a reminder that volcanoes are part of a dynamic Earth system. Volcanic eruptions influence all surface environments, including climate and weather13. Changes in those surface environments, such as heavy rainfall, might also influence eruptions. We are only just beginning to understand these interactions. </body>
<date id = '25'>22 April 2020</date>
<url id = '26'>https://nature.com/articles/d41586-020-01103-3</url>
<title id = '26'>Pancreatic cancer does not respond to certain anticancer treatments that boost immune responses. A mechanism active in tumour cells that contributes to this evasion of immune targeting has been uncovered.</title>
<body id = '26'>A consistent hallmark of pancreatic cancer is the inability to treat it with immunotherapy — an approach that harnesses the body’s immune response to target a cancer. Writing in Nature, Yamamoto et al.1 reveal a mechanism that enables pancreatic cancer cells to evade an immune response. The process implicated is usually associated with the normal degradation and recycling of cellular proteins. The authors find that inhibition of this pathway, using drugs or by genetic approaches, reverses this immune evasion in animal models of pancreatic cancer. The finding provides a compelling rationale for investigating whether targeting this pathway might be of benefit in the clinic.   The past decade has seen striking advances in the use of immunotherapy to treat numerous solid cancers (those not formed from blood cells) that had not responded to earlier therapy attempts. One such advance is the development of what is known as checkpoint blockade therapy. This targets the proteins PD-1 and CTLA-4, which are found on the surface of immune cells called cytotoxic T cells (also called CD8 T cells) and inhibit an immune response. The administration of two antibodies that, respectively, target these two inhibitory proteins is used widely to drive an antitumour immune response2. Unfortunately, treatment of pancreatic cancer with either antibody, or the two together, has not led to any notable success in terms of patient survival3. There are many factors that can contribute to the failure of immunotherapy. The tumour microenvironment of pancreatic cancer contains a variety of immune cells that suppress the function of cytotoxic T cells — they include myeloid cells, tumour-associated macrophages and regulatory B cells4. Moreover, signals released by the cancer cells themselves (often mediated by the aberrant activation of tumour-promoting genes such as those encoding the proteins Ras and Myc) have a key role in creating this profoundly immunosuppressive environment5. In addition, fibroblast cells in the tumour microenvironment secrete material that generates a physical barrier hindering the influx of T cells to the tumour site6. Another equally crucial cause of immune evasion is that the cancer cells themselves undergo changes. An immune response is triggered when a T cell recognizes as foreign a peptide fragment (called an antigen) that is ‘presented’ on the surface of a cancer cell, bound to a molecule called a major histocompatibility complex (MHC) class I molecule (Fig. 1). An inability of the MHC molecules to present tumour antigens has emerged7 as an explanation for how cancer cells can hide from the immune system. These molecules comprise an invariant β2-microglobulin protein (encoded by the B2M gene) and a protein encoded by HLA genes, which vary in the antigen-binding site they encode, thereby enabling different antigens to be bound by different MHC molecules. Tumour-derived-antigen presentation by MHC class I molecules is required for cancer cells to be recognized by the T-cell receptor on cytotoxic T cells. Figure 1 | Boosting the targeting of pancreatic cancer by the immune system. The ability of immune cells called cytotoxic T cells to attack tumours is thwarted by, for example, the action of inhibitory proteins called CTLA-4 and PD-1. Yamamoto et al.1 report a previously unsuspected mechanism that enables pancreatic cancer to evade immune cells. a, The T-cell receptor (TCR) on the surface of cytotoxic T cells enables them to recognize tumour cells, and this recognition depends on the presence of major histocompatibility complex (MHC) class I molecules on the surface of cancer cells. Yamamoto and colleagues report that MHC class I molecules in pancreatic cancer cells are destroyed by a process termed autophagy. This begins when the molecule binds to the protein NBR1 and is enveloped in a membrane to form an organelle called an autolysosome. The destruction of MHC class I molecules thus prevents them from ‘presenting’ peptide fragments (antigens) from the tumour that might be recognized by the TCR. b, In mouse models of pancreatic cancer, treatment combining an autophagy-inhibiting drug (chloroquine) and antibodies targeting PD-1 and CTLA-4 (termed checkpoint blockade therapy) provoked a robust immune response against the tumour compared, with the case for animals that did not receive such treatment. The inability to present antigens, and thus the absence of immune recognition, can occur through alterations in the genes encoding proteins needed for antigen presentation, such as B2M and HLA genes8,9. In pancreatic cancer, such genetic alterations are relatively uncommon, occurring in no more than 1% of cases10. However, lower-than-normal levels of MHC class I molecules, or their complete loss, occurs in more than 60% of cancers that arise in the pancreas11, and the decrease might be even greater in metastatic tumours (those that have spread beyond the pancreas)7. Until now, the mechanisms underlying the regulation of MHC class I molecules in pancreatic cancer have remained elusive. Yamamoto and colleagues reveal that a cellular pathway called autophagy is the means by which pancreatic cancer cells limit the amount of MHC class I molecules on their surface, thereby hindering antigen presentation. Autophagy is an essential cellular degradation pathway that recycles organelles and proteins to maintain cellular ‘fitness’12. It can act selectively through the binding of a specific receptor to a ‘cargo’ (for example, a protein or organelle) that has been targeted for destruction by being marked with a tag, such as the protein ubiquitin. A complex of receptor and cargo is enveloped in a lipid membrane to form a vesicle called an autophagosome, which fuses with an organelle known as a lysosome to form an autolysosome (Fig. 1). The cargo then undergoes enzyme-mediated digestion in the autolysosome and its contents are recycled for use in the cell.   Yamamoto et al. report that, remarkably, in pancreatic cancer cells, most MHC class I molecules do not exist on the cell surface, but instead are found in autophagosomes and autolysosomes. The authors identified an autophagy-associated receptor called NBR1 as being responsible for targeting MHC class I molecules to the autophagy machinery. Furthermore, they found that if autophagy was inhibited in mice, by drugs such as chloroquine or through genetic engineering, this restored the surface expression of MHC class I molecules, thereby enhancing antigen presentation. In mouse models of pancreatic cancer, autophagy inhibition resulted in an influx of cytotoxic T cells to the tumour microenvironment, and if the animals also received checkpoint blockade therapy, a robust antitumour immune response was generated. Increased autophagy has been known for around a decade13 to be a metabolic requirement for pancreatic cancer, but only now has a connection been made to the immune evasion of tumour cells. Thus far, clinical trials targeting autophagy in pancreatic cancer have relied on testing the antimalarial drug hydroxychloroquine (chloroquine and hydroxychloroquine are related molecules), which blocks one of the final steps in autophagy. Other drug candidates, directed at earlier components of the autophagy machinery, are in the pipeline. Early trials of hydroxychloroquine demonstrated only modest results, but there has been a resurgence of interest in combinatorial treatment approaches after evidence from animal models that, if signalling mediated downstream of mutant Ras by the enzyme MAP kinase is inhibited, pancreatic cancer cells become strongly dependent on autophagy for their survival14,15. Yamamoto and colleagues’ work will almost certainly lead to further additions to the compendium of autophagy-targeted clinical trials of pancreatic cancer treatments. Discoveries in the fields of autophagy and immunotherapy were, respectively, recognized by the Nobel Prize in Physiology or Medicine in 2016 and 2018. This new finding represents an unprecedented opportunity for the convergence of these two areas of study, in efforts to improve therapies for pancreatic cancer. </body>
<date id = '26'>22 April 2020</date>
<url id = '27'>https://nature.com/articles/d41586-020-01099-w</url>
<title id = '27'>The development of low-power methods for controlling a property of electrons known as spin could help to maintain the historic rates of progress that are occurring in computational power. Just such a method has now been reported.</title>
<body id = '27'>A promising technology for the next generation of computers is spintronics, a type of electronics that depends on the spin — the intrinsic angular momentum — of electrons, rather than their charge. However, available methods for controlling spin require electric currents that are too large for practical applications. In a paper in Nature, Noël et al.1 report an approach that allows low-power spin control using an electric field.   The exponential progress in increasing computational power over the past 50 years has been largely driven by the relentless miniaturization of the field-effect transistor2, the basic component of silicon chips. This consistent downscaling was anticipated3 in 1965 by electronic engineer Gordon Moore, and has led to the staggering 2 billion transistors that are now typically found in the processors of modern personal computers. The semiconductor industry has come up with a road map outlining the technological developments in computer materials, devices and systems that will be needed to maintain these historic rates of increase in computational power (https://irds.ieee.org). A growing section of the road map addresses a pressing problem for the field: transistors based on currently used technology cannot be scaled down much further, because the physical limits of miniaturization will soon be reached. There are no known solutions for several of the technical and materials issues associated with this problem. Materials scientists, physicists and engineers are therefore investigating an array of potential new working principles for computer technology. The development of new approaches also allows other goals to be targeted, such as lowering energy consumption, or incorporating multiple functionalities into components to speed up data processing. One way of reducing power consumption would be to eliminate the need for a continuous power supply to maintain the logic state (ON or OFF) of transistors. This can be achieved using ferroic materials (such as ferroelectric compounds, which have a permanent electric polarization) or piezoelectric mechanical devices, which require power to switch between the logic states, but not to retain those states4. Spintronics technology has also seen a surge of interest, because this approach is expected to reduce electrical dissipation5 — wasteful loss of electrical power as heat. Combinations of ferroic approaches with spintronics6 could be particularly effective in the race to develop more-efficient computing technology.   However, many of these approaches will require new materials — for example, the semiconductors used in conventional electronic devices do not have ferroic properties. A family of compounds known as complex oxides are of particular interest, because they host permanent electric and magnetic dipoles, thereby opening the door to applications that require permanent states. Although complex oxides are not as good as semiconductors for use in classical transistors because they produce more electrical dissipation, they have remarkable properties for spintronics7. Interesting electronic phases have been observed to form at the interfaces between two complex oxides. Noël and co-workers focus on a phase called an electron gas: an ultrathin (a few nanometres thick) layer of conducting electrons that forms at the surface of strontium titanate (STO) that has been covered by a layer of aluminium. STO probably provides the best illustration of the complexity of the electrical properties of transition-metal oxides. In its pure form, it is a dielectric material (an electrical insulator) that has a tendency to become ferroelectric at temperatures below 4 kelvin, but fails to do so because of quantum fluctuations8. However, tweaks to the chemistry of STO (such as the replacement of some of the strontium atoms by calcium atoms) can push the compound over the edge to become truly ferroelectric9. And the replacement of some of the strontium atoms by lanthanum atoms increases the number of electrons in STO through a process known as electron doping, and turns the material into a metallic conductor, and even into a superconductor10. One consequence of the electrons at the aluminium/STO interface becoming confined in a gas is that their spin is coupled to their momentum, a phenomenon called spin–orbit interaction11. Workers from the same research group as Noël et al. have previously demonstrated12 such spin–momentum entanglement: when they injected a spin-polarized current (a flow of spins that are oriented in one direction) into the electron gas, they observed a conventional electric current (a charge current) whose direction depends on the spin orientation and on the spin–orbit coupling. This is the result of the spin polarization being converted into electron motion by the spin–orbit interaction. Noël et al. now report surprising observations of the STO electron-gas system that adds to this complex behaviour. When the authors applied an electric field to the STO to control spin–orbit coupling in the gas, they observed a hysteresis effect — the direction of the charge current produced in the gas ‘remembers’ the polarity of the applied electric field, even after the field is removed (Fig. 1). Moreover, when they characterized the properties of the insulating STO beneath the gas, they observed features commonly attributed to ferroelectric compounds: when the polarity of the voltage applied across the STO is reversed, a spike of charge current is produced. Such a phenomenon is commonly associated with the reversal of electric dipoles in ferroelectric materials, and is at the core of the definition of electrical polarization in the modern theory of ferroelectricity13. Figure 1 | A ‘ferroelectric-like’ spin–orbit transistor. A type of electronics known as spintronics involves controlling the spin (the intrinsic angular momentum) of electrons. Noël et al.1 injected a spin current (arrows in circles indicate electron spins) from a magnetic nickel–iron alloy into strontium titanate (STO). The STO was covered by a thin layer of aluminium, which induces the formation of an electron gas (a layer of highly mobile electrons) at the STO surface. The electrons in the gas exhibit spin–orbit coupling — their spins couple to their momenta. This effect converts the spin current into a conventional charge current (red circles indicate electron charges). The authors applied a voltage across the insulating STO beneath the electron gas to change the sign of the spin–orbit coupling, and hence the direction of the charge current. The insulating STO exhibits surprising ‘ferroelectric-like’ behaviour: it has an overall electrical polarization whose direction (large arrow) depends on the applied voltage. The polarization remains in the absence of an electric field; this allows permanent control of the spin–orbit coupling and thereby of the direction of the charge current. The authors’ system has potential applications for spintronics, because it acts as a spin detector, analogous to optical polarizers that transmit light polarized only along a particular direction. Moreover, when the polarity of the applied voltage is inverted, the selectivity of the spin filter changes so that electrons with ‘spin up’ polarization move right, instead of left. Crucially, this selectivity remains in the absence of an applied voltage. This minimizes power consumption and opens up applications for memory storage. Ferroelectric-like behaviour has been observed previously in STO (see refs 14–16, for example). However, a peculiarity of the effect observed by Noël and colleagues is that it occurs only when the applied electric field exceeds a critical value. This raises concerns: true ferroelectric materials don’t show polarization only at high fields; it is an intrinsic state that occurs even in the absence of an external electric field. Noël and colleagues’ data indicate that something similar to a ‘relaxor’ state occurs in their system at low temperatures, in which a fraction of the STO consists of nanometre-scale domains that have an electrical polarization, and move or reorient in an applied electric field. By contrast, all of the material in a true ferroelectric compound is polarized. One can speculate that the movement of polar walls17 — boundaries that form between two STO domains that have different crystallographic orientations18 — produces the spikes of current observed by Noël and co-workers. But other microscopic mechanisms might be at play, given the richness of STO’s electronic behaviour; point defects produced in STO during the fabrication of the authors’ device could also have a role. Research into the domain walls in STO is currently booming, and will probably find an explanation for the observed behaviour. In the meantime, the authors’ demonstration of a permanent switch of spin–orbit coupling at a complex-oxide interface shows the potential of this class of material to compete in the race for more-efficient computing. </body>
<date id = '27'>22 April 2020</date>
<url id = '28'>https://nature.com/articles/d41586-020-01105-1</url>
<title id = '28'>The mammalian gut must defend against a variety of infectious agents. Neurons, cells not usually thought of as first-responders during infection, are now found to aid the gut’s barrier function and stop bacteria from spreading elsewhere.</title>
<body id = '28'>The recognition and neutralization of harmful bacteria in the gut is generally thought to be orchestrated by epithelial and immune cells. Writing in Cell, Lai et al.1 report that a subset of gut neurons also have an unexpected crucial role in the intestinal response to infection. The gut is exposed to food, antigens (molecules that can trigger an immune response if they are recognized as ‘non-self’), resident microbes that are normally harmless (commensal microbes) and harmful microbes (pathogens). Thus, gut cells have the difficult task of discerning friend from foe. Cooperative interactions between epithelial cells and immune cells are key to managing this complex balancing act, coordinating tolerance to food antigens and to commensal microbes, but initiating protective immune responses against pathogens. Nerve cells in the gut (comprising the enteric nervous system) can sense microbe-derived molecules, and these neurons interact with epithelial cells and immune cells to promote defence responses against microbes2. The human enteric nervous system encompasses a complex network of an estimated 108 neurons, which are essential for regulating many gut functions, including blood flow and the movement of the intestine’s contents3. Moreover, certain subsets of these nerve cells interact closely with and modulate components of the gut’s immune system4. One such type of neuron is called a nociceptor. Nociceptors elicit the perception of pain or discomfort in response to potentially harmful stimuli such as intense heat and cold, reactive chemicals or mechanical injury5. They also directly detect pathogens and pathogen-produced molecules, which can evoke a sensation of pain during infection6. Whether nociceptors contribute directly to impeding bacterial invasion of host tissues has been a matter of speculation. Salmonella is a pathogenic bacterium that is a frequent cause of food-borne illness. It can trigger a variety of conditions, ranging from inflammatory diarrhoea (gastroenteritis) to life-threatening complications in situations in which infection spreads beyond the gut to other sites in the body7,8. When Salmonella reaches the gut, one of the major sites of tissue invasion are the dome-shaped follicles called Peyer’s patches (Fig. 1). As key sensors of the gut that aid immune defences, these follicles use immune cells and specialized epithelial cells called M cells to monitor and respond to pathogens and commensal microbes. M cells can take up antigens from the lumen of the gut and transfer them to underlying immune cells9, which then either initiate a protective immune response to the antigen or tolerate the antigen’s presence. Figure 1 | Neurons in the mammalian gut help to defend against bacterial infection. Lai et al.1 report that a type of gut neuron, called a nociceptor, which expresses the proteins TRPV1 and NaV1.8, can thwart infection by Salmonella bacteria in mice. a, Salmonella can invade gut tissue by leaving the gut lumen to enter M cells — specialized epithelial cells in a region called a Peyer’s patch. These microbes can then spread elsewhere. Segmented filamentous bacteria (SFB), which are normally present in the gut and can bind to M cells or other gut epithelial cells, can help to limit Salmonella infection13. b, The authors report that these nociceptors orchestrate a defence response to Salmonella infection, the hallmarks of which include a decrease in the number of M cells and an increase in SFB colonization of the gut. These changes were associated with a decrease in Salmonella infection and diminished spread of the bacteria beyond the gut. Lai et al. report that nociceptor secretion of the neuropeptide CGRP regulates the number of M cells, along with SFB levels. Although Peyer’s patches are important for monitoring the contents of the intestine, certain pathogenic agents, including Salmonella, norovirus (a common viral cause of gastroenteritis) and prions (infectious, disease-causing proteins), exploit M cells as sites of tissue entry10,11. Notably, although Peyer’s patches are adjacent to neurons, including nociceptors, the functional consequences of this close proximity were not fully understood previously. Lai et al. assessed the role of nociceptors in the gut of mice infected with the pathogen Salmonella enterica serovar Typhimurium. The authors report that the presence of a subset of gut nociceptors (specifically, those that express the ion-channel proteins TRPV1 and NaV1.8) protect the gut against invasion by Salmonella and the subsequent spread of this bacterium to sites such as the liver and spleen. Intriguingly, the authors found that the protective effects of nociceptors were not mediated by well-known antimicrobial defence mechanisms, such as activation of immune cells or alterations in the levels of antimicrobial peptides that are produced by gut cells. Instead, during infection with Salmonella, these nociceptors orchestrated a reduction in the number of M cells. Because M cells are a key entry point for Salmonella, this reduction would probably have the consequence of reducing the surface area available for Salmonella to invade.   The authors analysed the composition of gut bacteria in the absence of Salmonella infection, using mice with gut nociceptors that were genetically engineered to lack either TRPV1 or NaV1.8 channel proteins. Compared with animals that expressed these proteins, both types of engineered mouse had lower levels of segmented filamentous bacteria (SFB), a group of commensal microbes that attach to gut epithelial cells, and particularly to M cells12. Such commensal bacteria are crucial for providing resistance against gut colonization by pathogens, including Salmonella13. Lai and colleagues investigated whether there was a connection between a decrease in M cells and the extent of SFB colonization of the Peyer’s patches. The authors demonstrated that M-cell depletion mediated by nociceptors, or triggered through an antibody-mediated experimental approach, led to an increase in this colonization, suggesting that the number of M cells can modulate SFB colonization in the gut (although the exact mechanisms responsible were not fully determined). This outcome was beneficial because it limited Salmonella infection, presumably because the higher presence of SFB and the depletion of M cells together resulted in a reduction of invasion sites available for Salmonella. Finally, Lai et al. report that when TRPV1-expressing nociceptors encountered Salmonella, the neurons released a neuropeptide called CGRP. This small molecule enables communication between cells. CGRP was directly able to regulate M-cell abundance and function, as well as to regulate SFB levels in the gut. The authors have uncovered a previously unrecognized role for nociceptors in host defence against Salmonella infection. These remarkable findings reveal a complex loop of interactions between epithelial cells, neurons and microbes in the mammalian gut, adding another layer of complexity to our understanding of gut immunity. Whether nociceptor-mediated responses help to defend against a variety of other microbial pathogens remains to be determined. Indeed, nociceptors have been reported to protect mice during infection by the bacterial pathogen Citrobacter rodentium14. A key area for future investigation will be to determine whether Lai and colleagues’ findings have relevance for human health. For example, one area that would be worth studying is whether long-term use of pain-blocking opioid drugs, such as morphine, might affect nociceptor-mediated antibacterial defence. This is of interest because nociceptors are the main target of opioids, and administering morphine to mice changes the gut’s microbial composition15,16. Moreover, morphine use promotes the spread of certain types of microbe (Gram-negative bacteria) from the gut to elsewhere in the body, a process that can lead to sepsis, a potentially life-threatening immune response to infection15,16. Future research that explores interactions between neurons and immune cells during infection could uncover further exciting findings that will profoundly influence our understanding of host defence. </body>
<date id = '28'>20 April 2020</date>
<url id = '29'>https://nature.com/articles/d41586-020-01000-9</url>
<title id = '29'>In a mirror world, antiparticles should behave in the same way as particles. But it emerges that leptons — neutrinos, electrons and their more exotic cousins — might not obey this expected pattern.</title>
<body id = '29'>All visible matter in the Universe is made of fundamental building blocks, the elementary particles. The group of particles known as fermions consists of two types: quarks, which make up protons and neutrons; and leptons, namely, the electron, muon, tau particle and neutrino. For each elementary particle, there is an antiparticle that has the same properties but opposite charge. The best-known example is the antielectron, or positron. It was long thought that antiparticles would behave in the same way as particles in a mirror world made of antimatter, but since the 1960s we have known that quarks and antiquarks break this particle–antiparticle mirror symmetry1,2. Writing in Nature, the T2K Collaboration reports possible findings of violation of this symmetry by leptons3.   Particle–antiparticle mirror symmetry is also known as charge-conjugation parity-reversal (CP) symmetry; it combines the charge symmetry between particles and their antiparticles with parity (the idea that physical laws should not change in an antimatter mirror world). Why is CP symmetry broken, and what are its consequences? This puzzling question lies at the core of our understanding of the laws of nature and the evolution of the Universe. As suggested4 by Andrei Sakharov in 1967, CP violation is one of the key ingredients needed to explain why there is a small excess of matter over antimatter in the Universe. This imbalance, at a level of a few particles per 10 billion photons5, is ultimately responsible for the existence of Earth, planets, stars and ourselves: if there were equal amounts of matter and antimatter, they would have destroyed each other in the early Universe and annihilated into photons. No matter would have remained. How did this tiny excess arise from an initial Universe that was perfectly symmetrical? The amount of CP violation observed in quarks is not enough to cause it6, so scientists have looked at leptonic CP violation in a well-studied mechanism called leptogenesis7. In models introduced to explain the observed neutrino masses, hypothetical heavy partners to neutrinos would have been copiously present in the early Universe and subsequently decayed. In the presence of CP violation, these decays could have generated the observed matter–antimatter asymmetry. The discovery of substantial leptonic CP violation would be groundbreaking. Its observation, together with evidence that a quantity known as lepton number has been violated (that is, not conserved), would provide strong circumstantial evidence for leptogenesis as the origin of the matter–antimatter imbalance8,9.   Leptonic CP violation is elusive, but can be searched for using neutrinos. These fundamental particles are remarkably reluctant to interact with ordinary matter, making them very hard to detect. They are the least understood known particle. Despite this, they are ubiquitous: your average coffee mug contains around 100,000 of the ‘cold’ neutrinos that permeate the Universe, and many times more produced by the Sun. Neutrinos come in three types (flavours), determined by their associated charged lepton, whether that is an electron, a muon or a tau particle. It was long thought that neutrinos were massless. However, the discovery of neutrino oscillations by the Super-Kamiokande experiment10 in 1998, and by the Sudbury Neutrino Observatory11 in 2002, proved that these particles do have mass. Neutrino oscillation is the phenomenon whereby neutrinos change from one flavour to another as they travel12. It is a quantum-mechanical effect that arises because each neutrino flavour is effectively a mixture — a quantum superposition — of three states that have different masses. Importantly, the superposition state can change over time because the components evolve differently (Fig. 1). For example, a neutrino that was produced as purely muon-flavoured can become partly an electron neutrino. Figure 1 | Neutrinos through the looking glass. The elementary particles known as neutrinos have a curious ability to transform between three flavours (νe, νµ and ντ) over time, because the three components (mass states) of their make-up evolve differently; waves are simplified depictions of the contribution of each mass state to the neutrino. Each neutrino type has its own antineutrino (indicated by a bar). Symmetry rules imply that, in a mirror world made of antimatter, the antineutrinos should behave like neutrinos. But results from Japan’s T2K experiment indicate that this symmetry might be broken3. The result could hint at how the Universe came to contain more matter than antimatter. Since their discovery, neutrino oscillations have been analysed in several experiments, but only in the past few years have tiny oscillations from muon neutrinos to electron neutrinos been observed13,14. The probability of this oscillation occurring is small, but it holds the key to leptonic CP violation: if CP symmetry is conserved, the oscillation probability for muon-to-electron neutrino conversion would be the same as that for muon-to-electron antineutrino conversion. The T2K Collaboration has been able to study these oscillations with unprecedented precision, and has observed possible evidence of leptonic CP violation. In the T2K experiment15, a neutrino beam is generated at the Japan Proton Accelerator Research Complex in Tokai. Here, highly accelerated protons hit a dense graphite target, producing large quantities of particles known as pions and kaons. These particles decay, giving rise to a neutrino beam (or an antineutrino beam, depending on the conditions used), which is monitored by two detectors 280 metres away. The neutrinos subsequently travel through Earth without being stopped, but some are detected by the underground detector at the Kamioka Observatory 295 km away, deep beneath Japan’s Mount Ikeno. The detector consists of 50,000 tonnes of ultrapure water surrounded by a vast array of light sensors. When a neutrino interacts with a neutron in the water, it can produce a muon or an electron, depending on its flavour. The T2K experiment detects the muons and electrons and discriminates between them, thereby identifying the flavour of the impinging neutrino and measuring the oscillation probability of muon-to-electron neutrino conversion. The T2K Collaboration analysed data collected between 2009 and 2018, in both neutrino and antineutrino mode. By combining this with input from other neutrino-oscillation experiments, the researchers have disentangled the dependence of the conversion probability on various parameters and thus provide evidence of CP violation. The results exclude CP conservation (that is, they suggest that CP violation has occurred) at a 95% confidence level, and show that the CP-violating parameter is likely to be large. These results could be the first indications of the origin of the matter–antimatter asymmetry in our Universe. The measurement is undeniably exciting. But extraordinary claims need extraordinary evidence — a confidence level of more than 99.9999% will be needed to be certain that leptonic CP violation has occurred. This requires a more precise measurement of the oscillation probability, with more intense beams, larger detectors and better-understood experimental features. The next generation of large-scale, multi-purpose neutrino experiments is preparing for the challenge. The T2HK experiment in Japan16 is based on the same technology as T2K but will use the Hyper-Kamiokande detector, which will have ten times the mass of water and a more intense beam. Hyper-Kamiokande received official approval this February, and construction will start soon. And the Deep Underground Neutrino Experiment17 (DUNE) will be based at the Sanford Lab in Lead, South Dakota; its technical-design report was published in February18,19. DUNE will use a different detector technology consisting of four modules filled with several thousand tonnes of liquid argon, to detect an intense beam of neutrinos produced 1,300 km away at Fermilab in Batavia, Illinois. Smaller prototypes tested at CERN, Europe’s particle-physics lab near Geneva, Switzerland, have demonstrated the feasibility of the large-scale DUNE detector. T2HK and DUNE therefore provide complementary techniques and measurements. They will probably give us a definitive answer in the quest for CP violation in the next 15 years. </body>
<date id = '29'>15 April 2020</date>
<url id = '30'>https://nature.com/articles/d41586-020-00750-w</url>
<title id = '30'>Tools have been developed to project inequalities in education around the world to 2030. They reveal that overall inequality will decline, but that all world regions will fall short of achieving universal secondary education.</title>
<body id = '30'>Increased years of schooling have been linked to better health and survival1, slower population growth2 and greater economic growth3. Because of its importance, access to “inclusive and equitable quality education” was included as one of the Sustainable Development Goals (SDGs) ratified by the United Nations General Assembly in 2015 (see go.nature.com/3ana8ob). The SDGs are an ambitious set of international development targets to be achieved by 2030. Writing in Nature, Friedman et al.4 provide evidence that, although most nations are projected to achieve near-universal primary education by 2030, large inter-regional disparities in the rates of secondary-school completion will persist.   The authors set out to assess whether countries are on track to achieve the SDGs for education by 2030. They assembled a database of 3,180 nationally representative censuses and surveys from 195 nations and territories. This database is an improvement on previous efforts to monitor education, which relied either on data back-projected from a single time point5 or on a database derived from one-fifth as many data sources6. Friedman et al. developed a model that combines all the data sources in their data set and extrapolates single-year estimates of educational attainment for all populations, separately by sex and country, from 1970 to 2018. The model then uses this information to project future trends in educational attainment for individual countries or territories, and for seven ‘major world regions’ chosen by the authors, which include high-income countries, sub-Saharan Africa and Eastern Europe and central Asia. Friedman and colleagues conclude that most countries are in line to achieve near-universal levels of primary-school attainment by 2030 — that is, for almost 90% of children to complete 6 years of education (Fig. 1). The exceptions to this trend are places such as Afghanistan, Papua New Guinea and parts of northern sub-Saharan Africa. By contrast, progress towards near-universal secondary attainment (12 years of schooling) is more uneven. Only 61% of young adults aged 25–29 years old are expected to have completed secondary school by 2030, and no major world region is expected to reach near-universal levels of secondary-school attainment. Furthermore, access to tertiary schooling is expanding faster in some world regions than in others; as a result, disparities are expected to increase until 2030. Figure 1 | Primary schoolchildren in Dhaka, Bangladesh. Friedman et al.4 analysed the number of years of schooling obtained by children in some 195 nations and territories between 1970 and 2018, and modelled predicted changes to 2030, to assess whether the world will meet the Sustainable Development Goals for education set by the United Nations.Credit: Zakir Hossain Chowdhury/NurPhoto/Getty The authors also find that gaps in educational attainment between men and women are expected to have changed substantially by 2030. In 1970, men achieved significantly more years of education than did women in 142 countries. By 2018, this had narrowed to 27 countries, and by 2030 it is projected to be just 4. Moreover, in 18 nations and regions, women are expected to achieve significantly higher mean years of schooling than men. This changing gap is largely attributable to girls’ gains in primary schooling. Next, Friedman et al. developed a metric for monitoring educational inequality within countries or territories — average interpersonal difference (AID), which measures the average difference in educational attainment between any two individuals in a population in a given year. The AID provides a different perspective from those of other commonly used metrics. For example, consider the Gini coefficient, which is perhaps the most commonly used indicator of inequality. Under this coefficient, inequality is highest when education is concentrated in the hands of a few. As access to education expands, the Gini coefficient declines. By contrast, the AID equates inequality with heterogeneity in educational attainment within a population. When education is concentrated in the hands of the few, the AID is low because most people have the same low level of educational attainment. As access to schooling expands, inequality as measured by the AID rises because the population now contains many people who have no education and many who have several years of schooling. As school enrolment becomes universal and members of the population begin to achieve similarly high levels of education, the AID declines again. From this metric, Friedman and colleagues conclude that global educational inequality peaked in 2017 and is projected to decline until 2030.   Even though this project involves an impressive volume of data, it is still limited by problems of data scarcity. Whereas some high-income countries, such as France and Germany, contribute more than 55 data points to the model, the time series for many resource-constrained and small-population countries or territories are extrapolated from fewer than 5 data points, or rely on data last collected in or before 2008. Although the validity of the model was evaluated by checking how well its predictions matched real data across many simulations in which one subset of data had been removed, there is no way of assessing how well it estimates attainment trajectories for places such as Malaysia, for which no data were available after 2003. The authors leverage regional trends to inform analyses of countries or territories for which data are scarce, but the results should be interpreted with caution. The smoothed trajectories of predicted change in the study also hide the profound and often sudden impact of education policies on schooling. The authors note nonlinearities in the rates of change consistent with a sudden increase in schooling, which might result from the elimination of school fees or an increase in the years of compulsory schooling. The recent expansion of free secondary education in many lower-income countries has the potential to further advance progress towards the SDG education goals, beyond what is currently predicted by Friedman and colleagues’ model. Ultimately, we can monitor only what we can measure: we track trends in educational attainment and in gaps between the sexes because those are the data that exist. Socio-economic gaps in schooling are now substantially larger than are gender gaps in most world regions7, but sufficient data on the socio-economic status of students are scarce. Likewise, almost three-quarters of countries have inadequate data with which to monitor progress in learning outcomes (such as mathematics or reading skills), rather than merely in years of schooling (see go.nature.com/39kd4o1). Global commitments to inclusive and equitable quality education run the risk of failing to achieve their true goals when we lack the data to properly track progress. </body>
<date id = '30'>15 April 2020</date>
<url id = '31'>https://nature.com/articles/d41586-020-01020-5</url>
<title id = '31'>A method for identifying atmospheric contamination of volcanic-gas samples reveals variations in the isotopic composition of nitrogen in the mantle, and provides a clearer view of the origins of this element in Earth’s interior.</title>
<body id = '31'>Earth’s nitrogen-rich atmosphere contributes to the pleasant surface environment in which we live and breathe — but makes it very difficult to determine the nitrogen isotope composition of anything else. Pervasive atmospheric contamination of samples derived from Earth’s mantle poses a formidable challenge to anyone investigating the origins and transport of volatile species, such as nitrogen and the noble gases, in the deep Earth. In a paper in Nature, Labidi et al.1 report that they have used a ‘clumped isotope’ method to identify uncontaminated mantle nitrogen in volcanic-gas effusions and gases trapped in volcanic-rock samples. The relative abundances of isotopes in uncontaminated nitrogen vary among samples from different locations. The authors argue that these differences originate from Earth’s formation and have survived approximately 4.5 billion years of mixing associated with mantle convection.   There are two stable nitrogen isotopes, 14N and 15N, and their relative abundances are expressed as δ15N values — the parts per thousand deviation of the 15N/14N ratio from a standard value. The nitrogen isotopic compositions of mantle-derived samples can provide insight into a wide range of topics, from the mix of planetary building blocks that brought volatile species to Earth during its formation2, to the transport of atmospheric nitrogen into the mantle through the sinking of tectonic plates over time3. Apart from the proportions of 14N and 15N in a sample, the way that isotopes are distributed between molecules also provides information. An isotopologue is a molecule that has a specific combination of isotopes of its constituent elements. For example, diatomic nitrogen molecules (N2, which constitute about 78% of the atmosphere by volume) can incorporate either 14N or 15N, yielding three possible isotopologues: 14N14N, 14N15N and 15N15N. Because the vast majority of nitrogen is 14N, the most common isotopologue is 14N14N. Substitution of a single 15N for 14N is rare; a doubly substituted isotopologue (15N15N) is rarer still. A random distribution of 14N and 15N between N2 molecules produces a specific mixture of the three isotopologues. Any measured deviation from the expected proportion of 15N15N is described as a clumped-isotope anomaly. Earth’s atmospheric N2 exhibits a well-resolved clumped-isotope anomaly4, and Labidi et al. used this signature to identify atmospheric contamination of volcanic gases. The authors established that mantle N2 has no clumped-isotope anomaly by analysing nitrogen released from unusually gas-rich samples of mid-ocean ridge basalt, confirming the expectation that magmatic gases have a random distribution of isotopes among N2 isotopologues. With this information in hand, the authors examined nitrogen isotope compositions in hydrothermal gases sampled from Yellowstone National Park in the United States, Iceland and other volcanic localities. They identified the nitrogen isotope compositions of the mantle sampled at locations at which trends showing varying degrees of atmospheric contamination were evident.   In previous studies3,5 of nitrogen in mantle-derived gases, systematic variations among measured nitrogen and noble-gas compositions were sought to identify atmospheric contamination, but contradictory signatures were sometimes observed — some metrics indicated that there was contamination, whereas others suggested there was none. Labidi and colleagues show that data that might have been interpreted as mantle compositions on the basis of relationships between nitrogen and noble gases are, in fact, affected by atmospheric N2 contamination. Their study also indicates that δ15N variations are produced in atmospheric N2 as it circulates through hydrothermal systems. However, the processes that generate such changes in the bulk proportions of 14N and 15N do not redistribute isotopes among isotopologues, so that the atmospheric clumped-isotope anomaly is preserved — which means that any contamination remains identifiable. There is no place for atmospheric N2 to hide if one is looking through a clumped-isotope lens. An important feature of the authors’ analytical approach is that it is not necessary to measure pure, uncontaminated magmatic gas to estimate the mantle composition. Even if multiple atmospheric contaminants are present, evidence of mixing trends in the data can be used to identify the mantle δ15N value of magmatic gas, which has no clumped-isotope anomaly. Labidi et al. report a mantle δ15N value for the potentially deep-seated6,7 Yellowstone mantle plume that is distinct from those determined for mid-ocean ridge basalts. With uncertainties regarding atmospheric contamination eliminated, nitrogen isotope variations in the mantle can be interpreted in the context of Earth’s formation, differentiation into distinct layers, and the long-term coevolution of the deep Earth and surface owing to plate tectonics (Fig. 1). Figure 1 | Nitrogen in the deep Earth. Labidi et al.1 report a new method for identifying contamination of volcanic gases by nitrogen from the atmosphere. The authors find that the nitrogen isotope composition of gases extracted from mid-ocean ridge basalts, which sample the convective mantle, is different from that of volcanic gas from Yellowstone National Park in the United States, which is thought to sample an upwelling mantle plume that originates in the deep mantle. By modelling transport of surface nitrogen into the mantle through subduction (the process in which one tectonic plate dives beneath another and descends into the mantle) and nitrogen loss (outgassing) from the mantle, the authors argue that only a limited amount of nitrogen from Earth’s surface has been incorporated into the mantle. They conclude that the observed variations in mantle nitrogen isotope compositions reflect differences that originated early in Earth’s history. To test whether nitrogen exchange between the surface and mantle over time explains their results, Labidi and co-workers developed a mathematical model of nitrogen evolution in the mantle. Intriguingly, the results suggest that there has been a net loss of nitrogen from the convective mantle over most of Earth’s history, and little incorporation of surface nitrogen into the mantle. This contrasts with previously reported evidence of substantial incorporation of atmospheric xenon into the mantle8,9. Given the limited role of surface nitrogen in the mantle, the authors argue that the observed nitrogen isotope variations are a remnant from Earth’s formation and early differentiation, when volatile species were delivered to the growing Earth as it separated into the core, mantle, crust and atmosphere. Evidence that early-formed mantle heterogeneities survive in the modern mantle has come from studies of signatures formed by rapidly decaying radioactive isotopes that decayed within the first 100 million years of Earth’s history8,10. It will be challenging to confirm that the nitrogen isotope variations identified by Labidi et al. arose early in Earth’s evolution, given that neither of the element’s two isotopes is produced by radioactive decay and that surface signatures might have a confounding role, however limited. Determination of δ15N values at other plume localities, including regions thought to be influenced by the recycling of surface materials11, would provide an interesting test of the authors’ primordial hypothesis. The application of clumped-isotope analysis reported by Labidi et al. provides an exciting method for such future studies — we now have an improved tool with which to view the origins and evolution of volatile species in the mantle. </body>
<date id = '31'>15 April 2020</date>
<url id = '32'>https://nature.com/articles/d41586-020-01021-4</url>
<title id = '32'>Evolutionary-tree diagrams, which show the branching relationships between species, are widely used to estimate the rates at which new species arise and existing ones become extinct. New work casts doubt on this approach.</title>
<body id = '32'>Scientists often want to make inferences about what the biological past was like, and how that past gave rise to the present, because doing so allows them to understand the processes that drive evolution. But, writing in Nature, Louca and Pennell1 challenge a major aspect of that enterprise.   Specifically, their work regards the issue of estimating past rates of speciation and extinction, which are, respectively, the rates at which new species arise and existing species go extinct. These rates determine the number of contemporary species of various forms. There are, for instance, around 6,600 species of songbird (passerines), which constitute more than half of all existing bird species, and we might therefore be tempted to say that songbirds have a high rate of speciation in comparison with that of other birds. But it’s also possible to speculate that they have a low extinction rate. Louca and Pennell show that the uncertainty is even worse than this: not only can we not estimate these two rates, but also there is an infinite number of different sets of these two parameters that are equally good at describing any particular outcome, such as the number of species of contemporary songbird. Because fossils are scarce or non-existent for the vast majority of species, evolutionary scientists instead estimate speciation and extinction rates from phylogenies — tree diagrams that describe the patterns of descent among a group of contemporary species (Fig. 1a,b). For any such phylogeny, it is easy to construct what is termed a lineage-through-time plot; this records the cumulative number of lineages up to that point in time on the tree that will eventually leave one or more living descendent species (Fig. 1c). The slope of the curve fitted to such a plot, often denoted by λ, is the net speciation rate. This is equal to the difference between the rate of speciation, termed b (or birth), and the rate of extinction, termed d (or death). It is described by the equation λ = b – d. Figure 1 | Assessing evolutionary histories. Louca and Pennell1 raise questions about a standard approach to estimating past rates of species formation (speciation) and extinction that uses data from a lineage-through-time plot. The number of species in the present depends on how speciation and extinction rates varied over time in the past. Using mathematical modelling, the authors reveal that an infinite number of pairs of speciation and extinction rates could give rise to any given outcome, and it is thus unclear how to determine the correct rates. a, b, Examples of known extinctions are rare, and are shown in these hypothetical tree diagrams only to illustrate how different rates of extinction (and different speciation rates) can yield the same lineage-through-time plot. c, Information taken from a tree diagram can be represented in a lineage-through-time plot as shown. Red dots indicate the number of lineages at a given time that gave rise to lineages existing in the present. The slope of the curve equals the speciation rate minus the extinction rate. This plot is valid for both trees even though they have different speciation and extinction rates. This underscores the authors’ demonstration that many different data inputs can give identical lineage-through-time plots. However, it is known that a difficulty arises in estimating b and d, because if all that is available is the number of species that have survived to the present, such as our 6,600 songbirds, any pair of b and d that returns the same value of λ will produce an identical lineage-through-time curve, and there is an infinite number of these pairs. In fact, it turns out that for the simple case of estimating b – d, such as described here, a feature of the shape of the lineage-through-time curve can be exploited to estimate the rate of extinction, and then the rate of speciation can be found by subtraction2. But to do so requires making the assumption that both of these rates are constant throughout the entire time span of the tree, when instead they almost certainly vary between the different branches (lineages) of the phylogeny, and through time. This is where Louca and Pennell step in, because the novelty and mathematical sophistication of their work lie in showing that we cannot estimate these ‘time-varying’ speciation and extinction rates. The authors invoke earlier work3 that defines the existence of a tree’s ‘deterministic’ lineage-through-time curve: this is a set of differential equations (equations describing rates of change) that fully determine the number of lineages in a tree at any given time. Louca and Pennell’s key result is then to show that there is an infinite number of alternative sets of time-varying speciation–extinction rates that yield the same number of lineages at any given time as does the deterministic lineage-through-time curve. They further show that the most probable estimates of the two rates (calculated by maximum-likelihood methods) do not necessarily identify the correct underlying model — as demonstrated by an analysis of hypothetical cases for which the true time-varying speciation–extinction rates are known.   Even worse for those who want to use the rates of speciation and extinction to study evolution, the possible alternative scenarios of time-varying speciation and extinction rates that are consistent with the deterministic lineage-through-time model often differ qualitatively. For example, the authors show that a phylogeny of approximately 80,000 species of seed plant is equally well described by speciation and extinction rates that both gradually increase through time or that both gradually decrease through time. Other scenarios, including rates that vary wildly with time, provide equally good descriptions of the numbers of lineages through time as derived from the deterministic lineage-through-time model. Louca and Pennell’s conclusions will be dispiriting to evolutionary scientists who are looking for a link between past levels of speciation and extinction and historical climate change or other environmental events, or who want to test ideas about what features of a species — such as diet, mating system or the length of a generation — might be used to predict speciation and extinction rates4. The limitations that Louca and Pennell have identified for estimating speciation and extinction rates do not go away as the size of the phylogenetic tree increases. Nor do other common features of trees provide much help: for example, if a group of species has never suffered any extinctions, estimating their speciation rate would be straightforward. But this is rare, and unlikely to be known in advance. Having abundant fossils could help, because they provide evidence needed to estimate extinction rates; however, fossils are seldom abundant. We can make assumptions about how speciation and extinction might vary with each other, through time, or with the number of species, but these assumptions are being made about the things that we would like to estimate. Amid this epistemological carnage regarding what we can possibly know, the authors helpfully offer some consolation by showing that it is possible to estimate a parameter they call the pulled speciation rate, or λp. This measures the rate of change (the slope of the curve) of the deterministic model of the lineage-through-time plot. The pulled speciation rate can be compared between lineages, or at different times, and might be useful for understanding the processes that gave rise to the species that are alive today, even if not necessarily providing information about those species that didn’t make it. And this aspect — the ones that became extinct — is the deeper lesson of Louca and Pennell’s work. Without fossils, all evolutionary scientists, whether studying speciation and extinction or attempting to reconstruct the features of distant ancestors, need to be aware that the evolutionary processes they identify are those that operated in the species that would survive and eventually leave descendants in the present. We can’t be sure what was going on in those that went extinct. It is the evolutionary version of the observation that history is written by the victors. The supreme irony of this predicament is that Charles Darwin’s idea about the survival of the fittest, the story that we want to understand, by its very nature renders elusive some of the key components needed to study it. </body>
<date id = '32'>15 April 2020</date>
<url id = '33'>https://nature.com/articles/d41586-020-00975-9</url>
<title id = '33'>The timing of disruptions to biodiversity associated with global warming is a key, but little-explored, dimension of change. Will losses in biodiversity occur all at once, or be spread out over time?</title>
<body id = '33'>Projections of the effects of climate change on multiple species are often made by estimating the change predicted for a single future time point; for example, by asking how the geographical distributions of multiple species will differ in 2100 from those today1. However, this approach does not capture the pace, timing or possible synchrony of biodiversity changes across time. Acute synchronous impacts can potentially be more damaging to a system than those spread over time, in terms of both human adaptation to biodiversity losses and ecosystem resilience. Writing in Nature, Trisos et al.2 report an approach for predicting how climate change will affect future biodiversity patterns.   The authors estimated the timing and synchrony of climate impacts on organisms globally by asking when species in a given region will be exposed to temperatures outside their normal global experience (by considering projected future temperatures due to climate change). They did this by compiling geographical-range maps for approximately 30,000 species, including birds, mammals, reptiles, amphibians, fishes, marine invertebrates, corals and seagrasses, and using temperature-projection models to identify the warmest average annual temperature experienced between 1850 and 2005 by each species within its range. Dividing Earth into grid cells of 100 square kilometres and using predicted climate information, the authors determined when each species would experience annual average temperatures above its historical annual average, encountered anywhere in its range, for an extended period. The result provides an estimate of when a species will be exposed to unprecedentedly high temperatures. Trisos and co-workers’ approach builds on ‘time of emergence’, a concept used when analysing climate change. Time of emergence describes the time at which a climate variable, such as temperature, emerges beyond the historical values of variation observed for a particular location — in other words, when the average value of the measurement of interest becomes more extreme than the previously encountered natural variability. Trisos et al. offer innovation in applying this concept to the realm of biodiversity. First, rather than considering the variation experienced at just one location, they considered the full breadth of variation experienced across each species’ geographical range, defining an organism as being ‘exposed’ in a specific grid cell only after it has experienced temperatures above its range-wide maximum (and with annual temperatures remaining above this value for a minimum of five years). Second, because the authors considered multiple species in an assemblage (the group of species present in a given grid cell), it was possible to assess the relative timing of exposure in a graphical format that the authors call a horizon profile (Fig. 1). This enables the synchrony in the timing of exposure events for the species in a region to be quantified and easily visualized. Figure 1 | Exposure of species to unprecedented temperatures owing to climate change. Trisos et al.2 report a global analysis describing when species (as assessed in grids of 100 square kilometres) are predicted to encounter, for more than five years, higher annual average temperatures than they have previously experienced anywhere in their geographical range. This state is called species exposure, and the authors assessed more than 30,000 terrestrial and marine species. a, For the majority of locations, the timing of when most species in a grid are exposed occurs highly abruptly, as in this example from the Amazon basin. Temperatures are from projection models, and the predicted future temperatures due to climate change are from the RCP8.5 model, which depicts a scenario of high greenhouse-gas emissions7. b, By contrast, in the Gobi Desert (located in northern China and southern Mongolia), species exposure occurs with low abruptness. (Graphs based on Fig. 1 of ref. 2.) The authors’ results predict that the greatest levels of exposure will occur at latitudes nearer the Equator, and, most notably, that there will be high synchrony in the timing of exposure between species in the same grid cell, for grid cells both on land and in the ocean. Trisos et al. find that most species in a given cell will usually become exposed to unprecedentedly high temperatures within the same decade. If this exposure results in local extinction, it suggests the following disturbing scenario. We might initially see a small trickle of species being lost from an assemblage, but this will be followed by an abrupt loss of most species in the assemblage within the same decade. What mechanism might explain this predicted pattern? The abruptness in exposures predicted by Trisos and colleagues is not due to any particular abruptness in the timing of climate change itself — although similar predictions of abrupt ecological change have been based on the additive effects of gradual climate change with abrupt natural climate variability, including weather3. Instead, it seems to be attributable to the similarity of the thermal niches occupied by the species in each grid cell. Trisos and colleagues find that more than half of the species in a given cell (and almost 90% in most marine assemblages) tend to have geographical ranges that encompass similarly warm temperatures, such that they would all face exposure at around the same time. Such a striking pattern of shared thermal niches within assemblages has been observed before, in a global analysis of marine fishes and invertebrates4. In that study, species’ thermal niches were found not to change gradually with latitude, but instead to have distinct transition points, indicating that species belong to what are termed thermal guilds4. These shared thermal niches could be due to physical boundaries or ecological interactions that restrict the ranges — and temperatures experienced — of multiple species similarly. Or this phenomenon might be the result of a low rate of evolution in the range of temperatures across which the species can fundamentally persist, leading to the maintenance of thermal guilds.   When does this abrupt exposure happen? It is predicted that it will occur at different times for grid cells around the world, from some predicted to be occurring already in the ocean, to others occurring towards the end of the projected time range, in 2100. That the timing is different across grid cells is a good thing, because at least all of the assemblages aren’t predicted to experience abrupt losses at the same time. But, notably, the timing of exposure does not correlate with the timing of climate-change emergence in temperature, suggesting that the latter metric might be a poor predictor of major biodiversity change within a given grid cell. Trying to project the timing of biodiversity shifts is a noble objective that will surely help us to develop management systems and anticipate crises. Although Trisos et al. provide an initial approach that offers useful insights, further studies should attempt to validate and qualify these predictions. For example, Trisos and colleagues used temperatures outside species’ current thermal niches to define climate exposure, but we don’t know what will really occur when species experience such temperatures — many can certainly tolerate temperatures beyond those found in their current ranges5,6. The timing of exposure to truly limiting environments might turn out to be more diverse across species than currently predicted by Trisos et al. if variation in species’ fundamental climatic niches (the range of temperatures and other climate variables across which an organism can survive) is considered. It will also be useful to consider the flip side of the range-shift issue: the timing and abruptness with which new species enter an assemblage as a result of range extensions arising from climate change. Most crucially, as climate change progresses, we should be able to test and refine projections such as these using real-time observations. Where are biodiversity changes already occurring abruptly? The need for systematic global biodiversity monitoring has never been stronger. </body>
<date id = '33'>08 April 2020</date>
<url id = '34'>https://nature.com/articles/d41586-020-00962-0</url>
<title id = '34'>Will mature forests absorb enough carbon from the atmosphere to mitigate climate change as levels of carbon dioxide increase? An experiment in a eucalyptus forest provides fresh evidence.</title>
<body id = '34'>When atmospheric concentrations of carbon dioxide increase, land ecosystems take up more carbon from the atmosphere as a result of increased photosynthesis, a process known as CO2 fertilization. It has long been suggested that CO2 fertilization will slow the rate of increase of CO2 levels in the atmosphere1, potentially mitigating climate change. To quantify the effect, ecologists have conducted experiments in which the atmosphere around a confined environment is enriched with CO2 — mostly in ecosystems for which the vegetation is short in stature, to reduce costs. A small number of enrichment experiments have been conducted in young forests, but there is a paucity of knowledge about the CO2-fertilization effect in mature forests. Writing in Nature, Jiang et al.2 present results of the Free-Air CO2 Enrichment (FACE) experiment in a mature forest in Australia. Their estimate of the CO2-fertilization effect is among the lowest yet reported. Jiang and colleagues carried out their study in a warm-temperate evergreen forest that has been undisturbed for the past 90 years, and which is dominated by eucalyptus trees (Eucalyptus tereticornis). They collected data for all the main carbon pools and fluxes in three circular plots (each 490 square metres; Fig. 1) in which the atmospheric CO2 concentration was elevated by 150 parts per million for 4 years, from 2013 to 2016. These data were compared with those from three control plots that were not enriched in CO2. Figure 1 | Carbon dioxide enrichment in an Australian eucalyptus forest. Jiang et al.2 increased the atmospheric CO2 concentration in circular plots to observe the effects on the main carbon pools and fluxes in the forest. The cranes were used to take researchers to the top of the forest canopy, and the ring structures transport and pump CO2 into the plots.Credit: John W. Whale The authors report that CO2 enrichment induced a 12% increase in carbon uptake, equivalent to an extra 247 grams of carbon per square metre per year, through gross primary production (GPP; the conversion of CO2 to organic carbon through photosynthesis). Of this, 28% ended up as net primary production (NPP; the fraction of GPP that is used for biomass growth, rather than consumed for metabolic processes) and 12.8% as an increase in the total carbon pools of the ecosystem (that is, in wood and soil). Their results add more uncertainty to already highly variable estimates of CO2 fertilization from previous CO2-enrichment experiments.   How does Jiang and colleagues’ estimate of the CO2-fertilization effect in this mature forest compare with results of other studies? One difference involves the leaf area of the forest canopy (the total surface area of leaves, counting only one side of the leaves), which is a major amplifier of the fertilization effect on the efficiency of carboxylation3 — the biochemical reaction that converts CO2 into organic compounds. A previous investigation4 of the same forest indicates that increased CO2 levels do not have much of an effect on the leaf-area index (LAI, a measure of total canopy leaf area) in this location, whereas CO2 enrichment did stimulate leaf-area expansion in field experiments in other ecosystems5,6. Furthermore, the plant carbon-use efficiency — the ratio of NPP to GPP — in the Australian forest, as in other mature forests7, is relatively low compared with that of young forests. This low carbon-use efficiency substantially truncates the CO2-fertilization effect. The two factors discussed above therefore jointly caused the CO2-fertilization effect in the Australian forest to be small. How can Jiang and colleagues’ results be interpreted from a more theoretical perspective? As atmospheric CO2 concentration increases, carboxylation is stimulated. This biochemical stimulation is scaled up through a biological hierarchy that progresses from leaf photosynthesis to canopy GPP, vegetation NPP, and to net changes in the carbon-pool sizes of plants and soil3. Across those scales, the carboxylation stimulation is amplified by some processes, but diminished by others. For example, if the extra carbohydrate produced as a result of rising CO2 levels is used for leaf-area expansion to capture more CO2, then stimulation is amplified at the canopy scale (that is, through GPP). By contrast, the stimulation is diminished when the extra carbon taken up at the canopy scale is allocated for plant respiration or transferred to microorganisms for their respiration. This theoretical framework of hierarchical responses allows the fertilization effects on GPP and on other carbon-cycle processes to be approximately estimated for a scenario in which the LAI does not change much and where the CO2 concentration increases by 150 p.p.m. Indeed, Jiang and colleagues’ observation-based estimates of a 12% increase in GPP, 12.8% of which ends up in the carbon pools, are very close to the lower limits of the theoretically derived estimates8.   Jiang and co-workers’ data, and data from similar studies, can aid estimates of the global effect of CO2 fertilization. The size of this effect depends directly on the sensitivity of carboxylation efficiency to rising atmospheric CO2 levels; this sensitivity should be similar at the eucalyptus forest and at all other sites around the globe, according to a theoretical analysis8. However, as CO2 levels increase, the capacity of photosynthetic carboxylation to process more CO2 diminishes, lowering the sensitivity of carboxylation efficiency to further CO2-level increases. In other words, the CO2-fertilization effect is dwindling at the biochemical level8. To work out the global fertilization effect, the carboxylation sensitivity is multiplied by the yearly increase in atmospheric CO2 concentration, which is becoming larger over time. The yearly increase in CO2 levels offsets the diminishing CO2-fertilization effect. Another factor that affects the size of the global CO2-fertilization effect is the LAI3. The change in LAI observed at the Australian study site in response to CO2 enrichment is at the low end of the wide spectrum of LAI changes that have been observed elsewhere4–6. At the global scale, however, the LAI is increasing over time — satellite observations show that Earth is literally becoming greener9,10. The increase of LAI amplifies the CO2-fertilization effect. The plant carbon-use efficiency reported in the current study is also at the low end of a wide range of reported values7, and contributes to the low CO2-fertilization effect observed in the study. However, we do not know much about how plant carbon-use efficiency varies over time at regional and global scales. This makes it difficult to assess whether the global fertilization effect will change because of shifts in this efficiency. The bottom line is that it is currently difficult to estimate the size of the global CO2-fertilization effect accurately. To solve this problem, we need to know more about hierarchical constraints not only across spatial scales, from ecosystem sites to regions and the globe, but also across biological scales — from the molecular level of biochemical reactions, to the leaf and canopy scale, and through to the larger scales associated with plant production and ecosystem carbon pools. </body>
<date id = '34'>08 April 2020</date>
<url id = '35'>https://nature.com/articles/d41586-020-00976-8</url>
<title id = '35'>Silicon used for electronics has a cubic crystal lattice, which makes the material unsuitable for photonics applications. A method for producing germanium and silicon–germanium alloys that have hexagonal lattices offers a solution.</title>
<body id = '35'>Silicon is a prodigious material for electronics. Its useful electronic properties, high abundance, low cost and excellent processability helped to stimulate a revolution in silicon technology: the development of mass-produced silicon chips, which allow computing capabilities to be integrated into almost any device. But, alas, silicon is an inefficient absorber and emitter of light, preventing it from being used in many photonics applications. Writing in Nature, Fadaly et al.1 report the development of silicon–germanium alloys that have excellent optoelectronic properties, and could thus aid the development of photonics technologies that are compatible with currently available silicon electronic devices.   Silicon’s lack of useful optoelectronic capabilities is due to its electronic properties — it is said to be an indirect-bandgap semiconductor. As an example of the problem, solar cells based on silicon must be at least 100 times thicker than those based on gallium arsenide (a direct-bandgap semiconductor, which absorbs and emits light efficiently) to collect the same amount of light, but they still convert the light into electricity much less efficiently2. And silicon-based lasers remain an unrealized dream, even after decades of intense research efforts. Instead, lasers are typically made using ‘compound’ semiconductors, which incorporate costly elements such as indium or gallium. The components used to absorb or emit light in currently available silicon photonics schemes are also mostly made from compound semiconductors, and are usually bonded to the silicon or used off-chip3. Several generations of scientists have tried to convert silicon and silicon-containing alloys into materials suitable for optoelectronics (optoelectronic-grade materials) by modifying the electronic band structure of silicon in different ways. Fadaly et al. make use of a strategy known as zone folding, which was originally outlined4 in the 1970s. The idea is that the presence of a periodic electric potential in an indirect-bandgap semiconductor could transform it into a direct-bandgap semiconductor. Until now, the best example of this approach was the production of a pseudodirect-bandgap semiconductor (which absorbs and emits light more efficiently than do indirect-bandgap semiconductors, but less efficiently than do direct-bandgap ones), in a special type of silicon–germanium alloy known as a superlattice, reported5 in 1992. This was achieved by alternating atomic layers that have different compositions of atoms, but the resulting material still couldn’t absorb or emit light efficiently enough for potential applications. Nearly three decades later, Fadaly and colleagues have taken a different approach. Instead of modifying the atomic potential by alternating layers of different composition, they alternate two types of atomic stacking in germanium and in silicon–germanium alloys. This changes the symmetry of the materials’ crystal lattices from a cubic form to a hexagonal one (Fig. 1). Figure 1 | Cubic and hexagonal crystal lattices. a, The silicon used for electronics has a cubic crystal lattice, which causes the material to be a poor absorber and emitter of light — limiting its use in optoelectronics. b, Fadaly et al.1 report a way of producing germanium and silicon–germanium alloys that have a hexagonal lattice. The resulting materials are good light absorbers and emitters, and would be compatible with existing silicon electronics technology, potentially opening the way to the development of new optoelectronic devices. The unit cell (the smallest repeating unit) of the hexagonal lattice contains twice as many atoms as the unit cell of the cubic form. This halves the size of the Brillouin zone — a unit cell of the abstract ‘momentum space’ that is used to describe the properties of electrons in semiconductors. This size reduction, in turn, results in the folding of the materials’ electronic bands in momentum space, moving the position at which the energy value of the conduction band is at a minimum to the centre of the Brillouin zone, and thus generating a direct bandgap. Fadaly et al. use quantum-mechanical calculations to determine the exact band structure of germanium and silicon–germanium alloys in the hexagonal crystal structure, thereby confirming that these materials have a direct bandgap. Most importantly, the authors demonstrate that hexagonal germanium behaves as an optoelectronic-grade direct-bandgap semiconductor. Moreover, by alloying hexagonal germanium with different amounts of silicon, they find that they can tune the energy of photons emitted from the resulting materials from 0.3 to 0.67 electronvolts. These emission energies are extremely relevant for chemical sensing and optical-communication technologies3,6.   All of this work was made possible by producing the materials in the form of nanowires, filamentary crystals that have a tailored diameter of between a few and a few hundred nanometres. In simple terms, the high surface-to-volume ratio of nanowires enables the formation of metastable crystalline phases such as hexagonal silicon or germanium7,8. Fadaly et al. are the first to report that defect-free hexagonal germanium and silicon–germanium alloys can be made in a scalable manner using this approach. The nanowire structure also offers another advantage: it causes light to interact with the nanowire material in a way that is ideal for photonic applications9,10. For example, nanowire shape can be engineered to ensure efficient light absorption and to prevent light from being trapped in the nanostructure by internal reflection. Nanowires could potentially also be used in light detectors for the ultra-rapid collection of the charge carriers produced from incoming photons, an effect that might be extremely useful for high-speed telecommunications. Fadaly and colleagues’ findings could potentially lead to the development of the first silicon-based laser, or be used to make mid-infrared light detectors, both of which would be compatible with the complementary metal-oxide semiconductor (CMOS) silicon technology that underpins much of the circuitry in computers. Such mid-IR detectors could be used in a scalable and economic lidar platform — a laser-based surveying technology that could be used by self-driving vehicles to detect objects. Mid-IR light does not damage the human eye, which means that mid-IR lasers could be used at high power in lidar systems; this enables object detection at long distances, thus allowing self-driving vehicles to travel safely at high speeds11. More broadly, the development of silicon-based alloys that have optoelectronic functionality could spark a second revolution in silicon technology, this time in silicon photonics. </body>
<date id = '35'>08 April 2020</date>
<url id = '36'>https://nature.com/articles/d41586-020-00322-y</url>
<title id = '36'>Visualization of the rhythmic oscillations of the mouse and human segmentation clocks, which are crucial to spine development, is now possible thanks to the development of sophisticated cell-culture systems.</title>
<body id = '36'>What do the flashes of a firefly and the chirpings of a cricket have in common? Both occur in a regular rhythm, which is controlled by an oscillating biological clock1. Another oscillating genetic clock controls the development of embryonic structures called somites, which give rise to the vertebrae that protect the spinal cord. Our knowledge of this segmentation clock stems almost entirely from research on animals2,3, because technical and ethical considerations limit the study of human embryos in culture. Writing in Nature, Diaz-Cuadros et al.4 and Matsuda et al.5 now report a breakthrough that enables studies of the human segmentation clock in vitro. In addition, Yoshioka-Kobayashi et al.6 use sophisticated techniques in mice to provide insights into the mechanisms that control the mammalian segmentation clock.   Somites arise from a tissue called the presomitic mesoderm (PSM). During somite formation, temporally and spatially controlled oscillations in transcription yield gene-expression waves that propagate through the PSM along the embryo’s head-to-tail axis. The result is a striped pattern of somites that forms the blueprint for the spine. Although the molecular components of the segmentation clock are highly evolutionarily conserved across vertebrates, new somites form with different rhythms in each species. For instance, gene oscillations have a period of 30 minutes in zebrafish and 2 hours in mice. Oscillations have been estimated to occur every 4 to 5 hours in humans2 — although until now they have never been directly observed. Diaz-Cuadros et al. and Matsuda et al. set out to model the human clock using induced pluripotent stem cells (iPSCs) — cells that are generated in vitro from differentiated human cells and, similarly to embryonic stem cells, can give rise to every cell type in the body. The groups used established protocols7–9 to convert iPSCs into PSM in vitro. To visualize and monitor the dynamic oscillations of clock genes in the cultured PSM in real time, each group used a different ‘reporter’ protein. Matsuda and colleagues used a reporter in which a key segmentation-clock gene10, Hes7, drives production of the bioluminescent enzyme luciferase. As Hes7 expression oscillates, levels of the reporter increase and decrease. Diaz-Cuadros et al. used an engineered version of Hes7 fused to a gene that encodes Achilles, which is a more rapidly generated variant of yellow fluorescent protein developed by Yoshioka-Kobayashi and colleagues. The use of Achilles enabled Diaz-Cuadros and co-workers to track fluorescent waves of Hes7 expression at the single-cell level4 — a resolution not possible with the luciferase reporter. Analyses using both reporters provide the first definitive evidence that the human segmentation clock has a period of approximately 5 hours (Fig. 1a). Figure 1 | Modelling embryonic segmentation in vitro. A tissue called the presomitic mesoderm (PSM) gives rise to somites — embryonic precursors of vertebrae. This process involves a ‘segmentation clock’ that drives rhythmic oscillations of gene expression, including that of the gene Hes7. Three groups have developed systems to analyse the clock in culture using live-cell imaging. a, Diaz-Cuadros et al.4 and Matsuda et al.5 directed wild-type (WT) human induced pluripotent stem cells (iPSCs) to become PSM cells. The iPSCs had been engineered to express a version of Hes7 that drives expression (arrow) of genes encoding the fluorescent molecule Achilles4 or the luminescent molecule luciferase5. Monitoring the oscillations of these genes in PSM cells revealed that the human segmentation clock has a period of about 5 hours. b, Matsuda et al. performed the same experiment using iPSCs in which Hes7 is mutated, as in the skeletal disorder spondylocostal dysostosis, and found a lack of oscillations. c, Yoshioka-Kobayashi et al.6 isolated the PSM from mouse embryos carrying a Hes7–Achilles reporter, and monitored oscillations, which have a 2-hour period. Three key signalling pathways — the Notch, Wnt and FGF pathways — act in sequential negative feedback loops to regulate oscillating gene expression during somite formation2,3,11,12. Diaz-Cuadros and colleagues used their culture system to investigate these pathways in detail. They confirmed the roles of these pathways in PSM cells taken from mouse embryos, and then showed that similar pathways govern segmentation in human PSM differentiated from iPSCs, with oscillations dependent on Notch signalling and another pathway, mediated by a protein called YAP. They found that FGF signalling not only determines the positions along the body axis at which oscillations stop, as previously reported2, but also regulates the complex dynamics of the oscillations — their period, phase and amplitude.   Matsuda and colleagues used their culture protocol to study a human genetic disease, congenital spondylocostal dysostosis, in which defects in segmentation of the vertebrae lead to skeletal anomalies13,14. The authors generated PSM from iPSCs derived from two people with the disease, who each had mutations in a different gene of the Notch signalling pathway. Surprisingly, despite these mutations and differences in overall gene expression, the authors observed normal oscillations in the PSM. By contrast, when the authors produced PSM from cells genetically engineered to carry a Hes7 mutation that had previously been identified as a cause of spondylocostal dysostosis15, they observed a dramatic loss of oscillations (Fig. 1b). This work highlights the potential of using iPSC-derived PSM to determine the relative roles of various clock components in development. It is known that, although individual PSM cells show autonomous oscillations, Notch signalling between cell neighbours synchronizes these oscillations1,16 to produce gene-expression waves at the population level. Yoshioka-Kobayashi et al. set out to examine this role for Notch signalling in detail. The authors engineered mice to carry a Hes7–Achilles reporter, and to lack a protein called Lunatic fringe that modulates Notch signalling. They then isolated the entire PSM from embryos that lacked Lunatic fringe and from controls that did not, and made use of optogenetics, a light-triggered gene-expression system, to visualize somite development in culture by tracking Hes7 oscillations over time (Fig. 1c). Although the autonomous oscillations of single PSM cells were unaffected by loss of Lunatic fringe, the researchers observed oscillation defects at the population level.   Notch signalling involves the release of the protein DLL1 from one cell and its binding by Notch receptors on another. This interaction triggers a downstream signalling cascade in the receiving cell that causes increases in the expression of various genes, including Hes117. This sender–receiver system can be modulated using a genetically engineered optogenetic variant of the Dll1 gene that is expressed in response to stimulation by light18. The authors stimulated Dll1, and compared how long it took for neighbouring cells to exhibit Hes1 upregulation in mice lacking Lunatic fringe with the time it took in controls. The study revealed that Lunatic fringe controls population-level oscillations by regulating the timing and amplitude of the signal-sending and signal-receiving process in adjacent cells. This work underscores the intricate role of Notch components in the cell–cell interactions that control clock oscillations. Together, the current studies provide a remarkable demonstration that simple iPSC culture systems can be used for in-depth analysis of the oscillatory gene expression associated with somite segmentation at single-cell resolution. However, they also have limitations. For instance, Diaz-Cuadros et al. and Matsuda et al. did not observe final stages of somite development and vertebra formation in their human culture systems. Nonetheless, their protocols will undoubtedly help to advance our understanding of the molecular basis of normal segmentation and to reveal the genes that, when mutated, lead to the development of disorders of the spine. More broadly, gene-regulatory networks are highly conserved between mammals, regardless of the animals’ size or whether they are bipedal or quadrupedal. This is in stark contrast to the species-specific timing of gene oscillations, which is fundamental to body-plan development. What causes these crucial differences in timing remains an enigma — but one that can now begin to be unravelled. </body>
<date id = '36'>01 April 2020</date>
<url id = '37'>https://nature.com/articles/d41586-020-00874-z</url>
<title id = '37'>In many neurodegenerative disorders, the spread of protein aggregates underlies disease progression in the brain. A receptor molecule has now been found that mediates the neuronal uptake of one such harmful protein.</title>
<body id = '37'>A key characteristic of many neurodegenerative diseases is the slow accumulation of misfolded protein deposits in the neurons of the brain. In particular, the accumulation and spread of a protein known as tau is a feature of several forms of dementia, ranging from the most common form, Alzheimer’s disease, to chronic traumatic encephalopathy, a dementia associated with repetitive head injuries. Writing in Nature, Rauch et al.1 provide a clue to how this harmful protein spreads: they identify a cell-surface receptor that enables tau to move between neurons.   In tau-associated forms of dementia, or tauopathies, disease progression correlates with the spread of tau deposits throughout the brain. This is thought to occur because of misfolded, disease-associated (pathological) tau entering healthy neurons. Pathological tau interacts with normal (physiological) tau already present in the neuron and acts as a template for the misfolding of the normal protein, thus propagating tau pathology across neuronal networks. There is therefore great interest in elucidating the mechanisms that allow pathogenic tau to exit one neuron and enter the next. The spread of pathogenic proteins throughout the brain is an active process, rather than simply the result of affected neurons dying, disintegrating and passively dispersing their contents2. The outer membranes of both the originating and the target cell need to be actively crossed, so that the misfolded tau can interact with physiological tau in the cytoplasm of the receiving neuron (Fig. 1a). Rauch and colleagues wondered whether a member of the low-density-lipoprotein receptor (LDLR) protein family — present at the neuronal surface — could hold the key to entry. Figure 1 | A surface receptor for tau uptake. Misfolded forms of the protein tau can propagate through neurons in neurodegenerative disease. a, Rauch and colleagues1 have discovered that the membrane-spanning protein LRP1 acts as a receptor for both normal (physiological) tau and for short strings (oligomers) of misfolded tau. Binding between LRP1 and tau leads to the uptake of tau into neurons and its spread through connected neuronal networks. Once internalized, tau resides in membrane-bound compartments called endosomes. How it escapes across the endosomal membrane into the cytoplasm is unknown. b, Rauch et al. show that an absence of LRP1 prevents tau transmission through the brain in live mice, potentially inhibiting the spread of pathology. (Physiological tau is generated in all neurons, and so is present throughout neural networks in spite of the lack of transmission.) The authors eliminated all LDLR-family members individually from neurons grown in culture. They showed that the loss of LRP1 specifically reduced tau internalization into neurons. Interestingly, this loss interfered with the internalization of all forms of soluble, physiological tau and of small aggregating clumps (oligomers) of pathological tau. This suggests that LRP1 could mediate the transfer of both physiological and pathological tau. (The transmission of physiological tau across neuronal networks has been described previously3, although its role is unclear.) Rauch et al. also found that the loss of LRP1 only partially blocked the uptake of larger ‘fibril fragments’ of tau. However, these fragments might also be taken up through less specific engulfing mechanisms that are known to occur in neurons4. Furthermore, the authors showed that tau competes with known LRP1 partners, including the lipid transporter ApoE, for binding to LRP1. They went on to map the areas of tau and LRP1 that interact, pinpointing two domains in the portion of LRP1 located outside the cell (its ectodomain), as well as a series of lysine amino-acid residues in tau that are exposed on the pathogenic protein. Targeting these residues using chemical inactivation prevented neuronal tau uptake, highlighting their importance. Findings from cell culture do not always translate to complex in vivo settings. To test the relevance of LRP1 in tau transmission across networks in the intact brain, the researchers went on to downregulate the expression of LRP1 in mouse brains and then to express mutant human tau in a defined brain region. This mutant tau readily spreads through the brains of wild-type mice. But the authors found that it remained highly restricted to the expression site in mice lacking LRP1 (Fig. 1b).   These exciting findings suggest that LRP1 indeed holds a key to tau transmission across intact brains. Of note, the authors limit their analysis to the detection of misfolded human tau in brain areas far away from the site at which the group had induced the protein’s expression, and analysed the mice at early stages of tau spread. In addition, there is no indication that the transmitted mutant tau in control brains is rich in β-sheets — a feature of tau deposits in degenerating brains. Indeed, there is also no evidence that neuronal health is adversely affected in these control brains. So the findings stop short of conclusively demonstrating a role for LRP1 in the spread of tau pathology, or, conversely, of demonstrating that downregulation of LRP1 can prevent the progression of pathology. Nevertheless, the identification of a receptor that allows tau access to neurons constitutes a major advance in our understanding of tau biology and its spread in the brain. It opens the door to detailed analysis of the intracellular trafficking and signalling events that are activated following the internalization of tau, under both normal and pathological conditions. This might help to unpick the poorly understood role for transmission of physiological tau across neuronal networks. This is directly relevant to efforts aimed at stopping tau spread using antibody therapies, because most of these approaches do not discriminate between physiological and pathological tau. The identification of LRP1 as a neuronal entry gate for tau will also enable direct investigation of the potentially different signalling pathways downstream of LRP1 that are activated by distinct tau species, giving insight into the earliest changes that occur in healthy neurons receiving pathogenic tau. Understanding these early events is crucial to finding strategies to combat devastating tau-related diseases before they cause irreversible damage in the brain.   LRP1 is widely expressed in the brain. In mice, loss of this receptor from neurons causes deficits in excitatory neuronal transmission5 and in motor function6. In addition, LRP1 has been suggested to have a role in clearing build-ups of the amyloid-β peptide7 (accumulation of which is associated with Alzheimer’s disease) and in repairing the myelin coat that insulates neurons8. So, although the current results might point to the potential of blocking LRP1 function, this intervention might not necessarily be therapeutically beneficial: the possible positive effect of stopping the spread of tau in the brain might be offset by defects in network function and increased amyloid deposition. Knowing the cell-surface receptor for tau, however, does allow for a mechanistic investigation of tau trafficking in the neuron. Following receptor-mediated internalization, tau resides in intracellular compartments known as endosomes, from which cargo normally gets degraded (in another compartment, the lysosome) or recycled back to the cell surface. It remains an open question how tau escapes the endosome to interact with and act as a template for the misfolding of native tau in the cytoplasm. A better understanding of this pathway might highlight options for rerouting internalized tau to be degraded or exported. Finally, Rauch and colleagues’ mapping of amino-acid residues in tau that allow it to interact with LRP1, in combination with emerging structures of different conformations of pathogenic tau9–11, could enable the design of molecules that target tau to combat its spread. Perhaps this work marks a first step towards preventing the progression of tau-related disease. </body>
<date id = '37'>01 April 2020</date>
<url id = '38'>https://nature.com/articles/d41586-020-00898-5</url>
<title id = '38'>The principle of mirror symmetry, which states that nuclear structure remains the same when protons are swapped for neutrons and vice versa, has been found to be broken in the lowest-energy forms of a mirror pair of nuclei.</title>
<body id = '38'>Nature likes symmetry. Examples range across size scales from macroscopic objects, such as spiderwebs or honeycombs, to the microscopic world with its arrangement of atoms in molecules, or of electrons around an atomic nucleus. Symmetry also exists at the level of nuclei, but in a paper in Nature, Hoff et al.1 report one way of breaking it.   Atomic nuclei are composed of two different types of particle — protons and neutrons —which, if we ignore the charge on the proton, resemble each other so much that they are often treated as a single particle, the nucleon. Mirror pairs of nuclei, in which the numbers of neutrons and protons have been exchanged, therefore have similar properties. In particular, the sequence of energies of a mirror pair’s nuclear states should be the same, from the ground state in which the nucleons are in the lowest possible energy level, to excited states of increasing energy2. A change in this sequence has, however, previously been observed for excited states of mirror partners3. Hoff and co-workers now report the breaking of mirror symmetry at the level of bound nuclear ground states (Fig. 1). They report that the ground states of the mirror partners bromine-73 and strontium-73 are not simply ‘mirror images’ in which protons and neutrons have been swapped, but have a different configuration of protons and neutrons. Figure 1 | Breaking nuclear mirror symmetry. a, In a pair of mirror nuclei, the number of protons in one nucleus equals the number of neutrons in the other, and vice versa. For perfect mirror symmetry, the nuclear structure and energy levels of the ground and excited states (shown schematically; dashed lines connect equivalent states) are essentially the same on swapping protons for neutrons, apart from a small overall shift caused by proton repulsion in the proton-rich nucleus. b, Hoff et al.1 report that the lowest-energy states of a mirror pair can have a different configuration of protons and neutrons; red dashed lines indicate that the lowest energy levels in one nucleus have swapped places compared with a. The cartoon illustrates a simple example of mirror symmetry and how it might be broken. How does this difference arise? The most basic building blocks of matter known today are quarks, of which there are six types. Protons and neutrons are both constructed from three quarks, and the most important difference between them is that their different quark combinations give the proton an electric charge of +1, whereas the neutron ends up neutral. The strong nuclear interaction that binds nucleons together in an atomic nucleus is essentially the same between protons and neutrons. For protons, however, the electric repulsion between identically charged particles adds together. When building two mirror-symmetric atomic nuclei, one with Z protons and N neutrons and the other with N protons and Z neutrons, this repulsion adds an extra global energy (mass) to the nucleus that has the more protons, but does not modify the arrangement of protons and neutrons. This symmetry explains why several of the properties of mirror partners are nearly identical: in their shape; their behaviour when excited (that is, when energy is added); and the properties of the decay processes through which unstable nuclei lose energy by emitting particles or radiation. To determine nuclear properties such as energy levels, energy is pumped into a nucleus (for instance, by colliding it with another nucleus), and the decay process in which γ-rays are emitted from the resulting excited nucleus is observed. The previously observed difference3 in the sequence of energy levels for the excited states of mirror partners occurred particularly at higher excitation energies, in which the density of states increases (that is, the neighbouring states come closer to each other). This difference of energy levels is a sign that mirror symmetry is only approximate and can be broken in particular circumstances.   A different structure in nuclear ground states has been observed4 previously for only one pair of mirror nuclei, nitrogen-16 and fluorine-16. In that case, however, one of the two partners (fluorine-16) is unbound — that is, the repulsion between protons outweighs the attraction from the strong nuclear force. It therefore decays rapidly by ejecting a proton in around 10–20 seconds5, comparable to the time it takes a nucleon to travel across the nucleus. However, nitrogen-16 is much more stable, with a half-life of about 7 seconds6. So the mirror difference there can be explained by the unbound nature of one partner. Hoff et al. reveal that the situation is different for bromine-73 and strontium-73, because both are long-lived and quasi-stable. To break mirror symmetry, nature had to play a trick: the ground states of these two nuclei are very close in energy to their respective first excited states. Mirror symmetry, being only an approximate symmetry, can therefore be violated by exchanging the ground and the first excited states in one of the two nuclei. The properties of bromine-73 have been well characterized for 50 years7, whereas information about strontium-73 is limited: we have a rough value for its half-life8, and know its strongest mode of decay9. The originality of Hoff and co-workers’ study is that the authors did not study the properties of strontium-73 directly, but through its two consecutive radioactive decays: the first decay occurs through the emission of β-particles and produces a particular state in the daughter nucleus, rubidium-73, which immediately decays by proton emission to produce krypton-72. The observed properties of the proton emission allowed the authors to deduce the structure of the proton-emitting state in rubidium-73, and, from this, the structure of the ground state of strontium-73. The results allowed a nuclear property known as spin to be characterized, and revealed something unexpected. The ground state of strontium-73 turns out not to have a spin of 1/2, as the ground state of bromine-73 does, but instead has a spin of 5/2, which corresponds to the first excited state of its mirror partner. Thus, mirror symmetry has now been shown to be broken in bound nuclear ground states. Is this breaking of mirror symmetry a disaster for our understanding of the structure of the atomic nucleus? Not at all. Deviations from expectations challenge our knowledge of nuclear structure, and allow nuclear scientists to fine-tune their models to describe atomic nuclei. As Hoff et al. show, the observed mirror-symmetry breaking might be triggered by the existence of two competing nuclear shapes, a prolate (rugby-ball) shape and an oblate (disk) shape. Both structures give the nuclei approximately the same energy and mass. These two shapes can mix, and the symmetry breaking in bromine-73 and strontium-73 might arise because there is a different degree of mixing in the two nuclei. It will be interesting to see whether other cases of ground-state mirror-symmetry breaking can be found. No other candidates seem to exist for nuclei that have similar numbers of nucleons to bromine-73 and strontium-73, because no nucleus is known for which the first excited state lies very close to the ground state. However, heavier nuclei are promising candidates. With more nucleons, more nuclear energy levels can be built, and the energy levels come closer together. By contrast, no mirror partners exist for nuclei whose mass number (the sum of the proton number and the neutron number) is greater than about 10010, because the nuclear interaction can no longer overcome the electrical repulsion associated with interactions between the protons in the ‘proton-rich’ mirror partner. The race is on to find more cases of broken mirror symmetry in nuclear ground states. </body>
<date id = '38'>01 April 2020</date>
<url id = '39'>https://nature.com/articles/d41586-020-00872-1</url>
<title id = '39'>An electrically neutral radical has been found to be a potent chemical reducing agent when excited by light. Remarkably, it is produced from a positively charged precursor that has long been used as a strong excited-state oxidizing agent.</title>
<body id = '39'>When molecules absorb light, they enter an excited state and become more reactive than when in their ground state. Light energy can therefore be used to generate reactive molecules that undergo chemical transformations that would otherwise be difficult to achieve. Several powerful oxidizing agents have been generated using light excitation, but strong reductants have been more difficult to produce. Writing in Nature, MacKenzie et al.1 report the discovery of a light-generated molecular species that exhibits reducing properties comparable to those of alkali metals — and which is therefore one of the strongest known chemical reductants.   Chemical reactions mediated by visible light are important tools in organic synthesis. These reactions occur analogously to light-driven biological processes such as photosynthesis — with the help of a light-absorbing catalyst. In photoredox catalysis2, an excited catalyst molecule exchanges a single electron with a reaction partner (the substrate). During this process, which is known as photoinduced electron transfer (PET), the substrate is transformed into a reactive free radical; this undergoes a subsequent reaction to give one or more final products. Such processes usually occur at ambient temperature because their energy barrier is overcome using light energy. Photoredox catalysis has undergone unprecedented development in the past decade, but some challenges remain. One is that no photoredox catalyst provides a reductant comparable in strength to that of alkali metals such as lithium and sodium. Alkali metals are still used in various reactions as potent reductants, despite their associated hazards and their tendency to produce undesired side products (that is, they have relatively low selectivity). One example of a photoredox reductive process is the generation of molecular species called aryl radicals, which, when organic compounds are being synthesized, can be used as a source of aryl groups (groups derived from a benzene ring or a benzene analogue by the removal of a hydrogen atom). Aryl halide compounds, in which an aryl group is attached to a halogen atom (chlorine, bromine or iodine), are preferred starting materials for generating aryl radicals because they are widely available and easy to handle. Aryl chlorides are the most preferred, but they are the most difficult aryl halides to reduce — as reflected by their highly negative reduction potentials. Reduction potentials quantify the tendency of a compound to acquire electrons from other compounds; for example, the reduction potential of chlorobenzene, a simple aryl chloride, is −2.78 volts relative to the potential of a saturated calomel electrode (SCE), a standard reference used in reduction-potential measurements3. It has not been possible to reduce aryl chlorides using a single PET process with visible light, because visible-light photons don’t have enough energy for the task. To reduce another compound, an excited photoredox catalyst must have an oxidation potential (a measure of its ability to lose electrons to other compounds) lower than the reduction potential of the compound to be reduced. 10-Phenylphenothiazine, for example, is one of the most strongly reducing photoredox catalysts when excited by light, but the oxidation potential of excited phenothiazine is only −2.1 V relative to SCE4 (versus SCE) — insufficient to convert chlorobenzene into aryl radicals, for instance. To overcome this problem, various systems have been reported that involve the use of two consecutive PET steps (see ref. 5, for example). In these approaches, a ‘sacrificial’ reducing agent reduces the excited catalyst molecule produced in the first step, forming a radical anion that is then excited by another photon. The resulting excited radical anion is a strong reducing agent. For instance, the excited radical anion formed from the catalyst Rhodamine 6G has an oxidation potential of −2.4 V versus SCE, which is sufficiently negative to reduce aryl bromides and aryl chlorides that have a reduction-facilitating group6. MacKenzie et al. now report an approach based on a salt that contains a mesitylacridinium ion (Mes-Acr+; Fig. 1). Mesitylacridinium salts have been used for almost two decades in photo-oxidation reactions7 — when irradiated by visible light, the resulting excited species is a potent oxidant that takes an electron from a substrate and is thereby converted into an acridine radical (Mes-Acr•). The electrically neutral radical is converted back to Mes-Acr+ by an oxidant for subsequent catalytic cycles. Figure 1 | An excited neutral radical acts as a potent reductant. The strength of chemical reductants is quantified by their oxidation potential, which is measured in volts relative to the potential of a reference electrode (such as a saturated calomel electrode; SCE). Two of the strongest reductants are the alkali metals sodium and lithium. Relatively strong reductants can also be produced from organic molecules in light-driven processes called photoinduced electron transfers (PETs), but the oxidation potentials are insufficiently negative for many reductions3. More-negative values can be achieved using two consecutive PET steps (see ref. 5, for example), or in electrophotoreduction processes that combine an electrochemical step with a PET step8,9. The mesitylacridinium ion (Mes-Acr+) can be converted into a radical (Mes-Acr•) when irradiated by light of wavelength 450 nanometres in the presence of a sacrificial reductant. Mackenzie et al.1 report that when this radical is irradiated by light of wavelength 390 nm, it produces an excited radical, (Mes-Acr•)*, that is a potent reductant. Me, methyl group; tBu, tertiary butyl group. The authors recognized that Mes-Acr• is a relatively stable species that absorbs light mainly from two ranges of wavelengths: 350–400 nanometres and 450–550 nm. They report that, when Mes-Acr• is irradiated with light of wavelength 390 nm, it forms an excited neutral radical that acts as an extremely strong reducing agent, with a maximum oxidation potential of −3.36 V versus SCE. They propose that this large negative value is the result of charge transfer within the excited radical. The use of an excited neutral organic radical is rare in photoredox catalysis. MacKenzie and colleagues formulated a reductive photocatalytic cycle based on Mes-Acr• using 390-nm light and a sacrificial reducing agent. This system can carry out several reduction reactions, such as the removal of tosyl groups from tosylated amine compounds (a type of reaction commonly used in organic synthesis; see Fig. 3 of the paper1). The researchers demonstrated that the new system is robust enough to work on scales that are useful for preparing compounds in the laboratory, by performing a detosylation reaction with 1.28 grams of a starting material. The same approach can also be used to replace bromine or chlorine atoms with hydrogen atoms in aryl bromides and chlorides, respectively — such reactions are known as dehalogenations (see Fig. 2 of the paper1). This procedure is possible when various groups are present in the substrates, and it even works with 4-chloroanisole, an aryl chloride that has a reduction potential of −2.9 V versus SCE.   Another approach for the catalytic production of strongly reducing species was reported simultaneously earlier this year in two papers from different groups8,9. In both cases, a neutral organic molecule acts as the catalyst; this is reduced electrochemically on a cathode to produce a radical anion, which is then excited by visible light to form a strong reductant with an oxidation potential more negative than −3.0 V versus SCE. These electrophotochemical systems were used to dehalogenate electron-rich aryl chlorides, and also in a series of arylation reactions (transformations in which an aryl group is attached to another molecule). The use of electrochemical reduction, instead of photochemical methods, to generate radicals allows catalysts to be used that do not absorb visible light. For example, naphthalene monoimide, a catalyst used in one9 of the two papers, falls into this category and cannot undergo the initial conversion to a radical anion using visible light. By contrast, once it is transformed electrochemically into a visible-light-absorbing radical, it can enter a photocatalytic cycle. MacKenzie and colleagues’ observation of the strong reductant character of excited neutral Me-Acr• will inspire investigations into whether other molecules show similar behaviour. One can also expect increased interest in other photocatalytic approaches for the production of reductive systems10–13. Taking into account the highly negative oxidation potentials observed for various light-generated agents in the current work and by other research groups, we can look forward to new arylation reactions, and even to ambitious applications such as the Birch reduction14 — a classic synthetic reaction typically performed using alkali metals. </body>
<date id = '39'>01 April 2020</date>
<url id = '40'>https://nature.com/articles/d41586-020-00873-0</url>
<title id = '40'>Psychological stress can trigger physiological responses, including an increase in body temperature. A neural circuit that underlies this stress-induced heat response has been identified.</title>
<body id = '40'>You are about to take the stage to speak in front of a large audience. As you wait, your heart starts to pound, your breathing quickens, your blood pressure rises and your palms sweat. These physiological responses are evolutionarily conserved mechanisms to prepare your body to fight against imminent dangers, or to run away quickly. Another key response is an increase in body temperature. Emotional stress can cause this psychogenic fever in many mammalian species, from rodents to humans1,2. What is the neural mechanism that underlies this phenomenon? Writing in Science, Kataoka et al.3 describe a key neural circuit in psychologically induced hyperthermia. The current work builds on a long legacy of research by the same group, who began their quest for a neuronal circuit that triggers heat production in 2004, using brown fat tissue as an entry point4. Brown fat is a type of ‘good’ fat that can generate heat when needed. Blocking the activity of β3-adrenergic receptor proteins, which are abundant in brown fat and enable the tissue to respond to signals from neurons, attenuates stress-induced hyperthermia5. In the 2004 study, the researchers injected viral ‘retrograde tracers’ into brown fat in rats; the tracers move through connected neurons, allowing the authors to identify brain regions from which neurons project to the fat4. This revealed that neurons in a brainstem area called the rostral medullary raphe (rMR) connect to brown fat. Later on, the same group identified2 the dorsomedial hypothalamus (DMH) as a key brain region upstream of the rMR. When the authors artificially activated the DMH-to-rMR pathway, they found an increase in neuronal activity and heat production in brown fat. Unexpectedly, activating this pathway also increased heart rate and blood pressure, suggesting that DMH–rMR could coordinate various physiological responses during stress.   In humans, psychological stress often involves an understanding of complicated situations, and thus probably requires instructions from regions of the brain’s cortex, which is involved in cognition. In the current study, Kataoka et al. set out to identify the cortical regions that could send these instructions to the DMH. As in their previous work, the authors used retrograde tracers — this time, injected into the DMH — to look for neurons that link into their heat-generating circuit. They found that only one, little-studied, region of the cortex was strongly labelled by the tracer. This region, called the dorsal peduncular cortex and dorsal taenia tecta (DP/DTT), is also highly active in rats in the wake of social defeat (a hostile interaction in which the animal has lost a fight with another, dominant rat). To examine the role of this region in stress responses, the authors impaired its connection to the DMH in three ways. They blocked activity throughout the DP/DTT using a chemical inhibitor; they used a virus to kill cells projecting from the DP/DTT to the DMH; and they used a sophisticated genetic approach to inhibit activity specifically in the projections that DP/DTT neurons send to the DMH. In each case, their intervention reduced stress-induced hyperthermia. By contrast, artificial activation of the neuronal projections between the two regions elicited a battery of responses, including increases in heart rate, blood pressure, and heat production in brown fat. The group provided evidence that the DP/DTT neurons send excitatory signals to the DMH, and demonstrated that the projections from the DP/DTT terminate close to the DMH cells that, in turn, project to the rMR. Taken together, Kataoka and colleagues’ experiments support the idea of a DP/DTT–DMH–rMR–brown fat circuit for heat production in response to stress (Fig. 1). Figure 1 | Stressful connections. Kataoka et al.3 report that, in rats, a brain region called the dorsal peduncular cortex and dorsal taenia tecta (DP/DTT) is involved in psychogenic fever — an increase in body temperature in response to social stress. Stress-related information reaches the DP/DTT from two other brain regions: the paraventricular (PVT) and mediodorsal (MD) thalamic nuclei. Neurons from the DP/DTT then project to and excite neurons in the brain’s dorsomedial hypothalamus (DMH), which in turn sends neuronal projections to the rostral medullary raphe (rMR). Finally, neurons from this region connect indirectly to brown fat tissue, which generates heat. How does the stress-related information reach the DP/DTT? Further retrograde tracing experiments revealed that the strongest inputs to the DP/DTT are from the brain’s midline thalamic regions, including the paraventricular (PVT) and mediodorsal (MD) thalamic nuclei. The PVT is highly sensitive to various physical and psychological stressors, such as predator cues and pain6. By contrast, the MD interacts with the prefrontal cortex to mediate complex cognitive functions, such as rule learning, abstraction, evaluation and (in humans) imagination7. Thus, every possible stressor, from physical pain to anticipated legal trouble, can find their way to the DP/DTT. It remains unclear, however, how different stressors are encoded in the DP/DTT, whether the responses of the DP/DTT to stressors are influenced by experience, and whether deficits in DP/DTT cells could be responsible for abnormal physiological responses to stress. Future studies using electrophysiological or optical recordings of the DP/DTT cells will help to address these questions.   The philosopher and psychologist William James suggested that fear is an interpretation of physiological responses to threat, instead of the other way around8. In other words, rather than running from a bear because we are afraid, we are afraid because we are running from a bear. If James is right, rats should stop being afraid if their physiological responses to a threat are blocked. Kataoka et al. therefore asked whether inhibiting the DP/DTT–DMH pathway can suppress the fear that a rat shows when presented with an aggressive, dominant counterpart that has recently defeated it in a stressful social interaction. Under normal conditions, a defeated animal will try to stay away from the aggressor to avoid incurring further damage. By contrast, naive animals that have not previously gone through a social defeat show no signs of fear, and investigate the dominant rat with great interest. Remarkably, when the authors blocked the DP/DTT–DMH pathway in rats that had been defeated, the animals behaved like naive rats. Thus, the behavioural manifestation of fear, and perhaps the perception of fear (which can only be inferred from behaviours in rats), depends on bodily responses to threat. These data provide an indication of why taking a deep breath before that big public speech might help to calm us down. The data also suggest that suppressing physiological responses to stress could be an effective way to alleviate stressful feelings. Of importance in this context, non-stress-related thermoregulation — changes in internal temperature caused by infections or external temperature change, for instance — is mediated, not by the DP/DTT, but by another region upstream of the DMH, the preoptic area9. Blocking the DP/DTT–DMH pathway would therefore be expected to leave day-to-day regulation of temperature unchanged. It is early days, but manipulation of the DP/DTT could potentially be a way to curb chronic psychological stress. </body>
<date id = '40'>30 March 2020</date>
<url id = '41'>https://nature.com/articles/d41586-020-00819-6</url>
<title id = '41'>Clinicians use ultrasound videos of heartbeats to assess subtle changes in the heart’s pumping function. A method that uses artificial intelligence might simplify these complex assessments when heartbeats are out of rhythm.</title>
<body id = '41'>The heart is a specialized muscle that contracts rhythmically around its closed chambers to propel blood. However, this pumping function fluctuates throughout the day as the circulating blood flow adapts to the body’s ever-changing metabolic demands1. Understanding the variations in cardiac pump activity with each heartbeat might have relevance for explaining the intricacies of heart function in health and disease. However, the tools for scrutinizing such changes remain imprecise. Writing in Nature, Ouyang et al.2 report the development of a computational platform that uses an artificial-intelligence (AI) approach to assess cardiac ultrasound video and to provide continuous, beat-by-beat measurement of cardiac pump function.   Clinicians commonly assess cardiac function using a value termed the ejection fraction, which is the percentage of the blood volume in the left heart chamber (the left ventricle) that is pumped out when the heart contracts. In a normal heart, just over half of the blood is ejected; thus, the calculated ejection fraction is more than 50%. Highly trained physicians can ‘eyeball’ ultrasound video loops of a beating heart and make a precise estimate of the ejection fraction3. However, if two isolated frames from the video were presented, showing only the beginning and the end of the ejection, even a trained physician would struggle to estimate the ejection fraction. Given that training and expertise vary from person to person, eyeballing is not relied on, and the ejection fraction is calculated by tracing the boundaries of the left ventricle on a digital image to estimate the blood volume at the beginning and end of ejection. It is recommended4 that clinicians estimate the ejection fraction of a heart by tracking it over three to five heartbeats; however, in typical clinical practice, often just one beat is assessed. If the accuracy of estimates of ejection fraction could be improved by having an easy way to routinely determine its precise value by tracking and averaging several heartbeats, this would be of immense benefit, particularly for people whose hearts are beating out of rhythm (a condition termed arrhythmia). If arrhythmia occurs, the changing duration of heartbeats alters the volume of blood filled and ejected from the left ventricle, thereby resulting in variations in the ejection fraction (Fig. 1). This variability makes the ejection fraction challenging to estimate for a type of arrhythmia known as atrial fibrillation. It is predicted5 that this condition will affect between 6 million and 12 million people in the United States by 2050, and 17.9 million in Europe by 2060. Moreover, ejection fraction needs to be assessed frequently in people who have atrial fibrillation, because heart failure (a state characterized by a poor ability of the heart to pump blood) occurs in more than one-third of such individuals6. And more than half of people with heart failure have atrial fibrillation6. Figure 1 | A computational approach for assessing cardiac pump function over several heartbeats. Ouyang et al.2 report the development of a method that uses artificial intelligence (AI) to monitor a standard clinical measurement of cardiac function. This measurement is called the ejection fraction (EF), and it is the percentage of the volume of blood in the left ventricle (one of the heart’s chambers) that is pumped out of the chamber when the heart contracts. The authors developed a system that can analyse ultrasound video frames to determine the EF by continuously comparing changes in the borders of the left ventricle over space and time, using an AI architecture called a 3D convolutional neural network. a, Regular heartbeats have a fairly uniform EF. b, By contrast, irregularities (arrhythmic heartbeats) might result in shorter heartbeat cycles and a lower EF. The automation of heartbeat assessment by the use of AI (rather than depending on a clinician to monitor this) would make it easier to track several heartbeats. The averaging of EF over multiple heartbeats would provide a better heartbeat assessment for a person with arrhythmia than would be the case for the measurement of a single heartbeat. To develop an AI-based method for assessing ejection fraction, Ouyang et al. used 10,030 cardiac ultrasound videos. These videos were stored along with images containing human-generated tracings that marked the inner border of the left ventricle at the beginning and end of the ejection cycle. The authors used a type of AI architecture called a convolutional neural network (CNN), first to perform a semi-automatic detection of a pattern of pixel-based information (segmentation) to recognize the left ventricle in the video frames; and second, to track the borders of the ventricle during the heartbeat cycle. Using CNN architecture to find the left-ventricle border in ultrasound images is not new7,8, but the innovation here is that Ouyang and colleagues evaluated new forms of three-dimensional CNN. This enabled them to integrate recognition of the left-ventricular border (the spatial information) from the 2D display in single video frames with the changes over time (the temporal information), to determine the information needed regarding the moving heart border. Forms of 3D CNN have been used previously in realms as diverse as general video analysis9,10, assessment of human physical activity11 and medical imaging12. However, Ouyang and colleagues’ work is, to our knowledge, the first attempt to take this approach in analysing cardiac ultrasound information over such a strikingly large number of videos. After Ouyang and colleagues had ‘trained’ the 3D CNN using the video data, they compared the AI-generated estimates for the ejection fraction with human-measured ejection fractions. Their 3D CNN method estimated the ejection fraction with a mean error of 4.1% and 6%, respectively, for two different sets of data used for validation. In other words, on average, using the authors’ proposed 3D CNN method, the ejection fraction was estimated to within 95.9% and 94%, respectively, of the corresponding ejection-fraction measurement reported by a clinician. These reported AI errors are substantially lower than those reported in previous attempts to use CNN to estimate the ejection fraction7, and are well within the inter-observer variability in ejection-fraction measurements between experienced clinicians3. Ouyang et al. then tested a further 55 patients for whom 2 ultrasound specialists separately assessed the heartbeat videos. The authors found that when the variability in the human- and AI-generated ejection-fraction estimates for each patient was compared, the 3D CNN method produced results with the least variability in the ejection fractions noted between the two recorded measurements. Furthermore, results obtained with the 3D CNN were extremely consistent across different ultrasound machines, and for measurements taken on different occasions. These results also indicate the importance of assessing the kinetics of cardiac-wall motion in developing a system for gauging cardiac function. Several avenues of possible future work building on this research should be explored. Efforts to reduce the overall computational burden would be welcome, so that the technique could be performed inexpensively and instantaneously during an ultrasound examination. Ouyang and colleagues’ approach required 0.05 seconds per video frame, which they reported as being faster than the estimation speed of human experts. However, this is not yet as fast as real time, which would be less than 0.02 seconds per video frame (for a rate of 64 frames in 1.28 seconds). A careful look at the different stages in the overall architecture of the 3D-CNN deep-learning approach will be needed to determine the best architecture for use in existing cardiac-ultrasound technologies, such as 3D echocardiography and ultrafast cardiac ultrasound. Moreover, the choice of computational approach for handling videos that contain suboptimal images, or those in which the image quality has been improved by the injection of image-enhancing agents, will need to be considered.   This tool for the continuous assessment of cardiac pumping has the potential to affect other areas of cardiology. For example, such an approach might be adapted to monitor ultrasound changes in ejection fraction in people undergoing complex medical procedures, such as catheter-based cardiac interventions, surgery or when receiving medication or mechanical circulatory support for a condition termed acutely decompensated heart failure. Furthermore, the use of 3D CNN to track other parameters that are more sensitive than ejection fraction for determining early changes in cardiac function (such as physical measures of heart-muscle deformation or changes in cardiac shape or geometry) that develop before a person shows disease symptoms might lead to new ways of measuring or identifying cardiac biomarkers (hallmarks of disease)13–17. Such automated approaches might be particularly relevant for the burgeoning ‘multi-omics’ approaches for data integration that incorporate different layers of biological information to define different stages of cardiac dysfunction18. In this regard, we applaud the authors for making available to the research community a large data set of annotated ultrasound videos (presented stripped of information that could identify the individuals). This resource will be extremely useful, and will probably spur yet more innovations in automated analysis that will boost our understanding of cardiac function. Moreover, such steps will be needed to achieve greater consistency in results obtained using different imaging systems for assessing cardiac function (such as cardiac ultrasound, computed tomography and magnetic resonance imaging). The ongoing efforts to improve the accuracy of automated measurements and disease prediction will, undoubtedly, ultimately free up extra time for physicians, enabling them to provide higher-quality clinical care and have better interactions with patients. Given the high health-care burden of cardiovascular disease worldwide, Ouyang and colleagues’ work is timely, and hints at an ensuing technological revolution that could have a profound effect on risk prediction of cardiovascular disease and on routine clinical decision-making. </body>
<date id = '41'>25 March 2020</date>
<url id = '42'>https://nature.com/articles/d41586-020-00787-x</url>
<title id = '42'>The Antarctic ozone hole shifted the jet stream in the Southern Hemisphere poleward, leading to hemisphere-wide climatic changes. But the Montreal Protocol, which banned ozone-depleting substances, has halted the shift.</title>
<body id = '42'>The discovery1 of a hole in the springtime atmospheric ozone layer over the Antarctic in the mid-1980s revealed the threat posed by human-made ozone-depleting substances (ODSs): the damage caused by these compounds exposes people and Earth’s ecosystems to harmful ultraviolet radiation. A related, unexpected effect was revealed in the early 2000s, when studies2,3 showed that the Antarctic ozone hole, which resides at altitudes of around 10–20 kilometres, has affected atmospheric circulation all the way down to the surface in the Southern Hemisphere — most notably, by shifting the summertime jet stream poleward. The production and use of ODSs was banned by the Montreal Protocol of 1987 and its subsequent amendments. Atmospheric ODS concentrations are therefore decreasing, and the first signs of ozone-layer recovery have emerged4,5. Writing in Nature, Banerjee et al.6 report that the hole-associated circulation effects have paused since ozone recovery started.   Stratospheric ozone absorbs ultraviolet solar radiation, and the absorbed energy heats the stratosphere, the atmosphere’s second-lowest layer. Consequently, ozone depletion and the related lack of heating cool the stratosphere — indeed, the Antarctic springtime stratosphere cooled by about 7 °C between the late 1960s and late 1990s as a result of the ozone hole2. This cooling increased the north–south temperature gradient between the southern mid-latitudes and the Antarctic, which strengthened the stratospheric westerly winds in the Southern Hemisphere and, in turn, caused a poleward shift of the jet stream in the troposphere (the lowest layer of the atmosphere). It is not obvious why changes to stratospheric winds affect circulation in the underlying troposphere. The stratosphere represents only about 15% of the atmospheric mass, and therefore, when in motion, has much less momentum than the troposphere. Yet observations and computational modelling confirm that the tropospheric jet stream is sensitive to changes in stratospheric winds at monthly, seasonal and decadal timescales, and that the cooling of the polar stratosphere is associated with a poleward shift of the tropospheric jet stream7. By the end of the twentieth century, the summertime tropospheric jet stream had shifted by about 2° of latitude, altering the transport of atmospheric heat and moisture. This contributed to warming of the Antarctic Peninsula, Patagonia and New Zealand, and to drying of western Tasmania and western New Zealand, and affected the circulation, temperature and salinity of the Southern Ocean8,9. A cessation of the tropospheric-circulation trends since the beginning of the twenty-first century has previously been noted10, but Banerjee and colleagues are the first to formally attribute it to the effects of the Montreal Protocol. The authors had to overcome several difficulties to demonstrate that the poleward shift has paused, and to attribute the pause to changes in stratospheric ozone levels. First, the natural year-to-year variability of atmospheric circulation in the mid- and polar latitudes is large, which makes it challenging to detect atmospheric-circulation trends in those regions11. Therefore, even though no statistically significant poleward shift has been observed since 2000 (as Banerjee and co-authors report; Fig. 1), this is the first time that the lack of statistically significant trends has been shown to represent a genuine pause in the shift. Figure 1 | Changes in the summertime jet stream in the Southern Hemisphere. a, The jet stream in the Southern Hemisphere forms a near-circle around Antarctica in the troposphere — the lowest layer of the atmosphere. b, Banerjee et al.6 have reanalysed data that record the position of the summertime jet stream during the past few decades. The grey line shows the average position determined from their analysis; the envelope around the line indicates the minimum to maximum range. The blue line indicates the underlying trend, and shows that the jet stream shifted from about 49° S to 51° S between 1980 and 2000 — the years when the stratospheric ozone layer over Antarctica was becoming depleted. The trend alters after 2000, when the ozone layer began to recover as a result of the Montreal Protocol, which banned ozone-depleting substances; the position of the jet stream did not alter significantly during this period. Banerjee and colleagues show that ozone recovery is the cause of the pause in the jet-stream shift. (Graph from Fig. 1b of ref. 6.) Another difficulty is working out which of the many factors that simultaneously affect atmospheric circulation is responsible for the pause. Banerjee and co-authors addressed this problem by carrying out multiple computational simulations using climate models. One group of simulations aimed to reproduce the observed climate change, and thus accounted for all known factors: anthropogenic factors such as increasing greenhouse-gas concentrations, and changes in atmospheric levels of ozone and ODSs; and natural factors, such as volcanic eruptions and solar cycles. These model simulations faithfully reproduced observed climate variability, including the poleward shift of the jet streams during the ozone-depletion period (from the late 1970s to 2000), and cessation of the shift thereafter. Further experiments were conducted in which only one or a few factors at a time were included in the models. This allowed Banerjee and colleagues to estimate the contribution of each factor to the circulation trends. They found that only simulations that accounted for changes in ODS levels and associated changes in ozone concentration reproduced both the poleward shift of the jet streams during the depletion period and the subsequent pause. Increases in greenhouse-gas concentrations, by contrast, pushed the tropospheric jet poleward during both ozone depletion and recovery periods. Other factors, such as changes in aerosol levels or solar variability, did not have a statistically significant effect on the simulated trends. Banerjee et al. also showed that their results are relatively insensitive to the details of the model formulations. For example, some models included atmospheric chemical reactions and explicitly calculated the amount of ozone depletion and recovery from ODS emissions. However, these models did not include interactions between the ocean and the atmosphere, and therefore would miss any ocean–atmosphere feedbacks that might affect the tropospheric trends of interest. Another set of models did include ocean–atmosphere interactions, but did not include interactions between atmospheric chemistry and physics, and thus could not reproduce any chemistry–climate feedbacks. Regardless of the nuances of the models, a similar picture emerged: the tropospheric circulation trends stopped when ozone recovery started.   So, what are the implications of these findings? First, the results show that — at least, during the past 20 years — ozone recovery exerted a strong enough force on the tropospheric circulation to overcome the opposing effect of greenhouse-gas increases. This is a crucial contribution to the long-standing debate about the relative role of these two factors in past and future circulation trends12. There is, however, no guarantee that the effects of ozone will dominate these trends in the future. As ozone levels continue to recover, their rate of change and the associated influence on climate will weaken, increasing the relative role of greenhouse-gas increases, especially in ‘business-as-usual’ scenarios in which nothing is done to abate future greenhouse-gas emissions. Such emissions increases might therefore dominate future tropospheric-circulation changes, and push the jet stream back towards the pole. Second, the findings add to the evidence that stratospheric changes can affect the climate in the troposphere. This is important because, despite decades of research, the exact mechanism of stratosphere–troposphere coupling is poorly understood7. The lack of understanding does not compromise Banerjee and colleagues’ results, but more research is needed to increase confidence in future climate-change projections. Finally, the authors’ results provide a clear signal that human actions can affect Earth’s climate: the Montreal Protocol has paused the climate change associated with ozone depletion. This is an object lesson in how the international community should react to global environmental challenges. Restricting dangerous emissions and changing business practices is also the way to combat global warming caused by greenhouse gases. </body>
<date id = '42'>25 March 2020</date>
<url id = '43'>https://nature.com/articles/d41586-020-00818-7</url>
<title id = '43'>Studies have pointed to a link between colon cancer and a gut bacterium that produces DNA-damaging molecules. The discovery of a mutational signature linked to these bacteria in human colon cancer supports this association.</title>
<body id = '43'>Understanding what causes colorectal cancer (CRC) could help to combat this disease of the colon. Writing in Nature, Pleguezuelos-Manzano et al.1 report evidence that strengthens a previously suspected connection to a type of gut bacterium. The authors implicate this microbe by pinpointing bacterial ‘fingerprints’ in DNA alterations found in CRC cells.   Certain bacteria produce genotoxic molecules — those capable of damaging DNA. These molecules can cause mutations if, for example, mistakes occur during the DNA-repair process that tries to fix genotoxic damage. In 2006, a genotoxin called colibactin, which is made by certain strains of the gut-dwelling bacterium Escherichia coli, was discovered2. That original description also shed light on how colibactin is produced by E. coli, identifying a key region of bacterial DNA, termed the pks island (the microbes that have this island are called pks+ E. coli), which encodes various components of an ‘assembly line’ that makes colibactin. By producing colibactin, pks+ E. coli can accelerate tumour formation in animal models3. Moreover, these bacterial strains are more prevalent in close association with the epithelial cells in the mucosa of the colon in people who have CRC than in those who don’t3. However, the complexity of the colibactin-producing assembly line and the molecule’s considerable instability pose substantial challenges to researchers trying to decode the workings of the pks island and to characterize colibactin’s structure. There are several questions to be answered. For example, what is the mechanism of action of colibactin? What types of change might it make to DNA nucleotides? And does colibactin activity have relevance to human cancer? It is known4 that pks+ E. coli damages the DNA of cells it infects by causing adenine nucleotides to undergo a type of modification called alkylation. Subsequent evidence proposing a symmetrical colibactin structure indicates that the molecule has two ‘warheads’ made of a structure called cyclopropane, which target adenines5. How common pks+ E. coli is in the gut of human populations is not fully known. To determine the details of DNA changes that might be induced by pks+ E. coli, Pleguezuelos-Manzano and colleagues turned to a ‘mini-gut’ cellular system that mimics the human intestine (Fig. 1). This approach uses a clump of human epithelial cells grown in vitro called an organoid or, specifically, a colonoid, because it is made of colon cells. The authors exposed colonoids either to pks+ E. coli isolated from a person with CRC or to an engineered version of the bacterium that did not make colibactin. This set-up enabled the bacteria to interact with the type of cellular surface they would encounter in the lumen of the colon. Whole-genome sequencing of colonoid cells enabled the authors to compare the mutations in cells exposed to E. coli that produced colibactin or that were defective in producing it. Figure 1 | Specific DNA mutations that can be induced by a bacterium also occur in human cancer. Pleguezuelos-Manzano et al.1 report the discovery of types of DNA mutation caused by a gut-dwelling bacterium. a, The authors used a model of the human gut consisting of epithelial cells grown in vitro and exposed these cells to Escherichia coli bacteria that make the molecule colibactin (such microbes are termed pks+ E. coli). Colibactin modifies DNA at adenine (A) nucleotides. This results in DNA damage, and errors in the repair process lead to mutations. Pleguezuelos-Manzano et al. identified specific types of mutation induced by colibactin. One type was single-base substitutions (called SBS-pks), typically identified as the replacement of a thymine nucleotide with a different nucleotide, leading to a different base pair at that site. Another type was insertion or deletion mutations (ID-pks), characterized by the deletion of thymine (T) nucleotides in a string of thymines. b, Using whole-genome sequencing data already available for cells from human colorectal cancer (CRC), the authors analysed the mutations present, and identified a subset of tumours that had mutations characteristic of colibactin exposure. This raises the question of whether these mutations arose because of exposure to pks+ E. coli. From this analysis, the authors determined a unique colibactin mutational signature — specific patterns of DNA alterations that arose in the presence of colibactin. This signature predominantly included two types of change. One type is the substitution of a single DNA nucleotide base for a different nucleotide (single-base substitutions, termed SBS-pks). These were skewed towards a change described as T→N, in which a thymine (T) nucleotide is replaced by any other type of nucleotide (N). The other type of change is a small insertion or deletion of nucleotides, characterized by deletions of single nucleotides in stretches of thymine nucleotides (known as T homopolymers). This sort of alteration is termed ID-pks. Interestingly, both SBS-pks and ID-pks occur preferentially downstream of adenine nucleotides, consistent with the proposed mode of action of colibactin, with two warheads targeting adenine nucleotides that are located in close proximity on opposite strands of the DNA (one warhead targets an adenine upstream of the site of damage and the other targets the site of damage)4,5. To determine whether this colibactin-associated mutational signature might be relevant to human disease, the researchers analysed a data set6 of whole-genome sequences for 496 human CRC tumours that had migrated from their primary site in the colon to form secondary growths termed metastases. Remarkably, the authors found that SBS-pks and ID-pks mutations were present in 7.5% and 8.8%, respectively, of CRC metastases, which is more frequent than in metastases of cancers of other primary origins. For example, SBS-pks and ID-pks mutations were found in 2.1% and 4.2%, respectively, of metastases of urinary-tract cancers, and in 1.6% and 1.6%, respectively, of head and neck tumour metastases. This pattern is consistent with the probability of exposure to pks+ E. coli at these different body sites, considering that the urinary tract, head and neck are only occasionally exposed to E. coli. When the authors assessed 2,208 predominantly primary CRC tumours from an independent data set (see go.nature.com/3d6utsx), 5.0% and 4.4% of the tumours, respectively, had high SBS-pks and ID-pks signatures, which supports the idea that pks+ E. coli are involved in the early stages of tumour formation. Further incriminating evidence is the observation that 2.4% of the most common CRC ‘driver mutations’ (those implicated in having a causal role in the disease), and 5.3% of mutations in the gene APC, which is frequently mutated in CRC, match the SBS-pks or ID-pks target motifs. Interestingly, SBS-pks- and ID-pks-like mutational signatures that are thought to arise from a mutational event in early childhood are found in the ‘crypt’ region of healthy human colons7, suggesting that exposure to pks+ E. coli early in life might be a causative event that contributes to cancer formation. Together, these findings depict a potential pathway by which a genotoxic bacterium could contribute to the development of cancer. Pleguezuelos-Manzano and colleagues’ work fills an important gap in our knowledge regarding the mutational consequence of exposure to pks+ E. coli in the context of cancer formation. However, some issues should be considered before sounding the alarm bells on pks+ E. coli, conducting screening tests to identify people who have these bacteria or initiating procedures to eradicate the microbes.   Of note, one widely used probiotic strain of E. coli (a strain used with the aim of promoting health) is the Nissle 1917 strain that carries the pks island and requires the strain’s DNA-damaging activity for its inflammation-attenuating activity in animal models8. This probiotic strain has a long history of safety in its use for human treatment9, which suggests that the cancer-inducing activity of pks+ E. coli is context-dependent. Although the mini-gut system is a powerful tool for investigating interactions between bacteria and human cells, it bypasses numerous other layers of interaction in the gut, such as the mucus barrier, cells of the gut immune system, and other types of microbe. Any of these components might influence the ability of pks+ E. coli to attach to epithelial cells — an absolute requirement for colibactin to have a genotoxic effect2. Indeed, mice colonized with pks+ E. coli rarely develop colon tumours in the absence of factors such as inflammation3,10, cancer-initiating mutations3,10,11 or other cancer-promoting bacteria11. The fact that pks+ E. coli are found in approximately 67% of people who have CRC3, yet the mutational signature was observed in only about 5% of the primary CRC tumours as shown by Pleguezuelos-Manzano et al., supports the concept of context-dependent genotoxicity. It will be crucial to define the interplays between human and bacterial cells and between different bacterial cells that might lead to conditions that would permit cancer formation mediated by pks+ E. coli. In addition, determining the evolution of this mutational signature in relation to tumour development over time in animal models would strengthen the evidence supporting a direct cause-and-effect relationship. This work also raises the question of what mutational signatures might be induced by other microbial genotoxins. For example, a toxin called cytolethal-distending toxin, which is produced by certain Campylobacter bacteria, damages DNA and promotes colon-tumour development in mice12. Comparing the mutational signatures arising from colibactin and other microbial genotoxins13 might offer a clearer picture of the mechanisms that can drive cancer formation and the potential synergistic effects that might arise. Moreover, obtaining a comprehensive catalogue of the distinctive DNA ‘scars’ left by encounters with various microorganisms might be useful for tracking down possible culprits that drive tumour formation. Pleguezuelos-Manzano et al. have highlighted the potential contribution of colibactin to cancer formation in humans, thereby adding further evidence that the community of gut microorganisms contains members with possible cancer-promoting properties. There is growing interest in manipulating gut bacteria for therapeutic purposes, and one could consider investigating strategies against pks+ E. coli strains in humans once the causal link to disease is firmly established. </body>
<date id = '43'>23 March 2020</date>
<url id = '44'>https://nature.com/articles/d41586-020-00809-8</url>
<title id = '44'>In 1980, a method was found to determine the amount of carbon dioxide in ancient air trapped in polar ice — providing direct evidence that CO2 is coupled to climate, and affects global temperatures in the past, present and future.</title>
<body id = '44'>In the ancient ice sheets near Earth’s poles, tiny bubbles trapped in the compacted layers of snowfall provide a natural archive of air from ages past, but one that is not straightforward to read. Forty years ago, the glaciologist Robert Delmas and colleagues developed a technique for reliably measuring the amount of carbon dioxide in the bubbles1. Their work paved the way for modern measurements showing that atmospheric CO2 levels have been linked to Earth’s temperature for hundreds of thousands of years.   Towards the end of the nineteenth century, Swedish scientist Svante Arrhenius became interested in the causes of Earth’s ice ages. Through calculations, he estimated how changes in CO2 levels might affect the atmosphere’s ability to absorb heat from the ground, rather than allowing it to escape into space. He suggested that the cooler temperatures during glacial periods resulted from a decrease in this absorption2. This greenhouse theory was an alternative to the theory backed by James Croll, who proposed3 that glaciation cycles arose from changes in Earth’s orbit, amplified by natural feedbacks such as changes in snow cover. Another scientist, Milutin Milankovitch4, refined this idea by looking at fine variations in Earth’s movements and the resulting changes in insolation (incoming sunlight) at different latitudes. His orbital theory gained support from marine sediment records5, which exhibit periodicities similar to Earth’s orbital parameters. Thus, the orbital theory was promoted as a pacemaker of the ice age, with various feedback loops amplifying tiny changes in insolation. In the 1960s, air inclusions in ice cores (Fig. 1) drilled from the depths of glaciers sparked interest as natural samples of the ancient atmosphere. To quote researchers at the time6: “if the CO2 concentration were found to vary in time, one would have the means for testing the well-known greenhouse theory of climate change.” Attempts were made during the 1960s and 1970s to obtain a reliable atmospheric CO2 record from gas extracted by melting ice samples (‘wet extraction’). But results showed unexplained high concentrations of CO2, sometimes ten times that of today’s atmosphere, and scattered over a bafflingly wide range of values. Figure 1 | Ice record. This thin section of glacier ice, visualized using polarized light, contains ice crystals and trapped bubbles of ancient air.Credit: Vladimir Lipenkov The answer to the puzzle came from studies of the chemistry of polar ice. At Grenoble in France, Delmas was researching the acid rain that was decimating northern European forests. Suspecting that the industrial sulfur emissions thought to be responsible might be recorded in Greenland ice, he and his students Michel Legrand and Jean-Marc Ascencio developed a technique7 for measuring ice acidity, adapted to be exceptionally sensitive to impurities and to avoid bias from ambient atmospheric CO2. They discovered that, even without volcanic or human activity, the ice was tinged with sulfuric acid arising from sulfur compounds emitted by plankton. If an ice sample contained carbonaceous dust, melting the ice would allow the background acid to react with the carbonate. This, they thought, might be producing extra CO2 in analyses of ice-core bubbles and confounding the CO2 measurements. To test their idea, Delmas and colleagues set up a dry-extraction system that avoided melting the ice — instead, the ice was crushed under vacuum in a vessel at –40 °C. They analysed selected samples from two Antarctic ice cores (Dome C and D10), which contained very few carbonates. For ice from the past 10,000 years, they found CO2 levels comparable to then-current atmospheric values (around 300 parts per million; p.p.m.), with good reproducibility. For ice deposited during the last glacial period, around 20,000 years ago, levels were much lower, around 190 p.p.m. (Fig. 2a), thus confirming Arrhenius’s prediction. Figure 2 | Ice-core records past and present. a, Forty years ago, Delmas et al.1 found a way to measure ancient atmospheric carbon dioxide levels from bubbles trapped in ice. Their results show that a dip in CO2 concentrations coincided with the temperature drop associated with the glacial period around 20,000 years ago. Top: red dots show the authors’ CO2 measurements in parts per million (p.p.m.) and the corresponding depth of the ice; the dashed red line represents the CO2 concentration in 1980, the time of the study. Bottom: blue line indicates oxygen isotope ratios18 (a proxy for temperature) expressed as the quantity δ18O. b, Similar methods have now produced CO2 records for much longer periods of time from deep ice cores. Here, CO2 results from Antarctic ice march in step with temperature cycles (temperature anomaly represents the difference between the actual temperature and the mean temperature of the past millennium) over the past 800,000 years, revealing how strongly this greenhouse gas is coupled to Earth’s temperature13. At around the time that Delmas et al. carried out their work, a Swiss group, using a refined version of wet extraction to analyse ice cores, also suggested8 that CO2 levels were depleted during the last glacial period. But the dry technique subsequently became widely adopted because of its greater reliability and accuracy. Delmas and co-workers’ paper thus paved the way for today’s more extensive comparisons of climate and CO2 concentrations. A milestone was reached in 1987 with dry analyses of an ice core more than 2 kilometres deep from the Vostok site in Antarctica. This provided a 160,000-year record9–11 of temperature and CO2, depicting a complete climate cycle in which CO2 levels varied from 290 p.p.m. during the warm period to 190 p.p.m. in the cold period, and correlated with temperature. Over the following decades, the climate–CO2 correlation was demonstrated over a period of 400,000 years from extended Vostok records12, and then as far back as 800,000 years13 (Fig. 2b) from ice drilled at another Antarctic site, EPICA Dome C. So Delmas and colleagues’ pioneering work covering a few tens of thousands of years sparked research that now gives us an accurate record of atmospheric CO2 levels undulating with glacial–interglacial climate cycles for several hundred thousand years. This close correlation attests to the tight coupling between climate and carbon cycles during the past 800,000 years, and has stimulated climate scientists to consider them to be part of the same global system.   Four decades of ‘dry’ sampling of ice-core bubbles have contributed to our current understanding of the succession of glacial–interglacial cycles. Lying somewhere between the initial suggestions of Arrhenius and Croll, these cycles are seen as the combined effects of orbital variations and greenhouse gases, amplified by a series of natural feedbacks. Insolation changes driven by Earth’s orbital and axial oscillations (Milankovitch cycles) set the timing of the glacial cycles. But greenhouse gases (mainly CO2), together with the growth and retreat of the northern ice sheets, affect the radiation balance between absorbed sunlight and energy escaping back to space, thereby driving the size and shape of the changes. The reliability of Antarctic ice as a natural sampler of past air has been confirmed by a reconstruction of atmospheric CO2 levels over the past millennium14, which covers pre-industrial CO2 levels (280 p.p.m.) and which partly overlaps with the period when direct atmospheric measurements were made. Caution should be taken in analyses of ice from Greenland, where Delmas noted that windborne carbonaceous dust could cause in situ CO2 production15. This is not the case for Antarctic ice, especially farther inland; but because outer parts of ice cores are always slightly contaminated by dust (from handling, storage or the atmosphere), dry extraction is needed even when analysing Antarctic samples. Last but not least, we know that current atmospheric CO2 levels (407 p.p.m. in 2018; see go.nature.com/2j4heej) are probably unprecedented for the past 800,000 years16. One of the most intriguing puzzles in palaeoclimate science is why the periodicity of the climate cycle has changed: marine sediment records show that climate followed a 40,000-year cycle before 1.2 million years ago, but changed to 100,000-year cycles during the Mid-Pleistocene Transition, about 1 million years ago. Delmas and colleagues’ legacy can be seen in an ongoing international Antarctic project that might shed light on this17. Known as ‘Beyond EPICA Oldest Ice’, it aims to extend the ice record to 1.5 million years ago by drilling down to 3 kilometres, to investigate greenhouse-gas levels and climate at that time. </body>
<date id = '44'>23 March 2020</date>
<url id = '45'>https://nature.com/articles/d41586-020-00765-3</url>
<title id = '45'>Interfaces between the tiny crystal grains that make up solid copper have been shown to change from one ordered phase to another, independently of the phase adopted by the crystals, opening up prospects for materials development.</title>
<body id = '45'>Some solid metals exist in various structural forms, a phenomenon known as polymorphism. Iron, for example, adopts one type of cubic lattice (α-iron) at room temperature, but transforms into another (γ-iron) above 912 °C. This is an example of a phase transformation — an abrupt change in the atomic structure of a material that occurs during a gradual change in temperature or pressure. The transformation of γ- to α-iron that occurs when iron alloyed with carbon is rapidly cooled from a high temperature has long been used to produce hard and strong steels; by contrast, pure iron is soft and ductile. Writing in Nature, Meiners et al.1 report that polymorphic phase transformations can also occur at the interfaces between the tiny crystals that make up most pure metals. This discovery suggests a fresh approach for processing metallic materials to optimize their properties for applications.   The vast majority of solid metals and their alloys are polycrystals — assemblies of billions of minuscule single crystals called grains, which are separated by grain boundaries. These boundaries are often the ‘weak links’ that cause a material to be brittle and to fracture. However, they can also be used to strengthen materials, because some grain boundaries efficiently block dislocation glide (a fundamental mechanism of plasticity in metals that involves the movement of lattice defects). About a century ago, it was thought2 that grain boundaries were amorphous layers about one micrometre thick. However, modern microscopy tools have revealed3 that the distorted atomic structure in these boundaries is only a few ångströms thick, comparable to interatomic distances. It is also now known that the atomic structures of most grain boundaries can be thought of as periodic arrangements of certain atomic structural units4 — that is, the grain boundaries can be thought of as two-dimensional crystals that have their own atomic structure, which is very different from the structure of the grains that they separate. A key question is whether these 2D structures undergo phase transformations that are unrelated to those in adjoining grains. For alloys that consist of two or more components, the answer is a resounding ‘yes’. Such phase transformations have been extensively characterized in experiments, and have been described by theoretical and computational models5,6. The situation was less clear for pure metals: indirect evidence7,8 hinted that phase transformations are possible at grain boundaries in pure tin and copper, but no direct observations had been made. Observing changes in the atomic structure of a grain boundary is a daunting task, because only minor displacements of atoms at the boundary are required to change its structure, and the atoms move much faster in the boundary than in a grain9. Meiners et al. now report that grain-boundary phase transformations occur in pure copper. The authors studied several grain boundaries in thin copper films that had been deposited on sapphire substrates, under ultra-clean conditions to exclude any potential effects of impurities. Using a scanning transmission electron microscope, they directly imaged the positions of columns of atoms in the thin-film samples. Their atomic-resolution images reveal the coexistence of two distinct atomic structures in two grain boundaries that had similar geometric parameters, as would be expected during phase transformations. The authors refer to these structures as domino and pearl phases, on the basis of the patterns formed by the atoms within them (Fig. 1). Figure 1 | Dominos and pearls. Most metals are assemblies of billions of tiny single crystals, called grains, which are separated by grain boundaries — layers of atoms that have a different periodic arrangement from that of the grains themselves. Meiners et al.1 used high-resolution microscopy techniques to investigate grain boundaries in thin copper films deposited on sapphire, and observed two coexisting phases (pictured), which they named domino and pearl phases. The authors also observed these phases in atomic computer simulations (not shown) of a grain boundary that had the same geometry as the ones in the films. They conclude that phase transformations occur in the grain boundaries of copper, and therefore probably also in those of other pure metals. Scale bar, 1 nanometre. (Image taken from Fig. 1 of ref. 1.) However, this observation alone does not constitute proof of a phase transformation, because one of the two phases could be in a highly unstable state that formed during the deposition of copper film, and that was preserved in the solid film on cooling. The authors therefore obtained further proof using a machine-learning approach (an evolutionary algorithm) in atomic computer simulations of a grain boundary that had the same geometry as the experimentally observed boundaries. The simulations showed that the pearl phase corresponds to the lowest-energy state of the grain boundary, whereas the domino phase is in a metastable state. The simulations also showed that the metastable domino phase is stabilized when stress is applied perpendicularly to the plane of the simulated grain boundary, so that its energy matches that of the stable pearl phase — thereby establishing a true thermodynamic equilibrium between the two phases.   Meiners and colleagues’ work clearly proves that phase transformations occur in the grain boundaries of pure metals, and thus opens up fresh opportunities for materials design. The number of possible polymorphs of bulk metals is generally limited, but the variety of grain-boundary structures and their possible metastable polymorphs (sometimes referred to as complexions6) is essentially boundless10,11. One can therefore envisage a processing technique that optimizes the overall performance of a material by producing grain-boundary phases (either stable or metastable) that maximize the positive effects of boundaries, but minimize their negative effects. For example, if one could produce grain-boundary polymorphs in aluminium that efficiently block dislocation glide (to maximize mechanical strength) and minimize electron scattering (minimizing electrical resistivity), the resulting metal would be a ‘dream material’ for making wire conductors in overhead power lines — eliminating the need for more-expensive aluminium-based composite wires. However, it remains to be seen whether the full potential of engineering phase transformations at grain boundaries can be realized in practice. One reason is that it is not clear how processing methods could be designed that produce desired grain-boundary phases. Moreover, a similar concept known as grain-boundary engineering12 — the use of processing methods to obtain grain boundaries that have a desired geometry and properties, without using phase transformations — has so far yielded only modest practical results. Another issue is that the large number of possible grain-boundary polymorphs will make it difficult to systematically determine polymorph properties. High-throughput computational methods based on machine learning and big data will be of help here1,13. Indeed, Meiners and colleagues’ work is a promising example of how the synergistic combination of high-resolution microscopy techniques and computational methods can lead to conceptual breakthroughs in the study of grain boundaries. </body>
<date id = '45'>18 March 2020</date>
<url id = '46'>https://nature.com/articles/d41586-020-00640-1</url>
<title id = '46'>Satellite tracking of marine predators in the Southern Ocean has revealed key ecological areas under disproportionate pressure from human activities. These results show the value of tracking data for informing conservation efforts.</title>
<body id = '46'>Even the most remote marine ecosystems on Earth — such as those at high latitudes, including in the Southern Ocean around Antarctica — can no longer be considered pristine1. The effects of humans on marine ecosystems now have a global footprint2–4, and mitigation of associated threats requires knowledge of the areas of particular ecological and biological significance. Such areas sustain the healthy functioning of marine ecosystems and should therefore be protected. Writing in Nature, Hindell et al.5 report analyses of tracking data for marine species that reveal these key areas in the Southern Ocean. The waters of the Southern Ocean encircle the Earth through the Drake Passage, the ocean region between the tip of South America and Antarctica. Because of this passage, the Southern Ocean has a key role in global climate and ocean circulation6. This ocean is also home to a unique range of marine fauna, including many charismatic predators, such as penguins (Fig. 1) and seals, as well as the precious Antarctic krill (Euphausia superba). These krill are at the base of the marine food web, and, alongside species of toothfish (Dissostichus eleginoides and Dissostichus mawsoni), are the target of the largest fishing industries in the Southern Ocean7,8. The fisheries compete with animals for food resources, and fishing activities along with the pressures from climate change are raising concerns about the possibility of ecosystem collapses there8,9. Figure 1 | Emperor penguins (Aptenodytes forsteri) in Antarctica. Hindell et al.5 report analyses of tracking data for marine predators, including this penguin species. The authors’ results pinpoint regions of the Southern Ocean around Antarctica that should be protected.Credit: Tui De Roy/Minden Pictures/National Geographic The Commission for the Conservation of Antarctic Marine Living Resources is the main management body for the Southern Ocean, and is tasked with ensuring the preservation of this ecosystem. To succeed, the commission needs to take precautionary steps, including the establishment of more and better-designed marine reserves as has been suggested8, and sites for these should be chosen on the basis of knowledge of the whereabouts of ecologically significant marine areas10. However, accurately defining these areas in a highly dynamic, changing environment is challenging. Monitoring predators at the top of a marine food web can help with this task. Such predators migrate within and between ecosystems, and can be used as indicator species11 — those able to provide information on the status of an ecosystem or habitat if alterations occur in their movement patterns, behaviour or reproductive success. In particular, tracking top predators can assist with identifying the areas that they use most, which can be considered as regions of great ecosystem importance, not only for the predators but also for a wide range of other species11. Indeed, tracking data are increasingly being used to inform conservation policy around the world12, and have been used to quantify the extent of spatial overlaps between species and fishing activities globally3.   Hindell et al. report analyses of tracking data from 4,060 individuals of 17 species of marine predators (seabirds and mammals), and suggest a way to use such data to predict key ecological regions in the Southern Ocean. Tracking data were collected between 1991 and 2016 using electronic tags attached to the animals. These tags provided location estimates (obtained using satellite information or other methods) as the animals migrated. The authors used some of these data (for 2,823 individuals) to develop predictive models to identify crucial habitats in the Antarctic region for all of the predator species combined. These integrated results provide a spatially defined assessment of areas of high biodiversity that includes species across multiple levels of the food chain (termed trophic levels) in the Southern Ocean. Defining a single, integrated result from such varied data sets and from so many species is a complex undertaking. Predators in the Southern Ocean include a large range of species from across different taxonomic groups. These include species living in the Antarctic region and species residing immediately north of it (in the sub-Antarctic), all with different diets and patterns of movement. The authors used a series of data-processing steps to generate a value they termed ‘habitat importance’, which they predicted using data across all of these species together (assemblage-level maps). To do this, Hindell and colleagues first mapped habitat importance for the species living in the Antarctic separately from those living in the sub-Antarctic, and then selected the maximum habitat-suitability values in those two maps to generate an overall assemblage-level map for all of the predator species combined. Finally, the authors defined the regions in the top 10% of their calculated habitat importance value as the areas of the most ecological significance in the Southern Ocean. This final step was a central part of their study. It enabled comparisons to be made between the areas of ecological significance and the areas affected by human activities, as well as between the levels of existing protection inside and outside these areas.   Hindell and co-workers report that the predicted areas of ecological significance they identified match the ocean regions of known elevated productivity for Antarctic krill13 and for other organisms at the base of the food web, including myctophids (lanternfish)14. This result is consistent with the idea that marine predators can be used as indicators to identify areas of ecological significance. The authors report the particularly striking finding that a disproportionately higher level of human pressures (fishing and the effects of climate change) occurred inside rather than outside the areas identified as being of ecological significance. On the basis of this, the authors recommend that the current network of protected marine areas in the Southern Ocean be extended. They confirm that these extensions should include the areas for which protection is already being planned. It would have been interesting if the authors had suggested how an approach similar to theirs could best be used to tackle comparable problems on a global scale. For example, the authors’ views on the best strategy for contributing scientific knowledge to inform efforts to protect biodiversity on the high seas (the waters outside national jurisdictions) would have been a valuable addition. This topical issue is currently being discussed by the United Nations General Assembly, and negotiations are under way to develop an international legally binding solution to address the problem15. Scientists have tracked marine predators for decades3,4,12. It is time to pool all these existing data sets to address pressing conservation challenges on a global scale. To succeed, a worldwide movement is needed within the community of animal-tracking researchers, to drive the sharing of these data and to combine them with information about human activities at sea. Combining such information will deliver much-needed evidence of the extent of existing threats, to inform managers and policymakers in a timely manner. As Hindell and colleagues state, the Southern Ocean has the potential to provide an example of how “science, policy and management can interact to meet the challenges of a changing planet”, and their work highlights a pathway for how best to direct policy efforts. </body>
<date id = '46'>18 March 2020</date>
<url id = '47'>https://nature.com/articles/d41586-020-00728-8</url>
<title id = '47'>A zinc-sensing ion channel, Hodor, has now been found in the intestine of fruit flies. Hodor activates the TORC1 signalling pathway, and in doing so, influences organism-wide growth and metabolism.</title>
<body id = '47'>Cells and organisms must sense nutrients in their surroundings and adjust internal conditions in response. An abundance of nutrients (such as sugars, fats and amino acids) triggers programmes that lead to proliferation, whereas a scarcity of nutrients blocks growth and often results in a redistribution of internal resources. Metal ions such as zinc (Zn2+), iron and copper are a subset of nutrients called micronutrients, and act as cofactors for proteins that have roles in growth and development1. But how organisms sense metal availability is unclear. Writing in Nature, Redhai et al.2 report the identification of a Zn2+ sensor in flies. Characterization of this protein reveals a pathway for Zn2+-dependent control of food intake and growth.   A plethora of proteins rely on Zn2+ to carry out their functions. As a result, extensive cellular resources are devoted to ensuring that Zn2+ concentrations in cells are kept within an optimal range. Notably, many DNA-binding proteins require Zn2+, including some that coordinate the production of proteins that themselves help to balance metal levels. Thus, a cellular feedback loop keeps Zn2+ levels in check. Proteins that shuttle Zn2+ into or out of the cell are part of this feedback mechanism, along with those that transport Zn2+ between intracellular compartments3. Much of our understanding of Zn2+ regulation comes from studying fruit flies, because genetic, biochemical and metabolic analyses are relatively straightforward to perform in this model organism3. However, few studies have moved beyond investigating how intracellular Zn2+ regulation helps to maintain steady physiological conditions to ask whether and how Zn2+ abundance can affect organism-wide programs for growth and development. A specialized set of cells that absorbs copper and iron ions has been identified in the fruit-fly gut4. Redhai et al. showed that Zn2+ also amasses in the region of the gut in which these cells reside. The researchers downregulated the expression of the genes that encode 111 putative nutrient-sensing proteins in the specialized cells of this region, and identified one protein whose downregulation caused a delay in fly development. In reference to this delay, they named the protein Hodor (short for ‘hold on, don’t rush’). The authors showed that mutation of Hodor, a transmembrane protein, leads not only to a reduction in growth of the fruit-fly larvae, but also to a diminished body-fat content and to lower food intake throughout the flies’ development. They demonstrated that Hodor is not a Zn2+ transporter, but instead behaves as a Zn2+-regulated channel that, when activated by Zn2+ binding, allows chloride ions (Cl−) to cross plasma membranes.   Zn2+-regulated Cl− transport could affect metabolism in multiple ways. For instance, Cl− influx could alter the concentrations of intracellular solutes or the acidity of membrane-bound organelles called lysosomes that have key roles in waste disposal and regulatory signalling in the cell5. In line with the latter idea, the authors found high levels of Hodor on the membranes of lysosomes, and observed a loss of lysosomal acidification when they downregulated Hodor in flies. How can a single protein, which is expressed in only a small subset of cells, have such profound effects on the physiology of a whole organism? Redhai et al. made several observations that could help to explain the broad effect of Hodor. First, increasing the Zn2+ content of flies’ diets led to increased feeding; this effect was abrogated by depleting Hodor. Second, growth-promoting insulin-like peptides (ILPs) built up in the brain of Hodor-depleted larvae. When ILPs are activated, they are secreted from the brain; the authors’ observation therefore suggests that Hodor is required for the activation of insulin signalling after feeding. Third, Hodor acts upstream of a protein complex that is integral to growth regulation: target of rapamycin complex 1 (TORC1). Mutations that would usually cause hyperactivation of TORC1 signalling instead restored normal growth and food intake in Hodor mutant flies. Taking this evidence together, Redhai and colleagues propose a model whereby Zn2+-dependent Hodor activity in the mid-gut drives TORC1-dependent metabolic programs that enable larval feeding and growth. That Hodor acts through TORC1 signalling is not surprising, although it is difficult to draw conclusions about the exact nature of the link between the two. In a similar way to insulin in mammals, ILPs are potent activators of TORC1 in flies6. In turn, TORC1 is a prime driver of metabolic processes and growth in all animals. At the cellular level, TORC1 activation occurs on the lysosomal membrane and requires lysosomal acidification7. Thus, loss of Hodor might impair TORC1-driven growth through loss of insulin signalling, loss of lysosomal acidity, or both (Fig. 1). Figure 1 | Complex control of fly feeding and growth by zinc ions. Redhai et al.2 have identified a membrane-spanning protein, Hodor, that senses zinc ions (Zn2+) in the guts of fruit flies. Following binding by Zn2+, Hodor enables passage of chloride ions (Cl−) into organelles called lysosomes in the cell. Activation of Hodor leads, through unknown mechanisms (perhaps involving a signalling factor), to the release of insulin-like peptides (ILPs) in the brain. ILPs activate the protein complex known as target of rapamycin complex 1 (TORC1), which is enriched on lysosomes. Furthermore, activation of Hodor causes a Cl− influx into the lysosome; this has an acidifying effect that also stimulates TORC1. Activation of TORC1 triggers signalling pathways that might have organism-wide effects, including promotion of feeding and growth. The link between Hodor and TORC1 is not the only avenue for further research opened up by the current study. Another question concerns the relationship between feeding behaviour and TORC1 signalling, which is currently only partially characterized. ILPs are secreted from the brain in response to feeding and stimulate TORC1, placing TORC1 signalling downstream of feeding5. However, Redhai and co-workers’ observation that increasing TORC1 activity restores growth and feeding behaviour in Hodor mutants, in which ILP secretion seems to be impaired, suggests that the picture is more complex. TORC1 might act both upstream of feeding (in the brain) and downstream of it (in the gut and other tissues). How exactly does activation of Hodor by Zn2+ stimulate feeding and ILP release? It seems reasonable to suppose that a factor secreted from the gut in response to Hodor activation might affect the neuronal circuits that control feeding in the brain8. But identification of such a factor will require more work. Finally, Hodor belongs to the family of Cys-loop channels, which have been a target of efforts to develop insecticides9. Redhai et al. provide evidence that Hodor is expressed only in insects, and show that mosquitoes engineered to lack the hodor gene die at larval stages. Given the protein’s gut-specific expression, the authors suggest that ingestible substances could be laced with drugs that block Hodor activity, and these substances could be placed at known larval breeding sites. Thus, Redhai and colleagues’ study could have broader implications than might have been anticipated in the hunt for a micronutrient sensor. </body>
<date id = '47'>18 March 2020</date>
<url id = '48'>https://nature.com/articles/d41586-020-00766-2</url>
<title id = '48'>A newly discovered 66.7-million-year-old fossil bird excavated in Belgium provides us with the best evidence so far for understanding when the living groups of birds first evolved and began to diverge.</title>
<body id = '48'>The living groups of birds are amazingly diverse, numbering some 10,000 species, and endlessly fascinating. But when did they first evolve? The answer depends on how you define a bird, how you recognize the most basal (earliest diverging) birds in the fossil record, and how you account for the palaeontological and genetic gaps in our knowledge of bird evolution. Writing in Nature, Field et al.1 report the discovery of a fossil bird from 66.7 million years (Myr) ago, during the Late Cretaceous period, the most recent point in time currently known when the first representatives of today’s birds evolved. In the process, the authors offer what might be a useful corrective to genetics-based estimates of the timing of bird diversification.   Most narratives of bird evolution begin with the pioneer Archaeopteryx, which first took wing in the Late Jurassic period (some 150 Myr ago) in present-day Germany2. Archaeopteryx is a bird in the broad sense of the term — it had a full complement of feathers and flew by flapping its wings — but it’s far from having the hallmarks needed to group it with members of any living birds. Archaeopteryx has features that are so unspecialized that they don’t prevent it from being the ancestor of all later bird groups, but they don’t tell us whether living birds arose from this exact lineage. When considering the relationships between any large group of organisms, such as birds, a key split on the evolutionary tree (Fig. 1) is the distinction between what are called crown-group and stem-group members3. For birds, the crown group includes all living birds (from ostriches to warblers, including quail, gulls, finches, woodpeckers, crows and all their relations), plus all descendants of their most recent common ancestor (that is, all the ancient ostriches, warblers and relatives of the other living bird groups). By contrast, the stem-group birds are those placed outside the living groups of birds but nevertheless still closer to them than they are to other major related groups, such as extinct dinosaurs: in other words, birds from Archaeopteryx (the most basal known bird) up to, but not including, living bird groups. The question is whether Field et al. are reporting on another stem-group bird or the first well-established crown-group bird, and what the age of their fossil discovery tells us about the timing of avian evolution. Figure 1 | An evolutionary tree for birds. Birds can be divided into crown-group birds (all living birds plus all relatives of their most recent common ancestor) and stem-group birds, which fall outside this group but are closer to it than they are to other major related groups, such as the dinosaurs ancestral to birds. Fossils of stem-group birds include specimens of Archaeopteryx, Enantiornithes, Hesperornithes and Ichthyornithes. Such stem-group creatures had wings, but lacked some hallmarks of crown-group birds. Field et al.1 report the discovery of a 66.7-million-year-old crown-group fossil bird that they call Asteriornis maastrichtensis. This fits on the tree near Anseriformes (duck- and goose-like birds) and Galliformes (chicken- or quail-like birds), but the fossil remains are insufficient to determine whether it is closer to the Galliformes than to the Anseriformes, or whether it is outside the group formed by Galliformes and Anseriformes. Regardless of this, the fossil reveals that the duck and chicken lineages, together with the mostly flightless birds called ratites (such as ostriches) plus other living bird lineages, had evolved by at least 66.7 million years ago. Earlier examples of crown-group birds are, as yet, unknown. All available evidence indicates that birds evolved from a group of carnivorous dinosaurs called theropods during the Jurassic period (about 200–145 Myr ago), and that bird flight had evolved by then, at least considering Archaeopteryx2. Through the Cretaceous period (145–66.5 Myr ago) there was considerable evolutionary experimentation in the early offshoots of bird lineages (in such diverse groups as the Enantiornithes, Hesperornithes and Ichthyornithes)4. But these ancient birds are outside the crown group because they lack the structural and physiological features characteristic of living birds. These stem-group birds seem to have grown much like small dinosaurs had done ever since the Triassic period (about 250–200 Myr ago) — faster than typical reptiles but slower than today’s birds, reaching maturity in a few years, on the basis of examination of their bone tissues5. However, sometime during the latest Cretaceous, a stem-group lineage of birds evolved that had much higher growth rates than these more basal lineages, and that generally matured within a year or even sooner2,5. These became the crown-group birds. Their relationship to the closely related stem-group birds remains fuzzy, partly because fossil birds are usually rare and poorly preserved4. This issue underlies the importance of the fossil bird reported by Field and colleagues. Although some previously discovered specimens have helped to pinpoint the ultimate origin of living birds, the authors’ discovery is the best evidence yet of when and how the first known crown birds evolved. Field and colleagues named their Belgian fossil specimen Asteriornis maastrichtensis. It is from the latest Cretaceous period, and was relatively small (with an estimated body weight of about 400 grams).   The remains are confined to an excellently preserved skull and some other fragmentary bones (see Fig. 1 of ref. 1), which is enough to establish not only that it is a crown-group bird, but also that it is an early member of the group of land birds called Galloanserae. Think of this group informally as ‘poultry’: chickens, quail and the like (Galliformes), plus ducks, geese and so on (Anseriformes). Asteriornis seems to be a basal member of this group and possibly (Fig. 1) on the chicken–quail branch (Galliformes), but the available information is too scarce and fragmentary for scientists to be sure. Most other known ancient specimens of fossil bird are either well-established members of living groups or outside this crown group entirely4. The position of Asteriornis on the bird family tree is particularly interesting because, among crown birds, the first major branch that diverged from the others (Fig. 1) is the ratites (most of which are flightless birds, including ostriches)6,7 — so Asteriornis is, if you’ll forgive the expression, well nested within the crown group of birds. What does the age of this Asteriornis fossil tell us about the timing of avian diversification? It’s only one bird, so it doesn’t reveal much about that. But we know that this specimen was on the scene about 200,000 years before the end of the Cretaceous. That establishes that crown-group birds evolved before the end of the Cretaceous, but perhaps only barely before then. This is something of a corrective to the conventional estimates of the earliest origin and diversification of living bird groups based on molecular phylogenetic analyses, which have proposed estimates of this divergence timing ranging from 139 to 95 to 89 Myr ago (to mention just a few such studies)8–11. If these values seem all over the place, let’s remember that such studies usually analyse changes in DNA sequences for a few genes and assume that the rate of molecular evolution is relatively constant over time. The very effort that such work takes to resolve these evolutionary divergences with such indirect molecular evidence is heroic. Why should palaeontological evidence be ‘smarter’ than molecular evidence in cases such as this? Fossil remains are a crucial test of molecular-based projections of evolutionary divergence times. If a molecular study estimates a given age of origin for a group, then fossils from the corresponding timeframe should have the diagnostic features that identify those groups. If fossils from the time of interest provide no evidence of the expected newly evolved features, then the molecular projections are not supported. The evidence for Asteriornis reported by Field and colleagues implies that crown-group birds first evolved when the Cretaceous period was nearly over. That places a strong constraint on hypotheses for basal divergence times, but there will always be more fossils to find. </body>
<date id = '48'>18 March 2020</date>
<url id = '49'>https://nature.com/articles/d41586-020-00641-0</url>
<title id = '49'>Cell death by a process called apoptosis inhibits inflammation in surrounding tissue. The finding that dying apoptotic cells release a tailored cocktail of metabolite molecules reveals a way in which they influence their living neighbours.</title>
<body id = '49'>“Marley was dead, to begin with. There is no doubt whatever about that.” The iconic opening lines of Charles Dickens’s novel A Christmas Carol convey the idea of the finality of death, a concept that pervades our thinking even when considering the demise of cells. A dead cell is, to echo Dickens’s description of Marley, “as dead as a doornail”. But just as Marley had an influence from beyond the grave to change the character of Ebenezer Scrooge in the story, cells that die can have a vital effect on the living cells around them. Writing in Nature, Medina et al.1 bring this process to life by uncovering metabolic processes in dying cells that have important consequences for the organism.   Every second, millions of cells die in our bodies owing to processes that are a normal part of life, such as tissue turnover and responses to environmental stresses2. The vast majority of these deaths occur by a process called apoptosis. This is a form of cellular suicide that is orchestrated by the actions of enzymes called caspases, which cleave hundreds of different intracellular proteins3. This regulated cleavage of various caspase targets effectively ‘packages’ the dying cell through an orderly dismantling process. DNA in the nucleus is cut into small pieces, the cytoplasmic ‘skeleton’ of filamentous actin protein is remodelled to break the cell into smaller fragments, and the exposure of a particular lipid on the cell surface signals to immune cells, such as macrophages, to take up (engulf) and digest the dying cell2. Ever since the original description of apoptosis4, it has been known that this form of cell death does not trigger an inflammatory response, as occurs in other types of cell death, such as necrosis. Subsequent research5 confirmed that apoptotic cell death is anti-inflammatory, leading to proposals that the injection of apoptotic cells might be used to control inflammatory disease. The inflammation caused by necrotic cell death has been attributed to the release of molecules called damage-associated molecular patterns (DAMPs), of which several have been identified6. By contrast, little is known about the mechanism underlying the anti-inflammatory properties of apoptotic cells. The engulfment of apoptotic cells by macrophages promotes tissue repair7, and the apoptosis-associated molecules responsible for this effect are unknown. Medina and colleagues discovered that, during the apoptosis of mammalian cells (including human cells) grown in vitro, small molecules released from the dying cells can induce macrophages to express genes involved in tissue repair and the inhibition of inflammation. The authors speculated that metabolites — molecules arising from metabolic processes — were responsible for this effect. By profiling different cell types undergoing apoptosis in response to different triggers, Medina et al. identified metabolites that were consistently released from all dying cells, whereas other metabolites in the cells were not released. This specificity was due, at least in part, to the selectivity of a particular protein channel on the cell surface, pannexin 1 (PANX1), which opens when it is cleaved by caspases8. Apoptotic cells engineered to lack PANX1 did not release the apoptosis-associated metabolites. The authors examined six metabolites released from all apoptotic cells and found that none, individually, had a significant effect on the gene-expression profile of macrophages. However, administration of all six had a robust effect on the gene-expression pattern, and a similar expression profile could be induced, at least partially, by exposing macrophages to a mixture of just three of the metabolites: spermidine, guanosine monophosphate and inosine monophosphate. The authors report that administering a mixture of these three metabolites had remarkable anti-inflammatory effects in vivo — inhibiting disease in a mouse model of arthritis and limiting the rejection of lung transplants in mice. Spermidine is a type of molecule called a polyamine. It is mainly produced from a metabolic pathway that converts the amino acid arginine to polyamines through intermediates that include the molecule ornithine (Fig. 1). Medina and colleagues traced the conversion of arginine to spermidine by this pathway, and found that cells induced to undergo apoptosis increased their synthesis of spermidine and its precursor, the molecule putrescine, before dying. The apoptotic cells released spermidine, but not putrescine. Spermidine release occurred in a PANX1-dependent manner. Figure 1 | Cells that die by a process called apoptosis signal to neighbouring cells. Medina et al.1 report that dying human apoptotic cells release molecules produced by metabolic processes, and that these metabolites have anti-inflammatory and tissue-repair effects. a, In healthy human cells, the amino-acid arginine is often converted to the molecule ornithine, which is either used in a pathway that generates the molecule spermidine or transported into a mitochondrion (a type of organelle), where it is converted to citrulline and other metabolites. Until the cell starts to die, a channel protein on the cell surface called pannexin 1 (PANX1) remains closed. b, As the cell undergoes apoptosis, the core apoptotic machinery activates enzymes termed caspases, which cleave PANX1, and the channel then opens. Production of the molecules spermidine and putrescine becomes higher than normal. One possible way to explain this is if the core apoptotic machinery prevents ornithine from entering the mitochondrion and instead diverts it towards spermidine production. Spermidine and other specific metabolites (not shown) are selectively released through PANX1 and influence adjacent cells. Although this phenomenon was monitored using just one apoptosis-inducing condition (namely, ultraviolet radiation), the finding raises the possibility that activation of apoptosis drives this pathway, which synthesizes spermidine. The hint that suggests this is the authors’ observation of the effects of administering a type of drug called a BH3 mimetic. This drug directly triggers a core step in apoptosis, the permeabilization of mitochondrial organelles in an event called mitochondrial outer membrane permeabilization (MOMP) — and its use led to spermidine release at levels comparable to those observed in apoptosis mediated by ultraviolet radiation. Perhaps MOMP prevents the transport of ornithine into mitochondria (where ornithine is converted to the molecule citrulline), and leads instead to ornithine being mobilized in cytoplasmic pathways leading to spermidine production. This model could be tested in cells engineered to lack components required for MOMP and exposed to BH3 mimetics. The molecule urea is formed as a by-product of the conversion of arginine to ornithine. Urea is an inflammatory DAMP that is released from necrotic cells6, but the authors did not determine whether urea is released through PANX1 during apoptosis. However, because Medina and colleagues observed a rise in arginine metabolism during apoptosis, if urea is not released through PANX1, this might provide a further reason why apoptosis is not inflammatory.   How do spermidine, guanosine monophosphate and inosine monophosphate induce responses in macrophages, and why do the three metabolites work only when given together? Guanosine monophosphate and inosine monophosphate are known to signal to G-protein-coupled adenosine receptors9, and spermidine can participate in a broad range of activities. The molecule inosine (which can be derived from inosine monophosphate) has anti-inflammatory effects9 and can prevent lethal inflammation in response to a bacterial toxin in mice10. It is possible that spermidine acts to increase such anti-inflammatory signalling from the adenosine receptors. Human cells are ten times less sensitive than mouse cells to the anti-inflammatory effects of inosine, probably owing to differences in adenosine-receptor expression and function between the species8, and therefore efforts to use these metabolites to treat human disease might prove challenging. Medina and colleagues’ work opens rich possibilities for future investigations into how apoptosis triggers metabolic changes, and how the regulated release of metabolites influences tissues. In contrast to apoptosis, other forms of cell death, such as regulated forms of necrosis, have profoundly different effects on surrounding cells, and whether and how changes in metabolism triggered by those cell-death pathways influence their surroundings is unknown. Cells that die by a form of regulated necrosis termed necroptosis continue to synthesize and secrete molecules called cytokines that affect inflammation11. In these dead ‘zombie’ cells, this synthesis occurs in an organelle called the endoplasmic reticulum11, raising the possibility that metabolites produced in the functioning endoplasmic reticulum of these zombie cells also signal to living cells in the surrounding tissue. Marley’s ghost appears in chains that he said were forged in life; what other chains are forged in cell death? </body>
<date id = '49'>18 March 2020</date>
<url id = '50'>https://nature.com/articles/d41586-020-00764-4</url>
<title id = '50'>Organic compounds can be synthesized in a continuous flow of solutions, but the need to balance mass flow across multiple reactors complicates the development of such systems. A new platform for flow chemistry addresses this issue.</title>
<body id = '50'>The desire to perform chemical synthesis quickly and without tedious manual manipulations has long driven the development of automated chemical synthesizers. In a paper in Nature, Chatterjee and colleagues1 report an automated approach that they describe as radial synthesis. In their system, individually accessible compartments for performing reactions are arranged around a central hub that coordinates reagent delivery, product sampling and chemical analysis, and the temporary storage of compounds produced as intermediates. The authors’ approach not only promises to reduce manual manipulation, but also eliminates the need to customize a synthesizer for each target molecule.   The synthesis of structurally complex organic molecules is the first task in the discovery of functional compounds needed for new technologies, including those in medicine and flexible electronics. Starting with relatively simple, purchasable reactants, the process involves sequences of chemical reactions in which the complexity of the molecules produced gradually increases with each step towards the final target. The sequences can be linear, or convergent (different parts of the target molecule are made in separate sequences and then joined together). To ensure that large amounts of the final product are prepared, each synthetic step must be high-yielding and reproducible, and must generate few side products. Conventional chemical synthesis is performed ‘in batch’ (in a flask). However, this requires multiple manual interventions from chemists to prepare, run and stop each reaction, and to isolate the desired product from any by-products that cannot be tolerated in subsequent steps. An approach known as flow chemistry2 has therefore been developed to address this problem. Flow chemistry can also increase the productivity of synthetic routes compared with batch procedures, and is safer for syntheses that involve potentially hazardous components. In flow chemistry, a continuous stream of reactants is pumped through a heated or cooled reactor (typically, a tube) to form the product. To carry out multistep synthesis, multiple reactor units are usually connected in series, so that the output of one reactor becomes — along with any further reagents — the input to the next (Fig. 1a). In-line separation systems can be added to purify intermediate compounds or to switch solvents. Figure 1 | Linear and radial flow systems for chemical synthesis. a, Multistep routes for synthesizing organic molecules can be carried out in a continuous flow of solvent. A solution of reagents is typically passed through a series of reactors; further reagents are introduced as needed. A different reaction occurs in each reactor, the product of which becomes the input for the next reactor. A solution of target molecules is produced as the output of the flow system. However, reactors and other modules in these systems often need to be manually replaced to optimize the platform for each synthetic route. b, Chatterjee et al.1 report a flow platform in which a central hub is surrounded by individually accessible reactors. The hub coordinates reagent delivery to the reactors in any sequence, thereby allowing the most suitable reactor to be used for any particular reaction (black dots indicate ports through which solutions are passed from or to the hub). This removes the need to customize the system manually for each synthetic route. The mass flow must be balanced so that the flow exiting each reaction or separation step equals the sum of the input flows from the previous step and from any added reagents and solvents, minus any diverted streams (which occur when compounds are extracted from one solvent into another). This mass-flow constraint means that different reactor types and volumes are needed to achieve the best-possible product yield in each step: slow reactions require large reactors, whereas fast reactions need small reactors. Because reaction rates increase with temperature, flow-reactor temperatures can sometimes provide flexibility in the choice of reactor sizes. For example, high temperatures can make relatively slow reactions fast enough to enable the use of a small reactor. However, high temperatures can also promote side reactions, including those that cause decomposition of the desired product. Therefore, the need to balance mass flow often dictates a specific sequence of reactor volumes and temperatures for a particular multistep synthesis. This, in turn, means that the platforms used for flow chemistry must be reconfigured — reactors must be replaced with others that are more appropriately sized — for each new target molecule, decreasing flexibility and increasing the development time (the time needed to optimize the platform and chemistry for each new synthesis). Chatterjee and colleagues’ radial synthesizer is designed to overcome this issue. A master controller in the central hub of their system uses multi-positionable valves to direct reagents to different reactor modules around it (Fig. 1b), so that each reaction in a synthesis can be performed at optimal conditions — for example, at the best temperature and concentration of reagents, and in the most appropriate reactor type. The effluent from each reactor returns to the central hub for in-line analysis and subsequent distribution to the next synthesis step, which could be in the same reactor unit or in a different one. This process repeats for each reaction step until the target molecule has been synthesized (in three steps, for most of the examples reported by Chatterjee and co-workers). The system allows both linear and convergent synthesis — in the latter case, intermediate products are stored and then joined together in the final step. But the key advance of Chatterjee and colleagues’ radial synthesizer is that it decouples the steps by directing reagents to an appropriate module for each reaction, independently of the other steps. This eliminates the mass-flow constraints and the need to reconfigure the platform for different synthetic sequences, reducing the development time. The need to store intermediate compounds produced from different steps until they are required for the next reaction increases the overall process time and the total volume of the flow platform, compared with linear sequences in conventional flow synthesis. The longer process times are likely to be offset by the shorter development times when synthesizing many different compounds in laboratory quantities (milligrams to grams). However, longer process times could be a disadvantage in the commercial synthesis of individual organic compounds, for which productivity per unit volume of the flow system matters. The need to store intermediates also limits applications in which the intermediates are potentially hazardous — a disadvantage compared with conventional flow-chemistry platforms, which contain only small amounts of intermediates at any given time. For such cases, the radial synthesizer would need to be configured without interim storage and to have a short transfer time between successive reactor units, effectively creating a conventional linear system.   Previously reported automated synthesis platforms have been designed so that standardized reactors and other modules (such as those used for extractions) can be easily plugged into and taken out of reconfigurable linear sequences by hand, enabling simple customization for different syntheses3,4. More recently, a system has been reported5 in which machine learning is combined with a chemist’s expert knowledge to plan the synthesis of a target compound. The synthesis is automatically converted into a robotically assembled flow system that prepares the compound. Automated multistep reaction sequences can also be carried out in robotically controlled batch reactors integrated with liquid handlers, purification and analytical systems6. And a robotic system that uses conventional laboratory apparatus, such as round-bottomed flasks, and which uses a standardized approach (the ‘chemputer’) to translate chemical-synthesis methods into physical operations, was reported last year7. Such automated systems ensure reproducibility, because a given instruction set for a synthesis will be carried out in exactly the same way on an identical system at another location, assuming that the input materials are of the same quality. Furthermore, these systems make it feasible for potentially dangerous compounds, such as potent pharmaceuticals or radioactively labelled compounds used for medical diagnostics, to be made without exposure to humans. Automated systems could also be used to generate reaction data — such as the reagents used, yields and conditions — for machine learning in organic chemistry. However, this will require further miniaturization of existing systems, which are currently too large, and therefore too slow to produce sufficient data (equivalent to tens of thousands of experiments) in a useful time frame (weeks) to enable machine learning. Modules for analysing reactions will also need to be incorporated into flow systems to produce such data. Chatterjee and co-workers’ radial synthesizer and other automated platforms go a long way towards eliminating tedious manual operations from chemical synthesis. Nevertheless, challenges remain, particularly in the handling of solids — whether solid reagents or solids formed during reactions. Solids can be suspended in slurries by stirring reaction vessels, but the transfer of slurries through tubes (or even more problematically, through valves) leads to clogging. Another problem is how to seamlessly integrate purification, isolation and analytical procedures. Robotic systems and the radial synthesizer offer opportunities to develop hybrid platforms that integrate the best elements of flow and batch technologies with purification and analytical methods. This would enable automated synthesis that incorporates state-of-the-art reactions and can produce more-complex molecular structures than have been achieved so far. As the hardware matures, the emphasis will shift to developing the control and artificial-intelligence infrastructure necessary to generate and implement chemical syntheses5 — freeing chemists from carrying out routine procedures, so that they can focus on discovering new chemical reactions. </body>
<date id = '50'>18 March 2020</date>
<url id = '51'>https://nature.com/articles/d41586-020-00767-1</url>
<title id = '51'>How Nature reported deep diving on dry land in 1970, and the history of electrotherapy in 1920.</title>
<body id = '51'> The Royal Navy last week broke the world record for deep diving on dry land by taking two volunteers to an equivalent of 1,500 feet in a pressure chamber. The subjects endured ten hours in an atmosphere of oxygen and helium at 666 pounds/inch2 (just over 45 atmospheres) and spent the next eleven days in a gradual mock ascent … There is no sign yet of any physiological limit for diving, and at least a further 300 feet of the sea are now opened up to explorers who take the requisite precautions. Besides the intrinsic interest of seeing what depths the human frame can attain, the experiment has provided an opportunity for studying the tremors and dizziness often suffered when high-pressure helium mixtures are breathed, and at the same time has made possible a wide range of physiological observations which have yet to be analysed. Because of the experimental nature of the exercise, frequent safety checks were made during the descent; the subjects paused for 24 hours at a time in atmospheres corresponding to 600, 1,000 and 1,300 feet, and made intermediate one-hour halts at 1,100, 1,200 and 1,400 feet. One of the subjects was temporarily affected by helium tremors after changes of pressure, but the other only suffered from occasional dizziness. It seems that limitations of time rather than physiology are the chief obstacle to really deep dives. From Nature 21 March 1970 An interesting lecture on the history of electrotherapy by Dr. W. J. Turrell is published in the Archives of Radiology and Electrotherapy for February … In England electrical treatment appears to have been first practised by the clerical profession. In 1756 a book on the subject was published at Worcester by Richard Lovett, a lay clerk at the cathedral, in which he records the treatment of a number of diseases with electricity. In 1780 John Wesley, the great divine, anonymously published a book entitled “The Desideratum; or, Electricity made Plain and Useful.” In this he appeals to the medical profession for a trial of the curative effects of electricity, and records many alleged cures. From Nature 18 March 1920 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '51'>17 March 2020</date>
<url id = '52'>https://nature.com/articles/d41586-020-00576-6</url>
<title id = '52'>A tiny skull trapped in 99-million-year-old amber suggests that some of the earliest birds evolved to become miniature. The fossil illustrates how ancient amber can act as a window into the distant past.</title>
<body id = '52'>Dinosaurs were big, whereas birds — which evolved from dinosaurs — are small. This variation is of great importance, because body size affects lifespan, food requirements, sensory capabilities and many other fundamental aspects of biology. The smallest dinosaurs1 weighed hundreds of grams, but the smallest living bird, the bee hummingbird (Mellisuga helenae)2, weighs only 2 grams. How did this difference come about, and why? In a paper in Nature, Xing et al.3 describe the tiny, fossilized, bird-like skull of a previously unknown species, which they name Oculudentavis khaungraae. The discovery suggests that miniature body sizes in birds evolved earlier than previously recognized, and might provide insights into the evolutionary process of miniaturization.   Fossilization of bones in sediments such as clay, silt and sand can crush and destroy the remains of small animals, and can flatten and decay soft parts such as skin, scales and feathers. By contrast, preservation of small animals in Burmese amber (which formed from the resin flows of coniferous trees about 99 million years ago) helps to protect their soft parts. A wide range of invertebrates4 and small vertebrates, including lizards5 and birds6, have been found in Burmese amber. Specimens preserved in this material are rapidly emerging as an exceptional way to study tiny vertebrates from the age of dinosaurs5,6. It is in Burmese amber that the single known fossil skull of Oculudentavis has been preserved (see Fig. 1a of the paper3). Oculudentavis means eye tooth bird, a name that Xing et al. chose because of two unusual features of the skull, each of which provides evidence about the likely lifestyle of this 99-million-year-old species. First, the skull is dominated by two enormous eye sockets containing scleral ossicles — rings of bone that form the eye skeletons of birds (Fig. 1). The opening at the centre of these ossicles is narrow, restricting access for light into the eye and providing strong evidence that Oculudentavis was active in well-lit, daytime environments. Figure 1 | Computed tomography scan of the skull of Oculudentavis khaungraae. Xing et al.3 have characterized this 99-million-year-old fossil bird. Second, the jaws of Oculudentavis have many small teeth. This might seem odd, given the absence of teeth in today’s birds, but teeth are in fact common among early fossil birds7. However, Oculudentavis has more teeth than other birds of the period, and these extend unusually far back in the jaws to a point just under the eye. On the basis of these facts, along with observations of the fossilized tongue, the authors suggest that Oculudentavis was a predator that mainly ate invertebrates. This diet differs considerably from the nectar-based diet of the smallest living birds, and suggests that extinct and living birds took different paths to miniaturization (although how diet might be involved in this process remains unknown). Oculudentavis is just one fossil species. However, even single fossils can contribute greatly to our understanding of the history of life on Earth. In this case, weighing perhaps 2 grams, Oculudentavis is about one-sixth of the size of the smallest known early fossil bird1. This indicates that, only shortly after their origins late in the Jurassic period (which lasted from about 201 million to 145 million years ago), birds had already attained their minimum body sizes. By contrast, the smallest dinosaurs weighed hundreds of times more1 (Fig. 2). Understanding when, how and why the lower limits of body size shifted in this way requires greater knowledge of the earliest fossil birds. But Oculudentavis is a stepping stone towards this. Figure 2 | Different size ranges of dinosaurs and birds. Dinosaurs varied from about 500 grams to many tonnes in weight. By contrast, the first birds were much smaller. The smallest fossil bird found so far from the Cretaceous period weighs in at about 12 grams (data taken from ref. 9). Xing et al.3 report that the tiny Oculudentavis weighed just 2 grams. This discovery provides new insight into the lower limits of vertebrate body size in the age of dinosaurs. The evolutionary relationships between Oculudentavis and other dinosaurs and birds are difficult to determine, but are central to clarifying the evolutionary implications of this discovery. Xing and colleagues’ analysis suggests two possibilities. Oculudentavis could belong to the most common group of birds of the Cretaceous period (about 145 million to 66 million years ago), the enantiornithines. Alternatively, it could be much more closely related to dinsosaurs, lying almost midway on the evolutionary tree between the Cretaceous birds and Archaeopteryx, the iconic winged dinosaur from the Jurassic.   This confusion is a result of the bizarre features seen in Oculudentavis. These include many characteristics that differ from those of other birds, such as more-robust, fused bones, and proportionally enlarged sensory organs relative to the overall body size. The authors suggest that these features could have arisen from the constraints of evolutionary miniaturization or from ecological specialization. Both of these might have required Oculudentavis to have a strengthened skull and proportionally large eyes to maintain sensory capacity at such a tiny size. In addition, Oculudentavis has features that are not seen in dinosaurs or birds, but are present in lizards — these include the spoon shape of its scleral ossicles and the fact that its teeth are attached to the jaw bone by their sides, rather than being implanted in sockets. The challenge of determining how Oculudentavis is related to other early birds and bird-like dinosaurs would be greatly assisted by knowing more about its skeleton. The past decade has generated much data on the dinosaur–bird transition, greatly advancing our understanding of this major evolutionary event7,8. In the past few years, Burmese amber has yielded surprising insights, including previously unseen feather and skeletal structures in other extinct birds6. The study of small vertebrates preserved in amber, their ecosystems and their evolutionary relationships with one another is in a nascent phase. But Oculudentavis suggests that the potential for continued discovery remains large — especially for animals of diminutive sizes. </body>
<date id = '52'>11 March 2020</date>
<url id = '53'>https://nature.com/articles/d41586-020-00635-y</url>
<title id = '53'>Bits of a logic gate can be encoded by differently magnetized regions. A method has been developed in which the walls between these domains are manipulated electrically, rather than magnetically, to produce a logic gate.</title>
<body id = '53'>Computer logic based on standard electronic components is expected to hit its speed and power limits before long. To get around this, logic devices that use magnetic elements, rather than electronic ones, have been proposed — but these usually require external magnetic fields, which limits their application. A working logic gate that uses magnetic elements and is driven completely electrically, without the need for an external magnetic field, has yet to be demonstrated1. However, in a paper in Nature, Luo et al.2 report that they have harnessed the chirality (‘handedness’) of a system to invert the direction of magnetization of domains in a cobalt wire purely by means of an electric current. The resulting inverter device acts as a ‘NOT’ gate, which they use to build up other logic gates such as NAND and NOR.   First, some background. Magnetic domains are small regions of uniform magnetization in a material. The narrow boundaries separating different magnetization orientations are called domain walls; within the wall, the magnetization must gradually twist around. Such magnetic domains have long been used to encode data bits for storage, such as in hard disk drives, in which mechanical motion is needed to access the data. By contrast, in magnetic racetrack memory3 (which is still under development), no mechanical movement is needed. Instead, magnetic domains within hair-thin metal wires are moved by the flow of electric current in the wires. The trick to doing this lies in an interaction — the spin-transfer torque — between the magnetic moments of the domain walls and the spins of the moving electrons. Here, spin refers to a quantum property of the electrons; moment is the magnetic direction and strength; and the torque is a twisting interaction that tends to rotate the moments and thus move the walls. In spin-transfer systems, the torque arises from current in the magnetic wire itself. But another type of torque, known as spin–orbit torque, is produced when the wire is placed on a layer of a non-magnetic heavy metal such as platinum, and the electric current flows in that layer instead (Fig. 1). In such systems, two effects add to each other to drive twisting in the domain walls more efficiently than in spin-transfer systems4,5. The first is that a spin current arises in the non-magnetic heavy-metal layer; the second effect, which is crucial to Luo and colleagues’ work, is that an interaction between the two metals forces a specific chirality on the domain walls. Figure 1 | How to flip magnetic data bits electrically. Luo and co-workers2 have produced a NOT logic gate that flips the direction of magnetization of magnetic domains by electrical means. In the authors’ system, mobile domains of a cobalt wire that have up or down magnetic moments act as data bits. From the side, spheres with arrows represent the direction of magnetization; on the top surface, circled dots and crosses indicate up and down moments, respectively. a, The authors fabricated a fixed wall between up and down domains, in which the magnetization direction can rotate in a left- or right-handed direction — a property called chirality. b, An electric current in the platinum substrate propels another, moving chiral domain wall (blue) along the wire. c, When two opposite magnetic moments collide on the left-hand side of the fixed wall, the direction of the moment in the fixed region switches. A new domain forms on the other side of the wall, in which the moment is reversed, preserving the preferred chirality of the system. Overall, the bit has been flipped. (Adapted from Fig. 1 of the paper2.) Chirality means handedness: like left and right hands, objects of opposite chirality have mirror symmetry, but cannot be superimposed on each other. Domain walls known as Néel-type walls can exhibit chirality when the direction of their magnetic moment is reversed, because there are two ways in which reversal can happen: the moment can undergo a right-handed twist or a left-handed one. (Moments can also spin in the plane of domain walls known as Bloch walls, but this mechanism is not used in Luo and co-workers’ study.) The chirality is produced by an effect called the Dzyaloshinskii–Moriya exchange interaction (DMI), which acts between the magnetic and non-magnetic metals. The DMI both establishes a specific chirality and favours the generation of the correct type of wall — a Néel wall — in Luo and colleagues’ set-up. Researchers have previously recognized that chiral domain walls could be used in logic operations6, but an all-electrical working logic has been hampered by the lack of a reliable working inverter, the key component for logic operations. Luo et al. have ingeniously invented an inverter that flips the magnetic moments associated with incoming bits of data using spin–orbit torque. Building on their previous work on chirally coupled nanomagnets7, the authors fabricated a sort of artificial, stationary domain wall in a magnetic cobalt wire interfaced with non-magnetic platinum. The magnetization in the cobalt is perpendicular to the plane of the wire, except in the stationary region. There, it is magnetized in the direction of the wire’s long axis, like the region in the middle of an ordinary domain wall, but across a much larger width. This is crucial, because it allows smaller coercivity — that is, the magnetization here is easier to switch. To picture how the inverter works, consider an input consisting of a domain wall that has left-handed chirality (Fig. 1). This mobile wall is rolled along the wire by spin–orbit torque. When it reaches the fixed artificial boundary, two opposite magnetic moments collide, producing a region of the wire in which the moment changes abruptly. According to theories of magnetism, such an abrupt change has a high energy cost. To lower the energy of the system, one of the moments must be switched, or a new magnetic domain must be generated. In this case, the moment in the low-coercivity fixed wall switches to the same direction as that in the incoming wall. But a chirality effect now comes into play: this switch of magnetic moment produces a right-handed chirality at the other side of the fixed wall that conflicts with the chirality preferred by the DMI. To resolve this, a new domain wall forms on that side (the system is shaped in such a way as to promote this process) and sets off along the wire. The moments in the resulting outgoing bit thus have the preferred left-handed chirality, rather than the right-handed chirality originally produced at the wall.   By integrating their inverters into junctions, Luo et al. designed some simple logic gates (NAND and NOR), as well as more-complicated ones (such as exclusive-OR). Each junction has two inputs, an intrinsic bias towards one magnetic moment and one output. The output is determined by the two inverted inputs and by the bias at the junctions (rather like a ‘majority gate’8). So, when the inputs and bias are (0,0) and 1, respectively, inverters immediately before the junction invert them to (1,1) and 0 at the junction itself, which consequently outputs 1, acting as a NOT gate. But when the inputs are either (1,0) or (0,1), the value of the bias determines whether the gate behaves as a NOR or a NAND. This majority-gate behaviour mitigates the need to precisely synchronize the two inputs, offering reliable logic operations. This logic system satisfies key criteria known as cascadability and fan-out. Cascadability means that the output of one gate is produced in the correct form and is strong enough to drive the input of the next gate. And fan-out means that one gate output can be connected to several gate inputs9. Moreover, the data can be stored in the absence of an external power source, and evade damage by ionizing radiation. Challenges remain before chips based on Luo and co-workers’ system can reach the market. The operating current will need to be reduced so that it can be accommodated by tiny complementary metal oxide semiconductor (CMOS) transistors, which help to pick up inputs and outputs for use in chips. In theory, current decreases as the size of wires and transistors decreases, and so current density (the charge per unit time that flows through a given cross-section of the wire) remains constant with scaling. A reduction in current density will be needed to increase the speed and reduce the energy consumption of the authors’ system. Domain-wall velocity does not scale linearly with current, and so new materials might need to be used to reduce the current density10,11. Another issue is that the input and output states of Luo and colleagues’ system have to be detected by microscopy, rather than by an electrical method. A different read-out system will be needed for practical applications, but this could be technically challenging. An effect known as tunnelling magneto-resistance might offer one solution12. The implementation of a domain-wall logic chip that uses an electrically driven read-out system should be the next goal, following on from Luo and colleagues’ exciting discovery. </body>
<date id = '53'>11 March 2020</date>
<url id = '54'>https://nature.com/articles/d41586-020-00605-4</url>
<title id = '54'>Identifying Earth’s building blocks from terrestrial rocks is challenging because these ingredients have become mixed as the planet evolved. Evidence of an unknown building block in ancient rocks provides fresh insight.</title>
<body id = '54'>Earth formed from an unknown selection of meteoritic material. Writing in Nature, Fischer-Gödde et al.1 report that the composition of ruthenium isotopes in ancient rocks from southwest Greenland contains evidence of a previously unrecognized building block of Earth. Surprisingly, the inferred isotopic composition of ruthenium in the material does not match known meteorite compositions. The authors’ findings suggest that Earth’s volatile components, such as water and organic compounds, could have arrived during the final stages of the planet’s growth.   Our planet is the product of a series of collisions of increasingly large celestial bodies2–4. These building blocks accreted from a protoplanetary disk of dust and gas that orbited the proto-Sun about 4.6 billion years ago. Identifying the compositions of Earth’s building blocks is difficult because of our limited access to the disk’s remnants, and because of the complex, long-term geological processing of the mantle that has mixed Earth’s ancient ingredients. Potential answers to the question of what Earth is made of can come from studies in which the isotopic compositions of terrestrial rock samples are compared with those of meteorites that formed within the first few million years of the Solar System’s history. These meteorites are presumed to be representative of the smaller bodies that ultimately coalesced to form the rocky planets. Consequently, meteorites are our most promising candidates for Earth’s building blocks. Fischer-Gödde and colleagues’ study builds on the finding that meteorites have characteristic isotope compositions that serve as fingerprints to distinguish different types of potential building block. For example, meteorites such as carbonaceous chondrites, which are often ‘wet’ (that is, they contain volatile components), have different isotopic fingerprints from meteorites that are generally ‘dry’5. The differences in isotopic composition originate from the heterogeneous distribution of stardust in the protoplanetary disk, and are known as nucleosynthetic isotope variations. If the fingerprints could be identified in terrestrial rock samples, this might provide evidence of the material from meteorites that Earth was built from. The documentation of fingerprints in terrestrial rocks could help to constrain estimates of when volatile elements were delivered to Earth and where they came from. This is because the abundances of certain isotopes of some elements — ruthenium-100 (100Ru), for example — not only distinguish between wet and dry building blocks, but also trace different stages of Earth’s accretion history.   Ruthenium is classified as a highly siderophile (iron-loving) element, because it collects in metal-rich phases of Earth’s interior. Consequently, most of our planet’s ruthenium is concentrated in its metallic core. There are, however, traces of ruthenium and other highly siderophile elements (HSEs) in the mantle, and their relative proportions approximate to those measured in primitive meteorites6. One interpretation of this is that the HSEs were added to the mantle after the core formed, during an event called the late veneer — when the final approximately 0.5% (of the total percentage weight) of Earth’s mass accreted7,8. If so, then ruthenium and other HSEs in the mantle record the composition of the last material that accreted to Earth9. It has been proposed that Earth’s volatile elements were also added during the late veneer, possibly by the accretion of carbonaceous chondrites10,11. Studies in the past few years, however, have found a mismatch between the 100Ru-isotope composition (the abundances of 100Ru in terrestrial rocks) in Earth’s mantle and that in carbonaceous chondrites12,13. It could therefore be concluded that carbonaceous chondrites did not form part of the late veneer, thus casting doubt on the timing of the delivery of volatiles to Earth13. This conclusion rests on the assumption that HSEs in the mantle do not contain significant quantities of material from before the late veneer — a reasonable assertion, given that there is limited direct evidence of this. If the pre-late-veneer mantle did contain a substantial amount of 100Ru that did not collect in the core, and that was identifiable by having a different 100Ru-isotope composition from that of the modern mantle, then carbonaceous chondrites could still have been accreted during the late veneer. Nucleosynthetic ruthenium-isotope variations have not been reported for terrestrial rocks before now. This is, in part, because Earth has active plate tectonics and mantle convection, which mix and dilute the fingerprints of its building blocks. However, in the past few years, analytical methods14 have been further developed that enable isotope variations to be measured on the scale of parts per million, making it possible to search for these primitive isotopic signatures. By comparing the 100Ru-isotope compositions of terrestrial rocks with those of meteorites, Fischer-Gödde and co-workers report that an ancient part of Earth, preserved in rocks from southwest Greenland, retains the fingerprints of an unusual building block (Fig. 1). The fact that the inferred isotope compositions do not match known meteorite compositions indicates that current meteorite collections are considerably limited in their sampling of the protoplanetary disk. Figure 1 | A scenario for the preservation of ancient material in Earth’s mantle. a, Between 4.6 billion and about 4.5 billion years ago, Earth formed from the accretion of material from meteorites. Siderophile elements, which have a strong affinity for metals, segregated into the core. b, The final approximately 0.5% of the total percentage weight of Earth’s mass accreted from meteorites during an event called the late veneer, after the core had formed. c, Fischer-Gödde et al.1 report that ancient rocks from southwest Greenland have an unusual ruthenium-isotope composition. They attribute this to the presence of pre-late-veneer mantle material in the rocks. The distribution of pre-late-veneer material shown here is speculative; the actual amount and distribution cannot be derived from the available data. The authors interpret their unusual 100Ru data as the isotopic signature of pre-late-veneer ruthenium in the source of these rocks. Considering their findings in the context of the compositions of other HSEs in the mantle, the authors suggest that the modern composition of the mantle can be reconciled with their new data only if the late veneer contained carbonaceous chondrites to counterbalance the composition of the pre-late-veneer component of the mantle. This would mean that volatiles could have been delivered to Earth during the final stages of the planet’s formation. Fischer-Gödde and colleagues’ data answer the long-standing question of whether Earth’s diverse building blocks are preserved and accessible for study. But the data also raise key questions, the answers to which will undoubtedly determine the importance of the new findings. For example, how representative of the pre-late-veneer mantle is the suite of rock samples from southwest Greenland? Are nucleosynthetic fingerprints observed in the isotopic compositions of other elements in the mantle? What is the composition of the ‘missing’ meteorites that dominated the ruthenium composition of the pre-late-veneer mantle, and why has it not yet been identified? And how was the isotopic signature of these meteorites preserved in the convecting mantle? These questions can be addressed only by expanding the search for nucleosynthetic fingerprints in the mantle. </body>
<date id = '54'>11 March 2020</date>
<url id = '55'>https://nature.com/articles/d41586-020-00638-9</url>
<title id = '55'>Chromatin, the complex of DNA and protein in cell nuclei, can be modified by ubiquitin molecules. It emerges that this modification occurs in a molecular reaction chamber formed from an enzyme and a scaffold protein.</title>
<body id = '55'>DNA is packaged around histone proteins to form a macromolecular structure called chromatin. Histones are often modified by the attachment of small molecules, to guide gene activity and genome stability1,2. For example, ubiquitin modifications on histone H2B (termed H2Bub) are associated with domains of active gene expression3,4. But how these modified domains arise is far from understood. Previous work4 has indicated a simple model for the formation of H2Bub domains: an enzymatic complex that is needed to add ubiquitin to H2B is recruited when the enzyme that catalyses gene transcription, RNA polymerase II, passes along chromatin. Writing in Nature, Gallego et al.5 provide an alternative model, in which the enzymatic complex forms a liquid-like, ‘phase-separated’ reaction chamber that adds ubiquitin to H2B, independently of RNA polymerase II.   In yeast, ubiquitin attachment to H2B (a process called ubiquitination) is executed by the enzymes Bre1 and Rad6. A third protein, Lge1, is also required for H2B ubiquitination in yeast ; Lge1 physically binds to Bre1, but its molecular role in ubiquitination has been a mystery6. Gallego et al. revisited Lge1 because its amino-acid composition indicates that it has an intrinsically disordered region (IDR) containing a ‘sticker’ sequence at the amino terminus, enriched in arginine, tyrosine and glycine amino-acid residues. Other proteins that contain IDRs have been shown to weakly interact and undergo liquid–liquid phase separation (LLPS) — a process in which proteins self-associate into liquid-like condensates or droplets, similar to membraneless organelles7. LLPS is gaining prominence as a concept that can explain key aspects of chromatin structure and function8–12. Gallego et al. showed that, in a reaction carried out in a test tube, Lge1 undergoes LLPS to form condensates. This process is driven by the protein’s IDR — specifically, by tyrosine residues in the sticker region. The authors observed an odd phenomenon when they added Bre1 to the test tube: Lge1 acted as a scaffold around which Bre1 formed a shell, limiting growth of the condensate. The presence of Bre1 also led to the transient accumulation of Rad6 in the shell, along with arrays of nucleosomes (structural units of chromatin consisting of DNA coiled around eight histones). Rad6 and the nucleosomes subsequently spread evenly throughout the condensate. Thus, Lge1 and Bre1 form ‘core–shell condensates’ that act as reaction chambers, capturing the ubiquitination machinery and its substrate, H2B in nucleosomes. One of the challenges of studying LLPS is testing ideas generated in vitro and through modelling, in living cells13,14. This is mainly because proteins that have IDRs are not amenable to structural studies, and condensates might be too small and dynamic to be visualized easily under physiological conditions. Using complementary approaches in vitro and in yeast cells, Gallego and colleagues provided evidence for the existence of the reaction chamber in vivo. First, by analysing protein sedimentation in cell extracts, the group showed that Lge1 forms a large complex that can capture Bre1. Subsequently, they tagged Lge1 and Bre1 with fluorescent protein fragments to visualize interactions between the two proteins in cells using microscopy. The proteins formed concentrated clusters, consistent with LLPS. Finally, the authors provided evidence that the core–shell structure enhances the efficiency of H2B ubiquitination in vitro and in cells, where it mainly affected gene sequences that were undergoing active transcription. Their work provides an exciting model to describe the environment in which nucleosomes are modified. What can this study teach us about the fundamental concepts of chromatin organization and regulation? The Lge1–Bre1 condensate is unlike known LLPS condensates because it involves not only a phase-separated structure, but also an enzymatically active protein shell. The protein composition of the LLPS reaction chamber can regulate its own size and can control the rate at which Bre1, Rad6 and nucleosomes enter the condensate. When this reaction chamber captures nucleosome arrays, it could well facilitate modification of multiple nucleosomes in succession, producing H2Bub domains (Fig. 1). Figure 1 | The core–shell structure. DNA in cell nuclei is wrapped around histone proteins to form units called nucleosomes. The enzymes Rad6 and Bre1, along with the protein Lge1, add ubiquitin (Ub) molecules to histones in actively transcribed regions of the genome (a process called ubiquitination). Gallego et al.5 report that Lge1 condenses into liquid-like droplets, acting as a core scaffold around which Bre1 forms a shell. Nucleosomes at active genes are recruited to and then diffuse into the droplets, along with Rad6, temporarily bound to ubiquitin (binding not shown). The authors find that this core–shell structure promotes ubiquitination. The WAC protein is the counterpart of Lge1 in humans. Mutations in WAC have been linked to neurodevelopmental disorders15,16, and there is emerging evidence that alterations in LLPS are associated with human diseases7. Gallego and colleagues showed that WAC also has an IDR and that it can partially perform the role of Lge1 in yeast. Their data indicate that abnormal core–shell compartmentalization might have a role in disease. The idea that proteins can condense into reaction chambers raises several questions. For example, how do Lge1–Bre1 condensates affect other mechanisms of chromatin organization and modification, and how are they targeted to regions of the genome undergoing active transcription? Perhaps nucleosomes in the main body of a gene — which have different patterns of nucleosome packing and chromatin modification from those of other chromosomal regions — are preferred substrates for the condensates. Another possibility is that the transcription machinery promotes targeting of these gene-body nucleosomes to the core–shell condensate; this is because RNA polymerase II has a repetitive carboxy-terminal domain that can also undergo LLPS17. Similarly, researchers will now be asking whether reaction chambers decondense nucleosome arrays to enable access for other chromatin-modifying enzymes that are not part of the condensate, and whether these chambers dissolve once chromatin is ubiquitinated.   Another important consideration is how other macromolecules might co-assemble with the Lge1–Bre1 condensate in the crowded environment of the nucleus. H2B ubiquitination promotes the activity of the enzymes Dot1, Set1 and Set2, which add methyl groups to histones4,18 — does this cross-talk also occur within a condensate? The N terminus of Dot1 is predicted to be an IDR and can promote H2B ubiquitination when overexpressed19. This is compatible with a model in which Dot1 co-assembles with Lge1–Bre1 condensates to coordinate cascades of nucleosome modification. Alternatively, perhaps Dot1 forms a distinct condensate. In this scenario, nucleosome arrays might be handed off between adjacent condensates, or the condensates might interact through fusion events. The idea of phase-separated condensates is not new. Scientists have been investigating their molecular properties for decades, but it now seems that these structures provide a mechanism for regulating processes inside cells, in particular for chromatin organization. This probably reflects the fact that many reactions that occur in the nucleus involve disordered proteins acting in conjunction with electrically charged strings of nucleic acids, providing an optimal environment for LLPS20. Gallego et al. add to this body of work, and highlight the fact that simple linear models for enzymatic reactions on chromatin are probably over-simplifications. Instead, reactions occur in 3D, with layered condensates forming enzymatic microenvironments that promote and regulate the reactions, and that could simultaneously unfold chromatin11 — all in the milieu of a local reaction chamber. </body>
<date id = '55'>11 March 2020</date>
<url id = '56'>https://nature.com/articles/d41586-020-00637-w</url>
<title id = '56'>Analysis of nucleic-acid sequences from human cancers, along with samples from adjacent tissue and blood, reveals the presence of microorganisms in tumours and blood across cancers.</title>
<body id = '56'>The world is filled with microorganisms, which have a profound impact on many facets of life. Do these microbial communities influence cancer1? Many studies of microbes and their genomes (collectively called the microbiome) have focused on the gut, where most of the body’s microbes reside. This work has revealed a role for the gut microbiome in several types of cancer that arise in the intestinal lining itself2–4, and indicated that the gut microbiome might influence cancers at distant sites through its impact on the immune system1. In addition, emerging evidence indicates that microbial signatures (such as nucleic acids) can be found in tumours at other sites in the body5,6 and in the tissues and blood of individuals who don’t have cancer7,8. Writing in Nature, Poore et al.9 build on this evidence, identifying signatures of microbial DNA and RNA, both in tumours and in the blood, across multiple human cancers. The authors further suggest that these signatures might augment existing clinical diagnostic tools, although further work is needed in this area. Poore et al. used The Cancer Genome Atlas (TCGA) — an online resource that includes DNA and RNA sequences — to analyse data for 33 cancer types, totalling more than 17,000 samples from some 10,000 patients. They analysed data sets derived from bulk tumour samples (primary tumours as well as recurrent ones, and tumours that had spread through metastasis), normal adjacent tissue and blood samples. The authors used multiple computational approaches, including independently trained artificial-intelligence (AI) models, to filter, normalize and classify microbial sequences in these samples. After stringent filtering approaches to address potential contamination and other variables, the group classified 7.2% of the total sequencing reads as non-human. Approximately one-third of those mapped to known sequences of bacterial, archaeal or viral origin, and 12.6% of these resolved to a particular genus from one of these groups. The authors next trained machine-learning models to use these sequences to distinguish between cancer types and between different stages of the same cancer type, as well as between tumours and normal tissue. Overall, the models performed well in discriminating between cancer types and between cancer and normal tissue, but showed some variability in their ability to discriminate between various stages of cancer. The researchers also tested the biological relevance of the microbial profiles against known microbial associations with cancer. In line with previous reports, they found Fusobacterium in gastrointestinal tumours, and viruses such as Alphapapillomavirus and Hepacivirus in cervical cancer, head and neck cancer and hepatocellular cancer. Poore et al. next explored microbial signatures in the blood of people with cancer, using AI models that analysed whole-genome sequences from the TCGA cohort of individuals. Their findings suggest that blood-borne microbial DNA (mbDNA) could be used to discriminate between cancer types. The group sought to validate its mbDNA models against existing cell-free tumour DNA (ctDNA) assays in a separate cohort, which included 69 individuals without cancer and 100 who had prostate cancer, lung cancer or a skin cancer called melanoma (Fig. 1). The authors’ models were generally good at discriminating between cancer types, although there were some limitations. Further validation of these results is needed in larger cohorts across cancer types using dedicated methods. Figure 1 | Microbial signatures of cancer. Microorganisms can inhabit various tissues. Traces of these microbes’ DNA and RNA can be found in various tissues, including the blood (here, only DNA is shown, for simplicity). Poore et al.9 built on previous findings6 to show that microbial DNA and RNA can also be found in tumours and act as a signature of cancer. Artificial-intelligence programs can use these nucleic-acid signatures from tissue or blood samples to discriminate between types of cancer, and between healthy individuals and those who have certain cancers. These results, along with another study of microbes in tumours that used TCGA data6, are provocative. However, the studies had some limitations, suggesting that there is a tremendous opportunity to build on this work. One limitation is that TCGA samples were not collected in a manner that controlled for contamination by microbes or mbDNA. This contamination could have been introduced at any time between sample collection and sequencing. Poore et al. and others6 tried to control for this through stringent filtering of potential contaminants; however, such approaches might limit our ability to detect the full complement of microbes present in tumours. In addition, DNA and RNA sequencing for human studies might not be performed in a way that enables microbes to be characterized completely. Future studies that build on the current work should involve analysis of carefully curated tissues and blood using appropriate sequencing techniques to allow for characterization of microbial signatures. In addition to validating the presence of these microbes in tumours and blood in cancer, it will be important to gain insights into their distribution and function. Poore et al. and others6 identified microbial signatures in tumours on the basis of nucleic-acid sequences; however it is not known where these microbes are located (within or around tumour cells, immune cells or in connective tissue known as the stroma) and whether or not they are alive. And more work will be needed to determine whether the microbes are driving cancer or are merely passengers in an altered tumour microenvironment. There are clear examples of how microbes in tumours might contribute to cancer development and to resistance to cancer therapy3,10. However, other data suggest that the presence of microbes in tumours is associated with better long-term outcomes11. Finally, further mechanistic insights into how microbes enter and persist in cancerous tissue are needed, as well as research into how best to target them for treatment and even cancer prevention. Such strategies will need to be nuanced, and must take into account the potential effect on all microbial niches, because many of the body’s resident microbes have a crucial role in overall physiology. Although some preclinical studies suggest that co-targeting microbes and tumour cells with antibiotics and chemotherapy is associated with delayed tumour outgrowth10,12, other work suggests that treatment with broad-spectrum antibiotics13 can worsen the outcomes of people receiving immunotherapy, probably owing to disruption of the gut microbiome. Thus, there is context dependence, which must be taken into consideration. Nonetheless, the opportunities for both clinical advances and basic insights that are presented by an ability to monitor and manipulate the microbiome are tantalizing. </body>
<date id = '56'>11 March 2020</date>
<url id = '57'>https://nature.com/articles/d41586-020-01515-1</url>
<title id = '57'>Playful master of games who transformed mathematics.</title>
<body id = '57'>John Horton Conway was one of the most versatile mathematicians of the past century, who made influential contributions to group theory, analysis, topology, number theory, geometry, algebra and combinatorial game theory. His deep yet accessible work, larger-than-life personality, quirky sense of humour and ability to talk about mathematics with any and all who would listen made him the centre of attention and a pop icon everywhere he went, among mathematicians and amateurs alike. His lectures about numbers, games, magic, knots, rainbows, tilings, free will and more captured the public’s imagination. Conway, who died at the age of 82 from complications related to COVID-19, was a lover of games of all kinds. He spent hours in the common rooms of the University of Cambridge, UK, and Princeton University in New Jersey playing backgammon, Go and other diversions, some of his own creation. Several of Conway’s most celebrated contributions were made while he was thinking about games and their strategies. Perhaps his greatest discovery was a surprising correspondence between numbers and games that led him to a truly gigantic system, the surreal numbers, which stunned the mathematics community. It contained not only the positive and negative real numbers that we all know, but also new infinitely large numbers, infinitesimally small ones, and all sorts of new numbers in between. Conway’s work on surreal numbers emerged from the influential research project and book Winning Ways for your Mathematical Plays (1982), a compendium of information on the theory of games, written with Elwyn Berlekamp and Richard Guy. This fascination with games also led Conway to develop the Game of Life, a cellular automaton in which the pattern of live or dead cells in a two-dimensional grid evolves according to a set of rules for the ‘birth’ and ‘death’ of each cell, based on the status of its nearest neighbours. The simplicity and accessibility of this game was popularized in 1970 by Scientific American columnist Martin Gardner. By the mid-1970s, it was estimated that one-quarter of the world’s computers were running Conway’s Game of Life as their screensaver. Conway, who was the John von Neumann professor of mathematics at Princeton University before his retirement in 2013, was born in Liverpool, UK, in 1937. His father made his living playing cards, and later worked as a chemistry laboratory technician at a local high school attended by George Harrison and Paul McCartney. Conway, like his mother, was an avid reader. He showed early interests in mathematics; by the age of 11, he wanted to be a mathematician at Cambridge. He received his PhD from the University of Cambridge in 1964 under the advisership of Harold Davenport, was subsequently hired at Cambridge as a lecturer, and became professor in 1983. In 1987, he moved to Princeton. Conway first attained fame in 1968 for determining all 8,315,553,613,086,720,000 symmetries of the Leech lattice — a remarkably regular arrangement of points in 24-dimensional space discovered by John Leech in 1967. This led to his discovery of the Conway simple groups, which were fundamental in the classification of finite simple groups — one of the capstone achievements of twentieth-century mathematics. Conway had a primary role in researching and assembling the iconic symmetry book ATLAS of Finite Groups (1985). His deep knowledge of symmetries led him to propose, with his ATLAS co-author Simon Norton, the Monstrous Moonshine conjectures. These, for the first time, seriously connected finite symmetry groups to analysis — and thus discrete maths to non-discrete maths. Today, the Moonshine conjectures play a key part in physics — including in the understanding of black holes in string theory — inspiring a wave of further such discoveries connecting algebra, analysis, physics and beyond. Conway’s discovery of a new knot invariant — used to tell different knots apart — called the Conway polynomial became an important topic of research in topology. In geometry, he made key discoveries in the study of symmetries, sphere packings, lattices, polyhedra and tilings, including properties of quasi-periodic tilings as developed by Roger Penrose. In algebra, Conway discovered another important system of numbers, the icosians, with his long-time collaborator Neil Sloane. In number theory, Conway showed that every whole number is the sum of at most 37 fifth powers. He also developed the 15-theorem (with his student William Schneeberger) and the 290-conjecture; these were vast generalizations of the four-squares theorem, proved by eighteenth-century mathematician Joseph-Louis Lagrange, which states that every positive whole number is the sum of four square numbers (for example, 21 is the sum of 16, 4, 1 and 0). Conway was a memorable teacher and speaker, and the many tricks he performed to illustrate mathematical concepts included: stating immediately the day of the week for any date in history, twirling a hanger with a penny balanced on its inside edge, contorting his tongue into a variety of shapes, balancing objects on his chin, and delivering entire lectures in which every word he said had only one syllable. He loved to talk about mathematics and games, as well as history, etymology and philosophy. His contributions to culture, through his work and outreach, will have a lasting impact. For the remarkable profundity of his mathematical discoveries — and the playful and generous way in which he shared these with others — he will be sorely missed. </body>
<date id = '57'>23 May 2020</date>
<url id = '58'>https://nature.com/articles/d41586-020-01557-5</url>
<title id = '58'>The first results from vaccine trials are promising, but scientists still urge caution, and Trump issues an ultimatum to the WHO.</title>
<body id = '58'>  In this episode: President Trump has given the WHO an ultimatum in a tweet, threatening to pull out of the organisation within 30 days unless unclear demands are met. We discuss what this means for the pandemic, the USA and the future of international health cooperation. The first results from vaccine trials are in and they are encouraging, but scientists are still urging caution. We hear the lowdown on the types of vaccines being developed and what hope there is of rolling them out any time soon. News: Coronavirus vaccine trials have delivered their first results — but their promise is still unclear News: The race for coronavirus vaccines: a graphical guide News: If a coronavirus vaccine arrives, can the world make enough? Our hosts pick out things that have made them smile in the last week, including hopeful antibody research, at-home sketch comedy and printable board games. News: Potent human antibodies could inspire a vaccine Video: Whiskers R we - SNL Video:The wild affordable world of 1 Player Print’n’Play Games Video:MORE of the Very Best Solitaire Print'n'Play Games Video: Marble run league Video: BBC goals at home (Only available in the UK) Noah Baker takes a look through some of the key coronavirus papers of the last few weeks. News: Coronavirus research updates medRxiv: Saliva is more sensitive for SARS-CoV-2 detection in COVID-19 patients than nasopharangel swabs Nature: Effect of non-pharmaceutical interventions to contain COVID-19 in China Science: Changes in contact patterns shape the dynamics of the COVID-19 outbreak in China New England Journal of Medicine: Multiorgan and Renal Tropism of SARS-CoV-2 Never miss an episode: Subscribe to the Nature Podcast on Apple Podcasts, Google Podcasts, Spotify or your favourite podcast app. Head here for the Nature Podcast RSS feed﻿. Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 Public health News 25 MAY 20 News 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '58'>22 May 2020</date>
<url id = '59'>https://nature.com/articles/d41586-020-01555-7</url>
<title id = '59'>John Tregoning finds inspiration in conferences, music and running.</title>
<body id = '59'>Credit: Adapted from Getty One of my shorter-lived delusions at the beginning of lockdown was that it would be a fantastic time to write grant proposals. I imagined myself reclining on a chaise longue in my study, wearing a beret, inundated with ideas; the next Nobel Prize just a thought away. This didn’t happen. It didn’t help that I own neither a beret nor a chaise longue. More problematic has been juggling the kids’ schoolwork, the actual day job, writing columns about how busy I am, and baking. Another barrier is that I no longer have access to the places where my best ideas usually seed. Yes, I still have access to papers, but although reading papers is an excellent way to build on, understand and share existing ideas, I find it generally unhelpful in generating new ideas for my own work. Part of this is to do with the publication eruption of the past 20 years. When I started my PhD, in plant biochemistry, it was relatively straightforward. If you wanted to find a paper in subject X, you looked in the Journal of X. There were sometimes regional variations — the British Journal of X or the European Journal of X — and there were, of course, the bigger, more general journals. It was just about possible to scan most of the new work in your field. This has changed. My work now has to do with the interface of infection and immunity in the lungs; with more than 400 journals covering the field, there are far too many articles to keep up with. So, like many others, I need a filter to identify the most important ideas. One approach is to be selective about which journals you read, but this can be flawed if you choose only those with a high impact factor. Another is to meet other scientists and engage with new research in that way. My favourite space in which to get ideas and develop collaborations is a conference. Small residential conferences, where you get to speak to everyone attending (and often drink too much beer with them), are just as good for this as big ones, such as the annual gathering of UK immunologists at the British Society for Immunology’s congress, which is like an extended family reunion with added science.   I also get ideas during music gigs. This might sound odd — singing along to dad rock doesn’t seem the ideal way to do science — but gigs are a space where I don’t have to think about my kids, my laboratory or my chickens, which clears space for other thoughts to germinate. And yet, seven weeks into lockdown, I’ve been surprised and pleased to see the shoots of new ideas appear. One thing that has helped is still having access to one of the other times when my creative side emerges — when running, which is allowed under lockdown here in the United Kingdom. It’s not that I have brilliant ideas while I am running along; most of the time is spent trying to catch my breath and wondering why I am so much slower than I was this time last year. But running clears my head and provides clarity long after I’ve caught my breath. Music is still helping me think, as well. I might not be able to go to gigs but, as lockdown began, I invested in a subscription to Spotify. I now have a carefully curated playlist of dad music, Disney songs and dance tunes that I can listen to without hearing — pop music as white noise — to drown out distraction. The addition of sound-reducing headphones allows me to tune out the world and really focus on what I am doing. This has led to a few terrifying moments when I have been so deeply immersed in the work (and the music) that when someone else comes into the room to speak to me, I jump out of my skin. So, the good news is that I am beginning to have ideas. Whether these will develop into amazing giant sequoias, or suffer the same fate as my attempts to grow cress, is yet to be seen. Latest on: Careers Career Column 22 MAY 20 Career Column 21 MAY 20 Career Column 19 MAY 20 Lab life Career Feature 14 MAY 20 Technology Feature 11 MAY 20 World View 05 MAY 20 Research management News 20 MAY 20 Career Column 15 MAY 20 World View 11 MAY 20 Institut Pasteur of Shanghai, Chinese Academy of Sciences (IPS-CAS) Shanghai, China Institut Pasteur of Shanghai, Chinese Academy of Sciences (IPS-CAS) Shanghai, China Queensland University of Technology (QUT) Brisbane, Australia Charles Darwin University (CDU) Darwin, Australia An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '59'>22 May 2020</date>
<url id = '60'>https://nature.com/articles/d41586-020-01506-2</url>
<title id = '60'>Scientists across the country are battling anti-science sentiment alongside a rapid increase in COVID-19 cases.</title>
<body id = '60'>SARS-CoV-2 coronavirus particles imaged using an electron microscope.Credit: NIAID/National Institutes of Health/Science Photo Library Brazil’s researchers have a battle on their hands. The country has the world’s third-highest number of confirmed COVID-19 cases, with more than 300,000 infections and 20,000 deaths. Scientists there have to fight not only the coronavirus, but also the government’s anti-science stance. President Jair Bolsonaro, who has been photographed shaking hands with supporters in recent weeks, has rejected social-distancing measures while heavily promoting the antimalarial drug chloroquine as a coronavirus treatment despite a lack of evidence that it is effective. Former health minister Luiz Mandetta was fired in mid-April after a disagreement over Bolsonaro’s response to the pandemic. His successor, Nelson Teich, resigned on 15 May, after just a month in the job. Despite the turmoil, Brazilian researchers are working hard to overcome the challenges the pandemic has brought, says physicist Luiz Davidovich, president of the Brazilian Academy of Sciences in Rio de Janeiro. Davidovich spoke to Nature about the impact of anti-science attitudes and budget cuts in the country, as well as possibilities for post-pandemic research. Scientists are working intensely all over the country. Engineers are working to design reliable but less-expensive ventilators, chemists are exploring compounds for possible treatments, and mathematicians are using artificial intelligence to identify molecules that could help to alleviate patients’ pain. Physicist Luiz Davidovich.Credit: Luiz Davidovich There has also been research on possible vaccines, and clinical trials in the city of Manaus in the Amazon region on the effect of chloroquine and hydroxycholoroquine on people with COVID-19. But the researchers involved in these trials have had serious problems because of negative results. Because the trial results indicated that the drugs didn’t work, the scientists started getting calls from people threatening their lives and their families. That shows the situation we have here. The Brazilian Academy of Sciences has called for the government to stand by these scientists and protect them. Universities are closed, and it’s not clear when they will reopen. This is delaying research, especially experimental research. Also, being physically present is very important. You go to lunch with someone and you have ideas, you talk in an informal way. I don’t know how to Zoom someone and say, “Let’s have an idea” — it doesn’t work like that. I really miss the conversations when we’re talking about something else and it leads to a new idea. Not having that will definitely affect the development of science. Science organizations are issuing public statements criticizing the government’s anti-science stance. The president of the National Academy of Medicine and I signed a statement about the use of chloroquine and hydroxychloroquine, stating what science knows about these drugs and criticizing the government’s position. We are also working with the media. In a television news interview, I said that medications should be prescribed by medical doctors, not by the president of Brazil. We also organized a full-day virtual march for science in which these issues were discussed. Yes — a good example is the northeast of Brazil. That is one of the poorest regions, and there are many more scientists in the southeast than in the northeast. But in March, a scientific committee was formed to help the governors of the states in the northeast. The committee has issued reports on the development of science and ways to re-establish what we call the ‘new normal’ in the country. They are in close contact with the governors, and that is a good example for the whole country. [The governors of Rio de Janeiro and São Paulo have also formed scientific advisory committees.] We are trying to evaluate this more closely, but science is everywhere in the press. Scientists are being invited by television stations to talk about science. People say science is very important at the moment — but on the other hand some of them still think that Earth is flat, that humans don’t have any effect on the climate, and that natural selection is wrong. But I think Brazilian media are paying more attention to scientists. The national newspaper O Globo now has a section called ‘A Hora da Ciência’ [The science hour]. It features different scientists talking about science related to the pandemic each day. I hope they will continue this section after the pandemic. We learnt that you should prepare the country beforehand. When the Zika epidemic started, we had labs that could still work very well. We had cooperation between scientists all around the country, and researchers found ways to help mothers to avoid Zika, to take precautions. We learned that cooperation in science is important. Having good equipment is important. Having good people is important. Have we built a public policy based on this learning process? No, certainly not. Since 2013, science funding in Brazil has gone down steadily. We have obsolete equipment in many labs, and the labs have fewer supplies. Because of cuts to the science budget, young scientists have left Brazil. Four young people left my group last year — bright people who have gone to other countries. It’s not just a question of a lack of resources. It’s also the general ambience in the country, the fact that they feel there is no encouragement from the government to do science in Brazil. That’s bad for Brazil, because these are the people who bring new ideas, and they are very motivated. Brazil’s pharmaceutical industry is focused on generic drugs, producing pharmaceuticals that were originally developed by companies in other countries. There is a paradox there. Brazil has about 20% of the world’s biodiversity, and we don’t profit from that, because of an industrial policy that was betting on generics and not stimulating start-ups that could use Brazilian biodiversity for biomedicine. This pandemic has given rise to discussions on how to have a pharmaceutical industry centred on biopharmaceuticals based on Brazilian biodiversity. This interview has been edited for length and clarity. Latest on: Government Correspondence 19 MAY 20 World View 19 MAY 20 Obituary 15 MAY 20 Public health News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 Career Column 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '60'>22 May 2020</date>
<url id = '61'>https://nature.com/articles/d41586-020-01537-9</url>
<title id = '61'>When the coronavirus pandemic struck, field ecologist Nicholas Herrmann adopted a perspective inspired by experiences earlier in his PhD.</title>
<body id = '61'>Nicholas Herrmann seeks out lizards living by small streams in Costa Rica during the first year of his PhD.Credit: William Eldridge In February, two colleagues and I began a study of several hundred lizards in southern Florida to identify traits that determine which individuals succeed in new environments. I hoped the project would make up the final chapter of my PhD dissertation. Then came the COVID-19 pandemic. We ended the project prematurely in mid-March and went home to be with our loved ones. Abandoning several weeks of productive work and several thousand dollars of funding with nothing to show for it was deflating. Once home, my disappointment, combined with physical exhaustion from long days working in the field, resulted in a cycle of napping and playing video games to help me stop obsessing about what I had left behind. After a few days, I felt ready to re-enter the world. I turned on my computer and began to reflect. I thought about how, as a field ecologist, I am never fully in control of my research. My research subjects are literally wild. Often, this wildness makes work fun. In 2016, during the first year of my PhD, I studied semi-aquatic lizards living along small streams in Costa Rica. As I gleefully chased them from bank to bank, alert to the potential danger of encountering crocodiles or venomous snakes, I thought I had one of the best jobs in the world. On the other hand, sometimes that wildness makes work impossible: my projects are often thwarted by unanticipated acts of nature. For example, during a field season in Florida in June and July 2017 two years into my PhD, I collected data from lizard populations that were intended for a long-term experiment. But that September, Hurricane Irma blasted my study islands, knocking over trees and temporarily submerging the islands. With my study sites physically and ecologically altered, I scrapped my plans and went back to the drawing board. At the time, it was easy for me to remain calm. I still had four years of institutional support to complete a dissertation: that felt like forever. And my adviser is an expert at extracting interesting findings from fieldwork smashed by hurricanes. Although many of the data from that first summer were not useful or informative, some contributed to an analysis of post-hurricane changes to my original study populations. I also redesigned the abandoned experiment, modifying it to fit into a single field season to circumvent the potential for destruction by a hurricane. Attempt number two began in May 2018, when colleagues and I worked tirelessly to establish the same experiment on six islands with a total of several hundred lizards, all measured and uniquely marked so we could track their behaviour throughout the summer. We celebrated the successful set-up with a week-long vacation. But when we returned to survey our study islands, many of them seemed empty. Where had all the lizards gone? I don’t know for certain, but in the days that followed we encountered multiple herons, known predators of small lizards, lurking around our study sites. We ditched four of our six sites and tried to power through. Costa Rica’s forests are rewarding, but challenging, places to do fieldwork.Credit: William Eldridge This time, I did not handle adversity well. Overwhelmed by self-imposed pressure to salvage the project, I was irritable and angry in the field. I overreacted to small mishaps, such as loading our boat onto the trailer crookedly and accidentally breaking my lizard-catching pole. I also struggled to communicate. I dismissed my colleagues’ questions about our work as unimportant or uninteresting. This behaviour made me difficult to approach, so no one told me how my demeanour had changed, and my stress about the project blinded me from seeing it for myself until it was too late. I’ve spent the past two years reflecting on the circumstances that contributed to my loss of perspective and composure that summer. After Hurricane Irma the previous season, I just wasn’t ready for what felt like bad luck to ruin my progress again. I was fixated on finishing the experiment. Obsessed with the idea that I could still ‘win’ the field season, I failed to notice the strain I was putting on myself and my relationships with colleagues. Rather than accepting what I could not control, I broke. When the coronavirus outbreak escalated in the United States in March, my colleagues and I narrowed our options to two. Option one was to stay in the field. Our housing and study sites were very isolated, so we could finish our work without endangering ourselves or others. But amid uncertain projections about the spread of the coronavirus, it seemed unlikely that we would be able to travel safely in April, when we had planned to spend time at home between phases of the project. Option two was to abandon the work. The decision felt urgent. Several US cities and states, including our home states of Massachusetts, California and Hawaii, had already ramped up their restrictions on travel and social gatherings. If we were going to return, we should do so as soon as possible. At first it seemed foolish to abandon a project that we valued and had already invested in, knowing that we could potentially see it through. Then we thought about what we would truly be asking of ourselves if we stayed. Finishing the work and safely returning home at the end of May, without seeing our families in April as planned, was a best-case scenario. It was mid-March, and we had already been away for nearly four weeks. Could we be away for another ten? Maybe. But we didn’t know then, just as we don’t know now, when advice to avoid air travel would change. The possibility that we could get stuck away from home well past May was daunting.   We also considered the inevitable bumps in the road to completing any research project, even those that get off to a great start, as ours had that season. To handle setbacks with grace, it helps to have a reservoir of emotional energy to draw from lest you lose patience and overreact. I know this from experience. My colleagues and I also knew that regardless of how our work turned out, our emotional reservoirs would be repeatedly taxed by the strain of separation from home and our loved ones during a time of immense uncertainty. We chose option two and went home. I value my work, but for me, the decision was not merely a reflection of whether I think research is important. I have experience of bad fortune and how to accept it. This time, my desire to finish the project was just one component of my professional and personal identities and returning home felt like the right decision. My colleagues had similar feelings. Now, more than one month later, we have no regrets, and we’re eager to try the project again next year. Until then, I am desk-bound. I don’t always know what to do while sitting there. I’m a field ecologist. But I do know two things: I’m happy to be at home with my family; and when someone asks me what I accomplished during my PhD, I will tell them that nature taught me how to cope with events that I cannot control. This is an article from the Nature Careers Community, a place for Nature readers to share their professional experiences and advice. Guest posts are encouraged. Latest on: Careers Career Column 22 MAY 20 Career Column 21 MAY 20 Career Column 19 MAY 20 Ecology Article 13 MAY 20 Obituary 12 MAY 20 Editorial 12 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 Institut Pasteur of Shanghai, Chinese Academy of Sciences (IPS-CAS) Shanghai, China Institut Pasteur of Shanghai, Chinese Academy of Sciences (IPS-CAS) Shanghai, China Queensland University of Technology (QUT) Brisbane, Australia Charles Darwin University (CDU) Darwin, Australia An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '61'>22 May 2020</date>
<url id = '62'>https://nature.com/articles/d41586-020-01545-9</url>
<title id = '62'>See a baby planet forming at the heart of a spiralling disc of gas and dust. Plus: how coronavirus has cut carbon emissions, and why immunity passports are a bad idea.</title>
<body id = '62'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here This telltale ‘twist’ marks the site where a planet may be formingESO/Boccaletti et al. Astronomers have captured this stunning image of a planet forming at the heart of a spiralling disc of gas and dust near AB Aurigae, a star located around 520 light years from Earth. The snap was taken using a near-infrared camera on the Very Large Telescope in Chile. To get better contrast, researchers used a ‘coronagraph’ to block out the star’s light. But even then, it isn’t possible to glimpse the baby planet itself. “We see the structure that the planet produces on the spiral — this is what we call a twist,” says astronomer Anthony Boccaletti. Vice | 4 min read Source: Astronomy & Astrophysics paper Energy consumption is starting to rebound in China as lockdowns are eased.Hector Retamal/AFP/Getty Pandemic response slashes carbon emissions The international response to the coronavirus pandemic has so far cut global carbon emissions by more than 8%, according to estimates from a pair of independent research teams. That’s roughly three times the annual emissions of Italy. “We’ve never seen anything like this,” says climate scientist Corinne Le Quéré. But energy consumption is already rebounding in China and elsewhere, and the pandemic could register as little more than a blip in the climate system as government-imposed lockdowns come to an end. Nature | 5 min read Immunity passports are a bad ideaThe World Health Organization has spoken out against the idea of lifting restrictions for people who test positive for antibodies to the coronavirus, but some governments are still considering it. Natalie Kofler and Françoise Baylis point out ten practical problems and ethical objections — including the unreliability of tests that are currently available, the potential for discrimination and risks to privacy and public health. Nature | 11 min read Fierce hurricanes could threaten US efforts to fight COVID-19US forecasters are warning of an unusually active hurricane season this year, with predictions of dozens of severe Atlantic storms due to a La Niña climate event in the Pacific. Coping with the aftermath of hurricanes that make landfall will be challenging in light of the pandemic, researchers warn, as emergency shelters could become hotspots for infection. “This year is different because you can’t put a bunch of people in an elementary school with a disease spreading,” says Steven Davis, president of disaster-preparation firm All Hands Consulting. “We are having to redo all of our plans.” The Guardian | 6 min read Royal Society president Venki Ramakrishnan says scientists should not be penalized for giving advice to policymakers, after a UK cabinet minister appeared to blame mistakes in the country’s pandemic response on the ‘wrong’ science. (The Guardian | 6 min read) Recovered monkeys resist re-infection Monkeys that had recovered from infection with the coronavirus were shown to be protected from re-infection, although how long the protection lasts is unclear. Researchers gave doses of the coronavirus to nine rhesus macaques (Macaca mulatta), which developed mild symptoms and antibodies against the virus. A month later, they gave the monkeys another dose of virus. All nine mounted an antibody response to this second dose, suggesting that their immune systems had fought off the virus. Reference: Science paper Virus ravages organs from heart to brainAutopsies on 27 people with COVID-19 have found the coronavirus not only in the lungs, but also in the kidneys, liver, heart, brain and blood. By scrutinizing databases of genetic activity, researchers also found that three genes known to encourage SARS-CoV-2 infection are highly active in kidney cells. Additional analysis of six people detected virus in all examined kidney compartments, which helps to explain the kidney damage seen in some people with the illness. Reference: The New England Journal of Medicine paper Antibody against SARS could help to fight COVID-19An antibody discovered in the blood of a person who survived severe acute respiratory syndrome (SARS) could potentially help others to fight COVID-19. The new antibody, dubbed S309, recognizes and blocks both the COVID-19 coronavirus and the one that caused the 2003 SARS outbreak. It works by binding to a viral protein called spike that both viruses use to enter cells. Reference: Nature paper Get more of Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Researchers on the hunt for new materials are increasingly turning to artificial intelligence (AI) and machine learning. Algorithms can predict the physical characteristics of selected crystal structures from first principles, and neural networks can use that information to make guesses about much larger gamuts of possible materials. In future, automated labs might also help to make the materials more quickly. But even a robotic lab will need human overseers: synthesis still involves “a fair amount of artisanship,” says electrical engineer Ted Sargent. Wired | 6 min read Source: Nature paper NASA is naming its new space telescope after its former chief astronomer Nancy Grace Roman, who helped to make the Hubble Space Telescope a reality. (NASA | 6 min read) By now you’re probably aware of the concept of herd immunity — but what about nerd immunity? We’re loving this Twitter user’s epidemiological approach to fighting fake news. We’re not immune to (constructive) criticism! Send your feedback on this newsletter to briefing@nature.com. Emma Stoye, news editor, Nature With contributions by Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '62'>21 May 2020</date>
<url id = '63'>https://nature.com/articles/d41586-020-01544-w</url>
<title id = '63'>Scientists from both countries must rise above political rhetoric for the sake of all, says Xiao-Nong Zhou.</title>
<body id = '63'>China’s experience with the coronavirus could hold lessons for other countries.Credit: Kevin Frayer/Getty At the beginning of the year, I started receiving supportive messages from friends and colleagues across the globe. I realized that the fight against COVID-19 had caused worldwide concern, and that we all needed to work together to cope. It seems that we in China have weathered the coronavirus storm for now, just as other countries, such as the United States, are facing the worst of it. So, from my office at Shanghai Jiao Tong University School of Medicine, I find myself sending messages of support to overseas friends and colleagues, as they work from home and care for their children or support partners.   For most of us, this is our first experience of quarantines or lockdowns — and some will be longer than others. There are other firsts: our first attempt to explain virology to our children, our first bout of social distancing from those we love but don’t live with, our first day of spring spent unable to go outdoors. I find myself hoping for many things, both in my own life and in the lives of my colleagues here in China and overseas. I didn’t find working at home easy, and my sympathy goes out to those who are still under lockdown. I hope that the difficulties of working from home are the worst of their worries. Of course, many of our colleagues are treating patients in hospitals, or working in laboratories to develop the diagnostics and vaccines that might eventually be our best option against the coronavirus. The modelling labs I manage have been directly involved in researching COVID-19. This has created opportunities for frequent virtual meetings in which we exchange experiences with colleagues from Brazil, India, Switzerland, Tanzania, Zambia, the United States and elsewhere, collaborating on long-term strategies for how to fight the virus or other pathogens. Being quarantined made us care for each other more: we checked in with each other, and asked each other more often how we were feeling and how we could help. Learning how to support each other effectively has been a powerful thing — and I hope our lab continues to do it long after the coronavirus pandemic ends. But it needs to go further — we should help and support each other more as a worldwide scientific community, not just in individual labs, institutes or countries. The coronavirus doesn’t distinguish between geographical or political boundaries as it spreads. We shouldn’t make such distinctions when supporting each other, either. I hope we can proactively work together throughout the coronavirus outbreak. In particular, I hope my colleagues in China and the United States, the two largest economies and scientific powerhouses in the world, can work together effectively for the health of the world. I hope my US colleagues get through the coronavirus outbreak without facing its effects – both directly and in its impact on the global economy. I hope that US postdocs retain their funding, and that PhD students are able to continue their studies. I hope lab heads can continue leading research into the coronavirus and in every other scientific field, where their work will go on to improve the lives of future generations. I hope that my US colleagues endure, as we have, and learn to support each other, as we have. I can tell them it isn’t easy, but it is possible, and as scientists we’re used to things being not-easy-but-possible. The fight against COVID-19 will continue. More than 328,000 people have died as of 21 May 2020. More hard work lies ahead: I hope the two largest scientific superpowers in the world can shoulder their burden proactively and together. This is an article from the Nature Careers Community, a place for Nature readers to share their professional experiences and advice. Guest posts are encouraged. Latest on: Careers Career Column 22 MAY 20 Career Column 22 MAY 20 Career Column 19 MAY 20 Politics Obituary 21 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 Institut Pasteur of Shanghai, Chinese Academy of Sciences (IPS-CAS) Shanghai, China Institut Pasteur of Shanghai, Chinese Academy of Sciences (IPS-CAS) Shanghai, China Queensland University of Technology (QUT) Brisbane, Australia Charles Darwin University (CDU) Darwin, Australia An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '63'>21 May 2020</date>
<url id = '64'>https://nature.com/articles/d41586-020-01450-1</url>
<title id = '64'>Chemist and lawyer who shaped international weapons conventions.</title>
<body id = '64'>Credit: Carlos Saba In 1981, the US government publicly accused Soviet-backed forces in southeast Asia of waging toxin warfare and violating their legal obligations under the 1925 Geneva Protocol and 1972 Biological Weapons Convention. It alleged that aircraft dispersed ‘yellow rain’ containing mycotoxins that were “not indigenous to the region”. Julian Perry Robinson, working alongside biologist Matthew Meselson at Harvard University in Cambridge, Massachusetts, established that what actually fell was wild-honeybee faeces containing naturally occurring toxins. He died on 22 April, aged 78, This episode illustrates how Robinson helped to bring rationality into a field in which emotions often run high. His ideas influenced the negotiation and implementation of international law. In a major 1970 report for the World Health Organization, he began to articulate the idea that an ability to respond to natural-disease outbreaks could considerably diminish incentives to use chemical and biological warfare (CBW). He called for the strengthening of disease surveillance and other key areas in public health, and suggested that if there was to be “any chance of success”, a clear plan was needed for communicating information with the public. His death from complications of COVID-19 is therefore particularly poignant.   Robinson was born in Jerusalem in November 1941. His interest in CBW began during the final year of his chemistry degree at the University of Oxford, UK. Working under the economist John Jewkes, his dissertation examined how the study of chemical warfare during the Second World War stimulated the synthesis of new types of organic compounds. On graduating, he spent four years with Kilburn & Strode Chartered Patent Agents in London before taking a position at the fledgling Stockholm International Peace Research Institute (SIPRI) in 1968, and becoming its focal point for CBW studies. From the mid-1970s, he was also central to the highly influential CBW Study Groups convened by the Pugwash Conferences on Science and World Affairs, which brought together scientists from east and west to discuss disarmament. At SIPRI he had a prominent role in a major review of CBW. This resulted in the classic six-volume study The Problem of Chemical and Biological Warfare (1971–75). He developed the concept of a cross-cultural ‘taboo’ on such weapons, and detailed the process of ‘assimilation’ that leads to their acceptance into existing military organizations. For example, chemical weapons went unused during the Second World War not because of deterrence, but because specialized First World War-era chemical warfare organizations were sidelined by conventional military institutions. Julian Perry Robinson in the laboratory in 1957.Credit: Julian Perry Robinson He met Mary Kaldor, his partner of more than 50 years, at SIPRI, and they returned to the United Kingdom in 1971 to join the Science Policy Research Unit at the University of Sussex in Brighton. Robinson spent the rest of his career there establishing the Harvard Sussex Program, a group for research, communication and training in support of informed public policy towards CBW issues that he co-directed with Meselson. There, he spotted another barrier to assimilation: existing technologies constrain emerging ones as they pass through a ‘weapons succession process’. Part of Robinson’s influence lay in identifying and seeking to close loopholes through which suppressed CBW technology might develop. He provided key inputs into the negotiations for the Biological Weapons Convention and the 1993 Chemical Weapons Convention. He advocated for proper implementation of the ‘general purpose criterion’ — the concept that malign intentions rather than physical objects should be prohibited — as the main mechanism by which the treaties could avoid being undermined by advances in science and technology.   In the years just after the cold war, he stood firm against what he considered to be a “creeping legitimization” of those chemical and biological weapons not tooled for mass destruction (non-WMD CBW). In 2008, he warned that chemical weapons lent themselves particularly to the new types of conflict experienced in places such as the Balkans, Afghanistan and parts of Africa, suggesting the taboo against CBW could become harder to maintain. These became the hallmark issues that he encouraged numerous research students to tackle. His mentoring of generations of scholars and policy practitioners means his ideas will continue to shape both treaties for decades. He officially retired from the University of Sussex in 2007, but continued to work at the Harvard Sussex Program, happiest when receiving visitors who came to discuss ideas and work in the extensive and unique archive on CBW issues that he and Meselson established. One of his last research projects resulted in a series of vignettes from across history that he described as “lessons about CBW” for future generations. He also continued to write detailed chronologies — including one on ‘novichoks’, after the nerve agents were used in Salisbury, UK, in 2018. I became a student of Julian’s in 1996, and continued to work alongside him at the Harvard Sussex Program until he went into self-isolation. One of our last conversations was about establishing a small study group to consider whether his influential chronology on chemical weapons in Syria might reveal patterns that those guarding against CBW should know about. Always modest, he was hesitant about whether people would spend time reading it. When I suggested some might, he smiled and said “Well now, there’s a thought. Let’s talk more on the other side of all of this.” </body>
<date id = '64'>21 May 2020</date>
<url id = '65'>https://nature.com/articles/d41586-020-01526-y</url>
<title id = '65'>Funders require that researchers clearly explain their science to a general audience. Pakinam Amer discovers the secrets of sound science communication.</title>
<body id = '65'>  In a world currently facing an unparalleled health crisis, the need for clear science communication has never been greater. Explaining complex ideas in a concise manner does not come naturally to everyone, but there are some simple rules you can follow. In the opening episode of this six-part series about science communication, Pakinam Amer discusses the craft of clear storytelling and science writing with seasoned communicators and journalists. Siri Carpenter, editor of The Craft of Science Writing, a selection of resouces from science writing platform The Open Notebook, explains how science journalism and science communication differ, but share important characteristics, including “a search for some kind of truth, driven by curiosity and sometimes the desire to right some wrong.” But how do you structure a story so readers are hooked from the start, explain complicated ideas, avoid jargon, check facts? “There are so many skills that go into good science writing,” Carpenter says. “It takes time and practice to get better, learning from mistakes, and from feedback.” Islam Hussein, a virologist and senior scientist at biopharmaceutical company Micobiotix in Boston, Massachusetts, tells Amer why he took to the video platform YouTube in 2014 to tackle pseudoscience after a hand-held device was wrongly touted as a tool to detect viral infections in his native Egypt. “There was a lot of interaction between me and the public. Some of it was good. Some of it was not, in the form of insults and threats," he says. With the help of his son, he rigged up a home studio in his basement to create more multimedia content, using his wife and colleagues to get feedback before getting it live. “When I hit record I want to make sure I am saying something useful and accurate,” he says. Hussein now regularly appears on the TV to explain the emerging science behind COVID-19. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '65'>20 May 2020</date>
<url id = '66'>https://nature.com/articles/d41586-020-01527-x</url>
<title id = '66'>Listen to the latest science news, brought to you by Nick Howe and Shamini Bundell.</title>
<body id = '66'>  In this episode: Researchers fabricate an artificial eye complete with a human-like retina. Research Article: Gu et al.; News and Views: Artificial eye boosted by hemispherical retina Dazzling elephant seals to avoid predation, and helping blind people ‘see’ through brain stimulation. Research Highlight: Mighty seals humbled by prey that flickers and flashes; Research Highlight: Blind people ‘read’ letters traced on their brains with electricity There’s an open question about how disk-galaxies form, but now new observations are pointing to an answer, from the very early Universe. Research Article: Neeleman et al.; News and Views: Galaxy disk observed to have formed shortly after the Big Bang We pick our highlights from the Nature Briefing, including a HIV ‘vaccine’, and incredibly hardy bacteria. Science: Long-acting injectable drug prevents HIV infections; Quanta Magazine: Inside Deep Undersea Rocks, Life Thrives Without the Sun Subscribe to Nature Briefing, an unmissable daily round-up of science news, opinion and analysis free in your inbox every weekday. Never miss an episode: Subscribe to the Nature Podcast on Apple Podcasts, Google Podcasts, Spotify or your favourite podcast app. Head here for the Nature Podcast RSS feed﻿. Latest on: Applied physics News & Views 20 MAY 20 News & Views 04 MAR 20 News & Views 12 FEB 20 Astronomy and astrophysics News & Views 20 MAY 20 Article 20 MAY 20 News Q&A 19 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Host: Shamini Bundell Welcome back to the Nature Podcast. This week, making an artificial eye… Host: Nick Howe And how disk galaxies formed in the early Universe. I’m Nick Howe. Host: Shamini Bundell And I’m Shamini Bundell. [Jingle] Host: Shamini Bundell First up, Darwin once pointed to the human eye as an example of evolution through natural selection that people might find hard to accept. An eye is an incredibly complex organ, but researchers working on artificial vision have managed to craft an eye that mimics our own, complete with a working retina. Reporter Geoff Marsh spoke to the scientists involved to find out more. Interviewee: Zhiyong Fan Because I used to see a lot of sci-fi movies, they have the artificial eyes, so I found this technology very interesting. My name is Zhiyong Fan. I’m a professor working in Hong Kong University of Science and Technology. Interviewer: Geoff Marsh What’s your favourite sci-fi film with an artificial eye? Interviewee: Zhiyong Fan Oh, Star Trek for sure. In 2012, we developed some enabling technologies. Then in 2016, we started with the actual work to fabricate our biomimetic eyes. Interviewer: Geoff Marsh We have cameras already with incredible resolution. What’s so special about the layout of the human eye? Why is that something to aim for? Interviewee: Zhiyong Fan One of the unique features everybody knows – our eyes are spherical-shaped. So, the spherical shape makes it very easy for the eyeball to move in our eye sockets. Of course, inside our eyeball, we have this hemispherical retina. So, this hemispherical retina also makes our single eye field of view much bigger than our camera, which uses flat image sensors. Interviewer: Geoff Marsh And that hemispherical shape, that dome shape, of the retina, am I right in thinking that that is what has posed the greatest fabrication challenge for you and your team? Interviewee: Zhiyong Fan Exactly, even now it’s very difficult to fabricate the sensing devices, circuitry and the hemispherical substrate. It’s very difficult. So, that is one of the biggest challenges we encountered in the very beginning. Interviewer: Geoff Marsh What are your photo-sensing nanowires? What are they made of? Interviewee: Zhiyong Fan So, these are semi-conducting light-sensitive materials. In this work, we used a new material called a perovskite material. Interviewer: Geoff Marsh Perovskite? Interviewee: Zhiyong Fan Yes. We used perovskite nanowires. Interviewer: Geoff Marsh And that’s the same material that’s used in photovoltaics. Interviewee: Zhiyong Fan That’s right, the new type of material for photovoltaic technology. So, this material converts the photon into the charges, so these nanowires are light-sensing materials. They are very sensitive to light and their density is around six times higher than the density of the photoreceptors on the human retina, so they have a higher density for sure. Interviewer: Geoff Marsh What was the key to managing to embed your photosensor array on that dome shape? Interviewee: Zhiyong Fan I guess the key is we achieved the integration and the material fabrication in one step. In the past, people have already tried. People work on nanotechnologies and make small devices. Often, they make the material first, then they transfer the material to somewhere else, so they do things in multiple steps, two steps at least. Interviewer: Geoff Marsh So, what your team did differently was you constructed your dome-shaped retina with the photosensing nanowires in that shape. You didn’t start on a flat surface and then try and fold it up. Interviewee: Zhiyong Fan That’s right. That’s right. That’s the key. Interviewer: Geoff Marsh And so, that allowed you to make that array much denser because you didn’t have to leave gaps between the photosensors to allow for folding. Interviewee: Zhiyong Fan Right, right. Another additional feature which makes our technology very unique is our technology is basically an electrochemical photo detector, unlike those solid-state devices. So, it’s a liquid inside. It’s just like how our eyeball has liquid inside. So, every single sensing pixel they’re connected with is liquid. But on the back side, we chose to use liquid metal wires as artificial nerve fibres to connect with the individual sensing elements because they are very flexible and also highly conductive, and the same principle is our human eye. Interviewer: Geoff Marsh What could your device ‘see’? How did you test that? Interviewee: Zhiyong Fan Well, in principle, we can achieve much higher image resolution than the human eye. So, our human eye, we have a resolution limit. If we look far away, for example, a star far away from us, it might only project a dot on our retina. The size of the dot, if it’s smaller than 3 micrometres on our retina then we can’t even see it, right. But if we use our nanowires and the size of this dot created by the star on our retina, if it’s 1 micrometre then we can see it. Interviewer: Geoff Marsh Does that mean then that if in the future someone did have these eyes implanted and they looked up at the night sky that they would see a much richer array of stars? Interviewee: Zhiyong Fan Absolutely, definitely you would see more stars. If you don’t look at a star and just look at surroundings, you’ll have a much clearer image and you can see a much further distance. Interviewer: Geoff Marsh So, I understand that you’ve put this artificial eye through a series of kind of performance tests. Interviewee: Zhiyong Fan So, we tested the sensitivity of our artificial retina. So, we can measure very weak light, and we also measured the response speed of our artificial retina. The response speed is a little bit more than 10 milliseconds, so that is already much faster than the human eye. The human eye will be something like 40 milliseconds, so our artificial retina is four times faster. So, we also measured the imaging functionality, the object projecting onto the retina and then we use a computer to acquire data and reconstruct the object so that we also can demonstrate. So, it has a higher fidelity. Interviewer: Geoff Marsh I mean it sounds like if this ever ended up in a human that they would have superpowers. Interviewee: Zhiyong Fan That’s right. It would be a super eye, basically. Interviewer: Geoff Marsh Yeah, you could take this technology further and change the materials so that they’re sensitive to different parts of the light spectrum, and then wow, that’s a whole new visual world. Interviewee: Zhiyong Fan Exactly, all of a sudden, the whole electromagnetic spectrum is open to us. Interviewer: Geoff Marsh Ultimately, Zhiyong, what do you suppose this technology will be used for? Is this just going to be a new kind of camera technology or a more realistic robot gadget or do you think that they’re going to find their way into our eye sockets? Interviewee: Zhiyong Fan They are two major application scenarios. So, one is help the blind people if they have an illness on the retina. So, we can use our artificial retina to replace that. The other one is humanoid robotics, so their eye also needs to look like human eye. We can use our artificial eye to replace those eyes made by flat technologies. Interviewer: Geoff Marsh I suppose another whole challenge which your team have not directly addressed here would be how you could interface such a technology with the human brain. That’s a whole other story. Interviewee: Zhiyong Fan Yes, I’m looking for collaborators working on biomedical research to modify our system, for example, to use more biocompatible materials. Interviewer: Geoff Marsh Do you think that your enjoying Star Trek has actually led to this discovery? Interviewee: Zhiyong Fan Definitely. In my research, I’ve had a lot of crazy ideas from sci-fi movies, so this is just one of them. Host: Shamini Bundell That was Zhiyong Fan from Hong Kong University of Science and Technology. If you want to find out more about artificial eyes, then you can find Zhiyong’s paper over in the show notes. Host: Nick Howe In a bit, we’ll be hearing about the earliest detected disk galaxy. Right now, though, it’s time for the Research Highlights with Dan Fox. [Jingle] Dan Fox The bioluminescent prey of southern elephant seals have a trick up their sleeves when they’re being hunted – a dazzling flash. By fitting seven seals with light, movement and location sensors, a team of researchers have discovered that some of the bioluminescent creatures, such as squid and lantern fish, that the seals feed on dazzle their attackers with a burst of light. Seals pursuing flashing prey took longer hunting compared to those capturing non-flashing targets, suggesting that bedazzlement is a good defence mechanism. However, one of the seals seems to have wised up to this and devised a trick of its own – subtly twitching its head to cause prey to trigger their flash early and reveal themselves in the darkness. Read the rest of that dazzling discovery over at the Journal of Experimental Biology. [Jingle] Dan Fox Blind people can now see letters if the right part of their brain receives precise electric jolts. People who were sighted before becoming blind typically have damage to their eyes or optic nerve but not to the visual cortex – the region of the brain that processes visual information. In fact, electrically stimulating the visual cortex triggers flashes of light that could allow the brain to create a recognisable picture, but the image is often shapeless. But researchers from the US have harnessed a new technique using an approach similar to tracing a shape on the palm of a hand but instead, transmitting short bursts of electricity in a sequence that mimics the shape of letters. In trials, two individuals who had lost their sight could correctly identify 80% of the letters they were shown, and the researchers think the approach could also be used to trace the outlines of common objects such as houses or cars. Trace out the rest of that research in full at Cell. [Jingle] Interviewer: Nick Howe Next up, there are two main theories about how disk galaxies form. Now, some new observations are providing clues about how this would have happened in the very early Universe. A long time ago, a galaxy formed far, far away. It looked kind of similar to our own Milky Way. It had lots of gas and was a slowly spinning disk. But how do disk galaxies like this form? Interviewee: Marcel Neeleman So, there’s two ways that galaxies are forming. Interviewer: Nick Howe This is astrophysicist Marcel Neeleman. Interviewee: Marcel Neeleman So, one way is that gas accretes onto these systems, so gas from outside falls on to the galaxy and then gets really heated, as hot as possible, and then they slowly fall into the central region and then form stars and then also the external galaxy itself that we can see. Or the second way is that maybe this gas still falls in from outside, but doesn’t get heated as much. It stays cold and then falls directly on to the galaxy. The difference between them is that one will take a lot longer. If gas falls in and gets heated, it takes a long time, so if you can find a galaxy very early on, you might be able to rule that model out. Interviewer: Nick Howe These two methods of disk galaxy formation – slowly with hot gas or quicker accumulation of cold gas – are both thought to occur in the Universe. But to work out which method was occurring when, researchers have to look back in time. Simulations have suggested that cold gas method could have been a dominant mode of disk galaxy formation in the early Universe. To confirm this, researchers need to peer back around 12 billion years by looking at something 12 billion lightyears away, and not only detect it but determine it’s a disk galaxy too. Interviewee: Marcel Neeleman You need the best telescopes in the world. Interviewer: Nick Howe This week in Nature, Marcel has put one of the top telescopes to use. In stunning resolution and detail, he’s observed a disk galaxy that formed only 1.5 billion years after the Big Bang. Universe-wise, that is not a long time, making this the earliest detected disk galaxy so far. The fact that it must have formed so quickly is good evidence for the cold method of galaxy formation. Interviewee: Alfred Tiley I was initially very surprised that there was a disk at this epoch. Interviewer: Nick Howe This is Alfred Tiley, an astronomer who wasn’t associated with Marcel’s study. Interviewee: Alfred Tiley From an observational perspective, everything that we have seen so far, including imaging from the Hubble Space Telescope, studies with our best instrumentation on the ESO Very Large Telescope, all of that was telling us that disks in galaxies start to emerge around 4 billion years after the Big Bang. So, to see evidence for a disk in a galaxy only 1.5 billion years after the Big Bang, that was surprising to me. However, it kind of is in line with theoretical expectations that cold-mode accretion should dominate in the early Universe. I think it’s a very exciting result. Interviewer: Nick Howe Alfred was particularly impressed with how well this early disk galaxy was able to be observed. Interviewee: Alfred Tiley It’s shown that it’s technically possible to not only detect this light that’s emitted from the gas in galaxies only 1.5 billion years after the Big Bang, but also to detect it with sufficient sensitivity and spatial resolution to really do a proper study of its properties. Interviewer: Nick Howe Whilst Alfred does think this newly observed galaxy is good evidence for the cold method of formation being dominant in the early Universe, there are some other possibilities of how it could have formed, such as two non-disk galaxies colliding. So, Alfred would like to see more examples of disk galaxies 1.5 billion years after the Big Bang. Interviewee: Alfred Tiley And then you really start to get a clearer picture of, okay, is this systematic across all galaxies at that time or is their galaxy an outlier from the rest of the galaxy population at that time? Interviewer: Nick Howe Marcel agrees and his next steps are to try and locate more early disk galaxies. Interviewee: Marcel Neeleman We found one so that’s a good start, but we definitely need more to confirm that this is a common way of forming galaxies, but we have good reason to believe that this is more common that we thought it was going to be. So, we found a galaxy that was pretty normal. There was nothing special about this galaxy when you look at it. So, the fact that we found it in this kind of way, that we selected a normal galaxy that doesn’t stick out in any kind of possible way, gives us a good feeling that this was probably a very normal way of forming galaxies. Interviewer: Nick Howe Whether or not this cold method of formation was indeed common throughout the early Universe remains to be seen, but both Alfred and Marcel think this is a key first step and shows that observations like this are possible, and that’s something that’s going to be really exciting for the astronomy community who will be keen to peer back into the early Universe. Interviewee: Alfred Tiley In a way, given that it’s only one galaxy, that’s even more exciting because I think this is going to push people to try and construct large samples of galaxies at a similar epoch and so I think, in terms of what it’s going to do for astronomy, I think it’s going to really focus the attention of astronomers on galaxy evolution and galaxy formation at a much earlier epoch than has been possible with the previous generation of telescopes and instrumentation. Interviewer: Nick Howe That was Alfred Tiley from the University of Western Australia. You also heard from Marcel Neeleman from the Max Planck Institute in Germany. You can check out Marcel’s paper over in the show notes where you’ll also find a link to a News and Views article written by Alfred. Host: Shamini Bundell Last up, it’s time for a quick chat about some other non-corona science stories out there. Nick and I have been browsing the Nature Briefing for the past few weeks – that’s Nature’s daily pick of science news and stories – and Nick, what do you want to share this time? Host: Nick Howe So, I’ve been looking into a new preventive treatment for HIV. So, at the moment, if you are at high risk of getting HIV, what you would do is take a combination of two drugs every day, but that’s really hard to stick to and it can mean that you leave yourself vulnerable if you forget or you miss a dose or something like that, and so what this is it’s an injection that you take once every two months, and that would be enough to protect you from the virus. Host: Shamini Bundell Wow, going from once a day to once every two months would be a massive lifestyle change, and what kind of people is this going to benefit? Host: Nick Howe Well, I guess it would benefit those people that are most at risk from HIV, so that might be sex workers, it might be people in relationships with people who are HIV positive who don’t want to run the risk, and there are certain groups of individuals, such as men who have sex with men, that are at higher risk than other groups, so those would be the type of people that it would be for. But as you say, it would be really great to be able to switch from taking two things once a day to something every couple of months. It would be much easier to stick to, and that’s the hope of the researchers. Host: Shamini Bundell And so, this is something that they’re still trialling but is looking hopeful. Host: Nick Howe Yeah, so this is still in clinical trials and, I must say, it hasn’t been peer reviewed as of yet, and also the trial is being a bit disrupted by the coronavirus outbreak. But it looks good from what they’ve shown so far, and the incidence of HIV in the volunteers is the same as those people taking the current treatment, which are the drugs, and so it looks like it could be a good potential treatment. Host: Shamini Bundell Brilliant, well, we’ll keep an eye out for the results of those trials then. My story this week is about bacteria. Woo, love the bacteria, and it’s about bacteria that are apparently hanging out in rocks all over the place, well, under the surface of the oceans, deep within the oceans in the rock, in the Earth’s crust. Somewhere there’s no light, there’s really barely any food, and you really wouldn’t be expecting to find life. Host: Nick Howe Those bacteria, they seem to just get everywhere, don’t they? I swear like there is no place that they can’t exist, but I do wonder – as you said, there’s no light, there’s no food – what exactly are these bacteria living on? Host: Shamini Bundell Well, it’s not obviously completely unknown that creatures can exist without sunlight and photosynthesis, so of course, deep sea vents are a little fountain of life deep below the surface of the ocean, and some of the bacteria in rocks under the oceans could be sort of getting food that slowly makes its way through. Another possibility is some of them might be getting food from, well, essentially, radioactivity in the rocks, providing that energy that they’re able to convert into a power source. Host: Nick Howe Even with radioactivity and things like that, they’re can’t be a lot of energy down there for them to use, so what exactly are they doing? Host: Shamini Bundell Well, a lot of them seem to be living and growing, just extremely slowly. So, one of the researchers in this Quantum Magazine article talks about there maybe being a single cell living for 100 or 1,000 years before dividing or reproducing, so potentially a completely different sort of rate of life than us surface folks would ever be able to comprehend. And then studying them might actually be really interesting because if these bacteria can survive with just rock and water, that might give us clues about where to look for life on other planets and what sort of life that could be. Host: Nick Howe Well, I guess slow and steady really does win the race then. Thanks for that Shamini. Listeners, if you’d like more short snippets of science just like that we discussed but instead to your email inbox then make sure you check out the Nature Briefing. We’ll put a link to that in the show notes along with links to the articles we discussed. Host: Shamini Bundell That’s all for this week. If you want to get into contact with us then you can find us on Twitter – we’re @NaturePodcast – or if you are more email inclined then we’ve got you covered too – we’re podcast@nature.com. I’m Shamini Bundell. Host: Nick Howe And I’m Nick Howe. Thanks for listening. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '66'>20 May 2020</date>
<url id = '67'>https://nature.com/articles/d41586-020-01534-y</url>
<title id = '67'>Studies have shown tentatively positive results, but it’s still too soon to say whether the vaccines actually work. Plus: a major update for carbon dating, and the search for a ninth planet in our Solar System.</title>
<body id = '67'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Researchers use data from tree rings to calibrate the process of carbon dating.Philippe Clement/Arterra/Universal Images Group/Getty Radiocarbon dating is due to be recalibrated using a slew of new data from around the world, including those from tree rings, lake and ocean sediments, corals and stalagmites. The result could have implications for the estimated ages of many finds — such as Siberia’s oldest modern human fossils, which according to the latest calibrations are 1,000 years younger than previously thought. Archaeologists are downright giddy. “This is a particularly exciting time to be working on the past,” says archaeological chronologist Tom Higham. Nature | 5 min read Is there a ninth planet in the Solar System (or a tenth, for those who still refuse to accept Pluto’s demotion)? String-theory guru Edward Witten has suggested shooting tiny spacecraft into the outer Solar System to find it, and another team has proposed a slightly different technique. Astronomers have previously seen hints of the planet — orbiting some 500 times farther from the Sun than Earth is. Some have speculated that it could be a small black hole, but even the latest proposals would not be able to establish exactly what it is. Physics World | 6 min readSource: arXiv preprint 1 and preprint 2 Scientists have used satellite data to produce the first large-scale maps of microscopic algae growing in coastal Antarctica, where large blooms are creating patches of ‘green snow’. Their observations will be used to assess the speed at which the continent is turning green because of climate change; warmer temperatures create more of the slushy conditions the algae need to thrive. The blooms could even create a source of food for other species. “This could potentially form new habitats. It’s the beginning of a new ecosystem,” says biologist Matt Davey, who co-led the study. The Guardian | 5 min read Coronavirus vaccines are being tested in humans and animals.Juan Ignacio Roncoroni/EPA-EFE/Shutterstock Hope and caution greet vaccine trial results This week, US biotechnology firm Moderna revealed that its COVID-19 vaccine triggered an immune response in people, and protected mice from SARS-CoV-2 infections in the lungs. The data have not yet been published or peer-reviewed. Vaccines being developed in the United Kingdom and China have shown tentatively positive — but not Earth-shattering — results in monkeys. The good news is that all three vaccines appear to be safe enough to proceed to further clinical trials. It’s too soon to say whether they actually work. Nature | 5 min read The science of superspreaders Why do some people with COVID-19 infect many others, whereas most don’t spread the virus at all? Epidemiologists have been looking at superspreading events — large clusters of cases linked to a specific location or gathering, or even to a single person — to try to understand the factors that affect how the virus spreads in different settings. “If you can predict what circumstances are giving rise to these events, the math shows you can really, very quickly curtail the ability of the disease to spread,” says infectious-disease specialist Jamie Lloyd-Smith. Science | 8 min read Female academics held back by lockdowns Early analyses show that female academics are posting fewer preprints and starting fewer research projects than their male peers amid the pandemic. Ecologist Megan Frederickson studied preprint servers to investigate whether women were publishing fewer papers than they were before lockdowns began. The analysis — and several others — suggests that, across disciplines, women’s publishing rate has fallen relative to men’s. The findings support earlier, anecdotal reports that female academics’ productivity has suffered disproportionately under lockdown. Frederickson notes that the lockdowns so far have been relatively short compared with the usual research timeline, so the long-term implications for women’s careers are unclear. Nature | 6 min read  Cryptographer Matthew Green says there’s no guarantee that contact-tracing apps will help to curb the spread of coronavirus, especially if uptake is lower than expected. (Nature | 9 min read) Chemist Alice Ball is finding recognition long after she died at age 24. While she was an instructor at the College of Hawaii in 1915, Ball invented a process for producing an anti-leprosy drug from the tropical chaulmoogra tree. The compound she created became the standard treatment for leprosy until the advent of antibiotics in the 1940s, but her supervisor failed to mention her name when he reported the discovery in the medical literature. “Alice was not, as a woman and especially as a black woman, deemed the esteem and status of that of a man, particularly a white man,” says historian Kathryn Waddell Takara. Chemistry World | 7 min read It took PhD student Lisa Piccirillo less than a week to solve a decades-old mathematical problem about the Conway knot — a knot that was discovered by legendary mathematician John Conway more than 50 years ago. (Quanta | 8 min read) Check out these lab simulations showing how mud moves under Martian conditions — the way it oozes along reminds researchers Petr Brož and Manish Patel of thick lava, or ‘boiling toothpaste’. Help us to find our flow — send any feedback on this newsletter to briefing@nature.com. Emma Stoye, news editor, Nature With contributions by Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '67'>20 May 2020</date>
<url id = '68'>https://nature.com/articles/d41586-020-01497-0</url>
<title id = '68'>Near-real-time data on carbon emissions reveal the sectors, countries and events that had the most impact, but it is unclear how long the dip will last.</title>
<body id = '68'>Compared to last year, China's carbon emissions through March were roughly 10% lower in 2020.Credit: Hector Retamal/AFP/Getty The international response to the coronavirus pandemic has so far slashed global carbon emissions by more than 8%, according to detailed estimates from a pair of independent research teams. That’s roughly three times the annual emissions of Italy. But energy consumption is already rebounding in China and elsewhere, and the pandemic could register as little more than a blip in the climate system as government-imposed lockdowns come to an end. Most reporting on carbon emissions takes place annually, but the unprecedented social and economic shock brought about by the pandemic has spurred interest in tracking energy and emissions trends in real time. Pulling information from a variety of sources — including energy and weather reports, satellite-based observations and traffic data collected by vehicle-navigation systems in more than 400 cities around the world — two international teams have now provided the first estimates of how carbon emissions are changing daily across the globe.   Although they differ in certain details, the analyses come to similar conclusions: carbon emissions fell by more than one billion tonnes in the first four months of the year compared with the same period in 2019. “The question was in the air,” says Corinne Le Quéré, a climate scientist at the University of East Anglia in Norwich, UK, and lead author of one study, published on 19 May in Nature Climate Change1. “We developed two different methods, so it’s quite encouraging to see that our results are comparable.” Le Quéré’s team compiled information on daily emissions from different sectors in a variety of cities, regions and countries, and then analysed the measures taken by governments to control the spread of coronavirus. On the basis that these measures will be relaxed over the course of this year, the team projected that the cumulative global emissions for 2020 could drop by anything from 4–7%, which would represent the largest drop since the Second World War (see ‘A V-shaped recovery’). “We’ve never seen anything like this,” Le Quéré says. Source: ref. 1 In fact, the scale of the reduction in emissions this year could be similar to the annual emissions reductions that would be required in order to meet the objectives of the 2015 Paris climate agreement, which seeks to limit global warming to 1.5 to 2°C above preindustrial levels. If the 2008 economic recession is any guide, however, emissions could recover quickly. The question is whether societies will change, and whether governments will advance a low-carbon energy agenda as they seek to stimulate the economy, says Philippe Ciais, a carbon-cycle researcher at the Laboratory of Climate and Environmental Sciences in Gif-sur-Yvette, France, who is leading a second effort to monitor global emissions in near-real time. “With this kind of data, we hope to look for answers.” Ciais’s team is building a prototype monitoring system that operates on timescales of days to weeks2. They analysed energy data from more than 400 cities and 130 countries as well as weather data across the globe to produce daily estimates of carbon emissions for 2019 and 2020. According to their preliminary estimates, global emissions started to significantly diverge from last year’s in March, as countries around the world began shutting down businesses and enforcing social-distancing measures (see ‘Daily comparisons’). Source: ref. 2 China, unsurprisingly, led the decline (see ‘China leads’). Figures from the world’s largest producer of greenhouse gases started to fall sharply in January, but much of that decline coincided with an annual reduction in energy consumption at the start of the Chinese New Year. Lockdowns in China helped to maintain lower emission levels in the country, accounting for a 10% reduction until the end of March compared with last year. As economic activity picked up in China, other countries went into lockdown mode, depressing global emissions throughout April. Source: ref. 2 The aviation industry experienced the a dramatic decline, with emissions falling by more than 21% in the first four months of the year, but the largest emission reductions in absolute numbers were in the electric power and ground transport sectors (see ‘Roads and power’). Source: ref. 2 Commercial and industrial demand for electricity dropped off as businesses closed down, and people all over the world parked their cars and stayed at home (see ‘Divergent paths’). “That’s one of the things that stands out: people still drove during the 2008 economic recession, but they are not driving now,” says collaborator Steven Davis, an earth-system scientist at the University of California Irvine. The data and methodologies developed by Ciais and his team have yet to be published in a peer-reviewed journal. Source: ref. 2 Such work could help to fill a yawning gap in emissions monitoring. “This has enormous potential,” says Bill Hare, who heads Climate Analytics, a non-profit consultancy in Berlin. Monitoring emissions in near-real time will ultimately be required as countries move to curb emissions in the future, Hare says, and that probably means developing energy-tracking systems as well as satellite systems that can provide a detailed view of how energy consumption, and thus emissions, change in response to climate policies. Le Quéré, C. et al. Nature Clim. Change https://doi.org/10.1038/s41558-020-0797-x (2020). Liu, Z. et al. Preprint at https://arxiv.org/abs/2004.13614 (2020). Download references Latest on: Climate change News Feature 22 MAY 20 Correspondence 19 MAY 20 Obituary 15 MAY 20 Climate sciences News Feature 22 MAY 20 Article 20 MAY 20 Correspondence 19 MAY 20 Energy Correspondence 19 MAY 20 News Feature 22 APR 20 Article 02 MAR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '68'>20 May 2020</date>
<url id = '69'>https://nature.com/articles/d41586-020-01294-9</url>
<title id = '69'>Early analyses suggest that female academics are posting fewer preprints and starting fewer research projects than their male peers.</title>
<body id = '69'>Evidence shows that women perform more household work than men do.Credit: Getty Quarantined with a six-year-old child underfoot, Megan Frederickson wondered how academics were managing to write papers during the COVID-19 pandemic. Lockdowns implemented to stem coronavirus spread meant that, overnight, many households worldwide had become an intersection of work, school and home life. Conversations on Twitter seemed to confirm Frederickson’s suspicions about the consequences: female academics, taking up increased childcare responsibilities, were falling behind their male peers at work. But Frederickson, an ecologist at the University of Toronto, Canada, wanted to see what the data said. So, she looked at preprint servers to investigate whether women were posting fewer studies than they were before lockdowns began. The analysis — and several others — suggests that, across disciplines, women’s publishing rate has fallen relative to men’s amid the pandemic. The results are consistent with the literature on the division of childcare between men and women, says Molly King, a sociologist at Santa Clara University in California. Evidence suggests that male academics are more likely to have a partner who does not work outside the home; their female colleagues, especially those in the natural sciences, are more likely to have a partner who is also an academic. Even in those dual-academic households, the evidence shows that women perform more household labour than men do, she says. King suspects the same holds true for childcare. Source: M. Frederickson https://github.com/drfreder/pandemic-pub-bias/blob/master/README.md (2020; All-author analysis); P. Vincent-Lamarre, C. R. Sugimoto and V. Larivière Nature Index https://go.nature.com/2XhxqxR (2020; First-author analysis). In her analysis, Frederickson focused on the two preprint servers that she uses: the physical-sciences repository arXiv, and bioRxiv for the life sciences. To determine the gender of more than 73,000 author names on 36,529 preprints, she compared the names with those in the US Social Security Administration’s baby-name database, which registers the names and genders of children born in the United States. Frederickson looked at arXiv studies posted between 15 March and 15 April in 2019 and in 2020. The number of women who authored preprints grew by 2.7% from 2019 to 2020 — but the number of male authors increased by 6.4% over that period. The increase in male authorship of bioRxiv preprints also outstripped that of female authorship, although by a smaller margin (see ‘Preprint drop-off’). (The two servers are not directly comparable in Frederickson’s analysis because the program that she used pulled the names of only corresponding authors from bioRxiv, whereas all arXiv authors were included.) “The differences are modest, but they’re there,” Frederickson says. She notes that the lockdowns so far have been relatively short compared with the usual research timeline, so the long-term implications for women’s careers are still unclear. The limitations of these types of name-based analysis are well-known. Using names to predict gender can exclude non-binary people, and can misgender others. They are more likely to exclude authors with non-Western names. And between disciplines, their utility can vary because of naming conventions — such as the use of initials instead of given names, as is common in astrophysics. Still, says Frederickson, over a large sample size, they can provide valuable insights into gender disparities in academia. Other researchers are finding similar trends. Cassidy Sugimoto, an information scientist at Indiana University Bloomington who studies gender disparities in research, conducted a separate analysis of author gender on nine popular preprint servers. Methodological differences meant that the two analyses are not directly comparable, but Frederickson’s work “converges with what we’re seeing”, says Sugimoto. Sugimoto points out that the preprints being published even now probably rely on labour that was performed many months ago. “The scientific publication process doesn’t lend itself to timely analyses,” she says. So her study also included databases that log registered reports, which indicate the initiation of new research projects. P. Vincent-Lamarre, C. R. Sugimoto and V. Larivière Nature Index https://go.nature.com/2XhxqxR (2020). In 2 of the 3 registered-report repositories, covering more than 14,000 reports with authors whose genders could be matched, Sugimoto’s team found a decrease in the proportion of submissions by female principal investigators from March and April of 2019 to the same months in 2020, when lockdowns started. They also saw a declining proportion of women publishing on several preprint servers, including EarthArXiv and medRxiv. These differences were more pronounced when looking at first authors, who are usually early-career researchers, than at last authors, who are often the most senior faculty members on a study. “This is what’s the most worrying to me, because those consequences are long-term,” Sugimoto says. “The best predictor of a publication is a previous publication.” In economics, too, there are indications that the pandemic is disproportionately affecting younger researchers, says Noriko Amano-Patiño, an economist at the University of Cambridge, UK. Taken as a whole, there aren’t clear discrepancies in the overall number of working papers — a preprint-like publication format in economics — that have been submitted to three major repositories and invited commentaries submitted to a fourth site that publishes research-based policy analyses. She and her collaborators also examined who was working on pandemic-related research questions using a COVID-19-specific repository. Although women have consistently authored about 20% of working papers since 2015, they make up only 12% of the authors of new COVID-19-related research. Amano-Patiño suspects that, in addition to their childcare responsibilities, early- and mid-career researchers, especially women, might be more risk-averse and thus less likely to jump into a new field of research. “Mostly senior economists are taking their bite into these new areas,” says Amano-Patiño. “And junior women are the ones that seem to be missing out the most.” “Unfortunately, these findings are not surprising,” says Olga Shurchkov, an economist at Wellesley College in Massachusetts. Shurchkov came to similar conclusions in a separate analysis of economists’ productivity during the pandemic. And a preprint posted to arXiv on 13 May1 shows the same trends in pandemic-related medical literature (see ‘COVID-19 effect’). Compared with the proportion of women among authors of nearly 40,000 articles published in US medical journals in 2019, the proportion of female authors on COVID-19 papers has dropped by 16%. Source: Ref. 1 Increased childcare responsibility is one issue. In addition, women are more likely to take care of ailing relatives, says Rosario Rogel-Salazar, a sociologist at the Autonomous University of Mexico State in Toluca. These effects are probably exacerbated in the global south, she notes, because women there have more children on average than do their counterparts in the global north. And women face other barriers to productivity. Female faculty, on average, shoulder more teaching responsibilities, so the sudden shift to online teaching — and the curriculum adjustments that it requires — disproportionately affects women, King says. And because many institutions are shut owing to the pandemic, non-research university commitments — such as participation in hiring and curriculum committees — are probably taking up less time. These are often dominated by senior faculty members — more of whom are men. As a result, men could find themselves with more time to write papers while women experience the opposite. Because these effects will compound as lockdowns persist, universities and funders should take steps to mitigate gender disparities as quickly as possible, Shurchkov says. “They point to a problem that, if left unaddressed, can potentially have grave consequences for diversity in academia.” Andersen, J. P., Nielsen, M. W., Simone, N. L., Lewiss, R. E. & Jagsi, R. Preprint at https://arxiv.org/abs/2005.06303 (2020). Download references Latest on: Publishing Career Column 19 MAY 20 News 13 MAY 20 Career Column 13 MAY 20 Research management Career Column 22 MAY 20 Career Column 15 MAY 20 World View 11 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '69'>20 May 2020</date>
<url id = '70'>https://nature.com/articles/d41586-020-01443-0</url>
<title id = '70'>The latest science news, in brief.</title>
<body id = '70'>A Pomeranian in Hong Kong was one of the first dogs to test positive for the coronavirus.Credit: Xinhua News Agency/Shutterstock The first two dogs reported to have coronavirus probably caught the infection from their owners, say researchers who studied the animals and members of the infected households in Hong Kong. An analysis of viral genetic sequences from the dogs showed them to be identical to those in the infected people. Researchers suspected that the infection had been passed from the owners to the dogs, and the direct genomic link strongly supports that, says Malik Peiris, a virologist at the University of Hong Kong who led the study, which was published in Nature (T. H. C. Sit et al. Nature http://doi.org/dvt4; 2020). The study showed no evidence that dogs can pass the infection to other dogs or to people, but it is impossible to be certain in which direction the virus travelled “so we have to keep an open mind”, says Peiris. Although the analysis confirms that people with COVID-19 can infect dogs, the probability of this happening is low, says Arjan Stegeman, a veterinary epidemiologist at Utrecht University in the Netherlands. In the study, only 2 of the 15 dogs who lived with infected people got the disease. Since the infections in the two canines in Hong Kong — a Pomeranian and a German shepherd — were reported, other pets have tested positive for the SARS-CoV-2 virus, including a cat in Hong Kong and another two in New York state. Four tigers and three lions at New York City’s Bronx Zoo also tested positive. Studies in cats have found that they can pass the virus to other felines. The Hong Kong study detected viral RNA and antibodies in both dogs, and live virus in one of them. Neither dog became noticeably sick. The findings support the results of an April study, in which researchers in China infected dogs with SARS-CoV-2, says Thomas Mettenleiter, a virologist at the Federal Research Institute for Animal Health in Riems, Germany. Dog owners who test positive for the coronavirus should be cautious when handling their pets, he says. The world’s largest science publishers are teaming up to establish standards for catching suspicious images in research papers. A new working group — the first formal cross-industry initiative to discuss the issue — aims to set standards for software that screens papers for altered or duplicated images during peer review. Journal editors have long been concerned about how best to spot altered images, which can result from honest mistakes or efforts to improve the appearance of images, as well as from misconduct. So far, most journals haven’t employed image-checkers to screen manuscripts, saying that it is too expensive or time-consuming; and software that can screen papers on a large scale hasn’t been available. The new cross-publisher working group aims to lay out minimal requirements for software that spots problems with images, and to look at how publishers could use the technology across hundreds of thousands — or even millions — of papers. The group began meeting in April, having been set up by the standards and technology committee of the STM, a global trade association for publishers, based in Oxford, UK. It includes representatives from publishers including Elsevier, Wiley, Springer Nature and Taylor & Francis. “The ultimate goal is to have an environment that helps us, in an automated way, to identify image alterations,” says the group’s chair, IJsbrand Jan Aalbersberg, who is head of research integrity at Elsevier. As researchers worldwide struggle to understand COVID‑19’s effects on the body, they are clamouring for tissue samples from patients. But the raging pandemic and ongoing lockdowns have complicated efforts to do autopsies and collect the tissue needed to understand how the coronavirus attacks organs including the lungs, heart and brain. Autopsies are always painstaking work, but the pandemic means that health-care systems are overwhelmed, protective equipment is in short supply and pathologists are at high risk of infection. But some researchers have found ways around the obstacles. Pathologist Marisa Dolhnikoff at the University of São Paulo and her colleagues have been performing minimally invasive autopsies using needle biopsies to understand why some patients develop blood clots. Researchers now want to collect and share such samples and results systematically. A team of pathologists including Roberto Salgado at the GZA-ZNA Hospitals in Antwerp, Belgium, is creating a global COVID‑19 pathology repository. The group is working with the World Health Organization to create guidelines for the safe collection of autopsy samples and a standardized way of recording the results. Researchers need tissue samples to determine what is killing patients affected by COVID-19.Credit: Giorgos Moutafis/Reuters </body>
<date id = '70'>20 May 2020</date>
<url id = '71'>https://nature.com/articles/d41586-020-01351-3</url>
<title id = '71'>A message from the past.</title>
<body id = '71'>I knocked on the door of Life Echo Manor, a Victorian mansion surrounded by gardens. At the sound, a young woman appeared by the clothesline and eyed me as if I were an intruder. I lifted my AR glasses, and the apparition vanished. The only movement now was the wind in the trees and the buzzing of the drones that tended the hedges. I’m just doing my job. They send someone out whenever a biomonitor goes dead. “Is anybody home?” Yvette Lake, the founder and chief executive of Echo Life, had been the sole occupant for more than 20 years. All Echo Life clients, even her, had to consent to biomonitoring before the team could install the cameras. Too many clients had lost themselves in visions of the past. I saw a face in the window, early middle age. Hope, then disappointment, flashed in its eyes as it drifted off. I thought I heard a voice call “Claudia”. Yvette and Claudia had been married for nearly a decade before the car crash. How many times had she turned towards that window as a pair of headlights passed by, aching for them to turn into the driveway? “I’m coming in.” I turned the doorknob. The silence that followed the creak of the hinges had the awkwardness of a dinner party grinding to a halt at some grievous faux pas. Yvette, with unruly hair, stood by the sink.   “Hi Jake,” she said. I was not Jake. Jake was the biological father of Yvette’s only child, Pallas. He had drifted into Yvette’s life from time to time, mostly before Claudia entered the picture, but never since the crash. Another Yvette reclined on the sofa, lost in thought. A third strode down the hall, calling Pallas’s name. Pallas died in the crash, too, in the passenger seat. It was all over the news. Yvette was all over the news, too, until suddenly she wasn’t. I followed Yvette to Pallas’s room to find it left just as it was all those years ago. Yvette knelt by an empty bedside. Yvette dusted a trophy shelf. Pallas would have been a basketball star one day. Yvette looked out the window with shoulders hunched and her right hand balled into a fist. The room was too crowded. I took off the glasses and left the shrine empty. Why is there nobody else, Yvette? Of all the people who had walked these floors in four decades, why do I see only your face? Were you alone even in her memories? “Yvette!” I said. “I’m with Life Echo. Welfare check.” The house whispered replies, but I ignored them. I focused on the scanner. The biomonitor was in the house, but where? After an hour searching dusty corridors, I found a door with a smart padlock and the remains of a hasp lying on the floor beside it. Beyond the door, a narrow staircase built for servants rose and turned. As I climbed, the signal grew stronger. So did the whispers. I stood at the threshold of a cavernous attic, where stacks of boxes reached precariously to the peaked ceiling. “Yvette!” I called, but the chatter of echoes drowned out even the buzz of the drones who flit amid the piles of boxes, brushing dust ineffectually from surfaces into the musty air. I put the glasses on again, and saw Yvettes, dozens of them, filling the space between the boxes. They faced the left-hand corner and stood as if entranced. I waded through them, following the scanner display, and took off the glasses just before reaching the wall. There, propped against a wall stud, was Yvette Lake. The biomonitor hung from her cold wrist. A crowbar lay at her feet. Her rigid hand held a flat photograph of herself wearing a birthday hat, blowing out a candle shaped like the number eight. Her family had gathered around her for the photo. Was this how far back you had to go, I wondered, to see a face that didn’t cause you pain? “You can’t have her,” said a voice behind me. I turned to see the girl from the photograph, seated on the edge of a box. I reached for the AR glasses instinctively, but they were already in my hand. A projection? “Yvette?” I said. “No. Not Yvette. Echo Life Manor.” “I am Yvette.” The voice was a good deep fake, but not perfect. Maybe the house had only a limited corpus of old audio to work with. Her face radiated despair, like the others, but with a petulant energy beneath it. This image was no faded memory. She was something new. “You protected Yvette, is that it? You kept her from seeing the painful things in her past. You hid Claudia and Pallas from her and let her dwell on herself.” “She didn’t need them,” said the girl. “She had me, and I had every facet of her within myself.” “But you weren’t enough for her. She broke the smart lock and found her way here, until she remembered everything she’d been hiding from. Her heart couldn’t take it. Death from natural causes.” Or was it? “No,” said the girl. “I am Yvette now. I remember every word, every motion, every line of code.” She slid off the box and stepped forward. “Leave my house. Tell them I am alive, and I need no more intrusions. I am content to remember.” “I can’t do that,” I told her. “Yvette is dead. The house will pass to a new owner. Maybe it will be a museum. It won’t need Life Echo anymore.” The girl grew to be a young woman. “I will say what it needs.” Now its voice was flawless. It picked up the crowbar and stepped towards me. I backed away until I felt insulation at my back, while she smashed the biomonitor. How did it manage to project force? That wasn’t standard Life Echo tech. I looked down at the scanner, trying to make sense of it, and as I did, an impish, childlike smile came to the woman’s face. A drone whirred. I heard a box tip from three metres above me. Something made of cast iron slid against cardboard, and before I could look up to see what it was, my world went dark. S. R. Algernon reveals the inspiration behind his latest tale. Murder at the Tesseract House was inspired by a more benign, real-world example of similar technology. Researcher Deb Roy, a scientist working at the Massachusetts Institute of Technology extensively recorded interactions in his own home after the birth of his child. This allowed him to study learning and childhood development, but it also provided him and his family with a treasure trove of recorded memories. He presented his work in a TED talk. I thought about the experience of living in a house that could replay memories of everything that had happened within it, how that would change our experience of time. I’ve had similar thoughts when watching the TV series This is Us, which weaves stories together from slices of life across the decades, from the mid-twentieth century to the mid-twenty-first. I wonder if our constant exposure to memories has led us to be more comfortable with nonlinear experiences and more willing to experience our life as emergent amalgam of past and present. When I start writing, I often ask myself ‘what could go wrong?’ I imagined someone in a home full of memories, but bereft of the source of those memories. They might retreat into that house, reliving the past forever, until something jolts them out of their reminiscence, or until they pass away. I also imagined a smart home that existed to provide those memories. How might it react if it no longer had an audience and its reason for existing? It too, might strive to keep things the same. Those images set the stage for murder and deceit, and the ending of the story. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '71'>20 May 2020</date>
<url id = '72'>https://nature.com/articles/d41586-020-01441-2</url>
<title id = '72'>It took a collaborative global effort to reveal the structures of key coronavirus proteins. That spirit is being tested as vaccine development gets under way.</title>
<body id = '72'>The coronavirus SARS-CoV-2 as seen using a transmission electron microscope.Credit: National Infection Service/SPL Last week, the leaders of Ghana, Pakistan, Senegal and South Africa joined more than 100 former heads of government, senior officials and leading researchers in an open letter urging that scientific research and intellectual property on coronavirus vaccines be shared freely — and that vaccines be distributed fairly — so that the poorest countries do not lose out. It is unfortunate that such a letter needed to be written in the middle of the worst pandemic of the past 50 years. But it was unavoidable, because some governments — including those funding the first wave of research and clinical trials — have not yet committed to the principles of fully open science and innovation. This contrasts sharply with the rapid sharing of findings and expertise among researchers that is being reported daily. This week, we cover one of many examples of such collaboration. Since January, researchers have been working across the globe and around the clock to reveal the structures of key proteins that make up the new coronavirus. Their achievements are the result of free-flowing exchange between university laboratories and national synchrotron facilities in countries including China, Germany, the United Kingdom and the United States. Work that would normally have taken months — possibly even years — has been completed in weeks. But rather than building on this cooperation, some countries are retreating into a kind of techno-protectionism, which serves neither science nor society.   On 10 January, when researchers in China and Australia shared the genome sequence1 for SARS-CoV-2 online, a global network of biologists interested in the structure of viral proteins set to work. The network included the Center for Structural Genomics of Infectious Diseases, a consortium of 40 scientists across 8 institutions in the United States and Canada, which played a central part in the project. Top of the consortium’s to-do list was to plan which proteins to tackle first, and which lab would take on which protein; the teams then set about getting high-resolution snapshots of these proteins, which enable the virus to enter cells and replicate. Thanks to this work and similar efforts elsewhere, teams have now solved more than 170 structures of whole or partial proteins alone or bound to a drug or receptor. The visualizations generated by this work can be used to find ways to neutralize the virus with drugs or vaccines. Simultaneously, a team of structural biologists at ShanghaiTech University in China began the task of revealing the structure of a key enzyme, Mpro, that the virus needs to replicate. Work that took two months for SARS-CoV, the virus that caused the outbreak of severe acute respiratory syndrome (SARS) in 2003, this time took just one week. The team deposited its results in the Protein Data Bank — an open-access digital repository for 3D biological structures — ready for researchers around the world to access. As they worked, Shanghai team members collaborated with structural biologists at the University of Oxford, UK, to share knowledge and avoid overlap.   But when it comes to distributing some of the fruits of that knowledge, this spirit of cooperation looks to be at risk. It is crucial that any vaccine, once proved to work, can be manufactured and distributed quickly in every country. For this to happen, the holders of intellectual property must pool their know-how — as the developers of open-source software do — so that companies large and small can participate in this emergency effort. Such intellectual-property sharing initiatives are under way, but, as Nature went to press, neither the US nor UK governments seemed ready to support these efforts, preferring to work according to a more conventional model whereby intellectual property for a vaccine is held by those that developed it, and then licensed to anyone wanting to manufacture it. This is unacceptable during a pandemic, when lives are at stake and the world’s population needs to be immunized. The research that has got us to this point has been pooled, and governments around the world are shouldering much of the risk of funding the vaccine effort. For these reasons, intellectual property has to be shared. Patent pooling is not simple, but there’s a wealth of literature from life-sciences patent law and case studies from the field of development studies that can help to make it work. And there is an important principle at stake. There is little justice, as economist Mariana Mazzucato at University College London often argues, if citizens have to bear many of the financial risks in such an endeavour, but most of the profits go to a small group of companies (and possibly a few universities) once a vaccine is ready to be rolled out. Scientists are not exempt from competition: the race to publish a paper or patent a molecule is all too common. But in the race to solve the structure of SARS-CoV-2, the competitors have mostly worked together and shared credit — and that is how they, and the hundreds of researchers working in complementary fields, must continue to work as vaccines and drugs move into clinical trials. It is a tribute to those scientists involved so far that they immediately understood that a pandemic requires a vastly different way of working. It is a tragedy that some national governments do not. </body>
<date id = '72'>20 May 2020</date>
<url id = '73'>https://nature.com/articles/d41586-020-01366-w</url>
<title id = '73'>Geological samples must be archived for all if we are to solve the riddles of Earth’s complex history.</title>
<body id = '73'>Geologists think they know the basics of Earth’s history. Liquid water has flowed on the planet for 4 billion years1. Tiny amounts of oxygen first gathered in the atmosphere about 2.3 billion years ago2. And the planet went through many periods of climatic upheaval, from freezing completely 700 million years ago3 to warming so rapidly about 250 million years ago that more than 80% of marine species were lost4,5. It has had many more ups and downs. This story can be reconstructed using data wrestled from ancient rocks. But as geologists learn more, our planet’s tale is getting muddier rather than clearer. Controversies have erupted in the past two decades over many aspects of the chemical record of the early Earth, including the evolution of life, environments and past long-term climate (see ‘Contentious timeline’). For example, variations in carbon-isotope ratios in carbonate rocks have conventionally been interpreted as recording drastic global environmental changes, including huge episodes of volcanism or bursts of oxygen6. By contrast, some researchers suggest that these same records have been changed over time by local environmental processes, and that they do not provide information about Earth’s ancient history7. This debate can be resolved only by applying a variety of geological and chemical tools8,9 to the same samples used to generate the carbon-isotope results. Attempts over the past decade to answer questions using better tools and larger databases have only amplified disputes. To make matters worse, too often, rock samples are not archived or shared. It is common for samples to be held by researchers in private collections instead of in accessible, curated institutional archives or museums. That’s a problem, because different geoscience teams cannot check each other’s work to test whether published results are robust and can be replicated. We call on researchers, museums, funders, scientific societies and journals to ensure that all samples of sediment and sedimentary rock from which geochemical data have been produced and published are curated, archived and made available to members of the research community. Source: N. Planavsky et al. Geological records are complicated and hard to interpret. It is easy to reach contradictory conclusions, most commonly for the following four reasons. Proxies and archives. Several geochemical methods can be used to infer past conditions such as temperature. The same method applied to different sedimentary rock types can lead to inconsistencies. For example, the ratio of heavy to light oxygen isotopes in chemical precipitates (such as chert, carbonate or apatite) tracks the seawater temperatures under which these minerals formed. But even in the same piece of rock, the reconstructed temperatures can be different depending on whether they are measured in a fossil or in a bulk aggregate of the entire rock sample. This is because rocks are inherently combinations of different minerals, which might have formed during different stages of a rock’s long geological history. The consequences for understanding past climates can be dramatic. For example, it is still not clear whether an interval of extreme heat killed marine organisms during the ‘Great Dying’ 250 million years ago. Sulfide toxicity, ocean acidification and carbon dioxide poisoning have also been proposed as possible mechanisms for killing off organisms at this time4. Similarly, the question of whether oxygen levels were low enough to have delayed the emergence of animals for around 4 billion years — or most of Earth’s history, thus addressing Charles Darwin’s dilemma of why complex life appeared so late in the fossil record — depends on which rocks are studied and what analytical methods are used8. For example, an analysis of gas bubbles in sedimentary rocks9 has suggested that atmospheric oxygen levels on Earth’s surface would have been high enough to support animals as early as 2.6 billion years ago. However, this clashes with a compelling body of evidence indicating that atmospheric oxygen concentrations were vanishingly low at this time10,11. Refining such proxies is extremely challenging when different teams cannot work on the same samples.   Geographical and temporal variation. Rock samples that are used to tackle the same research question are often collected from different places, where the rocks were deposited at various times and in vastly different environments. This can result in completely distinct answers. For example, mercury enrichments in sediments are used as a tracer of large episodes of volcanic activity and their links to mass extinction events12. However, mercury enrichments can also result from wildfires or from local depositional conditions that lead to heavy-metal uptake by sedimentary organic matter12. Furthermore, diverse geographical settings can record mercury enrichments differently, depending on aspects such as water depth, dissolved oxygen concentrations, the rate of sediment deposition and the type and location of the volcanoes themselves12,13. All of this can lead to spurious correlations between volcanism and extinction events. It is difficult to disentangle signals of global changes in the Earth system through time from local environmental variability using only reported geochemical data sets. Analytical reproducibility. Experiments can be hard to repeat even if rocks are pristinely preserved. Measurements are routinely checked against those of geochemical standard materials, the compositions of which are internationally validated. Yet there is always the possibility of errors during analysis. These can arise from differences in sample preparation (such as in rock-crushing techniques or in the type of acid used to prepare a sample) and instrumentation (machine type, tuning) to variations in laboratory conditions. For instance, boron-isotope measurements on marine carbonates are one of the key tools used to reconstruct atmospheric CO2 levels14. Various approaches to making such measurements can lead to CO2 estimates that differ by more than 400 parts per million14,15 — roughly equivalent to the total concentration of CO2 found in the atmosphere today. Contamination and alteration. As sediments become rocks, they undergo many processes that can alter the geochemical signals of where and how they formed. Sediments laid down on sea floors or lake bottoms can experience changes in water level or salinity, for example if they are flushed with meltwater. Hydrothermal processes and heat at depth might leach chemicals from the rock and alter the mineral composition. Rocks collected near the surface can be altered by groundwater or other contaminants, such as oil used to drill cores. For example, organic remains in rocks once thought to be evidence for oxygen production by pioneering photosynthetic microorganisms 2.7 billion years ago are now acknowledged to be probable contamination from the modern petroleum products used to drill the rocks from the ground16. Similarly, debate is raging over whether the chemical composition of ancient rocks records microbial oxygen production extending as far back as 3 billion years ago, or whether those rocks have been compromised by contact with recent groundwater17. Rock-core samples at the Geological Survey of Northern Ireland in Belfast, UK.Credit: Stephen Barnes/Science/Alamy Without the ability to access and remeasure samples, it can be challenging to work out whether disparities in results and views stem from complexity in Earth’s history, from sampling of rocks with different levels of alteration or from analytical issues. Yet sample archiving is not part of the standard protocol for inorganic or organic geochemical work, nor for some palaeoclimate work (other than, for example, ocean drill-core or ice-core samples, which are stored). Why has this situation arisen? Many scientists are reluctant to share samples they have struggled hard to collect. After all, there are high costs associated with fieldwork on outcrops and drilling programmes. Research groups might want to perform multiple geochemical studies on a single set of samples, and this takes time. Large geochemical studies that use unconventional isotope systems can take several years to extract a data set18. Other obstacles to archiving samples include how to fund archiving, where to store samples and how they are to be managed. Clearly, no single museum can hold all geological and geochemical samples. Museums would need to increase staff, space and funds for such collections.   Initiatives for archiving materials in other fields could serve as models. These include the Global Genome Initiative, a shared data protocol for frozen tissue repositories (see go.nature.com/3f4erur), and the Integrated Digitized Biocollections project for biological digital data. Global databases of these sample archives and their accessory information, building on initiatives such as the International Geo Sample Number (IGSN), will also be needed to assign unique identifiers and maintain inter-collection records. Some Earth-science fields already deposit samples in publicly accessible museums. For instance, palaeontologists have been required to do so for samples formally described in scientific publications for more than 150 years. Likewise, museums hold type specimens of fossils, meteorites and biological samples. Well-funded drilling projects also have strict archiving policies and well-curated core libraries, such as that for the International Ocean Discovery Program (see go.nature.com/2xoumhh). The FAIR data initiative offers strict guidelines on data archiving and has been adopted by many journals that publish Earth- and environmental-science research, including Science and Nature (see go.nature.com/2wv2jxd). Although the recommended best practices of this initiative already include sample archiving, this is not yet strictly implemented as a formal requirement for publication. Together, researchers, natural history museums, journal editors, scientific societies and funding agencies must develop and implement standardized archival policies. We recommend that the following steps are taken. Geochemical researchers should routinely send their samples to museums. To encourage buy-in, we suggest an embargo period for delaying new studies by other research groups on each set of samples from which geochemical data have been published. Geochemists must also work with museums to broaden the conventional definition of collections to include a range of different materials, from fist-sized specimens to rock fragments, powders and mineral grains. Geochemists should work with custodians of protected lands to encourage the inclusion of archival policies and procedures for geochemical samples collected under research permits. Natural history museums should broaden their mission to archive and curate geological samples. They should assign unique identifiers that can be logged in digital databases. Curators must decide how much of a sample can be withdrawn, because geochemical tests are destructive. Where resources are tight, museums will need to evaluate the spatial, financial and scientific capacity of collections, and determine which samples are most essential to curate. Scientific societies must tackle the question of what constitutes an acceptable repository. For instance, the Meteoritical Society’s Committee on Meteorite Nomenclature does this. Scientific societies such as the Geochemical Society in Washington DC and the European Association of Geochemistry in Aubière, France, should begin to recommend suitable institutions. Recent decades have demonstrated that rapid changes in data archiving are possible when clear guidelines — and editorial mandates — are in place. So we would like to see journals go further in supporting the FAIR data initiative, by making requests to archive samples and assignment of database unique identifiers mandatory for publication.   Many scientific journals regulate data archiving using a checklist. We recommend that this practice be implemented for sample archiving, and that repository-issued sample identifiers (as well as unique identifiers assigned by inter-institutional database efforts such as the IGSN) be included in each paper. All major changes to a field take time to develop, and changes at the editorial level can help to nudge them along. Journals could implement these policies on a relatively short timescale, as long as exceptions are initially made to the archiving mandate when requests for sample deposition are declined. Funding agencies should require that researchers’ grant proposals include sample archival procedures and that budgets include curation fees. Critics might argue that archiving will decrease the money available for other scientific endeavours. In our view, a sample stewardship plan should be viewed as equivalent to budget-line items for data archiving, publishing fees or institutional overhead costs that support other essential components of the research workflow. We strongly recommend against setting universal fees. Samples will vary widely in nature and size, from kilogram-scale samples to micrograms of separated minerals. So the cost to museums will likewise depend on institutional resources and expertise. However, we have confidence that museums, working with funding agencies and researchers, will ensure that fees are self-regulating. Collections of palaeontological samples provide an analogue for the practices needed. They also show that large-scale archiving is possible. The Invertebrate Paleontology Division of the Yale Peabody Museum of Natural History in New Haven, Connecticut, for instance, holds about 4.5 million specimens and takes in more than 2,000 samples a year, on average. As well as its curatorial researchers, the division is supported by two full-time staff members, one of whom handles the new acquisitions. We estimate that roughly 200,000 new sedimentary geochemical samples are analysed each year. We therefore reiterate that curation fees — even modest ones — should be incorporated into the budgets of research-grant proposals. Regardless of the current availability of space and curatorial support in individual museums, extra funds will be needed to meet the demand for archiving sedimentary geochemical samples. The guidelines we offer will need to be discussed and revised by the community and institutions. Nonetheless, all best practices must rest on a shared commitment — to ensure that scientific data are not divorced from scientific samples. </body>
<date id = '73'>11 May 2020</date>
<url id = '74'>https://nature.com/articles/d41586-020-01181-3</url>
<title id = '74'>Countries must join forces to avert a global food crisis from COVID-19.</title>
<body id = '74'>The coronavirus pandemic has laid many things bare, none more so than how interconnected our world is. The impact of globalization is most obvious in the stuttering supply chains that threaten food security worldwide. Maintaining or reweaving these webs is going to take technology, innovation and political determination. As chief economist at the Food and Agriculture Organization of the United Nations (FAO), I fear that few countries have recognized that their measures to contain the virus and buffer economic shocks must be adjusted to keep food flowing. Without food, there can be no health. The policy prescriptions are straightforward, and isolationism can form no part of it. Countries must work together, not throw up trade walls and bar essential workers from crossing borders. Global food-supply chains are already buckling. In India, farmers are feeding strawberries to cows because they cannot transport the fruit to markets in cities. In Peru, producers are dumping tonnes of white cocoa into landfill because the restaurants and hotels that would normally buy it are closed. And in the United States and Canada, farmers have had to pour milk away for the same reason. Legions of migrant workers from Eastern Europe and North Africa are trapped at borders, instead of harvesting on the farms of France, Germany and Italy. The United States, Canada and Australia all rely heavily on seasonal farmworkers who are unable to travel because of virus restrictions, including the suspension of routine visa services by some embassies. There are also concerns that foreign workers could import cases of infection. Crops are rotting in the fields. Fortunately, cereal harvests are expected to be good this year. Already, the world’s stockpile of maize (corn) is more than twice what it was in 2007 and 2008, when severe droughts created food shortages in key exporting countries, leading to a global food crisis. Rice and soya-bean stockpiles have also increased over this period, by around 80% and 40%, respectively. A farmer in India feeds a cow with strawberries meant for city dwellers; the coronavirus pandemic has blocked access to markets.Credit: Rajendra Jadhav/Reuters But the bounty will not help to avert food shortages if countries cannot move food from where it is produced to where it is most needed. Ships laden with cereals, fresh fruit and vegetables are docking late and their crews cannot disembark. So perishables, unable to reach wholesale markets in time, are going to waste. Wheat prices have jumped by 8% and rice prices by 25% compared with those of March last year. Meanwhile, panic buying across the world is creating more waste and affecting the quality of diets as people struggle to access fresh food. Global action on food was a challenge even before COVID-19. That countries and regions are experiencing the pandemic at different times and in different ways — from China, to Europe, the United States, India and now Africa — has created an ethos of nations acting only for themselves. That has led to chaotic chain reactions. Earlier this month, Russia, the world’s leading wheat exporter, limited wheat exports for three months to ensure that local supplies were sufficient. Although the disruption is expected to be minimal (see, for example, ref. 1), the gesture set alarm bells ringing elsewhere. It was a decision driven by a confluence of events, including the sharp drop in oil prices — this weakened the rouble against the dollar, which in turn bumped up local prices of wheat. It is the same course that Vietnam took with paddy rice in March, which is why rice prices spiked. The pandemic has emboldened divisive arguments — such as that open borders have enabled the virus to spread, that refugees and immigrants must be kept out, and that outsourcing should end. But such political positions ignore how much nations depend on each other for staple ingredients, pesticides, fertilizers, animal feed, personnel and expertise.   What happens next depends on whether nations resist isolationist pressures. I urge them to commit to not imposing export restrictions in response to the pandemic. Instead, they should agree to eliminate tariffs and taxes to compensate for local price increases caused by currency devaluation. And they should designate workers at ports and on farms as essential personnel, protect the health of these people and ensure that they can travel and continue to work. Collaboration is possible. The agriculture ministers of 25 Latin American and Caribbean countries signed an agreement this month to work together to guarantee food supplies in the region. Such a political declaration can pave the way for real progress. And governments and investors can benefit from more transparency and information than ever before on market conditions, through tools such as the Agricultural Market Information System (www.amis-outlook.org), which can reduce uncertainty. At the FAO, we are focusing on mitigating the virus’s impact on the activities that deliver produce to people, using evidence and lessons learnt from past crises. This includes information about food-price increases and volatility2, and how access to food and nutrition was affected during recent outbreaks such as that of Ebola3. Using big data, we monitor trade and collect information on logistical issues, assess how problems have been resolved and then signal the outcome to the market to reduce uncertainty (see https://datalab.review.fao.org). For example, we know that the main delay in shipping happens during cargo unloading, which now takes three days instead of one because of labour restrictions at ports. The delay is expensive for exporters, but they make up for it with the gains from exchange rates. So global shipping is working. We also track news in multiple languages to see how the pandemic is affecting food and agriculture. This helps countries to make policy decisions. We work with developing nations to boost food supply by analysing their agro-ecological conditions and advising when and where to plant and harvest their key commodities. We forecast how various aspects of the agricultural sector could be affected by COVID-19 — from labour and decreased demand because of falling incomes to exchange rates and inflation4.   What the pandemic has underscored is that the world must use its land and water resources sustainably, to grow essential, nutritious food in a more resilient way. One way of doing this is to cut food loss5. The world squanders about US$400 billion of food annually — an amount that could feed around 1.26 billion people a year. The wastage is equivalent to 1.5 gigatonnes of carbon dioxide emissions. (Compare that to the roughly 33 gigatonnes emitted in 2019 to produce the world’s energy.) Another priority is better treatment for smallholders and migrant workers, who form the backbone of farming. For example, small-scale operations need access to markets and help to increase productivity and incomes, which goes far beyond mere subsidies. The pandemic is an opportunity to hit the reset button, with scientists and social scientists playing an important part. And innovation is happening: China is investing in drones, unpiloted vehicles and other agriculture technologies to reduce human contact. In Africa, mobile phones are improving access to markets, prices and weather data, as well as facilitating money transfers6. Peru is seeing the benefits of innovative legislation that has formalized the farm labour force and directly linked it to the seasonality of crops. The government now knows which farmers are affected by the lockdown and can ensure they receive the necessary support. Let us seize these huge opportunities collectively. It is precisely because the coronavirus doesn’t respect borders that global cooperation is the only shot at defeating it. The people who are working on vaccine trials, health care, drug discovery and economic recovery must all still eat. We can either stand together or many millions will starve separately. </body>
<date id = '74'>23 April 2020</date>
<url id = '75'>https://nature.com/articles/d41586-020-01118-w</url>
<title id = '75'>Car owners underestimate total vehicle costs. Giving consumers this information could encourage the switch to cleaner transport and reduce emissions.</title>
<body id = '75'>Private cars are responsible for about 11% of the world’s total carbon dioxide emissions. That’s the greatest share in the transport sector, which accounts for 24% of emissions overall1. Petrol and diesel cars are associated with many other harmful effects, such as air pollution, congestion and accidents. It is clear that these cars must be largely removed from the roads to achieve sustainable mobility. The good news is that some policies have been enacted to reduce greenhouse-gas emissions and air pollution from petrol and diesel cars. Some markets have tightened emissions limits. In the European Union from 2021 onwards, for instance, the fleet-wide average emission target for new cars will be reduced by more than 25% — from 130 grams of CO2 per kilometre (g CO2 km–1) in 2015–19 to 95 g CO2 km–1. The current average is 120 g CO2 km–1 (on the basis of 2018 data; see go.nature.com/39puqyy). And the United Kingdom will stop sales of new petrol, diesel and hybrid cars from 2035 onwards. Cities are taking action, too. For example, Oslo has decreased the number of parking spots and raised parking fees2. New York City and Shenzhen in China are electrifying their bus fleets, and London is reducing bus emissions and improving infrastructure for walking and biking. Stuttgart in Germany is among the cities banning older, polluting diesel cars. The bad news is that more than 99% of new passenger cars sold worldwide still rely on fossil fuels1,3, and overall vehicle ownership in Europe grew by 25% between 2000 and 20174. The continued demand for vehicle ownership stems from several factors, including increased income5 and more mobility as people travel farther to their jobs as a result of greater city sprawl6. The transition away from conventional vehicles is hindered by the high upfront costs of electric cars7–9 and too few charging stations, leading to ‘range anxiety’ from potential owners10,11.   Consumers decide whether to own a vehicle on the basis of considerations such as where they live and the vehicle’s upfront and lifetime costs12. If they systematically underestimate total costs, this could increase car ownership and its associated emissions. It could also make alternative forms of transport — car sharing, alternative-fuel vehicles, public transport, biking or walking, say — seem less attractive. We surveyed more than 6,000 citizens across Germany to investigate whether consumers grasp the total cost of car ownership. We also performed a simple analysis to explore the potential implications of this awareness on the number of cars on the road. We find that people underestimate the total cost of owning a car by about 50%. We also found that providing personalized information on the costs of car ownership increased respondents’ willingness to pay for a public-transport ticket by around 22% (see Supplementary information; SI). We estimate that educating people in Germany about the true cost could reduce car ownership by up to 37% and cut associated transport emissions by 23%. Here, we suggest labelling and communication policies that could help to speed the transition to cleaner transport. We conducted a survey of the heads of German households — the people who self-report as being responsible for financial decisions — between 23 April and 12 June 2018. For every car owner, the survey elicited responses on the cost of ownership, based on the individual’s car type and driving behaviour, as well as socio-economic characteristics such as income, number of children and education. The work was done in collaboration with the survey institute Forsa in Berlin. It used a random sample of Forsa’s household panel, which is representative of the German-speaking population aged 14 and older. Of the 7,823 individuals who started the survey, 6,812 completed it, 6,233 of whom own a car (92%). Of these, 5,483 stated what they thought their monthly car costs were. These respondents form the basis for our analyses. Taking households’ car type and travel behaviour into account, we use detailed information from the German Automobile Club (ADAC) and other sources to calculate the actual monthly costs of car ownership, on average, for depreciation, fuel, taxes and insurance, and repair (see SI). Our findings were striking. Consumers underestimate the total cost of vehicle ownership by €221 (US$240) per month on average. The misjudgement amounts to 52% of the actual costs, so the total cost is nearly twice what people think. Using only the respondents who provided an estimate for all cost factors, the underestimation is €161 on average, which is 35% of the actual costs. To be conservative, we proceed in our analysis using this sample (for the full sample, see SI). The difference between estimates and actual costs varies widely. This is not necessarily surprising. The mean and median of the difference is well below zero, clearly demonstrating that costs are underestimated on average. We also investigated the four main costs of car ownership: fuel, depreciation, repair, and tax and insurance. On average, respondents came very close to perfectly estimating how much they spent on fuel, consistent with the previous literature13. But they severely underestimated all other major expenditure for running their cars (see ‘Costly misjudgement’). To our knowledge, this misjudgement has not been reported previously, and it can provide leverage points for designing new transport policies. Source: Analysis by M. A. Andor et al. Let’s assume that it is possible to completely eliminate the degree to which people systematically underestimate the total cost of owning a car. Would this change the number of cars on the road? We explored this question by modelling how car ownership changes when the associated costs change, on the basis of previous work on US car ownership12. This allowed us to calculate the reduction in the 47.1 million passenger cars in Germany that one might expect if households were perfectly informed about running costs. We predict that being aware of the true cost of owning a car could result in almost 17.6 million (37%) fewer vehicles on the road in Germany. Such a drastic reduction would mean less congestion and cleaner air. It would also lead to a drop in CO2 emissions of about 37 million tonnes per year: 4.3% of Germany’s total, or 23% of emissions from its transportation sector (see SI). Although increased demand for public transport could lead to more CO2 emissions from bus and rail travel, this effect would probably be small. First, the carbon emissions per person for each kilometre travelled for these modes of transport are around half those of car travel (see go.nature.com/2upeejh). Second, emissions-trading schemes in the EU, for example, mean that the increased electricity consumed by the growing number of electric trains and vehicles is prevented from translating into extra carbon emissions for the economy, because total emissions are capped. Even a more cautious estimate of changes in car ownership in response to higher prices of new vehicles (as reported in a 2018 US impact analysis14) would still imply a 9% reduction in car ownership, taking more than 4 million cars off German roads (see SI). We further used our survey data and empirical estimates of the impact of car-ownership costs to investigate whether such misjudgements affect the use of public transport and electric vehicles. We predict an increase in demand for bus and rail travel of 8% and 12%, respectively. Purchases of electric vehicles could increase by about 73% (see SI).   Although our survey was conducted in Germany, we expect the results to be applicable throughout Europe, and probably to countries with similar economies elsewhere. In 2017, Germany had 561 passenger cars for every 1,000 inhabitants; the EU average in the same year was 51215. And people in other countries accurately estimate fuel costs13, as in our survey. These are preliminary findings. Our calculations require a number of assumptions. For example, we had to assume how much car ownership changes when household perceptions of total costs change, and how much the demand for other transport modes would respond to decreased car ownership (see SI, pages 5–6). To explain, consider the case in which the changes in car ownership and the demand for other transport modes were less pronounced. Then, eliminating cost beliefs would have smaller effects. Critics might argue that cost is merely one of many factors that influence individuals’ decisions to own a combustion-engine car, including status, the need for mobility in rural areas and the lack of infrastructure for electric-vehicle charging and for public transport. Although cost is indeed only one factor, it is a crucial one in the car-ownership decision12,16. Another potential criticism is that our consideration of vehicle list prices does not account for the discounts car buyers usually negotiate, which could partly explain why our respondents underestimated depreciation. We show that taking typical discounts into account does not change our main conclusions (see SI). Indeed, our approach is conservative because we do not consider other factors that increase the cost of owning a car, such as extra equipment (navigation systems, seat heating and sports seats) or premiums for leasing or financing. These raise car prices by 30–50% on average, and so widen our estimates of ownership-cost misjudgement. A busy cycle lane along a major route in central Hamburg, Germany.Credit: Güven Purtul/VISUM/eyevine It is unlikely that anything will entirely stop people from underestimating the total cost of owning a car. Nevertheless, we think that closing this ‘awareness gap’ can spur the transition away from conventional cars. How do we do this? Cars should be labelled with total costs at the point of sale and in registration letters. Such information-provision policies influence consumer purchasing behaviour in a variety of contexts, from buying property to durables such as refrigerators and air conditioning17–19. Many countries, including the United States, Japan and China, already mandate that new cars for sale are labelled with the average future fuel cost of driving them. Companies that promote alternative forms of transport with lower emissions — such as electric-vehicle dealers, car-sharing or public-transport firms — could boost business by including information on the cost of car ownership in their advertising. To prevent potential conflicts of interest, the information would need to be certified or come from trusted sources, such as scientific institutions or public ministries. But even a general marketing campaign could at least encourage consumers to calculate the cost of driving accurately. How successful might such interventions be, compared with other options such as a fuel tax or subsidizing public transport? We calculate that fuel prices would need to rise by a massive 1,242% to cut car ownership by the same 37% reduction that we predict as a result of improved consumer information (see SI). This is because fuel price changes largely target driving itself, rather than the decision of whether to buy a vehicle.   Another widely discussed policy is to subsidize public transport more. This might have less potential than correcting the misjudgement of car ownership costs. There is no evidence available solely from Germany, so we based our extrapolations on evidence for the relationship between prices for public transport and car ownership. This came from a meta study of 83 papers, predominantly from Europe and the United States20. We find that eliminating public-transport fares entirely would decrease car use by only 4.1–6.2%, and could have an even smaller impact on car ownership. This approach would also be burdensome on the public treasury. In Germany, for example, total ticket sales by local transport companies amounted to €13.3 billion in 2019. We believe that policies on labelling and information would be less costly and less politically fraught than would many other options. The primary resistance to such a policy might come from conventional car dealers and manufacturers who would be reluctant to see sales fall. But information provision is likely to get strong public support from consumer-protection agencies, for example, which could offset such lobbying. Furthermore, providing ownership cost information could be implemented by revising existing fuel labels, which would reduce institutional obstacles. Future research should focus on total car-ownership costs, rather than on fuel costs alone. To plug knowledge gaps, we have the following recommendations. We need to know why consumers underestimate the costs of car ownership. This could lead to information targeted to those who make the biggest misjudgements. Campaigns should be tested in the field to guide policymakers who are aiming to promote greener transport. Our survey should be replicated in other countries to clarify how and where the results apply. This could shed light on what drives the systematic underestimation. Furthermore, future studies should elicit how the underestimation can be reduced, for instance by means of surveys, laboratory experiments and, ideally, field experiments. It would also be useful to investigate the overall impact of car-ownership information on mobility behaviour more broadly, including the use of public transport, cycling and walking. Our analysis uncovered a need for further empirical evidence on the relationship between the cost of car ownership and the number of cars on the roads. We see great promise in this research agenda to inform policymakers about cost-effective approaches to reducing emissions from transportation. One of the goals of the European Green Deal, proposed by the European Commission last December, is to accelerate the shift to sustainable and smart mobility. And Horizon Europe, the EU’s €100-billion research programme, is currently defining its agenda for research during 2021–27. Both present an invaluable window of opportunity. </body>
<date id = '75'>20 April 2020</date>
<url id = '76'>https://nature.com/articles/d41586-020-00915-7</url>
<title id = '76'>Exploitation and degradation of the mysterious layer between the sunlit ocean surface and the abyss jeopardize fish stocks and the climate.</title>
<body id = '76'>The elongated bristlemouth (Sigmops elongatus) is abundant in the oceans’ twilight zone.Credit: Woods Hole Oceanographic Institution/Paul Caiger The twilight zone contains the largest and least exploited fish stocks of the world’s oceans. Spanning from just below 200 metres to 1,000 metres deep, it is an interface between the well-studied marine life in the sunlit zone above and the ecosystems of the abyss below. It has a major role in removing carbon dioxide from the atmosphere and storing it for centuries or longer. The twilight zone is also privy to the largest migration on Earth. Huge numbers of fishes and zooplankton move hundreds of metres towards the surface each night to feed, before retreating back down at dawn. Yet the zone is poorly understood — physically, biogeochemically and ecologically. Even the number of organisms that live there remains a mystery, let alone their diversity and function. It is alarming, then, that this vast ocean domain is at risk in three ways — even before any of the potential consequences are understood1. First, the world’s growing population has an increasing need for food. Second, sea-floor mining for minerals and metals could release waste into the region2. And third, climate change is altering temperature, acidification and oxygen levels in ways that are likely to affect life there3. The twilight zone is hard to study. Its organisms are difficult to sample and analyse, being sparsely distributed, elusive and often fragile. They also live at pressures of up to 100 atmospheres, which poses problems for laboratory-based investigations. Critics might argue that waters near coasts and above shelves are more deserving of study, given the huge environmental pressures there, as well as their importance to societies. And, of course, they need attention. Sadly, however, it is too late to avoid widespread environmental damage to these inshore regions. Instead, research efforts and local policies must aim at mitigating the worst effects. By contrast, the twilight zone is almost pristine. Moreover, the majority of it lies beyond national jurisdiction. This makes it of common interest and responsibility, and means that global agreement is necessary to manage it. Here, we outline the steps needed to ensure that enough is known about this complex global ecosystem to inform decisions about the impacts of climate change and potential future exploitation. We call on the international marine research community to focus its attention on the twilight zone during the upcoming United Nations Decade of the Ocean, which runs from 2021 to 2030. In the spirit of the UN’s Sustainable Development Goals, we should seize the opportunity to establish a global policy that will protect this vast ecosystem for present and future generations. At present, we know just enough about the twilight zone to recognize its importance in maintaining a healthy ocean. Phytoplankton growing in the sunlit layer fuel multiple food-supply routes into the zone that sustain organisms from bacteria to giant squid. In the process of consuming this food, and each other, the twilight-zone animals produce CO2, consume oxygen and release nutrients back into the water (see ‘Twilight zone’). In the winter, cold, windy weather mixes water containing the recycled nutrients with water from the surface layer4. In this way, the twilight zone has an important role in supporting phytoplankton growth the next spring. Source: Adapted from JETZON Although winter mixing can release carbon back into the atmosphere, a fraction of it ends up in deeper waters, where it can be locked away, typically for centuries. This downward transport of organic matter, mediated by life in the twilight zone, is called the biological carbon pump, and the twilight zone is central to its strength5. This deeper flux of material becomes food for the animals there. The small amount that eventually reaches the sea floor sustains everything from bacteria to sea cucumbers. Unfortunately, little is known about how the twilight zone performs these roles. This makes it difficult to predict future ocean oxygen levels or how organic carbon will be stored in the long term. Moreover, the effects of climate change on ocean temperatures and oxygen levels will alter how the biological pump operates. Knowledge gaps range from fundamental information, such as what species dwell there and what their metabolic rates are, to how they behave and adapt to their environment. Bacteria colonize ‘marine snow’ (sinking aggregates of organic material); krill form dense, localized swarms; and some fishes have evolved vision that is tuned to dawn and dusk. But how do such adaptations affect the functioning of the twilight zone? The patchiness of the information makes it hard to predict how the twilight zone might respond to human pressures. For example, the fishing industry is likely to target species that are very abundant, such as elongated bristlemouth (Sigmops elongatus), but will also remove many other animals in the process. This could reduce the ecosystem’s resilience and change global nutrient and carbon cycles6. The Deep-See sensor platform heads for its first dive into the twilight zone.Credit: Woods Hole Oceanographic Institution/Erik Olsen The following three questions should be prioritized to plug these knowledge gaps. How many organisms live in the twilight zone, and how diverse are they? Estimates for the volume of fishes there range between 1 billion and 20 billion tonnes7. Surface waters are estimated to hold 1 billion tonnes. (The world’s human population has a total weight of 0.5 billion tonnes.) But it is not clear what fraction of the organisms are siphonophores (relatives of jellyfish) or cephalopods (such as squid). Which ecological processes transform and consume organic material? Marine snow is common in the twilight zone. Zooplankton could be breaking it up so that it forms a slow-sinking substrate for bacteria — a nutritious food for the zooplankton8. But this theory, known as microbial gardening, is yet to be tested. How is organic material transported into and out of the twilight zone? Researchers need to determine the relative importance of a range of mechanisms that vary with location, time and depth9. These span from physical processes to animal behaviour. Ocean currents transport tiny particulate and dissolved organic matter to greater depths. Larger organic aggregates and faecal pellets sink. And daily and seasonal animal migrations release waste products at depth. Addressing these three questions will help to clarify what sets the balance between how much organic material is consumed in the twilight zone, restoring nutrients and sustaining the fish stock, and how much passes on to greater depths, sequestering carbon away from the atmosphere10. Only with this knowledge can the wider consequences of exploiting the region be predicted. A juvenile glass squid.Credit: Woods Hole Oceanographic Institution/Paul Caiger The following three steps will help in addressing these research priorities. They make use of a range of innovative tools and techniques. Conduct a census. Organisms ranging from bacteria to large cetaceans need to be counted. Devices such as the Underwater Vision Profiler (UVP) can be deployed in the twilight zone to capture images of plankton that can be identified and counted using a web-based application known as EcoTaxa, which is linked to a taxonomic database containing roughly 100 million images of planktonic organisms and particles. A smaller version of the UVP can be attached to autonomous vehicles to extend sampling beyond the times and places that research vessels can visit. Larger organisms can be identified with short-range high-frequency acoustic sensors. When deployed at depth, these sensors can help sort fishes from siphonophores. DNA harvested from the environment can also be used to infer the identity and diversity of elusive or fragile animals, including large fishes, marine mammals such as beaked whales and gelatinous organisms. Any new approaches should be calibrated against conventional physical sampling methods and follow internationally recognized, standardized procedures. Determine what is processing and consuming organic material. To do this, changes in the size, source and sinking speed of organic aggregates need to be observed in situ as the particles descend through the water column. This should be done mainly using optical sensors. The information gleaned could then be combined with simultaneous estimates of the abundance of zooplankton and fishes and the intensity of ocean mixing, to determine what is breaking up the aggregates and retaining them in the twilight zone11. A range of ‘omics’ approaches should be used to provide insight into how the associated organic material is being eaten by microorganisms12, including metagenomics, metatranscriptomics and metabarcodes. Track organic material. Argo floats already roam the ocean and collect information on properties such as temperature, phytoplankton abundance and nutrient levels as they shuttle between the surface and a depth of 2,000 metres every 10 days. Imaging systems capable of measuring the size and abundance of organic particles are being added to these floats. The current network needs to be increased from 200 to 1,000 operating floats, with imaging sensors added to all. Optical sensors on other autonomous underwater vehicles such as gliders can be used to yield information on the size and shape of organic particles. These data could be combined with information from the Plankton, Aerosol, Cloud, ocean Ecosystem (PACE) mission, which NASA plans to launch in December 2022. The satellite will use a spectrometer to measure the colour of the ocean. Those data will be useful in determining the types of phytoplankton in the surface layer of the ocean, fuelling the twilight zone. The discovery that laser-mapping technology such as LIDAR (Light Detection and Ranging) can observe the daily migrations of zooplankton from space should be combined with sparse local data13. To obtain the most complete picture possible of the global twilight zone, we call on national and international ocean projects to coordinate efforts, rather than duplicate them. We encourage researchers and institutions to link up with JETZON (Joint Exploration of the Twilight Zone Ocean Network), an initiative launched earlier this year to improve communication and coordination. Currently, 15 projects involving 12 countries are involved, each studying just a few locations. This is a good start. But given the vast size and complexity of the twilight zone, everyone, from independent researchers to international projects, needs to join forces to succeed. There is no time to waste. We cannot let climate warming and human exploitation fundamentally alter the twilight zone before we even begin to understand the potential consequences for the health of the planet. </body>
<date id = '76'>31 March 2020</date>
<url id = '77'>https://nature.com/articles/d41586-020-00837-4</url>
<title id = '77'>Many people now have two or more diseases at once. It is time to rethink funding, research, publishing, training and treatment for this growing problem.</title>
<body id = '77'>More and more people the world over have two or more long-term diseases at once — known as multimorbidity. This is the case for at least half of Europe’s population aged 65 or older (more than 50 million people), for example1. And in the United Kingdom, the proportion of the population with multi-morbidity is expected2 to rise from 54% in 2015 to 68% in 2035. Science, and medicine, have been slow to respond to this change. One of the things that has limited the ability to address disease co-occurrences is that they are usually seen as random. In reality, diseases often cluster because of a common risk factor. Only a fraction of the many thousands of potential clusters are well known. The canonical example is the link between diabetes and conditions in the skin, peripheral nerves, heart, eyes and brain. Now, advances in statistical methods, electronic medical-record keeping and machine learning could help to identify many more links. Mapping these clusters, and working out which are non-random, is crucial for three reasons: to uncover new mechanisms for disease; to develop treatments; and to reconfigure services to better meet patients’ needs. We urge researchers to prioritize the hunt for diseases that occur together because of shared risk factors — biological or environmental. Multimorbidity has always existed. In all societies studied, people who are less wealthy are more likely to acquire multiple diseases than are those of the same age who have more advantages3. In high-income settings, the conditions usually occur in adults and include common cancers, cardiovascular disease and other non-communicable chronic disorders, especially those associated with smoking. In low-income settings, the worst affected tend to be children under five — with clusters related mainly to diet and infection. In rural Tanzania, for instance, children can have malaria, stunting due to malnutrition, anaemia from chronic parasitic infections and a high risk of accidents. A master’s student in Tanzania checks a child for signs of malnutrition.Credit: Brian Sokol/Panos Some clusters vary by region, usually for a combination of socio-economic, cultural or genetic reasons4. For example, a higher prevalence of multimorbidity is linked to low educational attainment (see ‘The health benefits of learning’). In some low-to-middle income regions, such as parts of South Africa, physical conditions such as anaemia tend to group with infections such as HIV5. In rapidly developing Asia, diseases of affluence such as type-2 diabetes and coronary heart disease commonly co-occur with tuberculosis and other diseases of poverty6. Source: Ref. 3 Mental- and physical-health issues often co-occur, and causation can go in both directions. Those with serious mental-health problems such as major psychotic disease tend to have significantly shorter lives owing to physical health problems7. And physical illness can lead to mental-health problems such as depression. Across all settings, people under 20 with one major condition, such as a genetic or congenital abnormality, often have or acquire others. For example, children with sickle-cell disease can go on to develop infections, visual problems or stroke8. Children and adolescents with developmental disorders can be affected by mental-health conditions as well as physical ones9. And children with autism spectrum disorder often have comorbidities ranging from dyspraxias10 to tooth decay11. Some diseases cluster in pregnancy. These include infections such as listeriosis; heart disorders such as cardiomyopathy; and autoimmune conditions such as flares of systemic lupus erythematosus. What’s making an already important issue much more important is the strong link with ageing. As we grow old, the probability of developing more than one condition greatly increases12. And because the incidence and prevalence of diseases in people younger than 60 has dropped substantially, societies in every continent are projected to have longer lifespans — with east Asian countries such as Japan and Singapore in the lead13. In high-income settings, most older people now have either no major conditions or several. A shrinking minority have one significant disease (see ‘Trials of ageing’). That’s the message of a decade or more of studies and systematic reviews, including a 2018 report from the UK Academy of Medical Sciences14. Source: Ref. 12 Middle-income countries such as China6 and South Africa15, are also seeing a rise in multimorbidity in older people. In fact, the diseases that cluster here arise from the double burden of ageing and poverty, and could well be the most diverse. Research, clinical teams and training are organized mainly around single diseases or organ systems. Think coronary heart disease or breast cancer, haematology or gastroenterology. But the cumulative effect of several diseases often involves interactions that make the practical and medical impacts of the combination much greater than the sum of their parts16. Because each disease tends to be treated separately, someone with cardiovascular disease, inflammatory bowel disease and asthma, say, might have to attend three clinics to receive treatment from three specialists, with each physician working in isolation. People with more than one condition tend to be excluded from clinical trials testing potential new drugs. Yet any one drug might have different effects when prescribed to someone with multiple conditions, which could be most of the people it is given to. Also, interactions that occur when many drugs are used together – known as polypharmacy – can harm patients and increase the complexity of their care17. And there are cases in which one drug affects several conditions because they share a common, if unrecognized, biological basis. (Tumour necrosis factor-α inhibitors, for instance, can treat Crohn’s disease and psoriasis because both are autoimmune disorders18.)   In short, opportunities for tackling several problems at once are being missed. To address this already large, and rapidly growing, problem, clinical epidemiologists and others must first map out disease clusters. Some associations will occur simply because the component diseases are common. Diabetes and asthma often co-occur, for instance, through random assortment. A shared risk factor or biological interaction is implied only when diseases cluster with a frequency that is greater (or less) than chance alone based on their relative prevalence or incidence. Historically, the identification of such clusters has depended largely on clinicians simply noting the same groups in multiple patients — often before there was any scientific explanation for the combination. In the early 1980s, for instance, it was clinicians’ observations that pneumocystis pneumonia co-occurred with Kaposi’s sarcoma in men in San Francisco that led to the identification of HIV/AIDS as the common risk factor19. Epidemiologists have helped to flag associations that are too weak or rare for a clinician to spot. In the late 1800s, for example, statistical pioneers such as Florence Nightingale and John Snow identified links between poor sanitation and typhoid, diarrhoeal diseases and hospital-acquired infections, which led to improved sanitation worldwide. More recently, epidemiologists have established many diseases associated with smoking, including lung cancer, heart disease and chronic obstructive pulmonary disease by following cohorts of smokers and non-smokers20.   Today, non-random clustering will be much easier to find than it was even five years ago. There are millions of electronic medical records, an ageing population with high rates of multimorbidity, new statistical tools and artificial-intelligence methodologies for pattern recognition. Plus, patients are finding each other online, discovering they share seemingly unrelated problems and flagging these to physicians and researchers. It should therefore also be possible to identify the sequence in which individual diseases tend to accumulate. This will increase researchers’ understanding of mechanisms. It could also allow clinicians to intervene, or to screen people for other conditions when the first disease of the cluster becomes apparent. Given the thousands of probable clusters21, such mapping should be done without preconceptions about why conditions might co-occur. Starting with hypotheses about causes — genetic or environmental — could close off possibilities and mean that important links are missed. Other advances are making it easier to probe the underlying molecular pathology. Already, for example, the global Human Cell Atlas initiative is providing insights into subsets of T cell common to different tissues22. Meeting this new challenge requires action across many fronts, notably funding, training and publishing. At present, few grant-giving panels are equipped to assess proposals that cross disciplinary boundaries. Training tends to be in one disease or organ system. And it is rare to find the best papers addressing multiple conditions in major journals, because editors struggle to find referees who are able to assess them. These are the priorities: Funding. In the United Kingdom, the National Institute for Health Research and the Medical Research Council are taking steps to actively invest in multimorbidity research (see ‘UK steps’). Analogous organizations in other countries need to do so, too. Meanwhile, in clinical trials, it is important to develop ways to include people with more than one disease. Research funders prioritize the identification of diseases that co-occur. In the United Kingdom, the National Institute for Health Research and the Medical Research Council are doing four things: • Calling for proposals for research into multimorbidity. Examples include: strengthening statistical methods to detect clusters; the application of machine learning to large data sets; and the identification of the biological basis of clusters. • Supporting clinical and public-health research to prevent clusters of disease, and to treat them more effectively (in the UK and globally). • Supporting work that aims to understand and respond to the needs of people with multiple long-term conditions, as well as the needs of their carers. • Supporting studies that investigate new approaches to the configuration of health systems to account for clusters of disease and patients’ needs. Training. Identifying which conditions co-occur could lead to a rethink of how scientific specialties are defined and medical students are taught. A multi-disease speciality has already been created for diabetes and HIV/AIDS. We need to take a similar approach to training, so that early-career scientists think about diseases that cluster as their principal area of focus (horizontal integration) as well as the vertical integration of a single disease from the basic to the applied sciences (bench to bedside). Publishing. Journal editors need to proactively identify potentially good papers about disease clusters and adapt the current model of single-disease referees to ensure they are assessed fairly. New fields develop much faster if journals champion them. Identifying diseases that group together is exciting scientifically and important clinically. It could yield enormous health benefits globally — across all age groups, ethnicities and socio-economic backgrounds. It should now be a priority for medical science. </body>
<date id = '77'>24 March 2020</date>
<url id = '78'>https://nature.com/articles/d41586-020-00672-7</url>
<title id = '78'>For three years, part of DARPA has funded two teams for each project: one for research and one for reproducibility. The investment is paying off.</title>
<body id = '78'>In 2016, the US Defense Advanced Research Projects Agency (DARPA) told eight research groups that their proposals had made it through the review gauntlet and would soon get a few million dollars from its Biological Technologies Office (BTO). Along with congratulations, the teams received a reminder that their award came with an unusual requirement — an independent shadow team of scientists tasked with reproducing their results. Thus began an intense, multi-year controlled trial in reproducibility. Each shadow team consists of three to five researchers, who visit the ‘performer’ team’s laboratory and often host visits themselves. Between 3% and 8% of the programme’s total funds go to this independent validation and verification (IV&V) work. But DARPA has the flexibility and resources for such herculean efforts to assess essential techniques. In one unusual instance, an IV&V laboratory needed a sophisticated US$200,000 microscopy and microfluidic set-up to make an accurate assessment. These costs are high, but we think they are an essential investment to avoid wasting taxpayers’ money and to advance fundamental research towards beneficial applications. Here, we outline what we’ve learnt from implementing this programme, and how it could be applied more broadly. Engineers expect their work to be subject to an IV&V process, in which the organization conducting the research uses a separate set of engineers to test, for example, whether microprocessors or navigation software work as expected. NASA’s IV&V facility was established more than 25 years ago and has around 300 employees testing code and satellite components. In conventional electronics, IV&V relies on fundamental units such as transistors, diodes, capacitors and oscillators. The electronics industry takes great pains to promote the compatibility of these basic elements across platforms. For example, a given microprocessor developed by Intel can function in both children’s toys and advanced physics labs. That compatibility does not apply to biological systems: proteins and cell lines are often not interchangeable, and even identical proteins can function differently in different environments. First-principle explanations of how they work often do not exist. This complicates efforts to apply IV&V approaches developed for electronics and software. What’s more, increasingly sophisticated bioengineering tools are making cell-biology experiments more complicated, so thorough validation could take months or even years to complete. Few investigators have such resources.   Instead, the biological sciences have depended on other, less-reliable techniques for reproducibility. The most long-standing is the assumption that reproducibility studies will occur organically as different researchers work on related problems. In the past five years or so, funding agencies and journals have implemented more-stringent experimental-reporting and data-availability requirements for grant proposals and submitted manuscripts. A handful of initiatives have attempted to replicate select published studies. The peer-reviewed Journal of Visualized Experiments creates videos to disseminate details that are hard to convey in conventional methods sections. Yet pitfalls persist. Scientists might waste resources trying to build on unproven techniques. And real discoveries can be labelled irreproducible because too few resources are available to conduct a validation. We were lucky enough to have the time, money and mandate to try something different. The synthetic-biology focus of DARPA’s Biological Control programme is well suited to merging biological research with reproducibility studies. The programme aims to bring engineering principles of design and control to biology. By definition, this requires the adoption of best practices from the engineering community — such as IV&V — to improve the likelihood that technologies can advance. Awardees were told from the outset that they would be paired with an IV&V team consisting of unbiased, third-party scientists hired by and accountable to DARPA. In this programme, we relied on US Department of Defense laboratories, with specific teams selected for their technical competence and ability to solve problems creatively. To get comfortable with the concept of IV&V, investigators needed reassurance that replicating teams would not steal ideas or derail publications. They also needed to get used to their results being challenged even before peer-review submission, and they needed reminders that cooperating with these teams was a programme requirement.   Results so far show a high degree of experimental reproducibility. The technologies investigated include using chemical triggers to control how cells migrate1; introducing synthetic circuits that control other cell functions2; intricate protein switches that can be programmed to respond to various cellular conditions3; and timed bacterial expression that works even in the variable environment of the mammalian gut4. In the future, we expect replication efforts will be reported as supplemental data submitted with manuscripts. Especially when claims border on the fantastical, it is helpful to show peer reviewers and editors that an independent party has confirmed the finding. So far, one publication co-authored by performer and IV&V teams has been accepted5, and two more are nearing submission. Still, getting to this point was more difficult than we expected. It demanded intense coordination, communication and attention to detail. Successfully combining reproducibility studies with fundamental research required a level of coordination between laboratories and with the programme manager (P.E.S.) that none of us had experienced before. The manager worked with each project team to determine which of their many results merited validation on the basis of the desired impact and application. We wanted to know that the engineered organism — yeast, bacteria, slime moulds, mammalian cells or something else — could be modified reliably and that these modifications performed as expected, as well as what environmental conditions were essential for that performance. A typical academic lab trying to reproduce another lab’s results would probably limit itself to a month or so and perhaps three or four permutations before giving up. Our effort needed capable research groups that could dedicate much more time (in one case, 20 months) and that could flexibly follow evolving research. Ultimately, the technologies that DARPA is developing should end up being applied by many people for a broad range of uses. So in addition to assessing whether the technologies worked, IV&V teams had to assess robustness. For instance, we needed to know what fraction of cells would incorporate new genetic material, especially when multiple genes and control elements were involved. We tested whether cells would still work in the same way if frozen and thawed months later, and whether they would retain their functionality after being grown continuously. One IV&V team checked whether migration in a genetically modified cell line was faster than in its precursor, and fabricated guidance chips to determine what surfaces best directed cell migration. Achieving verification means communicating effectively. Performer teams, particularly those with several principal investigators, had to designate someone to facilitate teleconferences and site visits. Both teams present jointly to the programme manager at least twice a year. A key component of the IV&V teams’ effort has been to spend a day or more working with the performer teams in their laboratories. Often, members of a performer laboratory travel to the IV&V laboratory as well. These interactions lead to a better grasp of methodology than reading a paper, frequently revealing person-to-person differences that can affect results. This is especially true when the IV&V investigator does not regularly work with the same cell type as the performer team, and thus approaches experiments in a similar way to other researchers who are building on a newly reported technique. Real-time collaboration minimizes or avoids logistical roadblocks that are known to prevent basic research validation (for example, when the original samples cannot be located, or the postdoctoral researcher with the necessary expertise is no longer with the laboratory). Still, our IV&V efforts have been derailed for weeks at a time for trivial reasons (see ‘Hard lessons’), such as a typo that meant an ingredient in cell media was off by an order of magnitude. We lost more than a year after discovering that commonly used biochemicals that were thought to be interchangeable are not. A five-laboratory consortium testing how cultured cells responded to cancer drugs reported similar experiences, with minor differences causing major effects6. Recommendation What to do Our experience Document reagents Include the vendor, product number and lot number for all reagents. We lost weeks of work and performed useless experiments when we assumed that identically named reagents (for example, polyethylene glycol or fetal bovine serum) from different vendors could be used interchangeably. See it live Watch an experiment carried out by another team. In our hands, washing cells too vigorously or using the wrong-size pipette tip changed results unpredictably. Site visits are mandatory because witnessing experiments in action reveals valuable information, such as how to trap Hydra without harming them, or how to tilt a cell plate. The benefits of site visits in terms of achieving reproducibility are worth the cost of plane tickets and lodging. State a range Rather than a single number, state a range of acceptable conditions for temperature, convection and other control standards. Knowing whether 21 ° C means 20.5–21.5 ° C or 20–22 ° C can tell you whether cells will thrive or wither, and whether you’ll need to buy an incubator to make an experiment work. Test, then ship Immediately before shipping cells or a genetic construct for testing, check them or it. Incorrect, outdated or otherwise diminished products were sent to the IV&V team for verification many times. Double check If a standard protocol does not work, the performer and independent valuation and verification (IV&V) teams should work together on a step-by-step review. A typo in one protocol cost us four weeks of failed experiments, and in general, vague descriptions of formulation protocols (for example, for expressing genes and making proteins without cells) caused months of delay and cost thousands of dollars in wasted reagents. Pick a person Each performer team should designate one person to keep communication open, accurate and timely. The projects that lacked a dedicated and stable point of contact were the same ones that took the longest to reproduce. That is not coincidence. Keep in silico analysis up to date Data-analysis pipelines are replete with configuration decisions, assumptions, dependencies and contingencies that move quickly beyond documentation, making troubleshooting incredibly difficult. Teams had to visit each others’ labs more than once to understand and fully implement computational-analysis pipelines for large microscopy data sets. Now, our IV&V efforts begin by cataloguing all chemicals, media and cell types, their suppliers and, for animal-derived extracts, lot numbers. Instruments are calibrated and settings (for example, microscope light source power) coordinated to avoid unintended side effects such as phototoxicity. Routine measurements in cell culture, such as pH, osmolarity and testing for Mycoplasma, which often fall by the wayside, are prioritized. Each project creates a customized checklist depending on its cell lines, equipment and experiments. Without this essential level of research hygiene, troubleshooting efforts would become an uninformative time sink. We have learnt to note the flow rates used when washing cells from culture dishes, to optimize salt concentration in each batch of medium and to describe temperature and other conditions with a range rather than a single number. This last practice came about after we realized that diminished slime-mould viability in our Washington DC facility was due to lab temperatures that could fluctuate by 2 °C on warm summer days, versus the more tightly controlled temperature of the performer lab in Baltimore 63 kilometres away. Such observations can be written up in a protocol paper. Sometimes, validation requires new equipment. For the slime moulds, independent validation meant buying an incubator that could keep cells stably at 21.5 °C, slightly below the IV&V laboratory’s ambient temperature. In another case, the performer team had to help install customized microfluidic and optical equipment at the IV&V lab because the standard microscopes and analysis software used for live-cell imaging were not up to the task. All this makes for a considerably more variable IV&V programme than is found in microelectronics. But without these efforts, some promising technologies could have been abandoned prematurely as seeming dead ends. We think that the IV&V programme brings benefits beyond reproducing any individual project. Now, there is a process to make investigations of disparate results more transparent. Performing reproducibility studies invariably forces scientists to think more deeply about their own experimental protocols and techniques. As one of our scientists said, “IV&V forces performers to think more critically about what qualifies as a successful system, and facilitates candid discussion about system performance and limitations.” Trainees told us that they have gained skill in analysing data, providing constructive criticism and designing and documenting their own research so that it can be reproduced. IV&V teams gained further advantages. For example, because service laboratories become well-versed in the mindset and protocols for new technologies even before publications appear, they are well-poised to integrate them into their offerings, predict future directions for the field and move research more quickly to applications. The IV&V programme also expands networking opportunities between DARPA scientists and the top-quality labs DARPA funds, including the potential to recruit postdocs and graduate students across laboratories. Not surprisingly, many DARPA BTO programmes in recent years have incorporated some form of IV&V to help validate programme results. As we continue the Biological Control IV&V programme, we expect to find more ways to improve it, to better quantify its benefits and to codify best practices, such as incorporating automation and robotics where possible and keeping an open line of communication between performer groups and IV&V teams. Although some of the lessons learnt from the first stages might seem obvious and trite, that also reinforces their necessity. We think that a dedicated shift towards the IV&V model by more research institutions and funding agencies will bring more reliable and cost-effective science. Programme officers at other granting agencies should consider allocating a portion of their funding stream to independent reproducibility efforts. This will both reduce the number of papers that cannot be replicated and improve the quality of work that funding agencies support. Metrics will need to be established to quantify the cost savings of applying this model to synthetic biology and bioengineering, but given its successful integration throughout more conventional engineering disciplines, we are optimistic that the returns will be worth it. </body>
<date id = '78'>10 March 2020</date>
<url id = '79'>https://nature.com/articles/d41586-020-00571-x</url>
<title id = '79'>New synthesis shows what a wasted decade means for the climate pact made in Paris.</title>
<body id = '79'>The past decade of political failure on climate change has cost us all dear. It has shrunk the time left for action by two-thirds. In 2010, the world thought it had 30 years to halve global emissions of greenhouse gases. Today, we know that this must happen in ten years to minimize the effects of climate change. Incremental shifts that might once have been sufficient are no longer enough. The further bad news is that, even taken together, the proposed climate action by all countries is a long way from meeting this requirement. Rather than halving emissions by 2030, countries’ climate proposals will lead to a slight increase. Worse still, individual countries are not on track to achieve commitments that were insufficient from the outset and are now woefully inadequate. The better news is that more countries, regions, cities and businesses are implementing the deep, rapid transformations that are urgently required. At scale, these could achieve the collective climate goals that nations agreed in Paris more than four years ago. There are lessons to be learnt from places such as Costa Rica, Shenzhen in China and Copenhagen that have made strides through the use of renewable energy and electrified transport. The United Kingdom (together with 75 other parties) and California have at least set ambitious goals to become carbon neutral, which might send signals to industry even before supporting policies are implemented. Meanwhile, 26 banks have stopped directly financing new coal-fired power plants (see go.nature.com/32uped2). Much is happening on the ground. The question is how to ramp up these activities fast enough to keep warming to less than 1.5 °C above pre-industrial levels. Here we present a snapshot of the extent to which nations’ individual pledges are inconsistent with their stated collective goals. We also note some of the pockets of promise. We draw our conclusions from a synthesis of all ten editions of the Emissions Gap Report produced by the United Nations Environment Programme (UNEP)1–5. Each year for the past decade, this report has examined the difference between what countries have pledged to do individually to reduce greenhouse-gas emissions, and what they need to do collectively to meet agreed temperature goals — the ‘gap’. Our analysis shows that the gap has widened by as much as four times since 2010. There are three reasons for this. First, global annual greenhouse-gas emissions increased by 14% between 2008 and 20186. This means that emissions now have to decline faster than was previously estimated, because it is cumulative emissions that determine the long-term temperature increase. Second, the international community now agrees that it must ensure a lower global temperature rise than it decided ten years ago, because climate risks are better understood. And third, countries’ new climate pledges have been insufficient. The tenth anniversary of the report coincides with the 2020 milestone to which countries agreed in Paris. They undertook to communicate or update climate pledges, or ‘nationally determined contributions’, to the UN Framework Convention on Climate Change conference (COP26) this November in Glasgow, UK. Clearly, the promises must be overhauled — and then, crucially, kept — if the yawning gap between ‘talk and walk’ is going to close by 2030. Electric taxis at a charging station in Shenzhen, China.Credit: Qilai Shen/Bloomberg/Getty The scope of the UNEP emissions gap reports has evolved over time, in line with climate policy. So what has changed during the past decade? In the 2009 Copenhagen accord7 and the 2010 Cancun agreement8, countries collectively pledged to limit warming to below 2 °C, and 73 countries individually pledged emissions targets for 2020. The 2015 Paris agreement, responding to mounting concern over the impacts of climate change, tightened the collective temperature limit to “well below 2 °C” and agreed “to pursue efforts to limit the temperature increase to 1.5 °C”9. Under the Paris deal, 192 parties individually pledged emissions targets, typically for 2030 (see ‘More and faster’). Sources: Historical emissions: ref. 6; 2010 pledges and pathways: ref. 1; 2015 pledges and pathways: ref. 2. From 2010 to 2014, the gap reports projected the likely difference in 2020 between the expected result of countries’ pledges and the pathways towards 2 °C. The 2010 report documented a shortfall of 14%. Since 2015, the reports have forecast the expected shortfall in 2030 between the countries’ pledges and progress towards both 1.5 °C (current shortfall of 55%) and 2 °C (current shortfall of 25%; see ‘More and faster’). The report also examines the policies that countries are implementing domestically. Had serious climate action begun in 2010, the cuts required to meet the emissions levels for 2 °C would have been around 2% per year, on average, up to 2030. Instead, emissions increased. Consequently, the required cuts from 2020 are now more than 7% per year on average for 1.5 °C (close to 3% for 2 °C).   The time window for halving global emissions has also narrowed drastically. In 2010, it was 30 years; today, it is 10 years for 1.5 °C (25 years for 2 °C). Although many reports, scientists and policymakers continue to discuss rises of 2 °C, it must be emphasized that, in 2018, the Intergovernmental Panel on Climate Change reported that warming of more than 1.5 °C would be disastrous10. Countries are not even on track to achieve their now plainly inadequate 2015 pledges. Of the G20 countries, seven (Australia, Brazil, Canada, Japan, South Korea, South Africa and the United States) need to implement existing policy or roll out new measures. (The United States has begun the process of withdrawing from the Paris agreement, and will leave in November.) Russia and Turkey have set themselves unambitious targets that they can meet without new policies. Since 2015, estimated global emissions in 2030 have decreased by only 3%. For the leading seven emitters, 2030 estimates have slightly decreased, flatlined or increased (see ‘The seven top emitters’). Country or region (2018 emissions in gigatonnes CO2 equivalent)6,13 Change in projected greenhouse-gas emissions by 2030 since 2015 Potential reasons China (13.2) No change New climate and energy policies; altered growth projections. United States (6.6) No change Rollback of federal policies works against price drops in renewables and reductions in coal use. European Union (4.0) Lower Mostly attributable to implementation of new policies. India (3.8) Slightly lower Unclear. Russia (2.4) No change No change in policies or growth projections Indonesia (2.3) Higher Higher emissions projections from deforestation. Brazil (1.6) Higher Higher emissions projections from deforestation. Comparison of the 2015 and 2019 UNEP Emissions Gap Report2,20 and other sources21–25 provides information about changes in current policy projections for the leading emitters. Uncertainties for each estimate are large. See Supplementary Information for details. No single model can predict the future, and such analyses by necessity exclude the most recent developments. Nevertheless, it is clear that, collectively, current policies will not limit global warming to well below 2 °C, let alone 1.5 °C, as agreed in Paris. Clearly, the annual audit of the emissions gap has not altered poor performance. The gap concept has nonetheless proved useful. The reports and numbers have continuously informed the UN climate summits11 and the emissions gap was noted as a serious concern when parties were adopting the Paris agreement9. Fundamental policy transformations have begun to appear in some sectors, countries, regions, cities and businesses over the past ten years. These innovations seek to achieve the UN’s Sustainable Development Goals (SDGs), including climate ones. Slashing emissions now requires ‘leaving no one behind’. To recognize, monitor and understand these advances, the gap reports have included examples. Some are discussed here. Ambitious action. Most encouragingly, a wealth of agile nations, regions, cities and businesses have promised or made radical changes since the Paris agreement (see ‘Action gap’ and Supplementary Information. See also go.nature.com/2t22tth). At the last count, net-zero emissions goals have been set or are being considered by 76 countries or regions (the European Union is the largest) and 14 sub-national regions or states (the largest being California); some locations have begun implementation. Together, these places account for about 21% of global greenhouse-gas emissions12,13. Sources: 2017 emissions: refs 12,13; 2017 Energy production: ref. 14. Fifty-three countries and 31 states and regions have explicitly committed to an emissions-free electricity sector. Seven additional countries have done so implicitly by aiming for net-zero greenhouse-gas emissions. Together, these account for around 18% of global electricity generation14. Twenty-one countries, 5 regions and more than 52 cities have committed to make all vehicles emissions-free. Individual examples also exist for sectors in which reaching zero emissions was thought to be difficult, such as heavy industry and aviation. Steel giants ThyssenKrupp in Essen, Germany, and SSAB in Stockholm are aiming for zero-emissions steel production by 2050 and 2045, respectively. The building-materials company Heidelberg Cement, headquartered in Germany, is aiming for zero-emissions cement production by 2050. For aviation, Norway and Scotland hope to make short-haul and domestic flights zero emissions by 2040. Renewables. Costs of renewable energy are falling faster than expected15. Renewables are currently the cheapest source of new power generation in most of the world. Solar and wind power will be financially more competitive than will existing coal plants by next year15. These cost declines, and those of battery storage, are opening up possibilities for large-scale, low-carbon electrification.   Coal consumption. The rise of renewable energy can – must – facilitate a move away from coal. Emerging economies that depend on coal, such as China and India, have begun to address consumption by adjusting the fuel’s price, capping its consumption, reducing plans for new coal-fired power plants and supporting renewables. Much more must be done, and quickly — while addressing poverty, energy access and urbanization16–18. UN SDGs. Actions to reduce greenhouse-gas emissions are essential for achieving food security, healthy lives and many other SDGs, as confirmed by a growing body of research10,19. For example, renewable energy cuts air pollution, and improves health and energy security compared with fossil fuels. These few success stories must be scaled up and mirrored with progress in every sector. The fact that reductions in greenhouse-gas emissions are a prerequisite to achieving sustainable development must propel action. The gap is so huge that governments, the private sector and communities need to switch into crisis mode, make their climate pledges more ambitious and focus on early and aggressive action. Otherwise, the Paris agreement’s long-term goals are out of reach. We do not have another ten years. </body>
<date id = '79'>04 March 2020</date>
<url id = '80'>https://nature.com/articles/d41586-020-00500-y</url>
<title id = '80'>Journals, funders and scholars must work together to create an infrastructure to study peer review.</title>
<body id = '80'>Peer review is the defining feature of scholarly communication. In a 2018 survey of more than 11,000 researchers, 98% said that they considered peer review important or extremely important for ensuring the quality and integrity of scholarly communication1. Indeed, now that the Internet and social media have assumed journals’ original role of dissemination, a journal’s main function is curation. Both the public and the scientific community trust peer review to uphold shared values of rigour, ethics, originality and analysis by improving publications and filtering out weak or errant ones. Scholarly communities rely on peer review to establish common knowledge and credit.   Despite decades of calls for study, research on peer review is scarce2. Current investigations are fragmented, with few connections and limited knowledge-sharing, as manifested by how sparsely these researchers cite each other’s papers3. The most rigorous work is generally restricted to one or a few journals per study, often in the same field. There is a lack of systematic research on how journals manage the process (such as selecting, instructing and rewarding reviewers, managing conflicting reviews, or publishing reviewers’ reports); on how to define the quality and utility of individual reviews; and on how to assess peer review (such as who participates, how and why). Nor is there a way to compare the reactions of authors and reviewers at different journals or in different disciplines. The topic is under-studied partly because it is difficult to research. Access to data about the review process is hard won. It often depends on personal connections with journals, and is generally limited to such a small number of titles that generalizations are hard to make. Few dedicated grants are available. Yet greater transparency and study could determine which models and practices of peer review best promote research integrity and reliability4. Here we describe a pilot project to encourage broad, systematic study of peer review and what we hope this can accomplish. Tantalizing insights are possible when researchers can access journal data about peer review. For instance, sociologist Misha Teplitskiy at the University of Michigan in Ann Arbor and his colleagues were confidentially supplied with reviewers’ identities for 7,982 neuroscience manuscripts submitted to PLoS ONE, and created co-publishing networks of reviewers, authors and editors. They found that reviewers tended to favour authors connected to them through co-authorship and professional networks5. Data from the journal Functional Ecology indicated quantitatively that the use of author-suggested reviewers can bias editorial decisions6. An analysis of the writing styles and recommendations in reviewer reports from five Elsevier journals suggests that open peer review favours more objective and constructive remarks7. However, only 8.1% of those referees agreed to reveal their identity, and this was mostly when their recommendations were positive7. And analysts at Elsevier last year identified reviewers and an editor who seem to have unethically used peer review to boost citations of their own work (see Nature http://doi.org/gf7zjm; 2019).   Cross-disciplinary teams doing both qualitative and quantitative research will be essential for understanding which review models (single blind, double blind, published and unpublished reviews, confidential and disclosed reviewers) work best under which circumstances and why. Studies that probe reviewer behaviour under different peer-review models or before and after a change in process could assess, for instance, whether double-blind review (often used in the humanities and social sciences) is just convention or is a useful way to avoid favouring senior scientists. Such knowledge could stop journals implementing one-size-fits-all approaches when they are inappropriate, and might suggest how to harmonize peer-review processes to benefit authors and referees. There are many questions about the quality of journals that access to reviewer reports cannot address. In some cases, journals must take the initiative to perform internal experiments. In 1999, to see whether publishing reviewers’ names would affect the quality of reviews, the British Medical Journal ran a randomized trial. In 2014, Elsevier ran a trial on five journals that shifted from closed to open peer review. Nature Research journals are also studying the effects of publishing review reports. In other cases, relevant data can be found in the manuscripts themselves — for example, whether key experimental details of animal and human studies are included and comply with those registered before studies began. However, a systematic study of peer review could address crucial questions, such as when and where it has the most value — in screening out weak manuscripts, improving mediocre ones or adding essential caveats or context. It could help to reveal when and why editorial decisions are made on the basis of quality versus authors’ reputations. That could, in turn, help the development of tools for evaluating quality, rigour and integrity. Authors, editors and referees could use these in assessing individual manuscripts. Publishers, scientific associations and other organizations could use such tools to improve their reviewing processes. This will be most effective if evidence comes from a broad variety of sources. Accumulating the sort of data we envisage might seem like a pipe dream. The peer-review process varies greatly across publishers, and there are even idiosyncratic differences at the same journal. Yet data sharing and collaboration now occur across disparate domains outside scholarly publishing. Pharmaceutical companies and other research institutions pool clinical data through platforms such as Vivli and YODA, and drug-discovery data through Open Targets. Some 50 European firms involved in transportation, logistics and information technology (IT) are sharing data with an eye to, for instance, helping passengers to move more swiftly through airports or transporting goods more efficiently. The Global Alliance for Genomics and Health, a non-profit organization based in Toronto, Canada, has created frameworks and standards for responsible, voluntary sharing of genomic data.   Many digital innovations in scholarly publishing can be applied to the study of peer review. ORCID (which supplies unique identifiers for individual researchers) and Crossref (which can, for instance, link citations, data sets and individual publications) can help to disambiguate reviewers, authors and scholarly products, and so enable more-rigorous analyses. The Manuscript Exchange Common Approach, a framework for transferring manuscripts and reviews between different publishers and preprints, shows the feasibility of creating databases that can pull from different peer-review management systems. These include ScholarOne Manuscripts — used by the publishers Wiley and Sage, among others — and Editorial Manager, which is used by Springer Nature, Elsevier and many others. Big obstacles remain. Publishers, both private and non-profit, consider it risky to show their workings, not least because of concerns over confidentiality. There is no infrastructure for sharing data even on the number of reviewers per manuscript, the rate of accepted review requests and other data that would not violate any confidentiality rule. In 2014, a group of science and technology scholars, publishing professionals and funders — including many of the authors of this Comment — formed a collaboration under a European Union project called PEERE, funded by COST (the European Cooperation in Science and Technology). We wanted to enable broader research that involved many journals from many disciplines. In 2017, we released a PEERE protocol8 for sharing data on the peer-review process (see go.nature.com/2vbkc7m). It considers ethics, responsible management, data protection and privacy, and complies with the current EU legal framework, including the General Data Protection Regulation (see ‘Peer-review data’). The PEERE collaboration has a protocol8 for sharing data on the peer-review process (see go.nature.com/2vbkc7m). Information about each submitted manuscript — shown here in four categories — is anonymized, organized and protected within the PEERE infrastructure, and made available to researchers for studies on peer review. • Author characteristics (number, gender, country, seniority). Data are stored under an identifier, not a name. • Editorial decision (accepted, rejected). • Text of submitted manuscript (recoded to be readable only by machines). • Text of published manuscript. • Recognition metrics (citations, Altmetrics). • Discipline(s) covered. • Impact factor. A range is used to avoid identifying the specific journal or paper. • Number of yearly submissions. • Rejection rate. • Editor characteristics (gender, number). Data are stored under an identifier, not a name. • Submission date. • Time required for decisions by editors and reviewers. • Type of blinding. • Number of invited and actual referees. • Rounds of revisions. • Reviewer characteristics (number, gender, country, seniority). Data are stored under an identifier, not a name. • Reviewer score and recommendations (reject, accept with minor or major revisions). • Agreement between reviewers. • Length and text of report (recoded to be readable only by machines). • Degree of openness (whether report or reviewer identity is published). Subsequently, we piloted a series of data-sharing initiatives that we scaled up to cover more than 150 journals (we have not made the list available because the journals are anonymized). The publishers include Elsevier, the Royal Society, Springer Nature and Wiley. PEERE computer scientists worked with technical staff from ScholarOne and Editorial Manager to design a joint metadata set and to store data ready to be used for research. We have developed ways to, for example, anonymize factors such as the publisher and journal name. Author, referee and editor names are anonymized but transformed into a consistent identifier throughout samples. The text of reports is recoded as a list of machine-readable symbols with a hidden key that is accessible to natural-language computing techniques and other analytics. This provides the technical groundwork for a broad data-sharing infrastructure that can enable systematic research on peer review. Next, we need to develop a blueprint on data-sharing and commission a proof-of-concept infrastructure that complies with general data-protection regulations and other principles of ethical management. We have worked out ways to make data systems interoperable and to ensure that people using these data cannot access proprietary information, as well as ensuring that users follow ethical procedures. We hope to discuss broader plans when scholars come together at the PEERE conference in Valencia, Spain, in mid-March. Making all of this happen will require accountability and reliable funds. Public agencies and independent foundations have already started to recognize that research to improve research is an investment, supporting initiatives such as the Open Science Framework, the Research on Research Institute and others. Small and under-resourced publishers — which in some disciplines publish the leading journals — might still rely mainly on manual systems and could need help to participate or to overhaul their systems. As custodians of the scholarly record, publishers, independent journals and learned societies should participate in developing the blueprint and provide support with in-kind investments (such as the time of their IT staff). They should also work to remove obstacles to data sharing, such as the culture of secrecy and bug-ridden, patchworked content-management systems. Public bodies should support the establishment of an independent, representative governance group for the data-sharing infrastructure. In the ideal situation, researchers would need only to sign an ethics agreement, file a request detailing the type of data needed, and either run their data-analysis scripts on our infrastructure or obtain material for qualitative research. Our system is designed to prevent researchers from pulling out data from a single journal or identifying individual researchers. This infrastructure needs to be seen as a community public good — not as a resource that entrenches private interests. Of course, the infrastructure must take into account each stakeholder’s needs for responsible data management, confidentiality and accountability. Once in place, access for research studies could be greatly expanded: academics would not need to negotiate individual agreements on data sharing or forge direct connections with journal editors and publishers. Publishers, too, could rely on shared protocols to anonymize and manage data, reducing costs and reputational risk. Indeed, an agreement to share data on peer review could become a clear marker of legitimacy in a world increasingly plagued by predatory journals. It is not hard to imagine work that would support standards for data and journal management, or for training, certifying and crediting reviewers and editors. Although there is much to be done, supporting research on peer review promises to create better processes for authors, reviewers and editors. More importantly, it will boost the reliability, rigour and relevance of the scientific literature. Everyone will benefit from this. </body>
<date id = '80'>25 February 2020</date>
<url id = '81'>https://nature.com/articles/d41586-020-00446-1</url>
<title id = '81'>The conservation community must be able to track countries’ progress in protecting wetlands, reefs, forests and more, argue James Watson and colleagues.</title>
<body id = '81'>Coral in a mangrove swamp in the Raja Ampat Islands, Indonesia.Credit: Giordano Cipriani/Getty Next week, representatives of more than 190 nations are gathering in Rome to discuss how to halt the biodiversity crisis during this decade and beyond. Since 2010, targets for conserving species have shaped policy and galvanized efforts to halt species loss worldwide, as part of the Convention on Biological Diversity (CBD; see www.cbd.int/sp/targets). Yet no such targets exist for ecosystems — despite the wealth of evidence showing that their health and functions are essential to the processes that maintain all life1.   Targets that are specific, measurable, attainable, relevant and timely (SMART) are central to project planning and have proved to be effective in policies that seek to address global problems. For example, during the 1980s, a group of 20 nations agreed to set various limits on the production and consumption of chlorofluorocarbons. This helped to guide the phase-out of these substances under the Montreal Protocol, which came into effect in 19892. It is now possible to establish a SMART target for ecosystems, as well as metrics to track progress in meeting that goal. Nations are no longer limited by a lack of knowledge or methods when it comes to ecosystem mapping and assessment (see ‘Under pressure’). What’s more, they can use a proven and standardized approach for ecosystem risk assessment: the Red List of Ecosystems protocol, which was adopted by the International Union for Conservation of Nature (IUCN) in 2014. Sources: J. E. M. Watson et al. (map); data from O. Venter et al. Nature Commun. 7, 12558 (2016)/B. S. Halpern et al. Sci. Rep. 9, 11609 (2019). We urge those attending next week’s meeting to place an ecosystem-based goal and target alongside species-based ones in their discussions. Nations have a chance to ensure that all of the world’s remaining intact ecosystems are retained by 2030, that overall ecosystem area and integrity increase by 2050, and that all that fall below a level of degradation defined by the Red List of Ecosystems protocol are restored. The ratification of an international target will compel governments to act. This is the only way to halt the decline of ecosystems. In 2010, the 193 nations that were parties to the CBD agreed to work together to prevent the extinction of known threatened species and improve their conservation status by 2020. They did this by ratifying Target 12 of the CBD 2011–20 strategic plan for biodiversity (see www.cbd.int/sp/targets). Actions taken because of this and previous CBD targets have reduced the risk of extinction for many species, although direct links are hard to prove. For example, conservation efforts over the past 30 years have helped to cut the extinction rate of endangered birds by at least 40%, according to one analysis3. Previously endangered populations that are now growing include the Seychelles magpie-robin (Copsychus sechellarum) and a Brazilian parrot called Lear’s macaw (Anodorhynchus leari). Over the past decade, nations have been identifying and protecting the marine, terrestrial and freshwater sites that are of international importance to the conservation of vulnerable species. More than 16,000 of these ‘key biodiversity areas’ have now been identified worldwide (see go.nature.com/2xdtqb8). Government reports submitted to the CBD indicate that such areas are increasingly being protected4. One example is Itombwe Natural Reserve in the Democratic Republic of the Congo, which was formally established in 2016 to conserve several rare species, including the enigmatic Itombwe puddle frog (Phrynobatrachus sp.). Such species-focused conservation activities are crucial. But they are not sufficient to sustain biodiversity and the benefits of nature to humanity. Human impacts such as overfishing have affected the Amazon River ecosystem in Brazil.Credit: Ricardo Oliveira/AFP/Getty Ecosystems, from the boreal forest and wetlands to coral reefs and mangroves, are more than the total of the plants and animals living in them5. Complex interactions between biological and physical systems drive processes that sustain all life. This includes production of clean water, regulation of air quality and climate through carbon sequestration and storage, soil formation, pollination and the production of food and wood for houses1. Indeed, natural systems are key to dealing with the effects of climate change, as highlighted by a 2019 study6. It estimated that, between 2000 and 2013, the impact on carbon levels of losing intact tropical forests (including indirect effects such as reduced biodiversity and increased selective logging) might be six times greater than was originally proposed6. Thanks to substantial advances in mapping and monitoring, scientists can now diagnose ecosystems’ defining features and the processes that threaten them5,7. Take the demise of tidal flats revealed by satellite technology. Such mapping showed that coastal development and sea-level rise destroyed 16% of these ecosystems between 1984 and 2016. This has reduced storm protection and food provision for billions of people8. Remote sensing is similarly monitoring tropical forests9, ice cover10, coral reefs11 and mangroves12. For instance, at least 12% of the world’s mangroves were lost between 1996 and 2010 because of human activities13.   Pivotal to these efforts has been the development of the Red List of Ecosystems protocol, a set of criteria for identifying ecosystems that are most at risk of collapse14. It lays out how to define and map ecosystems, and enables systematic risk assessment using an array of indicators of extent and degradation. So far, the Red List criteria have been used to assess more than 2,800 ecosystems in 100 countries across all continents15; 45% of those systems were found to be at risk of collapse (D.A.K., unpublished observation). These efforts could serve as a starting point for work towards an international target for conserving ecosystems. Ecosystem-level conservation is already affecting decisions on resource use and management made by national governments, non-governmental organizations and industry15. For example, a 2017 assessment of ecosystems in Colombia — Amazon rainforests, tropical dry forests, high Andean cloud forests, lowland savannah and other types — classified almost half (44%) as either ‘endangered’ or ‘critically endangered’, as defined by the Red List protocol16. This results from human activities such as forest clearance for illegal coca crops, cattle ranching and mining. The finding has prompted the Colombian government to focus on the amount of land given protected area status, and to consider the restoration of critically endangered ecosystems. Workers cut down trees in southeast Cameroon as part of a sustainable initiative to help local groups with forest management.Credit: Brent Stirton/Getty In South Africa and Australia, businesses wanting to encroach on ecosystems that are classed as critically endangered or endangered must first conduct a full environmental impact assessment for their proposed project. Likewise, Finland’s first government-led systematic ecosystem assessment, completed in 2008, resulted in increased protection of threatened forest under the nation’s Environment Protection Act and Forest Act17. In China, assessments of the rapid decline of tidal-flat ecosystems has catalysed efforts to better understand, manage and protect them. Tidal flats surrounding the Yellow Sea in east Asia support the migration of up to three million shorebirds and stabilize the coastline for more than 150 million people, also providing them with storm protection and food18. In July 2019, two of these important migratory sites were added to the United Nations Educational, Scientific and Cultural Organization (UNESCO) World Heritage List after being classified as endangered under the IUCN criteria . It is difficult to accurately assess progress towards conservation targets at the species level — a major constraint on their effectiveness. Monitoring of at-risk species is often infrequent and numbers fluctuate naturally from year to year. Such species also tend to be elusive. At the ecosystem level, a SMART target should therefore enable frequent tracking of ecosystems using remote sensing and modelling. This could result in more-transparent reporting of the status of Earth’s ecosystems, enhancing public awareness of their current trajectories and the consequences of their decline. Any ecosystem target should set limits on degradation that mark the irreversible loss of key processes14. A target should also highlight the importance of conserving healthy ecosystems over restoring degraded ones. Such restoration is technologically and economically challenging and, as yet, there is no evidence that complete restoration of an ecosystem is even possible. Nevertheless, restoration has a key role in avoiding species extinctions and mitigating climate change, and should be part of an ecosystem goal. The Rome meeting is the second of three working-group meetings for negotiations leading up to a new set of biodiversity targets, which will replace those agreed in 2010. This 2030 global strategic plan for biodiversity will be formally established in October by the signatories to the CBD. This year marks the implementation of the pledges made in the Paris climate agreement, and the United Nations Decade on Ecosystem Restoration begins in 2021. The launch of the 2030 strategic plan in October is an unprecedented opportunity — perhaps the last — for humanity to address multiple environmental problems at once. Whereas a species target forces nations to report on their progress only in relation to biodiversity, an ecosystem target would necessitate simultaneous reporting on wins across three fronts: biodiversity, climate change and sustainability (specifically, on the United Nations Sustainable Development Goals for human development and well-being). World leaders must be held accountable for the current and future state of their countries’ ecosystems. </body>
<date id = '81'>18 February 2020</date>
<url id = '82'>https://nature.com/articles/d41586-020-00324-w</url>
<title id = '82'>A levy on fossil fuels can support and restore ecosystems that help to stem climate change.</title>
<body id = '82'>Deforestation must be stopped in tropical countries to tackle the existential threats of climate change and biodiversity loss. The vast majority of Earth’s species are in the tropics; forests there have taken in much of the carbon added to the atmosphere by human activities. Safeguarding these forests is central to slashing greenhouse-gas emissions and meeting the internationally agreed United Nations Sustainable Development Goals (SDGs)1. Sadly, in tropical countries and internationally, investments are woefully inadequate in conservation, restoration and improving land management to protect biodiversity and ecosystem services — collectively called ‘natural climate solutions’1,2. To plug this gap, we urge more countries that have tropical forests to adopt a tropical carbon tax — in South and Central America, Africa, Asia and the Pacific. This is a levy on fossil fuels that is invested in natural climate solutions. Such a policy can reduce the use of oil, gas and coal and mobilize domestic funds for adaptation and mitigation. Costa Rica and Colombia have done this. Our own analysis shows that, if 12 other countries roll out a tropical carbon tax similar to Colombia’s, they could raise US$1.8 billion each year between them to invest in natural habitats that benefit the climate (see Supplementary Information). We call on governments, development banks, financial investors and non-governmental organizations to support those countries that need financial and technical help to implement this policy, and to ensure that the money raised is spent efficiently and effectively. Almost one-quarter of the emissions caused by humans come from agriculture, forestry, fibre and livestock production3. It has been estimated that tropical deforestation can contribute as much to emissions as do some large nations (see go.nature.com/37gmwvy). If present trends continue, by 2050 the world will have lost a further area of tropical forest almost the size of India — 289 million hectares4. This could squander half of the remaining global carbon budget for limiting warming to 1.5 °C above pre-industrial levels4. Meanwhile, more than three-quarters of species live in the tropics. These are under greater threat of extinction than is life elsewhere, mainly because of deforestation5.   There is a quick, cheap way to halt these trends: reducing the conversion of land in the tropics, especially of forests, peatlands and mangroves. Alongside cuts to fossil-fuel emissions, up to 37% of the mitigation needed to hold warming to the Paris agreement goal (to avoid the catastrophic impacts of climate change) might be achieved in this way, at a cost of less than $100 per tonne of CO2 equivalent1 — the standard measure for greenhouse-gas emissions. One-third of these mitigation options could cost less than $10 per tonne1. But ecosystem conservation, restoration and management received just 3% of global finance for climate mitigation in 2017–18: an average of $18 billion6. Most of the remainder was spent on renewable-energy generation and on investments in low-carbon transport, such as railways and electric vehicles6. Extra cash is unlikely to come from the international community in the near future, and aid and other funding is already scarce for biodiversity conservation in tropical countries2. Such nations urgently need a new way to fund natural solutions to climate change. Colombia and Costa Rica have blazed a trail. Since 1997, Costa Rica has collected a 3.5% tax on fossil fuels. That now generates $26.5 million per year7 (see go.nature.com/3jdpmtk; in Spanish). The tax was negotiated in Costa Rica’s legislative assembly and supported by research from the non-governmental Tropical Science Center in San José, which examined the benefits of forests to the country’s economy. Implementation faced little opposition because the tax was incorporated with other fiscal reforms. Surveys of fossil-fuel users indicated that they did not object if revenues were directed to forest conservation.   To invest the money raised, Costa Rica created its National Forest Fund (FONAFIFO). For example, from 1997 to 2018, the fund paid out to landowners across 23.5% of the country — an area of 1.2 million hectares. They spent the money on projects to protect 1 million hectares of mature forest and 71,000 hectares under reforestation. The fund supports conservation of mature forests, reforestation using native or exotic species, and agroforestry systems that use a mix of trees and crops or grasslands. It has disbursed $500 million to roughly 18,000 people, including those living across 162,000 hectares of Indigenous lands, such as the Cabécar and Bribri territories. Transparency and accountability of the fund’s operations are important to its success and continued popularity, so strategic and operational plans, budgets, financial statements and other details are available online (see www.fonafifo.go.cr). In the 1980s, Costa Rica had the highest deforestation rates in the world. Forest cover more than doubled between 1986 and 2013, rising to 53%8. Although estimates remain uncertain, we think that the fossil-fuel tax, along with a decline in the profitability of livestock and the expansion of protected areas and ecotourism, contributed to this. The programme funded by the fuel tax has been especially effective away from protected areas and their buffer zones9. Colombia rolled out a carbon tax in 2016 as part of sweeping fiscal reforms. These garnered broad political support because of the need to raise money for the country’s peace process. The carbon tax was developed by the Ministry of Finance and Ministry of Environment and Sustainable Development, and is collected from companies producing or importing fossil fuels. People in the Democratic Republic of the Congo at a charcoal market — the fuel is one of the causes of deforestation in the country.Credit: Federico Scoppa/AFP/Getty Colombia’s tax of $5 per tonne of emitted carbon yielded revenues of $148 million in 2017 and $91 million in 2018 (see go.nature.com/3b8ufkj; in Spanish). These go to the Colombian Peace Fund (Fondo Colombia en Paz), from which 25% is used to manage coastal erosion, reduce and monitor deforestation, conserve water sources, protect strategic ecosystems and combat climate change. A further 5% is used to strengthen Colombia’s National System of Protected Areas. The revenue will be used for conservation projects in the following prioritized areas: flood-plain forests, tropical montane cloud forests, tropical humid forests, tropical savannahs and Andean forests. These projects are in the development phase and are waiting to access the fund. There is also a project to enhance the Colombian Environmental Information System (SIAC), a web-based platform that provides official information on the state of the country’s natural resources and which is under development (see go.nature.com/2hthzqw; in Spanish). A mechanism called carbon neutrality allows companies to reduce their tax burdens by buying certified carbon credits from conservation and restoration projects in Colombia that adhere to internationally recognized standards. For example, a company might buy a credit in a region that promotes social initiatives with communities that are involved in managing these projects. This is the case for communities in the Chocó departmental region of northwestern Colombia, such as those living near towns including Acandí, El Carmen del Darién and Baudó. Up to 70% of the world’s biodiversity is found in just 17 ‘megadiverse’ countries10. Thirteen contain tropical forests. In 2018, these countries lost almost 7.3 million hectares of forests — an area roughly the size of Panama. According to our estimates, that represented nearly 30% of global deforestation and may have released about 7% of worldwide carbon emissions (see Supplementary Information and www.globalforestwatch.org/map). Two scenarios illustrate how these countries could benefit from a tropical carbon tax11 (see also Supplementary Information). The first assumes that each follows a similar policy to that of Colombia, introducing a tax of $5 per tonne of carbon emitted, and allocating 30% of the revenues to natural solutions to climate change and measures that conserve forests. The second assumes a tax of $15 per tonne of carbon emitted and 70% allocation.   We provide this second option because we think that both the urgency and interest in addressing climate change and biodiversity loss will continue to grow. It is also likely that some governments will choose to adopt such a higher carbon price and allocate more revenues to natural climate solutions. For some countries, notably India, the Philippines, Mexico, Ecuador and Malaysia, the sums raised could provide hundreds of dollars per hectare to counter forest loss. The more ambitious policy could yield nearly $13 billion each year for natural climate solutions. Brazil, the Democratic Republic of the Congo and Indonesia would benefit the most, because they currently have the greatest amount of deforestation. Countries that have experience in developing high-quality carbon-offset projects, such as Peru and Ecuador, are well positioned to adopt a tropical carbon tax (see go.nature.com/2tptk21). There are three main criticisms of funding natural climate solutions through carbon taxes. First, that they cause ‘leakage’ — the displacement of deforestation to other areas. Second, that they reduce the incentive to reduce emissions through renewable energy. And third, that the tax revenue should be used for other purposes. We think that each of these problems can be addressed. National tax schemes reduce the likelihood of leakage in each country. Renewable-energy production and natural climate solutions are both essential, as indicated by scenarios from the Intergovernmental Panel on Climate Change3. Finally, although there are many worthy uses of tax revenue, the severity of climate change and biodiversity loss means that stemming both at once is a development priority for tropical-forest countries. A cotton-top tamarin (Saguinus oedipus) in one of Colombia’s protected national parks.Credit: Thomas Marent/NPL We also recognize that it can be politically challenging to introduce measures that increase the cost of living. But as the examples in Costa Rica and Colombia illustrate, investments in protecting biodiversity to reduce carbon emissions can favour poor people because such investments have wider social benefits beyond landowners and parks9. In Costa Rica, forests and high levels of poverty can often be found in the same districts, so revenues destined for conservation can also contribute to social development. The Costa Rican government prioritizes such districts for payouts for ecosystem services. It also assists smallholder farmers and Indigenous communities in submitting requests for funds. Today, 40% of beneficiaries in Costa Rica are communities that live below the poverty line. Ecosystem services such as drinking-water supply, food provision and cultural services are estimated to contribute between 50% and 90% of income and subsistence among the rural poor and those who live in forests12. Such services can make an important contribution to ending extreme poverty (SDG 1), achieving zero hunger (SDG 2), improving health (SDG 3) and meeting many of the other 14 SDGs12. The World Bank, the International Monetary Fund (IMF) and other multilateral agencies should encourage more countries to adopt a tropical carbon tax. The IMF already promotes carbon taxes as an efficient and fiscally responsible way of reducing emissions, with revenues being used for much-needed public investments in developing countries13. The international community can support more-widespread adoption of a tropical carbon tax in two important ways. First, some tropical-forest countries and other low-income nations will require extra financial assistance because they might be unable to raise sufficient funds from a carbon tax. For example, if Papua New Guinea, Madagascar and the Democratic Republic of the Congo adopted Colombia’s approach for combating each hectare of forest loss with natural climate solutions, they would generate only $23, $3 and $1 per hectare, respectively (see Supplementary Information). Top-up financing could come from bilateral assistance, or from the Special Climate Change Fund and the Least Developed Countries Fund. Both of these are managed by the Global Environmental Facility for the UN Framework Convention on Climate Change (UNFCCC).   Second, many tropical-forest countries will require technical support to guide and monitor their investments. Countries should comply with recognized global quality marks such as the Verified Carbon Standard and the Climate, Community and Biodiversity Standard. The first is the world’s most widely used voluntary programme for mitigating greenhouse-gas emissions. The second identifies projects that simultaneously address climate change, support local communities and smallholders, and conserve biodiversity. Currently, the projects that have been validated and verified encompass more than 10 million hectares, an area the size of Iceland (see https://verra.org/project/ccb-program). Tropical countries are already showing interest in carbon-pricing initiatives and natural climate solutions. Next week, Costa Rica will host a high-level meeting on the subject in San José with government and business leaders from Peru, Ecuador, Mexico and Chile, as well as Colombia. And several major international events in 2020 provide a platform for supporting global action towards a tropical carbon tax. These include the International Union for Conservation of Nature’s World Conservation Congress in June, the 15th meeting of the Conference of the Parties (COP15) to the Convention on Biological Diversity in Kunming, China, in October, and the 26th session of the UNFCCC Conference of the Parties (COP26) in Glasgow, UK, in November. We suggest that, at these meetings, policymakers explicitly highlight and incorporate a tropical carbon tax in agreements and decisions. Tropical deforestation and land-use change must be halted to safeguard the climate and global biodiversity. The widespread adoption of a tropical carbon tax is a practical way forward. </body>
<date id = '82'>12 February 2020</date>
<url id = '83'>https://nature.com/articles/d41586-020-00082-9</url>
<title id = '83'>Efforts to protect people’s privacy in a massive international cancer project offer lessons for data sharing.</title>
<body id = '83'>A migrating breast cancer cell.Credit: Steve Gschmeissner/SPL More than 800 terabytes of genomic data are available to investigators all over the world, thanks to a major international project to identify the genetic traits associated with various types of cancer. Researchers involved have just published six papers in Nature. (Another 16 papers have been published elsewhere.) All eight of us were involved in the six-year endeavour. And four of us helped put in place safeguards to protect the privacy of the thousands of patients and volunteers who consented to have their data used in the research. Here, we reflect on some of the lessons learnt for researchers sharing vast amounts of genomic data. Genomics researchers worldwide are increasingly dealing with vast data sets gathered by consortia spanning many countries. Most are unclear on what to do to protect people’s privacy and to comply with international and national data-protection laws, especially given recent and ongoing changes in legislation. An international code of conduct for genomic data is now crucial. Built by the genomics community, it could be updated as technologies and knowledge evolve more easily than is possible for national and international legislation. Between 2013 and 2019, 468 institutions from 34 countries in Asia, Australasia, Europe and North America amassed 2,658 cancer genomes — each paired with a non-cancerous sequence from the same person. The effort was led by the International Cancer Genome Consortium (ICGC). The combined data were made available to investigators largely thanks to cloud computing. The project — the Pan-Cancer Analysis of Whole Genomes (PCAWG) — is the first to try to aggregate so many subprojects across different jurisdictions and make the entire data set available across the world.   Much of the PCAWG data (and the tools for analysing them) were made available through the Cancer Genome Collaboratory, a cloud service built for the genomic research community. (The commercial cloud-service provider Amazon Web Services was also used.) But the data were first processed in high-performance computer centres and the clouds of academic institutions in Germany, the United Kingdom, the United States, Canada, Spain, Japan and South Korea. Some were also processed using commercial clouds (Amazon Web Services, Microsoft Azure and Seven Bridges). Since PCAWG began, several other international genomics projects have turned to the cloud (see ‘Cashing in on clouds’) including The Human Cell Atlas, an international project to create a reference map of all human cells1, and the European Open Science Cloud, which is for researchers and professionals in science, technology, the humanities and social sciences2. Cloud services have been transformative in enabling large-scale genomic analysis. Conventionally, any research team wanting to analyse an aggregate data set collected by a consortium would first have to seek authorization from each project partner’s research ethics or data-access compliance office. It would then have to download the data from each subproject over the Internet or — more likely — have the hard drives containing the data sent by post. In the case of the Pan-Cancer Analysis of Whole Genomes, which comprises 800 terabytes of raw data, investigators were able to save months, and thousands of dollars, by immediately accessing the data they needed, and experimenting with and customizing the analytical tools developed by the community. They could also obtain authorization to access most of the data from one place — the International Cancer Genome Consortium’s Data Access Compliance Office. This trend is likely to continue. Cloud services are becoming cheaper and more readily available, and researchers are increasingly reaping the benefits of sharing ever-larger amounts of genomic data with international colleagues3 (see ‘Open data’). Over the past three decades, geneticists around the world have been sharing more and more data. Last year, more than 83,000 researchers from 146 countries downloaded 6.7 petabytes of (mainly human) DNA data from the European Molecular Biology Laboratory’s European Bioinformatics Institute. This hosts many biological data sets and makes them accessible worldwide. That is equivalent to around 230 billion whole human genomes. Such sharing of genomic data will only increase as more data become available. By 2025, more than 60 million patients worldwide are expected to have had their genome or exome (protein-coding regions)sequenced as part of routine health care16 — potentially providing a formidable resource for researchers. Yet cloud services bring fresh challenges with respect to the protection of participants’ data — especially given that national governments, law enforcement and private corporations are increasingly showing interest in accessing them. Canadian border authorities, for example, are choosing which country to deport migrants to on the basis of DNA test results from consumer genomic services4. Organizations such as the Cancer Genome Collaboratory can persist only for as long as they are funded. Even in the case of Amazon and other major tech companies, service outages caused by technical problems, changes to the company’s terms of service or even sudden closure of the company could block researchers’ access to data at any time. Also, it is often unclear to what extent researchers using cloud services can ensure that their data are not disclosed to third parties, such as those conducting abusive state-level surveillance. Nor is it clear what steps must be taken to protect the data against such breaches of confidentiality. In the case of PCAWG, the ICGC’s Data Access Compliance Office helped to guard against some of these issues. Anyone wanting to use PCAWG data entered into a contract with the project’s data-access committee; they had to confirm, for instance, that they would not try to re-identify patients or volunteers once these people’s data had been stripped of personal information. No breach of donor confidentiality is known to have occurred. But even when researchers request the data from an associated data-access committee (as in the case of PCAWG and elsewhere), numerous issues remain unresolved. It is unclear, for instance, what vetting should occur before researchers get access to sensitive genomic data, or what checks should be made before genomic data are shared internationally. Even those involved in PCAWG could not establish a truly international cloud because of restrictions on the transfer of data across borders (caused, in this case, by European regulators having concerns about genomic data from Europeans being held in the United States). The US component of the project (The Cancer Genome Atlas; TCGA), which contributed one-third of all PCAWG samples, was made available to researchers through the University of Chicago’s Protected Data Cloud. Researchers wanting to obtain those data had to abide by an access agreement that was largely compatible with that provided by the ICGC’s Data Access Compliance Office. Ultimately, however, TCGA remained conceptually split off from the rest of the project, because researchers had to follow a different access procedure and to combine the two data sets themselves. Server racks in a centre in Berlin.Credit: Thomas Trutschel/Photothek/Getty Genomics researchers urgently need clear data-sharing rules that are harmonized across jurisdictions. An international code of conduct could help investigators to overcome some of the current hurdles, as well as others that might arise as legislation on data protection evolves. Such a code could outline the steps researchers must take to comply with the European Union’s General Data Protection Regulation (GDPR), implemented in 2018, and the US Health Insurance Portability and Accountability Act, among other laws. In fact, the GDPR explicitly encourages the development of sector-specific data-protection codes in its Article 40. And last June, the European Data Protection Board (an independent body tasked with issuing guidance on the GDPR and encouraging the drawing-up of codes of conduct), issued guidelines on the submission, approval and monitoring of such codes for data processing. It also promised further guidance on the use of codes as a potential way to facilitate the transfer of data across borders (see go.nature.com/322nkkv). A European biobanking research infrastructure, known as BBMRI-ERIC, announced in 2017 that it would develop an EU-wide Code of Conduct on Health-Related Data, to submit to the European Commission (see go.nature.com/2j3ihce). When completed, and if approved, such a European code could be beneficial. Meanwhile, we call on the genomics research community to prioritize the establishment of an international code of conduct that lays out how existing ethical and legal obligations can be satisfied in relation to international genomic clouds. At least five aspects must be considered. Identifiability. Despite the problems with it5, de-identification, in which health data are stripped of any information that could be used to identify the participant (such as name, social security number, address) has long been hailed as a way to protect people’s privacy in research6. Yet because of conflicting terminology and gaps in understanding, researchers rarely know what standard they must meet for their data to be properly anonymized or ‘pseudonymized’ (in which a code enables individuals to be re-identified)7. What’s more, laws are difficult to enforce in practice because it is often unclear how breaches of confidentiality occurred, or which organization or researcher was responsible8,9. Data-protection laws, such as the GDPR or the California Consumer Privacy Act, invariably require the identifiability of data to be analysed on a case-by-case basis, in part because the technological tools enabling identification are constantly changing. Even though it is difficult to lay out hard and fast rules in advance, a code could provide some guidance on how to evaluate when it is reasonable to deposit genomic (and health data more broadly) in open-access repositories. This might involve considering, say, whether a set of genomic variants is somatic or present in the germline and so inherited. (Researchers have shown that it is possible to identify an individual using only a few germline variants10; no one has yet been able to identify someone on the basis of somatic tumour variants.) Broad consent. The GDPR explicitly recognizes an exception to ‘specific consent’, meaning the consent people give for their data to be used in a specific research project. This is to allow participants’ data to be used for certain areas of scientific research, in keeping with recognized ethical standards11. Guidance is needed on what researchers must do to meet the requirements for broad consent. Furthermore, how should they keep patients and volunteers informed about how their data are ultimately used? Return of individual findings, portability and access. How to safeguard participants’ right to move their data around – by giving them their data in a machine-readable format, rather than as a printed PDF, for example — should be clarified. The code could also lay out what steps are necessary for responsible communication of health data to a patient or volunteer12,13. Should people who are being informed about the identification of genomic variants of malignant or unknown significance be offered genetic counselling, for instance? Withdrawal. Researchers need guidance on how they can meet participants’ right to withdraw from research. The GDPR requires that those entrusted with people’s data keep records of third parties to whom they have disclosed those data. And when consent is revoked, they must notify the third parties. Yet all sorts of questions remain, such as whether analyses on aggregate data should be revised with the participants’ data removed, and so on. Compelled disclosure. A code of conduct could provide researchers with guidance on how to deal with government requests for personal data, including what legal protections they can appeal to. In the United States, for example, the National Institutes of Health’s Certificates of Confidentiality are designed to shield researchers from such requests14. The achievements of PCAWG in relation to the sharing and handling of genomic data augur well for the development of an international code that researchers everywhere can refer to. Genomic research consortia, public and private funding bodies, and those working on existing regional codes (such as the one in Europe) might begin the process of building it. A first step would be to convene a meeting to determine the topics the code would touch on, the best way to consult research participants about their needs and a decision-making process that will allow the text to be finalized in a timely way. If genomics researchers are instead left in the dark about how to properly address data protection and sharing, they could either be excessively cautious and fail to share as consents allow, or fail to provide participants with appropriate protection15. In other words, further regulatory uncertainty risks stalling new genomic analyses and undermining people’s faith in scientific collaboration for the public good. </body>
<date id = '83'>05 February 2020</date>
<url id = '84'>https://nature.com/articles/d41586-020-00274-3</url>
<title id = '84'>Build models that identify and mitigate the causes of discrimination.</title>
<body id = '84'>An algorithm deployed across the United States is now known to underestimate the health needs of black patients1. The algorithm uses health-care costs as a proxy for health needs. But black patients’ health-care costs have historically been lower because systemic racism has impeded their access to treatment — not because they are healthier. This example illustrates how machine learning and artificial intelligence can maintain and amplify inequity. Most algorithms exploit crude correlations in data. Yet these correlations are often by-products of more salient social relationships (in the health-care example, treatment that is inaccessible is, by definition, cheaper), or chance occurrences that will not replicate. To identify and mitigate discriminatory relationships in data, we need models that capture or account for the causal pathways that give rise to them. Here we outline what is required to build models that would allow us to explore ethical issues underlying seemingly objective analyses. Only by unearthing the true causes of discrimination can we build algorithms that correct for these. Models that account for causal pathways have three advantages. These ‘causal models’ are: tailored to the data at hand; allow us to account for quantities that aren’t observed; and address shortcomings in current concepts of fairness (see ‘Fairness four ways’). A flurry of work has conceptualized fairness. Here are some of the most popular, and ways in which causal models offer alternatives. Fairness through unawareness12. This method works by removing any data that are considered prima facie to be unfair. For example, for an algorithm used by judges making parole decisions, fairness through unawareness could dictate that data on ethnic origin should be removed when training this algorithm, whereas data on the number of previous offences can be used. But most data are biased. For instance, number of previous offences can bear the stamp of historical racial bias in policing, as can the use of plea bargaining (pleading guilty being more likely to reduce a sentence than arguing innocence)13. This can leave researchers with a hard choice: either remove all data or keep biased data. Alternatively, causal models can directly quantify how data are biased. Demographic parity14. A predictive algorithm satisfies demographic parity if, on average, it gives the same predictions to different groups. For example, a university-admissions algorithm would satisfy demographic parity for gender if 50% of its offers went to women and 50% to men. It is currently more common in law to relax demographic parity so that predictions aren’t necessarily equal, but are not too imbalanced. Specifically, the US Equal Employment Opportunity Commission states that fair employment should satisfy the 80% rule: the acceptance rate for any group should be no less than 80% of that of the highest-accepted group. For instance, if 25% of women were offered jobs, and this is the highest acceptance rate, then at least 20% of men must be offered jobs4. One criticism of demographic parity is that it might not make sense to use it in certain settings, such as a fair arrest rate for violent crimes (men are significantly more likely to commit acts of violence)15. Instead, one could require that counterfactual versions of the same individual should get the same prediction4. Equality of opportunity16. This is the principle of giving the same beneficial predictions to individuals in each group. Consider a predictive algorithm that grants loans only to individuals who have paid back previous loans. It satisfies ‘disability-based equality of opportunity’ if it grants loans to the same percentage of individuals who both pay back and have a disability as it does to those who pay back and who do not have a disability. However, being able to pay back a loan in the first place can be affected by bias: discriminatory employers might be less likely to hire a person with a disability, which can make it harder for that person to pay back a loan. This societal unfairness is not captured by equality of opportunity. A causal model could be used to quantify the bias and estimate an unbiased version of loan repayment. Individual fairness17. This concept states that similar individuals should get similar predictions. If two people are alike except for their sexual orientation, say, an algorithm that displays job advertisements should display the same jobs to both. The main issue with this concept is how to define similar. In this example, training data will probably have been distorted by the fact that one in five individuals from sexual or gender minorities report discrimination against them in hiring, promotions and pay18. Thus similarity is hard to define, which makes individual fairness hard to use in practice. In causal modelling, counterfactuals offer a natural way to define a similar individual. A causal model2 represents how data are generated, and how variables might change in response to interventions. This can be shown as a graph in which each variable is a node and arrows represent the causal connections between them. Take, for example, a data set about who gets a visa to work in a country. There is information about the country each person comes from, the work they do, their religion and whether or not they obtained a visa. This model says that the country of origin directly influences a person’s religion and whether they obtain a visa; so, too, do religion and type of work. Having a causal model allows us to address questions related to ethics, such as does religion influence the visa process? But because many different causal models could have led to a particular observed data set, it is not generally possible to identify the right causal model from that data set alone3. For example, without any extra assumptions, data generated from the causal graph described here could seem identical to those from a graph in which religion is no longer linked to visa granting. A modeller must therefore also leverage experiments and expert knowledge, and probe assumptions.   Experiments can help in identifying factors that affect fairness. For example, a modeller wishing to explore whether ethnicity would affect treatment recommendations made online by health-care professionals could create two patient profiles that differ only in some respect that relates to ethnicity. For instance, one profile could have a name common to Americans of Chinese descent, and the other a name common to Americans of African descent. If the treatment recommendations are the same, then names can be ruled out as a source of bias, and the model can be stress-tested in another way. Few aspects of a deep, multifaceted concept can be tested as easily as changing a name. This means that experimental evidence can underestimate the effects of discrimination. Integration of expert knowledge, particularly from the social sciences and including qualitative methods, can help to overcome such limitations. This knowledge can be used to, for example, inform the modeller of variables that might be influential but unobserved (lighter circles in the causal models, below), or to determine where to put arrows. Assumptions about unobserved variables that might alter the predictions of a model need to be clearly stated. This is particularly important when experiments cannot be run or more detailed expert knowledge is not available. For example, if ‘health-care access’ is not observed in a model attempting to predict ‘health need’, then it is crucial to identify any potential impacts it might have on ‘health costs’ as well as how it is affected by ‘ethnicity’. This need for context and metadata makes causal models harder to build than non-causal ones. It can also make them a more powerful way to explore ethical questions. Causal models can test the fairness of predictive algorithms in three ways. Counterfactuals. A causal model allows us to ask and answer questions such as ‘Had the past been different, would the present or future have changed?’ In the visa example, algorithmic biases could be smoked out by tweaking parts of the model to explore, for instance: ‘Had individual X been Christian, would this algorithm have granted them a visa?’ A researcher could then identify what pieces of information an algorithm could use to achieve counterfactual fairness4: the algorithm’s output would not change regardless of the individual’s religion. For example, if the algorithm used just work and not country of origin or religion, it would satisfy counterfactual fairness.  Sensitivity. In many settings, unknowns alter knowns — data we can observe are influenced by data we cannot. Consider a causal model for a trial setting. This model shows how two independent sets of unobserved quantities, structural racism and jury racism, can unfairly lead to a guilty verdict. Although researchers often cannot precisely identify unobserved variables, they can reason about how sensitive a model is to them. For instance, they can explore how sensitive our estimate of the causal link between legal representation and guilty verdict is to different levels of jury racism. Simulations of the worst-case bias scenarios (that is, when jury racism is highest) can then be used to alter jury selection to minimize the bias.  Impacts. Data-driven decisions can have long-term consequences and spillover effects. These effects might not be obvious, especially in the standard machine-learning paradigm of predicting one short-term outcome. But carefully designed causal models can help researchers to use ‘interventions’ to probe the ripple effects of decisions far into the future5,6. For instance, the models can help regulatory agencies to understand how changing a scholarship algorithm influences who is accepted into law school. In this example, a single parent might need a scholarship so that they can reduce the hours they need to spend at a job, leaving them more time for study. That boosts their grades and therefore influences their chances of being admitted to law school. This complex chain can be explored using causal models.  Causal models are powerful tools, but they must be used appropriately. They are only models, and will thus fail to capture important aspects of the real world. Here we offer some guidelines on using them wisely. Collaborate across fields. Researchers in statistics and machine learning need to know more about the causes of unfairness in society. They should work closely with those in disciplines such as law, social sciences and the humanities. This will help them to incorporate the context of the data used to train the algorithms. For example, scholars should meet at interdisciplinary workshops and conferences. One such is next year’s Association for Computing and Machinery (ACM) conference on Fairness, Accountability and Transparency to derive a set of causal models for setting bail price and for immigration decisions. A great example of such collaborations is one between information scientist Solon Barocas at Cornell University in Ithaca, New York, and attorney Andrew Selbst at the Data & Society Research Institute in New York City. They described how current law is unable to deal with algorithmic bias7. Partly in response to this work, machine-learning researchers have launched a large subfield, known as algorithmic fairness, that looks into ways of removing bias from data. And we and other researchers now use causal models to quantify discrimination due to data.   Partner with stakeholders. Predictive algorithms should be developed with people they are likely to affect. Stakeholders are best placed to provide input on difficult ethical questions and historical context. One example is the work by statistician Kristian Lum at the Human Rights Data Analysis Group in San Francisco, California, which investigates criminal-justice algorithms8. Such algorithms decide whether to detain or release arrested individuals and how high to set their bail, yet they are known to be biased. Lum has invited people affected by such decisions to speak at academic conferences attended by people who research these algorithms. This has led to closer collaboration, including the tutorial ‘Understanding the context and consequences of pre-trial detention’ presented at the 2018 ACM conference on Fairness, Accountability and Transparency in New York. So far, most stakeholder work has focused on criminal justice. Another setting that would benefit from it is mortgage lending. We propose that a rotating, interdisciplinary panel of stakeholders investigates the impacts of algorithmic decisions, for example as part of a new international regulatory institute. Make the workforce equitable. Women and people from minority groups are under-represented in the fields of statistics and machine learning. This directly contributes to the creation of unfair algorithms. For example, if facial detection software struggles to detect faces of black people9, it is likely the algorithm was trained largely on data representing white people. Initiatives such as Black in AI (go.nature.com/38pbcaa) or Women in Machine Learning (go.nature.com/2s5km5g) are positive steps. And we can go further. Causal models can themselves help to address the field’s ‘pipeline problem’ by identifying where unfairness enters the process and which interventions can increase the participation of under-represented groups without shifting the burden to extra work for role models in those groups. Academic institutions should critically evaluate and use these models for fairer admissions in fields related to artificial intelligence.   Identify when algorithms are inappropriate. Statistics and machine learning are not all-powerful. Some problems should not be solved by expanding data-gathering capabilities and automating decisions. For example, a more accurate model for predictive policing won’t solve many of the ethical concerns related to the criminal legal system. In fact, these methods can mask structural issues, including the fact that many neighbourhoods are policed by people who do not live in them10. This disconnect means that police officers might not be invested in the community they police or the people they arrest. There are red flags when demographics, such as ethnic origin, influence nearly every piece of information in a causal graph, or when previous attempts to address a bias failed because people strategically changed behaviours in response. In these cases, an algorithmic solution would paper over a system that needs fundamental change. Foment criticism. A vibrant culture of feedback is essential. Researchers need to continually question their models, evaluation techniques and assumptions. Useful as causal models are, they should be scrutinized intensely: bad models can make discrimination worse11. At the very least, a scientist should check whether a model has the right data to make causal claims, and how much these claims would change when the assumptions are relaxed. Algorithms are increasingly used to make potentially life-changing decisions about people. By using causal models to formalize our understanding of discrimination, we must build these algorithms to respect the ethical standards required of human decision makers. </body>
<date id = '84'>04 February 2020</date>
<url id = '85'>https://nature.com/articles/d41586-020-00177-3</url>
<title id = '85'>Stop using the worst-case scenario for climate warming as the most likely outcome — more-realistic baselines make for better policy.</title>
<body id = '85'>More than a decade ago, climate scientists and energy modellers made a choice about how to describe the effects of emissions on Earth’s future climate. That choice has had unintended consequences which today are hotly debated. With the Sixth Assessment Report (AR6) from the Intergovernmental Panel on Climate Change (IPCC) moving into its final stages in 2020, there is now a rare opportunity to reboot. In the lead-up to the 2014 IPCC Fifth Assessment Report (AR5), researchers developed four scenarios for what might happen to greenhouse-gas emissions and climate warming by 2100. They gave these scenarios a catchy title: Representative Concentration Pathways (RCPs)1. One describes a world in which global warming is kept well below 2 °C relative to pre-industrial temperatures (as nations later pledged to do under the Paris climate agreement in 2015); it is called RCP2.6. Another paints a dystopian future that is fossil-fuel intensive and excludes any climate mitigation policies, leading to nearly 5 °C of warming by the end of the century2,3. That one is named RCP8.5. RCP8.5 was intended to explore an unlikely high-risk future2. But it has been widely used by some experts, policymakers and the media as something else entirely: as a likely ‘business as usual’ outcome. A sizeable portion of the literature on climate impacts refers to RCP8.5 as business as usual, implying that it is probable in the absence of stringent climate mitigation. The media then often amplifies this message, sometimes without communicating the nuances. This results in further confusion regarding probable emissions outcomes, because many climate researchers are not familiar with the details of these scenarios in the energy-modelling literature. This is particularly problematic when the worst-case scenario is contrasted with the most optimistic one, especially in high-profile scholarly work. This includes studies by the IPCC, such as AR5 and last year’s special report on the impact of climate change on the ocean and cryosphere4. The focus becomes the extremes, rather than the multitude of more likely pathways in between. Happily — and that’s a word we climatologists rarely get to use — the world imagined in RCP8.5 is one that, in our view, becomes increasingly implausible with every passing year5. Emission pathways to get to RCP8.5 generally require an unprecedented fivefold increase in coal use by the end of the century, an amount larger than some estimates of recoverable coal reserves6. It is thought that global coal use peaked in 2013, and although increases are still possible, many energy forecasts expect it to flatline over the next few decades7. Furthermore, the falling cost of clean energy sources is a trend that is unlikely to reverse, even in the absence of new climate policies7. Assessment of current policies suggests that the world is on course for around 3 °C of warming above pre-industrial levels by the end of the century — still a catastrophic outcome, but a long way from 5 °C7,8. We cannot settle for 3 °C; nor should we dismiss progress. Some researchers argue that RCP8.5 could be more likely than was originally proposed. This is because some important feedback effects — such as the release of greenhouse gases from thawing permafrost9,10 — might be much larger than has been estimated by current climate models. These researchers point out that current emissions are in line with such a worst-case scenario11. Yet, in our view, reports of emissions over the past decade suggest that they are actually closer to those in the median scenarios7. We contend that these critics are looking at the extremes and assuming that all the dice are loaded with the worst outcomes. Exposed permafrost, such as on the sides of this crevasse in Alaska, releases greenhouse gases as it thaws.Credit: Brandt Meixell/USGS/Smith Collection/Gado/Getty Asking ‘what’s the worst that could happen?’ is a helpful exercise. It flags potential risks that emerge only at the extremes. RCP8.5 was a useful way to benchmark climate models over an extended period of time, by keeping future scenarios consistent. Perhaps it is for these reasons that the climate-modelling community suggested RCP8.5 “should be considered the highest priority”12. We must all — from physical scientists and climate-impact modellers to communicators and policymakers — stop presenting the worst-case scenario as the most likely one. Overstating the likelihood of extreme climate impacts can make mitigation seem harder than it actually is. This could lead to defeatism, because the problem is perceived as being out of control and unsolvable. Pressingly, it might result in poor planning, whereas a more realistic range of baseline scenarios will strengthen the assessment of climate risk. This admission does not make climate action less urgent. The need to limit warming to 1.5 °C, as made clear in the IPCC’s 2018 special report13, does not depend on having a 5 °C counterpoint. The plethora of future emissions scenarios poses a challenge to users of climate data — from policymakers to investors14. More than 1,200 mitigation scenarios were assessed in AR5 in 2014. Another 400 scenarios were used in the IPCC’s 2018 special report on 1.5 °C of warming13. Most of these assume a baseline with no climate policy across a range of socio-economic developments. In our experience of working with scenario users, this proliferation leads to more confusion than clarity, particularly in the absence of any guidance on the relative likelihood of each scenario.   Other organizations present relatively few scenarios — the International Energy Agency (IEA), for example, now has just three main ones. Its Current Policies Scenario indicates what could happen to emissions with things as they stand. The Stated Policies Scenario includes current policy intentions and targets. The IEA’s Sustainable Development Scenario reflects emissions in a world that is already aligned with the goals set in Paris7. The United Nations Environment Programme’s Emissions Gap Report takes a similar approach, comparing countries’ emissions-reduction pledges with global pathways that limit warming to well below 2 °C15. These influential agencies do not focus on worst-case outcomes. They plot the gulf between where the world is heading and where it has agreed it should go. For those making real-life decisions, the choice of scenario becomes important14,16. Emphasizing ways of adapting to an extreme RCP8.5 scenario with around 5 °C warming in 2100 is out of step with the requirement to build resilience and reduce vulnerabilities in the near-term. Most users of climate scenarios care more about the world as it is now, rather than what might have been had global emissions not slowed over the past decade7. Users focused on mitigation are keen to capitalize on emerging opportunities such as cheap renewables, or to avoid overinvesting in stranded assets in dying industries. For example, they want to know whether the rapid cost declines in renewables might make investments in fossil fuels high risk. A RCP8.5 baseline renders these applications useless, because it implies that recent climate policies and technological progress are halted or even reversed. For policymakers, mitigation policies that depend on the assumptions underlying high-emission baseline scenarios such as RCP8.5 will seem exorbitant, because they do not incorporate the plummeting costs of many low-carbon technologies over the past decade. The marginal investments required to move from 3 °C of warming to well below 2 °C (the main Paris goal) will be much less than moving from 5 °C to well below 2 °C. A narrative of progress and opportunity can make the Paris targets seem feasible, rather than seemingly impossible. Sources: Historical data: Global Carbon Budget (2019); SSP data: ref. 19/J. Rogelj et al. Nature Clim. Change 8, 325–332 (2018)/SSP Database (v2); IEA data: Ref. 7 Those who are tasked with taking climate action on the basis of information from model scenarios are increasingly calling for a more risk-based approach to help with adaptation and mitigation14. This approach accounts for the relative likelihood of different outcomes. Controversially, it requires researchers to assign probabilities to scenarios16. Critics don’t want to do this, because many see it as an arbitrary process. But when specialists refuse to assign probabilities, users often do so themselves. Most do so poorly because they do not have a deep understanding of the assumptions that underpin these scenarios. Initially, the probabilities do not need to be elaborate, and could even just identify the most likely scenario resulting from current energy-system trends and policies. Now, scenarios are selected on the basis of their climate outcomes in 2100, not their likelihoods. More complex probabilistic approaches would require modellers to work differently17. For example, they would need to forge new alliances with those in the social sciences18 and involve policymakers, investors and industry14. This will require years of work. Meanwhile, three steps should be taken over the next year in the lead-up to AR6, to set the climate community on the right road. The latest generation of climate models has just come out, and many researchers are now selecting which future emissions scenarios to use in studies. First, the new generation of scenarios called the Shared Socioeconomic Pathways (SSPs; see ‘Parting of the pathways’) has a much more nuanced approach to baselines, and IPCC authors can highlight a range of outcomes in a world with no new policies19 (see also Nature Clim. Change 9, 727; 2019). The space between high-end and low-end scenarios should be more deeply explored in AR6, so that the climate impacts we are likely to experience can be communicated more clearly20. For example, according to many studies, we are heading for a 3 °C world. Therefore, it would be prudent to clearly outline the climate impacts for 3 °C in addition to those for 5 °C. The 2021–22 Sixth Assessment Report from the Intergovernmental Panel on Climate Change (IPCC) will compare different types of trajectory from those weighed in its 2014 report. The Representative Concentration Pathways (RCPs) are a set of four possible climate scenarios for the end of the century1. The RCPs were used extensively in the 2014 IPCC Fifth Assessment Report, but lack any consistent set of socio-economic assumptions driving future emissions and are simply intended to reflect different potential climate outcomes3. They include RCP2.6, RCP4.5, RCP6.0 and RCP8.5, with the number reflecting the additional radiative forcing in 2100, relative to pre-industrial times. Radiative forcing (in watts per square metre) measures the combined effect of greenhouse-gas emissions and other factors (such as atmospheric aerosol levels) on climate warming. Current radiative forcing relative to pre-industrial levels is around 2.5 watts per square metre. The Shared Socioeconomic Pathways (SSPs) are five socio-economic and technological trajectories that the world could follow this century19. Each has a baseline in which no climate policies are enacted after 2010 — resulting in between 3 °C and 5 °C of warming above pre-industrial levels by 2100. In addition, the SSPs can be linked to climate policies to generate different outcomes for the end of the century (analogous to RCPs), with radiative forcing of 1.9, 2.6, 3.4, 4.5, 6.0, 7.0 or 8.5 watts per square metre in 2100. A subset of SSP models has been selected for the 2021–22 IPCC report12, and will function in a similar way to the RCPs in its 2014 report. Second, scientists should recognize that different users need different tools. In the context of AR6, this could mean that the various working groups (focusing on climate science, impacts and mitigation) highlight different scenarios in their analyses and communications. The final AR6 synthesis could then integrate the different risk perspectives. Finally, we suggest that climate-impact studies using models developed for AR6 should include scenarios that reflect more-plausible outcomes, such as SSP2-4.5, SSP4-6.0 and SSP3-7.0 (see ’Possible futures’). When RCP8.5 or its successor SSP5-8.5 are deployed, they should be clearly labelled as unlikely worst cases rather than as business as usual. </body>
<date id = '85'>29 January 2020</date>
<url id = '86'>https://nature.com/articles/d41586-020-00112-6</url>
<title id = '86'>To the world leaders mustering in Davos: set your minds to reaching net-zero emissions, and you can forge the future we need.</title>
<body id = '86'>As political leaders, industry executives and celebrities gather this week for their yearly networking meeting in Davos, Switzerland, top of their agenda is the need to halve global carbon emissions by 2030. Of the many barriers to achieving this goal, the greatest is mindset. I had to learn this a decade ago when I was appointed to lead the international climate-change negotiations that resulted in the 2015 Paris agreement: ultimately, 195 nations pledged to reduce emissions and alter their economies to protect our planet. They also agreed to increase their efforts towards net-zero emissions substantially every five years. That makes 2020 a crucial year. We cannot afford for governments to let that key commitment slip. The Paris agreement was a breakthrough after a devastating collapse in Copenhagen in 2009, when years of preparation and two weeks of excruciating around-the-clock negotiations produced only a weak, legally irrelevant accord. Copenhagen was a free-for-all of political frustration, outrage and disagreement — with the global north and global south set against each other. Last month’s United Nations climate meeting in Madrid left many of us similarly bereft. That makes the lesson of how we got from Copenhagen to Paris all the more relevant. It started with my making a big mistake in the summer of 2010, at a press conference with 40 journalists in a windowless room at the Maritim Hotel in Bonn, Germany. When asked whether a global agreement on climate change would ever be possible, I blurted out, without thinking, what most already thought: “Not in my lifetime.” That’s how close I came to making failure a self-fulfilling prophecy.   I immediately realized that, before we could consider the political, technical and legal parameters of an eventual agreement, I had to dedicate myself to changing the mood: there could be no victory without optimism. I decided to set a clear intention: even if we did not know precisely how, a global deal would emerge, simply because it was necessary. It was that contagious frame of mind that led to effective decision-making, despite the enormous complexities under which we were operating. When the Paris agreement was achieved, the optimism that people felt about the future was palpable — but, in fact, optimism had been the primary input. Since then, science has become clearer about the threats of climate change: now, even our children know that business as usual will lead to destroyed infrastructure, devastating loss of plants and animals, and millions of people struggling in regions made uninhabitable from rising temperatures and lack of fresh water. What is much less clear is what life will look like in those places where we do what is necessary to limit warming to 1.5 °C, as stipulated by the Paris agreement. To get to what we achieved in Paris, we moved away from confrontational blaming-and-shaming to appreciating shared opportunities. Now, we must picture, say, cities full of green spaces pulling carbon dioxide from the atmosphere; widespread public transport; thriving wildernesses; rural economies rebooted for sustainable agriculture; and jobs in renewable-energy projects. Optimism is about acknowledging difficulties — and losses — yet still designing a better future. An excellent example is the European Union’s proposed European Green Deal, announced in December 2019. This explicitly reframes an urgent challenge as a unique opportunity to create a “resource-efficient and competitive economy” that will generate jobs, purify air and mobilize industry, agriculture and other sectors to deliver net-zero emissions by 2050. My own country, Costa Rica, has already launched an economy-wide plan to ‘decarbonize’ by 2050. This ambitious plan, the first of its kind when it was announced last February, will expand forests and promote electric taxis and public buses. It is based on respect for human rights and gender equity, and clearly recognizes the opportunity for decarbonization to revitalize the economy. Costa Rica has launched a decarbonization plan that will expand the country’s forest cover.Credit: Margarita Almpanezou/Getty Most executives already understand that they need to contribute to climate stabilization just to ensure that their businesses have a future. The number of companies setting science-based targets in line with a 1.5 °C trajectory doubled between September and December last year. Similarly, the combined assets managed by the Net-Zero Asset Owner Alliance — a group of investors aligning their portfolios with a 1.5 °C future — had surged from US$2.4 trillion to $4 trillion within two months of its launch in September 2019. Leaders in the oil and gas industries have told me privately that shareholder and public pressure, plus questions from their own children, have prompted them to shift their practices. Despite this, I posit that most people, including many of those attending the Davos meeting, still harbour the view that it is impossible to truly transform our economy in one decade. We cannot afford such fatalism. Swift change has happened before, and without being driven by planetary necessity: the global Internet is just 30 years old. If we can see where we are going — a future in which humanity does what is necessary to preserve the planet as we know and love it — we will take faster, surer steps to get there. That visualization is all the more important because how we are going to get to this future will feel unfamiliar. The transition of technologies and systems in music and information makes sense only because we have seen vinyl records yield to streaming services and paper superseded by mobile multimedia. We must be ready to shape the necessary transition for energy, transport and more. And we must understand that this transition will be driven collectively. The global economy is a huge, complex system. As I learnt during my stewardship of the Paris agreement, if you do not control the complex landscape of a challenge (and you rarely do), the most powerful thing you can do is to change how you behave in that landscape, using yourself as a catalyst for overall change. Imagine a person who wants to run a marathon and then concentrates on the fact that they can’t yet even run a mile: they begin to close the space of possibility. But, if that person adopts a different mindset, commits to a training schedule and visualizes passing the finish line, their goal is much more likely to be achieved. To all the people gathering in Davos, and all those watching from the outside, I urge you to move firmly into a state of stubborn optimism. The Anthropocene, the proposed geological age we now live in, does not need to go down in history as the age characterized by human-induced destruction. It can be the time when we rewrite our expected future for a better one: we still hold the pen. We must conceive of success and take immediate steps towards it. </body>
<date id = '86'>20 January 2020</date>
<url id = '87'>https://nature.com/articles/d41586-020-00032-5</url>
<title id = '87'>To understand how people use digital media, researchers need to move beyond screen time and capture everything we do and see on our screens.</title>
<body id = '87'>There has never been more anxiety about the effects of our love of screens — which now bombard us with social-media updates, news (real and fake), advertising and blue-spectrum light that could disrupt our sleep. Concerns are growing about impacts on mental and physical health, education, relationships, even on politics and democracy. Just last year, the World Health Organization issued new guidelines about limiting children’s screen time; the US Congress investigated the influence of social media on political bias and voting; and California introduced a law (Assembly Bill 272) that allows schools to restrict pupils’ use of smartphones. All the concerns expressed and actions taken, including by scientists, legislators, medical and public-health professionals and advocacy groups, are based on the assumption that digital media — in particular, social media — have powerful and invariably negative effects on human behaviour. Yet so far, it has been a challenge for researchers to demonstrate empirically what seems obvious experientially. Conversely, it has also been hard for them to demonstrate that such concerns are misplaced. A major limitation of the thousands of studies, carried out over the past decade or so, of the effects of digital media is that they do not analyse the types of data that could reveal exactly what people are seeing and doing on their screens — especially in relation to the problems that doctors, legislators and parents worry most about. Most use self-reports of ‘screen time’. These are people’s own estimates of the time they spend engaging with screens or with platforms that are categorized as ‘smartphone’, ’television’, ‘social media’, ‘political news’ or ‘entertainment media’. Yet today’s media experiences defy such simplistic characterization: the range of content has become too broad, patterns of consumption too fragmented1, information diets too idiosyncratic2, experiences too interactive and devices too mobile. Policies and advice must be informed by accurate assessments of media use. These should involve moment-by-moment capture of what people are doing and when, and machine analysis of the content on their screens and the order in which it appears. Technology now allows researchers to record digital life in exquisite detail. And thanks to shifting norms around data sharing, and the accumulation of experience and tools in fields such as genomics, it is becoming easier to collect data while meeting expectations and legal requirements around data security and personal privacy. We call for a Human Screenome Project — a collective effort to produce and analyse recordings of everything people see and do on their screens.   According to a 2019 systematic review and meta-analysis3, over the past 12 years, 226 studies have examined how media use is related to psychological well-being. These studies consider mental-health problems such as anxiety, depression and thoughts of suicide, as well as degrees of loneliness, life satisfaction and social integration. The meta-analysis found almost no systematic relationship between people’s levels of exposure to digital media and their well-being. But almost all of these 226 studies used responses to interviews or questionnaires about how long people had spent on social media, say, the previous day. The expectation is that if someone reports being on Facebook a lot, then somewhere among all those hours of screen time are the ingredients that influence well-being, for better or worse. But ‘time spent on Facebook’ could involve finding out what your friends are doing, attending a business meeting, shopping, fundraising, reading a news article, bullying, even stalking someone. These are vastly different activities that are likely to have very different effects on a person’s health and behaviour. Another problem is that people are unlikely to recollect exactly when they did what4,5. Recent studies that compared survey responses with computer logs of behaviour indicate that people both under- and over-report media exposure — often by as much as several hours per day6–8. In today’s complex media environment, survey questions about the past month or even the past day might be almost useless. How many times did you look at your phone yesterday? The US National Institutes of Health (NIH) is currently spending US$300 million on a vast neuroimaging and child-development study, eventually involving more than 10,000 children aged 9 and 10. Part of this investigates whether media use influences brain and cognitive development. To indicate screen use, participants simply pick from a list of five standard time ranges, giving separate answers for each media category and for weekdays and weekends. (The first report about media use from this study, published last year, showed a small or no relationship between media exposure and brain characteristics or cognitive performance in computer-based tasks9.) Instead, researchers need to observe in exquisite detail all the media that people engage with, the platforms they use and the content they see and create. How do they switch between platforms and between content within those? How do the moments of engagement with various types of media interact and evolve? In other words, academics need a multidimensional map of digital life. To illustrate, people tend to use their laptops and smartphones in bursts of, on average, 10–20 seconds10. Metrics that quantify the transitions people make between media segments within a session, and between media and the rest of life, would provide more temporally refined representations of actual use patterns. A session begins when the screen lights up and ends when it goes dark, and might last less than a second if it entails checking the time. Or it could start with a person responding to their friend’s post on Facebook, and end an hour later when they click on a link to read an article about politics.   Measures of media use must also take account of the scattering of content. Today’s devices allow digital content that used to be experienced as a whole (such as a film, news story or personal conversation) to be atomized, and the pieces viewed across several sessions, hours or days. We need measures that separate media use into content categories (political news, relationships, health information, work productivity and so on) — or, even better, weave dissimilar content into sequences that might not make sense to others but are meaningful for the user. To try to capture more of the complexity, some researchers have begun to use logging software. This was developed predominantly to provide marketers with information on what websites people are viewing, where people are located, or the time they spend using various applications. Although these data can provide more-detailed and -accurate pictures than self-reports of total screen time, they don’t reveal exactly what people are seeing and doing at any given moment. To record the moment-by-moment changes on a person’s screen2,11, we have built a platform called Screenomics. The software records, encrypts and transmits screenshots automatically and unobtrusively every 5 seconds, whenever a device is turned on (see go.nature.com/2fsy2j2). When it is deployed on several devices at once, the screenshots from each one are synced in time. This approach differs from other attempts to track human–computer interactions — for instance, through the use of smartwatches and fitness trackers, or diaries. It is more accurate, it follows use across platforms, and it samples more frequently. In fact, we are working on software that makes recordings every second. We have now collected more than 30 million screenshots — what we call ‘screenomes’ — from more than 600 people. Even just two of these reveal what can be learnt from a fine-grained look at media use (see ‘Under the microscope’). This higher-resolution insight into media use could help answer long-held questions and lead to new ones. It might turn out, for instance, that levels of well-being are related to how fragmented people’s use of media is, or the content that they engage with. Differences in brain structure might be related to how quickly people move through cycles of production and consumption of content. Differences in performance in cognitive tasks might be related to how much of a person’s multitasking involves switching between content (say, from politics to health) and applications (social media to games), and how long they spend on each task before switching. Recordings of smartphone use by two 14-year-olds living in the same northern California community reveal what can be learnt from a fine-grained analysis of media use (see ‘All in the details’). Dose. A typical question that researchers might ask is whether study participants are ‘heavy’ or ‘light’ phone users. Both adolescents might have characterized their phone use as ‘substantial’ had they been asked the usual survey questions. Both might have reported that they used their smartphones ‘every day’ for ‘2 or more hours’ each day, and that looking at their phones was the first thing they did each morning and the last thing they did every night. But detailed recordings of their actual phone use over 3 weeks in 2018 highlight dramatic differences2. For participant A, median use over the 3 weeks was 3.67 hours per day. For participant B, it was 4.68 hours, an hour (27.5%) more. Pattern. The distribution of time spent using phones during the day differed even more. On average, participant A’s time was spread over 186 sessions each day (with a session defined as the interval between the screen lighting up and going dark again). For A, sessions lasted 1.19 minutes on average. By contrast, participant B’s time was spread over 26 daily sessions that lasted, on average, 2.54 minutes. So one of the adolescents turned their phone on and off seven times more than the other, using it in bursts that were about one-third the length of the other’s sessions. These patterns could signal important psychological differences. Participant A’s days were more fragmented, maybe indicating issues with attentional control, or perhaps reflecting an ability to process information faster. Interactivity. Both adolescents spent time creating content as well as consuming it. They wrote text messages, recorded photos and videos, entered search terms and so on. On a questionnaire, both might have reported that they posted original material ‘sometimes’ or maybe ‘often’. But the screenshot data reflect patterns of interactivity that would be almost impossible for them to recall accurately. Participant A spent 2.6% of their screen time in production mode, creating content evenly throughout the day and usually within social-media apps. By contrast, participant B spent 7% of their total screen time producing content (and produced 2.5 times more). But they did so mainly in the evening while watching videos. Content. During the 3 weeks, participant A engaged with 26 distinct applications. More than half of these (53.2%) were social-media apps (mostly Snapchat and Instagram). Participant B engaged with 30 distinct applications, mostly YouTube (50.9% of the total). Zooming deeper into specific screen content reveals even more. For participant B, on average, 37% of the screenshots for a single day included food — pictures of food from various websites, photos of B’s own food, videos of other people eating or cooking, and food shown in a game involving the running of a virtual restaurant. In a survey, both adolescents might have reported that they used ‘a lot’ of apps, and might have given the names of some of them. But the content of their media diets would be impossible to capture.  So, how can we do better? What’s needed is a collective effort to record and analyse everything people see and do on their screens, the order in which that seeing and doing occurs, and the associated metadata that are available from the software and sensors built into digital devices (for instance, on time of day, location, even keystroke velocity). In any one screenome, screenshots are the fundamental unit of media use. But the particular pieces or features of the screenome that will be most valuable will depend on the question posed — as is true for other ‘omes’. If the concern is possible addiction to mobile devices, then arousal responses (detected by a change in heart rate, say) associated with the first screen experienced during a session might be important to measure. If the concern is the extent to which social relationships dictate how political news is evaluated, then the screenshots that exist between ‘social’ and ‘political’ fragments in the screenome sequence might be the crucial data to analyse. (News items flagged by a close friend might be perceived as more trustworthy than the same news obtained independently, for example.) How can researchers get access to such high-resolution data? And how can they extract meaning from data sets comprising millions of screenshots? One option is for investigators to collaborate with the companies that own the data, and that have already developed sophisticated ways to monitor people’s digital lives, at least in certain domains, such as Google, Facebook, Amazon, Apple and Microsoft. The Social Science One programme, established in 2018 at Harvard University in Cambridge, Massachusetts, involves academics partnering with companies for exactly this purpose12. Researchers can request to use certain anonymized Facebook data to study social media and democracy, for example. Largely because of fears about data leaks or study findings that might adversely affect business, such collaborations can require compromises in how research questions are defined and which data are made available, and involve lengthy and legally cumbersome administration. And ultimately, there is nothing to compel companies to share data relevant to academic research. To explore more freely, academics need to collect the data themselves. The same is true if they are to tackle questions that need answers within days — say, to better understand the effects of a terrorist attack, political scandal or financial catastrophe. Thankfully, Screenomics and similar platforms are making this possible. In our experience, people are willing to share their data with academics. The harder problem is that collecting screenomics data rightly raises concerns about privacy and surveillance. Through measures such as encryption, secure storage and de-identification, it is possible to collect screenomes with due attention to personal privacy. (All our project proposals are vetted by university institutional review boards, charged with protecting human participants.) Certainly, social scientists can learn a lot from best practice in the protection and sharing of electronic medical records13 and genomic data. Screenomics data should be sifted using a gamut of approaches — from deep-dive qualitative analyses to algorithms that mine and classify patterns and structures. Given how quickly people’s screens change, studies should focus on the variation in an individual’s use of media over time as much as on differences between individuals and groups. Ultimately, researchers will be able to investigate moment-by-moment influences on physiological and psychological states, the sociological dynamics of interpersonal and group relations over days and weeks, and even cultural and historical changes that accrue over months and years. Some might argue that screenomics data are so fine-grained that they invite researchers to focus on the minutiae rather than the big picture. We would counter that today’s digital technology is all about diffused shards of experience. Also, through the approach we propose, it is possible to zoom in and out, to investigate how the smallest pieces of the screenome relate to the whole. Others might argue that even with this better microscope, we will not find anything significant. But if relationships between the use of media and people’s thoughts, feelings and behaviours continue to be weak or non-existent, at least we could have greater confidence as to whether current concerns are overblown. The approach we propose is complex, but no more so than the assessment of genetic predictors of mental and physical states and behaviours. Many years and billions of US dollars have been invested in other ‘omics’ projects. In genomics, as in neuroscience, planetary science and particle physics, governments and private funders have stepped up to help researchers gather the right data, and to ensure that those data are accessible to investigators globally. Now that so much of our lives play out on our screens, that strategy could prove just as valuable for the study of media. </body>
<date id = '87'>15 January 2020</date>
<url id = '88'>https://nature.com/articles/d41586-019-03959-6</url>
<title id = '88'>A tool that focuses on papers — not researcher behaviour — can help readers, editors and institutions assess which publications to trust.</title>
<body id = '88'> If it is published in the scientific literature, can you trust it? All too often, that question gets lost, sidetracked or buried. Even when serious, credible concerns are sent to a journal, decisions over whether to correct or retract are more likely to take years than months — time during which potentially harmful misinformation can spread. Delays and inaction often happen because enquiries tend to focus on the thorny question of whether a researcher acted deliberately to deceive. The more important issue, however, is the integrity of the actual publication: is the research article reliable and are its conclusions valid? As researchers who have spent years enmeshed in investigations, we think a major obstacle to evaluating the integrity of publications is a lack of tools. The Committee on Publication Ethics (COPE) advises publishers to retract articles when there is “clear evidence that the findings are unreliable”, but does not advise on how to determine whether that is the case. Resources for editors also focus on how to manage communications, rather than on how to evaluate reliability and validity. The net effect is inaction: readers remain uninformed about potential problems with a paper, and that can lead to wasted time and resources, and sometimes put patients at risk. The integrity of a publication can be compromised in many ways. Some are unintentional: typos, transcription errors or incorrect analyses. Others are deliberate: image manipulation, data falsification and plagiarism1. How publication integrity was compromised is secondary to whether the paper is reliable. Unreliable data or conclusions are problems irrespective of the cause. Enter checklists, which have helped in the structuring of complex procedures in health care and other industries. One of us (C.K.G.) helped to develop a checklist for assessing the quality of institutional investigations of researchers’ conduct2. Academic publishing has also introduced a suite of checklists to be completed by authors when submitting papers, to make sure certain aspects of the research are fully reported. Here we present a tool — the REAPPRAISED checklist — that aims to help readers, journal editors and anyone else assess whether a paper has flaws that call its integrity into question. We developed it on the basis of our own experience and extensive consultation with research administrators and journal editors. Although designed for clinical and animal studies, the structured approach to investigation applies more broadly. Readers, peer reviewers, journals, publishers and institutions can use it to assess whether to trust a paper’s findings. Our checklist should not be confused with journals’ submission checklists, which are filled in by authors before publication and indicate what items are reported in the manuscript. The REAPPRAISED checklist can be used by anyone struggling to assess a submitted or published article, and includes common-sense assessments that go beyond the text itself. It can, and should, be applied independently of whether misconduct is suspected. Its use can help to speed up the identification and correction of flawed papers, preventing wasted resources and even protecting patients from harm. How did we come to see the need for this tool? From early 2013, three of us (A.A., A.G., M.J.B.) began to contact journals about multiple, serious problems we had identified in 33 reports of trials led by bone-health researchers Yoshihiro Sato and Jun Iwamoto3. The first retraction did not appear until late 2015. This delay is all the more regrettable given that concerns had been raised more than a decade earlier. In 2003, a publication by Sato4 assessed a very rare complication of treatment of Parkinson’s disease. Within a year, a letter5 to the editor remarked on how surprising it was that the research group had managed to identify 40 people with this complication in a very short time, because the writer’s specialist institute had seen only two cases “in living memory”. Others had brought up concerns about ethical oversight6, as well as failure of randomization, implausible recruitment and outcomes, and other problems7, but no editorial comment or correction occurred at that time; retraction notices were finally issued in 2015–18. Even after Sato admitted8 in 2016 to making up data, only 2 of the 34 journals we contacted took the initiative to assess other papers they had published by this group. Investigations conducted at the four institutions at which Sato and Iwamoto worked were also, in our opinion, misdirected and incomplete. They focused on identifying researcher misconduct, not publication reliability. They assessed only 90 of 351 potentially compromised publications, and only after we contacted them in 2017 with concerns9. Two institutions failed to reach a conclusion about the reliability of 56 publications of the 78 they considered, because they could not determine whether misconduct had occurred. The other two did not report assessments on individual papers. (The institutions maintain that their investigations were appropriate, but have not responded directly to the criticisms, and two stated that it was difficult to assess publication integrity independently of misconduct.) So far, at least 90 papers by Sato or Iwamoto have been retracted. More than twice that number remain in the literature, including 5 of the 33 clinical-trial reports. Readers have no way of knowing whether those reports are trustworthy. Nor do they have any guidance for making their own assessments. These papers continue to be influential. In the years since we brought our concerns to journals, those 33 clinical-trial reports have been cited more than 600 times. They have been used by other researchers as evidence to help justify at least eight clinical trials10. Others that were later retracted were used in an effectiveness review conducted by the US Agency for Healthcare Research and Quality. They provided the sole evidence that bisphosphonates, commonly used for osteoporosis, could prevent fractures in patients at high risk of falls10. Other cases had similar outcomes. More than ten years elapsed between the initial notification of concerns about publications of clinical studies in anaesthesia by Yoshitaka Fujii and the first of 183 retractions (see go.nature.com/2pvw2ax). Concerns raised about problems in a paper by Andrew Wakefield on autism and vaccination did not result in full retraction until 12 years after its publication. Flawed papers such as these influence future research initiatives, are incorporated into clinical guidelines and influence medical practice and public perceptions, so could potentially harm millions of patients. Our REAPPRAISED checklist facilitates systematic evaluation through 11 categories (see ‘The ‘REAPPRAISED’ list for evaluationof publication integrity’). It covers ethical oversight and funding, research productivity and investigator workload, validity of randomization, plausibility of results and duplicate data reporting. It can identify problems from isolated data errors to data fabrication or falsification. Some of these questions should ideally have been asked by reviewers and editors before publication, and not all questions will apply to every paper, but it is still useful to consider all the questions collectively when assessing an article. We developed the checklist in our roles as researchers (A.G., M.J.B., A.A.) and journal editor (A.A.K.) while evaluating thousands of publications; C.K.G., a specialist in institutional investigations, helped to refine it. Our experience suggests that using REAPPRAISED can assist in decision-making before and after publication. A.A.K. implemented an earlier version of the checklist as editor-in-chief of Anaesthesia, which reviews nearly 1,000 submissions per year. Each is screened with the checklist; if concerns arise, individual patient data are requested and reviewed carefully for errors, inconsistencies or other red flags. In the past two years of routine use, editors have identified integrity problems in 42 submissions. A large subset of these — comprising work from 12 research groups — was serious enough to notify authors’ institutions. Six of the 12 ensuing investigations confirmed that data had been falsified or fabricated; two laid the blame on errors; and four are ongoing (A.A.K., personal communication). We have also found this checklist effective in communicating with journals, having used it to submit a structured list of concerns to journal editors for the 56 publications for which institutional investigations were inconclusive. That has led to 29 retractions and 5 expressions of concern. We would like to see the checklist used during both manuscript review and post-publication evaluation. The fact that it separates assessment of publication integrity from the investigation of research misconduct will speed up evaluations. It could even be published alongside decisions to retract, issue an expression of concern, correct a paper, or let it stand. We expect that use of REAPPRAISED will lead to more detailed, efficient, consistent and transparent evaluations of publication integrity, and thus faster and more accurate reporting of corrections and retractions. These improvements will benefit the researchers, clinicians, policymakers, patients and others who rely on the literature to make decisions. People using the tool will be able to help refine it as they gain experience, and it will help them to develop standards to assess the integrity of publications and act accordingly. We hope others will join in our efforts to implement and refine REAPPRAISED both informally and in future publications. Integrity problems often cluster. Authors who have a paper retracted for misconduct are more likely to accrue multiple retractions than are those whose work was retracted for other reasons11. Publications by their co-authors can also be compromised12. The checklist could help journal staff and investigation committees to decide when an assessment should be broadened to include other papers by a particular researcher and collaborators. Ironically, a checklist that puts aside the question of misconduct might aid in evaluations of inappropriate behaviour. If multiple concerns are identified, or the concerns identified are those often associated with misconduct, the entire body of an author’s work should be systematically assessed. Publishers’ integrity groups should adopt the checklist (or ask those expressing concerns to do so). Funders and government regulators should disseminate it to publishers, research institutions and other stakeholders. Peer reviewers and readers can use it on their own initiative, and those who have a nagging feeling about a publication can use it to work through their concerns and, if merited, communicate those to a journal. If the goal is trustworthy literature, the integrity of publications — not just determination of misconduct — should be the focus of investigation. ☐ Are the locations where the research took place specified, and is this information plausible? ☐ Is a funding source reported? ☐ Has the study been registered? ☐ Are details such as dates and study methods in the publication consistent with those in the registration documents? ☐ Is there evidence that the work has been approved by a specific, recognized committee? ☐ Are there any concerns about unethical practice? ☐ Do all authors meet criteria for authorship? ☐ Are contributorship statements present? ☐ Are contributorship statements complete? ☐ Is authorship of related papers consistent? ☐ Can co-authors attest to the reliability of the paper? ☐ Is the volume of work reported by research group plausible, including that indicated by concurrent studies from the same group? ☐ Is the reported staffing adequate for the study conduct as reported? ☐ Is there evidence of copied work? ☐ Is there evidence of text recycling (cutting and pasting text between papers), including text that is inconsistent with the study? ☐ Is the recruitment of participants plausible within the stated time frame for the research? ☐ Is the recruitment of participants plausible considering the epidemiology of the disease in the area of the study location? ☐ Do the numbers of animals purchased and housed align with numbers in the publication? ☐ Is the number of participant withdrawals compatible with the disease, age and timeline? ☐ Is the number of participant deaths compatible with the disease, age and timeline? ☐ Is the interval between study completion and manuscript submission plausible? ☐ Could the study plausibly be completed as described? ☐ Are the study methods plausible, at the location specified? ☐ Have the correct analyses been undertaken and reported? ☐ Is there evidence of poor methodology, including: ☐ Missing data ☐ Inappropriate data handling ☐ ‘P-hacking’: biased or selective analyses that promote fragile results ☐ Other unacknowledged multiple statistical testing ☐ Is there outcome switching — that is, do the analysis and discussion focus on measures other than those specified in registered analysis plans? ☐ Is there evidence of manipulation or duplication of images? ☐ Are any data impossible? ☐ Are subgroup means incompatible with those for the whole cohort? ☐ Are the reported summary data compatible with the reported range? ☐ Are the summary outcome data identical across study groups? ☐ Are there any discrepancies between data reported in figures, tables and text? ☐ Are statistical test results compatible with reported data? ☐ Are any data implausible? ☐ Are any of the baseline data excessively similar or different between randomized groups? ☐ Are any of the outcome data unexpected outliers? ☐ Are the frequencies of the outcomes unusual? ☐ Are any data outside the expected range for sex, age or disease? ☐ Are there any discrepancies between the values for percentage and absolute change? ☐ Are there any discrepancies between reported data and participant inclusion criteria? ☐ Are the variances in biological variables surprisingly consistent over time? ☐ Are correct units reported? ☐ Are numbers of participants correct and consistent throughout the publication? ☐ Are calculations of proportions and percentages correct? ☐ Are results internally consistent? ☐ Are the results of statistical testing internally consistent and plausible? ☐ Are other data errors present? ☐ Are there typographical errors? ☐ Have the data been published elsewhere? ☐ Is any duplicate reporting acknowledged or explained? ☐ How many data are duplicate reported? ☐ Are duplicate-reported data consistent between publications? ☐ Are relevant methods consistent between publications? ☐ Is there evidence of duplication of figures? Download a PDF </body>
<date id = '88'>07 January 2020</date>
<url id = '89'>https://nature.com/articles/d41586-019-03913-6</url>
<title id = '89'>There is no room for squeamishness in the face of the world’s growing water shortage — three steps could vastly improve the image of reused water for drinking.</title>
<body id = '89'>One of five water-reuse plants in Singapore, which together supply about 40% of the nation’s water for drinking and other uses.Credit: Roslan Rahman/AFP/Getty Drinkable water is becoming increasingly scarce. Population growth, pollution and climate change mean that more cities are being forced to search for unconventional water sources1. In a growing number of places, drinking highly treated municipal wastewater, called ‘reused water’, has become the best option — and, in some cases, the only one (see ‘What is reused water?’). But anxieties about reused water, often heightened by sensational media coverage, have prevented several projects from going ahead. Some people are concerned that reused water will contain more pathogens and chemicals than drinking water sourced from lakes or rivers. Others are simply disgusted by the idea of consuming water that has passed through toilets and drains before being treated. Around two billion people now live in countries with ‘high water scarcity’ — mainly in northern Africa and western, central and southern Asia2. With the global population predicted to increase from 7.7 billion today to 10 billion in 2050 — an estimated 70% of whom will live in urban areas — the demand for safe drinking water is set to rise drastically. According to a 2019 United Nations assessment, water demand in general is likely to increase by 20% to 30% between now and 20503. ‘Reused’ water comes from highly treated wastewater. In middle- and high-income countries, domestic (municipal) wastewater — from houses, shops and businesses, but not from industries — is generally collected, treated in sewage plants and discharged into rivers, lakes and other natural water bodies. The ‘raw water’ is then collected, treated again and used by towns and cities downstream for drinking, agriculture, landscape irrigation or industrial processes. An alternative strategy is to treat municipal wastewater more rigorously so that it can be used for drinking. After it goes through the sewage plant, it is treated in a second plant (and sometimes a third) using advanced chemical, biological and physical treatments. The water is then fed directly into the drinking supply system or into the natural system (rivers, lakes, aquifers or reservoirs). In the latter scenario, water is subsequently extracted from the natural system, treated again and then supplied to people for drinking or other uses. In both cases, the resulting water is termed reused8. In many places, discharges of wastewater treated in the usual way (so just once) are making up an increasing proportion of river flow15. Yet authorities still consider such rivers ‘natural sources of freshwater’. Because downstream treatment methods might not be adapted to the actual quality of the raw water, this is increasingly posing a health risk. Thus, treating wastewater to higher standards in a controlled environment and reusing it for specific purposes can make more sense, for both economic and health reasons. Thus, conserving water is paramount. Supply infrastructure needs to be improved and better managed, including through the use of smart sensors and other technologies. Economic instruments, such as appropriate pricing, can boost efficient usage. Legislation needs to be implemented to lessen pollution. And all sectors — public and private — need to be educated about the importance of saving water, as does society more broadly. High on the list should be efforts to investigate the benefits and risks of drinking reused water, including ways to make it more acceptable to consumers. Opposition from citizens has stalled several projects aimed at providing people with reused drinking water over the past two decades. In 2000, the Los Angeles Daily News ran an article titled ‘Tapping toilet water’ about the East Valley Water Recycling project that had begun in the San Fernando Valley in Los Angeles, California, in 1995. People in the region were worried that the water, which they thought would be supplied only to those living in low-income areas, would be unsafe. The project was politicized by mayoral candidates, and the Los Angeles Department of Water and Power, which had proposed the project, eventually decided not to implement it. Since then, the reused water has been used only in irrigation and industry4. In Queensland in Australia, residents successfully opposed a reuse project in Toowoomba in 2006 and the Western Corridor Recycled Water Scheme in southeastern Queensland in 2009, even while the country was experiencing the most severe drought since records began. In Toowoomba, 62% of around 95,000 people voted against the project, largely because of safety concerns and fears that it would harm industries including tourism, food processing and property sales5. The Western Corridor Recycled Water Scheme cost Aus$2.4 billion (US$1.6 billion) to construct and aimed to produce up to 230,000 cubic metres of water per day to cover around 30% of southeastern Queensland’s water supply needs. But in 2009, following political pressure and a break in the drought, it was decided that the scheme would produce drinkable reused water only when the levels of the reservoir (where the reused water would be stored) fell below 40% of full capacity6. A worker fills a tanker with drinking water in Queensland, Australia, which is experiencing water shortages because of a long-term drought.Credit: William West/AFP/Getty Public scepticism over water safety is not completely unwarranted. In the United States and Canada, for example, there are still communities that lack access to safe drinking water — predominantly among low-income and minority ethnic populations7. In several cases, drinking water has been shown to be unsafe for the population, such as in Flint, Michigan, in 2014, and in several cities in Canada this year. In all of these instances, the water was found to contain higher concentrations of lead than those deemed safe by the regulatory authorities. In October last year, testing revealed that nearly 300 drinking-water wells and other water sources in California contain traces of chemicals known as PFASs (per- and poly-fluoroalkyl substances) that have been linked to certain cancers and other health problems. Currently, however, reused drinking water is actually subject to stricter regulations, monitoring, assessments and auditing than standard drinking water. Three steps would improve the image of reused water. Do more research. Wastewater contains hundreds of known chemical and pathogenic contaminants that, if not treated properly, can cause serious acute and chronic diseases, such as cholera or typhoid. Also, new chemicals are continually being introduced to the market, and new strains of bacteria and viruses discovered. Investigators at universities and those working for water-utility companies must study, quantify and effectively mitigate any emerging risks, and must keep appraising the overall benefits and costs of reused water on both human and environmental health. Especially as technologies for detection become more sensitive, more affordable and widely available, the presence of pathogens and chemicals must be continuously monitored (by daily or even more frequent testing8) to protect the public from problems that might emerge9. Chronic risks from long-term exposure to low levels of toxic chemical substances are just as important to track as acute risks resulting from a one-off exposure10. In middle- and high-income countries, drinking water, whether or not it is reused water, must meet national, regional and local health standards (or whichever apply) for pathogens, chemicals and any other types of contaminant8. So far, water agencies in cities using reused water have been able to meet these standards — through the use of multiple barrier-treatment steps from chemical to microbial, through real-time monitoring of microbes and chemicals, and by using various risk-management strategies throughout treatment and distribution11,12. Improve public outreach. Water-utility companies must develop more comprehensive strategies on information dissemination, public consultation, education and engagement. Community engagement is not — and must never be perceived to be — a means to convince the public that certain projects should go ahead. Rather, it should be about setting up platforms, so that people’s concerns can be heard and addressed early on, even if that means modifying plans. Some successful projects can offer a model. In the 1990s, for instance, the city of San Diego in California planned a water reuse project to reduce its dependence on water transfers from the Colorado River and other sources. The project was initially supported by the public. But that support fell away for various reasons, including inconsistencies in the information provided by expert panels on the safety of recycled wastewater. Following the use of terms in the media such as ‘toilet to tap’ and ‘sewerage beverage’, and claims that the reused water would be supplied only to low-income communities, resistance was such that the city council converted the project to a non-drinkable scheme in 199913. Yet San Diego still needed more drinking water. So in 2004, the company Public Utilities decided to develop more-comprehensive strategies for public outreach and education. Among the suite of approaches deployed were an online and telephone survey, research involving focus groups, opportunities for city staff to discuss the project with San Diego voluntary service organizations and others, and a dedicated website providing information. These efforts paid off. In 2004, only 26% of those surveyed approved of water reuse. By 2012, 73% did. The city approved the ‘Pure Water San Diego’ project in 201314. It is expected to produce some 114,000 m3 of drinking water per day by 2023 and to supply one-third of the city’s water needs by 2035. Implement projects where need is great. Competent water-utility companies should start implementing reuse projects in places where the need is greatest. They will need to have sufficient knowledge, technical know-how, staffing levels and financial capacity, and be operating in cities where there are strict water-quality regulations. Once such schemes have been proved safe and effective in places where the stakes are high, others will be more likely to support similar projects in their own communities. The key to these strategies working is the continuous involvement of all stakeholders — from city mayors to national governments, from businesses and local health and medical boards to community and environmental groups, religious leaders and the media. At least three important economic centres — Singapore, Windhoek in Namibia and Orange County in California — would not have progressed to where they are today without reused drinking water (see ‘Three successes’). In fact, without these reuse projects, the strict water rations that were likely to result could have had severe impacts on socio-economic development. Moreover, reused water can benefit streams, rivers, lakes, wetlands and aquifers, in part because the excess water from such projects that is returned to natural systems is of better quality than standard treated wastewater8. Located in an arid to semi-arid environment with little access to surface water sources, Windhoek was the first city to create a drinking-water supply from reused water in 1968. The Goreangab Water Reclamation Plant currently produces around 24% of Windhoek’s drinking water (21,000 cubic metres per day)16. During the 2014–16 drought, supplies from nearby reservoirs could meet only 10% of demand, instead of the expected 75%. Reused water from Goreangab then contributed up to 30% of the city’s total water supply. Operational since 2008, the Orange County Groundwater Replenishment System has become the largest reuse facility in the world. It produces 379,000 m3 of drinkable water per day. The project is widely accepted in part because the utility company, the Orange County Water District, prioritized public information and engagement from the start. Singapore spent decades planning a water reuse scheme now called NEWater. By the time the project was launched in 2003, comprehensive communication and education efforts on long-term safety and reliability issues, involving the government and other decision-makers, had already been established. Today, NEWater supplies about 40% of Singapore’s drinkable and non-drinkable water. If all goes to plan, by 2060, it will meet 55% of the city’s water demands17. Most people in Singapore are aware that their island city state is short of water, being too small to store the rainfall it receives. And they appreciate the importance of NEWater. On each Singapore National Day (9 August), thousands of people who attend the celebrations are given bottles of NEWater, which they drink without qualms. </body>
<date id = '89'>31 December 2019</date>
<url id = '90'>https://nature.com/articles/d41586-019-03841-5</url>
<title id = '90'>A pioneer in sustainable innovation explains why she has spent the past decade fighting the first lawsuit to force a government to act on global heating.</title>
<body id = '90'>The Urgenda Foundation’s co-founder Marjan Minnesma (right) and an 11-year-old co-plaintiff wait for a verdict in the Dutch appeals court in 2015.Credit: Peter Dejong/AP/Shutterstock I live in a nation where more than one-quarter of the land is already below sea level. For much of the past decade, I’ve been on a journey for climate justice. With 886 of my fellow Dutch citizens, the Urgenda Foundation that I co-founded brought the first lawsuit aiming to find a national government guilty of failing to safeguard its people from the ravages of climate change. We have won repeatedly, at several levels of the court. Our final win in the Supreme Court of the Netherlands in The Hague on 20 December is a fitting end to a watershed year for civil action on global heating (this article has been updated with the outcome). The case has inspired other national lawsuits that — along with those against corporations and investors — are creating a burgeoning toolkit of environmental jurisprudence. Together, these serve notice on contributors to the world’s still-growing emissions that their inaction is no longer defensible. In 2011 I read Revolution Justified by lawyer Roger Cox (who later acted with lawyer Koos van den Berg for Urgenda in the first court). In the book, Cox argued that catastrophic climate change is a major threat to us and our children, and that governments are not working to prevent it. One of the few democratic ways to make states act, he suggested, is through the legal system. What if judges read the facts? It would probably be obvious to them that climate change is a clear threat. Might they rule that ‘not acting’ is hazardous negligence that breaches a government’s duty of care towards its citizens? That’s certainly how I felt. I had been at the first Conference of the Parties to the United Nations Framework Convention on Climate Change (UNFCCC) in Berlin in 1995. The convention was the focus of my law thesis. In the intervening decades of trying to effect change as a scholar and champion of sustainable innovation, I’d also had three children. With every passing year of empty promises, growing greenhouse gases and rising temperatures, my attitude shifted from cerebral problem-solving to worrying for their future. I now give many speeches, around one-third of which are about the problem and two-thirds about solutions. But, most of all, I like starting projects that seem impossible, and finishing them to leave something concrete. I decided to bring a lawsuit to force the Dutch government to do what it had said for years was necessary — namely, to reduce the emissions of greenhouse gases by between 25 and 40% by the end of 2020, compared to 1990 levels. You can start a court case only if you have first tried to reach your goals in other ways. So, in November 2012, Urgenda organized a public seminar close to where the parliament of the Netherlands meets, in The Hague. In theory, the parliamentarians who visited could run straight back to the ongoing debate that day and demand of the government what we asked for. Presenters that day included the outspoken US climate scientist James Hansen, who is now assisting in several court cases brought by groups of young people in the United States and Norway. Another was Urgenda co-founder Jan Rotmans of the Netherlands National Institute for Public Health and the Environment (RIVM). He built the first integrated climate-assessment model, IMAGE, which has been used in international climate negotiations. The audience included politicians, members of the press and engaged citizens, to whom we explained the dangers of doing nothing and the overwhelming evidence of the severe effects of humans’ greenhouse-gas emissions on living conditions. The seminar had little effect. That month, we wrote a letter to the Dutch government demanding a 40% reduction of greenhouse-gas emissions by 2020. We got a friendly letter back. The government agreed that climate change is a severe problem and that it needed to take action. But, the government wrote, it “did not want to be a frontrunner”, claiming that such an approach could dent prosperity and businesses and raise carbon dioxide levels as a result. This was richly ironic coming from a world-class laggard in sustainable energy. The Netherlands’ international reputation for being ‘green’ is thanks to cycling and recycling. When it comes to climate change, it talks a lot and does little. At the time, out of the 27 nations of the European Union, only Luxembourg and Malta generated less energy from renewables than did the Netherlands. Owing to its rich reserves of fossil fuels in offshore natural-gas fields, as well as its massive ports, chemical industries, agriculture and use of coal, the Netherlands was listed 34th of the world’s roughly 200 countries in the league table of net emissions that year — more than 80% of all countries emitted less. In the most recent league table, from 2015, it is in 40th place. Looking at the biggest emitters of 2014–16 in absolute terms, the Netherlands was in the top ten for emissions per person, higher than China and way above India. Suggesting that the nation is ‘too small to act’, as the state argued in court in April 2015, implies that most countries of the world should also do nothing. Supporters of a US climate-change lawsuit brought by 21 young people joined a rally in Oregon in June 2019.Credit: Robin Loznak/Zuma Wire In mid-December 2012, the Urgenda Foundation decided to sue the government. We invented ‘crowd pleading’: a cross between crowd funding and citizen science. We asked people to join and help us to look for arguments in court cases all over the world. The foundation gathered the 886 co-plaintiffs, all Dutch citizens, including children — the youngest of whom was 5 years old when we began. On 20 November 2013, we handed in the summons to the front desk of the Supreme Court in The Hague, demanding a 40% reduction of greenhouse-gas emissions by 2020, or — if this was not possible — at least 25% compared to 1990 levels. After several rounds of written documents with arguments from us and from the Dutch state, we were called to a hearing at the District Court of The Hague in April 2015. Our hundreds of co-plaintiffs and attendant media could not fit into the court buildings. We produced our own live stream so people could watch together in buildings nearby and follow it at home from their computers. At the end of that day, the judges said they would give their verdict on 24 June 2015 — my 15th wedding anniversary. We hoped we’d win, but we were not sure at all. We put our chances at perhaps 50%. I never doubted our arguments, but we didn’t know whether the judges would have the time and willingness to dive deep enough into the science of climate change. At 10 a.m. on 24 June we were again in court, to hear the short summary of the three judges on our case. I sat at the front of the room watching the judges and trying to tweet the main conclusions. Halfway through the summary, I stopped tweeting because I started to realize that the judges were following our line of reasoning. I glanced at the lawyers to check whether I was right. They were concentrating too hard to catch my eye. The judges agreed that the Dutch government had breached its duty of care by taking insufficient measures to prevent dangerous climate change impairing the living conditions of its people. They based their arguments on tort law (also called civil law) and the doctrine of hazardous negligence. Because the government had signed many documents from the UNFCCC and the European Union declaring that industrial countries should reduce greenhouse gases by between 25 and 40% in 2020, the judges stated that the Netherlands should at an absolute minimum reduce emissions by 25%. Perhaps 40% is necessary, they declared, but the upper bound is at the government’s discretion. A second after the judges left the court room, it erupted with joy. People were yelling, crying, applauding and hugging. Hardly anybody had expected we would win. The verdict was announced in Dutch and English simultaneously, which helped to spread the word. In half an hour, the news was all over the world. We were overwhelmed by the reactions. Calls flooded in from people from Canada to New Zealand. Some were crying on the phone, saying that they had almost given up, but now had hope again. For Urgenda, the court case changed a lot. Begun in 2007 at the Erasmus University in Rotterdam, the foundation (now based in Amsterdam) had been a non-governmental organization that mainly worked on solutions to climate change for the Netherlands. In 2008, for instance, we imported the first electric vehicles from Norway and sold them to cities such as Amsterdam, while helping to create a network of charging stations. We kick-started the growth of solar power in the Netherlands by organizing the first collective buying initiative in Europe for solar panels and inverters. Our project ‘We Want Sun’ purchased 50,000 panels, which at the time brought down the prices for rooftop solar installations in the nation by one-third. After the win, we were framed by journalists and many others as climate activists. They didn’t mean it as a compliment. But I took it as one: an activist is one who acts, just as we’d always done. We are still working on climate solutions, but many know us only from the climate case. In September 2015, the government lodged an appeal with the court in The Hague, despite a spontaneous international campaign begging it not to — including messages from celebrities such as actor Mark Ruffalo (who has played the Hulk since 2012) and the model Cameron Russell. So began two years in which our lawyer, Koos van den Berg, produced hundreds of pages with more arguments to convince the appeals court. The second verdict came in October 2018, and again we won! All 29 grounds of appeal from the state were declined. Better still, this day in court was even more damning for our government (and potentially others) than the first. The district court had ruled that the citizen suit could not base arguments on the European Convention on Human Rights because it was brought by an organization (the Urgenda Foundation) rather than by a human — notwithstanding that its co-plaintiffs numbered hundreds of people. The Court of Appeal disagreed. It declared that the Dutch government is obliged, under articles 2 and 8 of the European Convention on Human Rights, to protect inhabitants by reducing emissions by 25% by 2020. So now we had two duties of care, one from tort law and one based on human rights (a higher-order law). Shortly after the second verdict, the government appealed again, this time to the Supreme Court of the Netherlands. This court always takes independent legal advice before ruling, normally from one person. In this case, everything was out of the ordinary, so two advisers were called upon: the deputy procurator general and the advocate general. On 13 September this year, they delivered their advice: to uphold the earlier judgments. In 80% of cases, the Supreme Court follows the guidance it is given. But this journey has taught us to brace for surprises. Meanwhile, six years have elapsed since we filed the case calling for action by 2020. Although the 2015 judgment spurred the state to set a more ambitious climate policy for 2030, little was done to meet the 2020 target. The government simply assumed that the judgment would be overturned on appeal. After the second win, that attitude finally changed. To implement the 2020 target, the government has taken measures to close one of the nation’s five coal-fired power plants, and has launched new subsidies for energy-saving activities and renewable energy. But with current national emissions reduced by only 15% from 1990 levels so far, a large gap still remains. To provide a road map for change, Urgenda published a plan on 24 June — the fourth birthday of the first verdict (see go.nature.com/345d4zr; in Dutch). It included more than 700 organizations, including paper manufacturers, farmers, local sustainable-energy co-operatives and large environmental organizations. It set out 40 measures for reducing greenhouse gases by 25% from 1990 levels by the end of 2020. These included driving at 100 instead of 130 kilometres per hour, raising water levels in nature reserves and energy-saving options for the health and industrial sectors. The foundation later added another ten measures. So there are now 50 ways for the government of the Netherlands to make up for its failure to protect its citizens from warming of more than 1.5 °C, as the judges of the Supreme Court decreed on 20 December that it must. The 700 partners are poised to help, once the government delivers the money and support that are needed. It has been a long, hard road, with many ups and downs for the whole team, from tense discussions to nights without sleep. But I’m glad we stayed the course and inspired others around the world to say to their leaders: step up. </body>
<date id = '90'>17 December 2019</date>
<url id = '91'>https://nature.com/articles/d41586-019-03759-y</url>
<title id = '91'>Leading scholars and publishers from ten countries have agreed a definition of predatory publishing that can protect scholarship. It took 12 hours of discussion, 18 questions and 3 rounds to reach.</title>
<body id = '91'>Illustration by David Parkins When ‘Jane’ turned to alternative medicine, she had already exhausted radiotherapy, chemotherapy and other standard treatments for breast cancer. Her alternative-medicine practitioner shared an article about a therapy involving vitamin infusions. To her and her practitioner, it seemed to be authentic grounds for hope. But when Jane showed the article to her son-in-law (one of the authors of this Comment), he realized it came from a predatory journal — meaning its promise was doubtful and its validity unlikely to have been vetted. Predatory journals are a global threat. They accept articles for publication — along with authors’ fees — without performing promised quality checks for issues such as plagiarism or ethical approval. Naive readers are not the only victims. Many researchers have been duped into submitting to predatory journals, in which their work can be overlooked. One study that focused on 46,000 researchers based in Italy found that about 5% of them published in such outlets1. A separate analysis suggests predatory publishers collect millions of dollars in publication fees that are ultimately paid out by funders such as the US National Institutes of Health (NIH)2. One barrier to combating predatory publishing is, in our view, the lack of an agreed definition. By analogy, consider the historical criteria for deciding whether an abnormal bulge in the aorta, the largest artery in the body, could be deemed an aneurysm — a dangerous condition. One accepted definition was based on population norms, another on the size of the bulge relative to the aorta and a third on an absolute measure of aorta width. Prevalence varied fourfold depending on the definition used. This complicated efforts to assess risk and interventions, and created uncertainty about who should be offered a high-risk operation3. Everyone agrees that predatory publishers sow confusion, promote shoddy scholarship and waste resources. What is needed is consensus on a definition of predatory journals. This would provide a reference point for research into their prevalence and influence, and would help in crafting coherent interventions. To hammer out such a consensus and to map solutions, we and others met in Ottawa, Canada, over two days in April this year. The 43 participants hailed from 10 countries and represented publishing societies, research funders, researchers, policymakers, academic institutions, libraries and patient partners (that is, patients and caregivers who proactively engage in research). Our focus was the biomedical sciences, but our recommendations should apply broadly. Here we put forward our definition. We describe what it took to achieve consensus and how we’ll move forward. The consensus definition reached was: “Predatory journals and publishers are entities that prioritize self-interest at the expense of scholarship and are characterized by false or misleading information, deviation from best editorial and publication practices, a lack of transparency, and/or the use of aggressive and indiscriminate solicitation practices.” Since the term ‘predatory publishers’ was coined in 2010, hundreds of scholarly articles, including 38 research papers, have been written warning about them. Scientific societies and publishers (including Springer Nature) have helped to establish the ‘Think. Check. Submit.’ campaign to guide authors. But it is not enough. More than 90 checklists exist to help identify predatory journals using characteristics such as sloppy presentation or titles that include words such as ‘international’. This is an overwhelming number for authors. Only three of the lists were developed using research evidence4. Paywalled lists of quality journals and predatory journals show that there is an appetite for clear, authoritative guidance. But these lists are inconsistent and sometimes out of reach5,6 (see ‘No list to rule them all’). A journal’s membership of agencies such as COPE (the Committee on Publication Ethics), curated indexes such as Web of Science, or being listed in the Directory of Open Access Journals (DOAJ) is insufficient to guarantee quality. Predatory journals have found ways to penetrate these lists, and new journals have to publish for at least a year before they can apply for indexing. Source: Adapted from ref. 5. A scoping review comparing publications about predatory journals found that their characterizations sometimes overlapped, sometimes did not and sometimes directly conflicted7. These inconsistencies suggest that crafting a practical definition would require building consensus across researchers, publishers, research institutions and the broader public. Participants in our summit completed a three-round modified Delphi survey (a structured technique to elicit input, offer feedback and build consensus) that included 18 questions and 28 sub-questions. There were also 12 hours of discussion, followed by 2 further rounds of feedback and revision. Crafting a consensus definition was hard. Even reaching agreement on the use of ‘predatory’ was a challenge. Part of the group wanted a term that acknowledges that some authors turn to these outlets fully aware of their low quality; these scholars willingly pay to publish in predatory journals to add a line to their CVs. We discussed replacing the term entirely with language that recognizes nuances in publishers’ quality and motivation. Alternatives considered included ‘dark’, ‘deceptive’, ‘illegitimate’ and ‘acting in bad faith’. Ultimately, we concluded that the term ‘predatory’ has become recognized in the scholarly community. Implementation science suggests that introducing new nomenclature would take considerable resources, which we felt could be better put towards combating predatory publishing directly. So we recommend keeping the word ‘predatory’ while noting its limitations. Predatory journals are driven by self-interest, usually financial, at the expense of scholarship. They are characterized by the following: False or misleading information. This applies to how the publisher presents itself. A predatory journal’s website or e-mails often present contradictory statements, fake impact factors, incorrect addresses, misrepresentations of the editorial board, false claims of indexing or membership of associations and misleading claims about the rigour of peer review. Deviation from best editorial and publication practices. Standards here have been set out in the joint statement on Principles of Transparency and Best Practice in Scholarly Publishing (see go.nature.com/35mq7mj), issued by the DOAJ, the Open Access Scholarly Publishers Association, COPE and the World Association of Medical Editors. Examples of substandard practice include not having a retraction policy, requesting a transfer of copyright when publishing an open-access article and not specifying a Creative Commons licence in an open-access journal. These characteristics can be difficult to know before submitting, although such information is easily obtained from legitimate journals. An unprofessional-looking web page — with spelling or grammar mistakes or irrelevant text — should also raise red flags. Warning signs should be assessed with care. For instance, journals are not eligible for listing on the DOAJ or joining COPE until after one year of operation. A well-meaning but poorly resourced journal might not be able to maintain a professional website. Also, some journals claim to follow best practice but do not. Summit participants agreed that the burden of proof rests on the journal. Lack of transparency. There are two reasons we list this separately from deviation from best practice. First, transparency in operational procedures (such as how editorial decisions are made, fees applied and peer review organized) is presently somewhat aspirational in academic publishing and thus cannot be considered a current best practice. Second, the absence of transparency in predatory journals makes it important enough to highlight separately. Predatory publishers often fail to provide their contact information or details about article processing charges. Editors and members of their editorial boards are often unverifiable. Aggressive, indiscriminate solicitation. Although legitimate journals might solicit submissions, predatory journals often use aggressive solicitation such as repeated e-mails. These might be excessively flattering in tone, or might mention researchers’ past publications while noting that related submissions are urgently needed for a forthcoming issue. A clear warning sign is that the invitee’s expertise is outside the journal’s scope. Criteria we left out. Some obvious candidates for this list — journal quality and intent to deceive — were deliberately left out. It can be tough to distinguish a predatory journal from a journal that is under-resourced. Both can be low quality, but the latter does not have an intention to deceive8,9 (see also go.nature.com/33gmjut and go.nature.com/2afaka7). Furthermore, such intent is hard to assess and, if many of the characteristics described in the definition are met, identifying intent might not be necessary. Most controversially, we omitted quality of peer review, even though negligent peer review is often a prominent feature of predatory journals. We are not saying that peer review is unimportant, only that it is currently impossible to assess. Unfortunately, many legitimate journals fail to make their peer-review processes sufficiently transparent, for instance by sharing peer reviewers’ comments and other data. At the moment, journal quality, adequacy of peer review and deceit are too subjective to include. Efforts to fight predatory publishing require collaboration and support. Organizations, researchers and governments have started to respond. To name just a few, in 2017, the NIH released a statement encouraging researchers it funds to publish in reputable journals. India’s University Grants Commission has created a reference list of respectable journals and is currently working to revise academic publication incentives and develop a training course to reinforce the message. In November 2018, COPE held a forum on predatory publishing to examine problems and solutions. So far, disparate attempts to address predatory publishing have been unable to control this ever-multiplying problem. The need will be greater as authors adjust to Plan S and other similar mandates, which will require researchers to publish their work in open-access journals or platforms if they are funded by most European agencies, the World Health Organization, the Bill & Melinda Gates Foundation and others (see www.coalition-s.org). Many might argue that, with predatory journals adapting so quickly, our group’s efforts would have been better spent crafting interventions or promoting outreach. We believe that with this consensus definition, we are better prepared to track the problem over time, compare the results of studies on predatory journals and develop and evaluate intervention strategies such as educational campaigns and policy mandates. Over the coming months, we will solicit input and make the definition usable so that funders and academic institutions can ensure that researchers avoid submitting manuscripts to predatory journals or listing such publications on their CVs. Our first step is to develop a portal that presents our definition and other educational resources in multiple languages — available at https://osf.io/8xvpm — and how to get involved. Next, we will establish an international observatory to compile data on the problem, tracking numbers of publications in predatory journals by discipline and geography. We will work with funders, institutions, patients and other stakeholders to iteratively develop resources to assess journal quality. We are seeking funding to create and test a digital tool to achieve these goals. Efforts to counter predatory publishing need to be constant and adaptable. The threat is unlikely to disappear as long as universities use how many publications a scholar has produced as a criterion for graduation or career advancement. The publish-or-perish culture, a lack of awareness of predatory publishing and difficulty in discerning legitimate from illegitimate publications fosters an environment for predatory publications to exist. Predatory journals are also quick to adapt to policies and measures designed to foil them. As scientific publishers experiments with new formats and business models online, it has become increasingly easy for fake publishers to masquerade as legitimate ones. We invite others to join us in our call to action. </body>
<date id = '91'>11 December 2019</date>
<url id = '92'>https://nature.com/articles/d41586-019-03688-w</url>
<title id = '92'>Researchers from racial and ethnic groups that are under-represented in US geoscience are the least likely to be offered opportunities to speak at the field’s biggest meeting.</title>
<body id = '92'>Some scientists opt to present posters, others are assigned them instead of being asked to talk.Credit: AGU/EPNAC Biases — structural, implicit and explicit — exclude many people from science, technology, engineering and mathematics (STEM) education and employment, and devalue their contributions1,2. Most studies focus on bias against women. Few data sets offer enough generalizability or statistical power to evaluate the representation of minority ethnic and racial groups, or to examine intersectionality3. The latter describes the interwoven forms of discrimination that affect a person from multiple marginalized groups (such as racism, sexism, classism or ageism), locate them in systems of oppression and limit their upward mobility — as might be experienced by a young African American woman in science in the United States. We offer just such a data set here. Presenting at scientific conferences is key to academic career progression. Scientists don’t just communicate results; they also develop relationships with collaborators and mentors, and identify job and funding opportunities. Giving a talk confers recognition and prestige, particularly for students and early-career researchers. Despite historical inequities, women are now presenting more at conferences4,5 and colloquia6. These gains are especially visible at conferences that are organized by women or that specifically support early-career participants. We found that US scientists from minority racial and ethnic populations already under-represented in science had relatively fewer speaking opportunities at a key scientific conference over a four-year period than their proportion in the sample would predict; the imbalance was most severe for women (see 'Fewest chances'). This disadvantage for under-represented minority groups held across career stage (see ‘Who gets the microphone?’). Our results underscore the pressing need to support minority groups at conferences — as elsewhere in STEM — to advance equity and improve research. Source: Am. Geophys. Union (Data); H. L. Ford, C. Brick et al. (Analysis) The American Geophysical Union (AGU) is an international non-profit scientific association with around 60,000 members in 137 countries. Since 2013, the AGU has collected self-reported demographic data from its membership, including gender, race or ethnicity (for US-based academics only), career stage and birth year. The AGU Fall Meeting is the world’s largest Earth- and space-science conference. The attendance each year from 2014 to 2017 was approximately 24,000–28,000 people. Around 22,000 abstracts are submitted for selection as talks or posters each year; few are rejected (<0.05%). Membership is necessary for submitting, although not for attending the meeting. Source: Am. Geophys. Union (Data); H. L. Ford, C. Brick et al. (Analysis) Abstracts are submitted to topical sessions. Sessions are proposed and organized, and abstracts vetted, by a group of conveners — academics, industry members, government scientists and others. The primary convener must be an AGU member. There are three tracks by which geoscientists get to present at the meeting — two by submission, one by invitation. Authors can submit abstracts to conveners, who decide which will become talks and which posters; or authors can submit abstracts just to give a poster. In addition, session conveners directly invite scientists to speak (strictly, to send in abstracts, which generally results in a talk). The database of 87,544 accepted abstracts from the meetings between 2014 and 2017 offers a unique opportunity to probe inequities of opportunity between demographic groups5. Presentations are approximately 34% talks (about one-third of which are directly invited) and 66% posters. Of US-based authors, 98% (n = 53,247) provided career information. Researchers had verified themselves as students (undergraduates and graduates) or the AGU had calculated career stage from years since highest degree obtained: early career (0–10 years); mid-career (10–25 years); and experienced (late career; more than 25 years). Controlling for career stage is crucial because minority racial and ethnic groups are concentrated in the student and early-career stages (see ‘Submissions: Fewer seniors’). This is due to both a leaky pipeline in education and professional advancement7 and the fact that senior groups more strongly bear the imprint of historical biases. Conferences such as the American Geophysical Union’s Fall Meeting offer opportunities to network.Credit: AGU/EPNAC The AGU recorded self-reported ethnicity and race from US-based authors only (n = 54,446). Of these, 71% (n = 38,768) reported a category (defined as per the US census, see Supplementary information): White (58%), Asian American (7.3%), Hispanic/Latino (3.9%), African American (1.1%), Native American (0.3%) or Pacific Islander (0.2%). The remainder marked ‘other’ (13%) or ‘prefer not to answer’ (13%), or didn’t respond (2.8%). We did not verify whether Native American respondents were citizens of tribal nations; we acknowledge that self-reported identity is not the same as tribal citizenship. ‘Other’ could refer to individuals who are multiracial or who do not identify with the categories listed. Before analysis, we decided to exclude authors who were based outside the United States (n = 33,098), who identified as ‘other’ or who did not report ethnicity or race. Of our sample of US-based authors who reported their race and ethnicity, more than 99% (n = 38,716) identified as female or male (the third option was ‘prefer not to answer’). We appreciate that this binary treatment does not incorporate the full spectrum of gender and sexual identity. Minority ethnic and racial groups make up 31% of the US population8. People from these groups are under-represented in the STEM workforce (11%), and specifically in the physical sciences, at 9%9. In the AGU abstracts data set, African American, Hispanic/Latino, Native American and Pacific Islander comprise 7.7% of the first-author abstracts in the analysed sample. We combined them into one measure — under-represented minority groups (URMs). We did so to increase the statistical power to detect differences, to limit the risk of multiple comparisons generating false positives and to avoid including potentially identifying information for people from rare groups. We admit that this approach erases meaningful differences in lived experiences between people in these groups, particularly those with the lowest representation. Scientists from each minority group have unique barriers to participation. We combined the groups White and Asian American into non-URM. We did so because Asian Americans (4.8% of the US population8) are well represented in the STEM workforce (20.6%), in physical sciences (17.5%)9 and in the analysed sample (10.2% of first-author abstracts). We appreciate that this bracketing, too, is suboptimal — it also erases many meaningful differences, pressingly that Asian American researchers do face career barriers, including implicit and explicit biases10,11 (see Supplementary information). Our analyses focus on the chances of scientists from minority racial and ethnic groups that are under-represented in Earth and space sciences being given speaking opportunities, compared with other applicants. The key proportions are normalized relative to the population of each group, so that the results indicate representation (see Supplementary information for all inferential statistics). First authors from under-represented minority groups contributed 7.7% of all the abstracts in the sample (n = 2,981; see ‘Submissions: Fewer abstracts’). The URM applicants were disproportionately students or early-career scientists (79% compared with 59% of non-URM authors; see ‘Submissions: Fewer seniors’). At some career stages, the small number of URM researchers sometimes led to low statistical power to detect differences. URM authors were invited to give talks less often than were other authors (8% versus 14%, normalized; see ‘Opportunities: Too few talks’). Crucially, this was statistically significant in the early-career stage (and overall). Source: Am. Geophys. Union (Data); H. L. Ford, C. Brick et al. (Analysis) From talk-or-poster submissions, URM authors were assigned talks less frequently than were other scientists (42.9% versus 50.8% normalized in each population; see ‘Opportunities: Too few talks’). Again, this difference was statistically significant in the crucial early-career stage as well as overall. Compared to others, URM authors were more likely to apply to give only a poster (35% versus 24%; see ‘Opportunities: Too few talks’). This was significant overall and for each career stage. Female URM authors had strikingly few opportunities at the AGU Fall Meetings. They had even less chance of being invited to talk (and applied for posters more often) than had URM men (and non-URM women), and were assigned talks less often than were non-URM women (see ‘Fewest chances’). This is despite the fact that women (taking all races and ethnicities together) had equal or more opportunities to speak than men had (see ‘Equity — why so slow?’)5. To sum up, scientists from under-represented racial and ethnic minority groups had the smallest chances of being selected and invited to speak, and opted for poster presentations more often than did their peers. In the United States, affirmative action is a set of laws, guidelines and policies that aim to increase the representation of historically excluded groups in higher education and professional careers. Overall, White women have been the primary beneficiaries17, as our results underscore. A report last year by the US National Science Foundation showed that minority ethnic and racial groups are under-represented in graduate programmes, and that this results in reduced economic and social opportunities16. An inclusive environment, visible role models and adequate funding are key to enabling people from under-represented minority groups to participate and succeed in science, technology, engineering and mathematics (STEM)18. A growing body of research has highlighted the subtle, indirect and often unintentional actions perpetrated against such researchers by majority groups, and which have an impact on a sense of belonging in STEM spaces19–21, as well as on career persistence and well-being22,23. Small interventions can help, such as asking STEM community members to be mindful of equity, diversity and inclusion. Reminding individuals, particularly men, to consider diversity when selecting potential reviewers can improve gender representation24. However, the effects of these reminders on ethnicity bias have not been studied, and reminders might not be effective in the long term in reducing implicit biases in STEM25. Implicit-bias training is well-meaning but largely ineffective26,27. We did not assess abstract quality. An alternative explanation for our results could be that URM scientists submitted abstracts of lower quality. Even if the AGU’s selection were perfectly meritocratic, any gap in abstract quality would still, in our view, suggest bias in the STEM pipeline — for example, as a result of discrimination in earlier education7 and career development. These obstacles result in fewer URM scientists than scientists from other groups holding positions at elite institutions that provide excellent resources and strong collaborators. Because of small sample sizes, it was not possible to control for career stage when we analysed by gender (see ‘Fewest chances’). We did not investigate why URM geoscientists applied to give only a poster more often than did others overall, and at every career stage. There could be several reasons. People might be held back by psychological factors such as lower self-confidence12. For example, people from under-represented minority groups often report ‘impostor syndrome’ — feeling isolated and vulnerable in academia because they perceive themselves as having lower competence than their peers11. Or, some URM scientists might value poster presentations — this format could align with different goals, interests or lived experiences, for example by enabling researchers to communicate findings in one-on-one conversations. Because we left out of our analysis those based outside the United States, those who identified as ‘other’ and those who did not report ethnicity or race, our results will probably have excluded relevant individuals — people who identify as multiracial, for example. Our main analyses therefore represent a conservative test of speaking opportunities between minority and majority groups. Notably, combining Asian Americans with under-represented minority groups would have yielded figures that, at face value, looked more representative. We did not do this because the US National Science Foundation (NSF) does not include Asian Americans as an under-represented group in STEM; its policy efforts are focused on the under-represented minorities we track here. In the Supplementary information, we report separate exploratory analyses concerning Asian Americans, and examine career stage further, because of geoscience-specific nuances in the recruitment and representation of people who identify thus10. We must also point out that other nations might apply different census definitions to those used here. For example, ‘White’ in the United States encompasses people who have origins in the Middle East or North Africa. To recap: a woman starting out in her career from a racial or ethnic minority group that is under-represented in US geoscience is less likely to gain a speaking slot at the field’s largest conference than are her male peers and her non-Hispanic White peers of both sexes. These findings hold sobering lessons for the AGU and other STEM conferences and activities. We pre-registered our data cleaning and main confirmatory analyses at the Open Science Framework to increase generalizability (see Supplementary information). One of the AGU’s goals for inviting speakers is to “enhance diversity and/or feature early-career scientists”. It is particularly concerning that where URM authors are most numerous — in the least-established career stages — they get fewer invitations than their proportion would predict. Such early inequities are likely to affect the retention and promotion of people from under-represented minority groups across geoscience. There are three clear steps for the AGU to take. First, conference conveners should be blinded to information that is not necessary to rate the quality of submissions. Identifying details such as names and institutions introduce bias13,14 even in people committed to equity, because many thinking processes, such as stereotype activation, occur outside awareness or control. Double-blind review has decreased bias in allocating time on the Hubble Space Telescope15. Second, the AGU should encourage more scholars from under-represented minority groups to participate as conveners. Third, the AGU should provide more travel grants to URM presenters, which could increase the overall population of URM attendees both directly and by shifting norms. We encourage other STEM conferences to make these changes. Meanwhile, the rest of the community has work to do to (see ‘Equity — why so slow?’). Established scholars can support scientists from minority groups by encouraging them to submit talk abstracts and by providing opportunities to practise presenting in local, domestic and international venues. These steps can increase confidence and foster the development of people’s identity as scientists. It is crucial for universities and funding agencies to support organizations that provide openings and mentorship to young scholars from minority groups, such as the Society for Advancement of Chicanos/Hispanics and Native Americans in Science. The NSF aims to broaden participation in STEM through its criteria for grant proposals and through initiatives such as NSF INCLUDES (Inclusion across the Nation of Communities of Learners of Underrepresented Discoverers in Engineering and Science)16. Such programmes can liaise with professional societies. Racial, ethnic and gender biases harm individuals and undermine the quality of science. Even if all demographic gaps were plugged tomorrow at the level of people graduating with PhDs, and even if these graduates did not have to run the gauntlet of systematic bias that their predecessors faced, it could still take generations to achieve fair representation among senior academics. We therefore urge more organizations to measure and share the outcomes for scholars from minority groups. With this information and the growing literature on effective interventions, together we can create a more equitable scientific community. </body>
<date id = '92'>04 December 2019</date>
<url id = '93'>https://nature.com/articles/d41586-019-03687-x</url>
<title id = '93'>Corporations selling DNA-profiling technology are aiding human-rights abuses. Governments, legislators, researchers, reviewers and publishers must act.</title>
<body id = '93'>Across the world, DNA databases that could be used for state-level surveillance are steadily growing. The most striking case is in China. Here police are using a national DNA database along with other kinds of surveillance data, such as from video cameras and facial scanners, to monitor the minority Muslim Uyghur population in the western province of Xinjiang. Concerns about the potential downsides of governments being able to interrogate people’s DNA have been voiced since the early 2000s1 by activist groups, such as the non-profit organization GeneWatch UK, and some geneticists (myself included). Partly thanks to such debate, legislation and best practices have emerged in many countries around the use of DNA profiling in law enforcement2. (In profiling, several regions across the genome, each consisting of tens of nucleotides, are sequenced to identify a person or their relatives.) Now the stakes are higher for two reasons. First, as technology gets cheaper, many countries might want to build massive DNA databases. Second, DNA-profiling technology can be used in conjunction with other tools for biometric identification — and alongside the analysis of many other types of personal data, including an individual’s posting behaviour on social networks. Last year, the Chinese firm Forensic Genomics International (FGI) announced that it was storing the DNA profiles of more than 100,000 people from across China (FGI, known as Shenzhen Huada Forensic Technology in China, is a subsidiary of the BGI, the world’s largest genome-research organization). It made the information available to the individuals through WeChat, China’s equivalent of WhatsApp, using an app accessed by facial recognition. Migrants from Central America, including a pregnant woman and children, being checked by a US border-patrol agent in Granjeno, Texas, in 2018.Credit: Adrees Latif/Reuters With stringent safeguards and oversight, it is legitimate for law-enforcement agencies to use DNA-profiling technology. But these uses can easily creep towards human-rights abuses. In October this year, the US Department of Homeland Security announced that it would authorize the mandatory collection of DNA samples from immigrants in federal custody at the US border, including children and those applying for asylum at legal ports of entry. The resulting DNA profiles will be available through a database called CODIS (Combined DNA Index System), which includes the profiles of convicted offenders and individuals arrested for serious offences. Such treatment could reinforce debunked claims that immigrants are more prone to criminal behaviour than the general population. A much broader array of stakeholders must engage with the problems that DNA databases present. In particular, governments, policymakers and legislators should tighten regulation and reduce the likelihood of corporations aiding potential human-rights abuses by selling DNA-profiling technology to bad actors — knowingly or negligently. Researchers working on biometric identification technologies should consider more deeply how their inventions could be used. And editors, reviewers and publishers must do more to ensure that published research on biometric identification has been done in an ethical way. In Xinjiang in China, police collected biometric information (including blood samples, fingerprints and eye scans) from nearly 19 million people in 2017, in a programme called ‘Physicals for All’. This was part of a suite of measures that are being used by the Chinese government to control the Uyghur ethnic group3. Other nations are building massive DNA databases or considering doing so. In 2015, Kuwait passed a law mandating DNA profiling of its entire population. Foreigners living in Kuwait and even visitors were to be included. In January this year, Kenya passed a law that would have enabled the government to require all citizens to submit any biometric information, including DNA profiles, to a national database. Both cases have hit obstacles. Kuwait’s Constitutional Court overruled the 2015 law two years later, because of concerns about how the database could be used in violations of privacy and due process. And, thanks to a decision taken by Kenya’s High Court in April, DNA is now excluded from national efforts to collect biometric data. But these and other examples indicate that governments keep being tempted to hoover up their citizens’ DNA data4. One way to reduce the likelihood of massive DNA databases being misused is to change the behaviour of the companies that invest in DNA-profiling technologies (see ‘Ethical divesting’). Investors could help to ensure ethical use of the products of DNA profiling firms. Public outcry can lead to divestment. Since March this year, for example, major US funds such as Goldman Sachs have divested all their shares from the Chinese surveillance company Hikvision, because of concerns about the use of the company’s products in human-rights breaches. Investors could even be motivated to scrutinize company ethics, thanks to studies over the past five years or so indicating that ‘good’ corporate social responsibility practices tend to correlate with better financial performance over the long term. Pressure from investors — and the public in general — might be increasingly powerful. Take Thermo Fisher Scientific’s February announcement that it would stop selling its DNA profiling technology in Xinjiang, China. Although Chinese authorities can easily transport such technology from elsewhere in the country, it is significant that a major corporation publicly acknowledged “the importance of considering how [its] products and services are used — or may be used — by [its] customers”. US and European corporations are still the dominant providers of such technologies. The deployment of DNA-surveillance infrastructure in Xinjiang, for example, was enabled by the Chinese government buying products from — and working with — the US company Thermo Fisher Scientific in Waltham, Massachusetts. The firm is currently the global leading supplier of DNA-profiling technology in law enforcement. Thermo Fisher Scientific researchers have worked with China’s Ministry of Justice, and with researchers at the People’s Public Security University of China, which falls directly under the Ministry of Public Security, to tailor the technology specifically for use in Tibetan and Uyghur populations5. (Thermo Fisher Scientific did not respond to a request for comment). However, in February, after two years of public outcry and intense pressure from high-profile US senators, the company announced that it would stop selling its DNA-profiling technology in Xinjiang. Marketing and lobbying by technology suppliers is often behind pushes for the broadest possible use of DNA profiling. In 2016, for instance, a representative of a US lobbying firm working for Thermo Fisher Scientific described in a conference presentation the development of universal DNA databases as “inevitable”. He noted that the expansion of these to “Western countries or other countries with democratic forms of government” faced “significant hurdles”, such as the “open and public parliamentary process” and the “culture of being influenced by opposition and protests” (see go.nature.com/337pjce). Restrictions on the use of technologies or services provided by corporations are currently too weak. Take export controls: either they do not pay due attention to these sensitive technologies, or they have loopholes that often render them useless. For example, US laws forbid the export of fingerprint-recognition technology to some destinations or users deemed problematic by the US government, such as the Chinese police. But the United States does not restrict the export of more-invasive DNA-profiling and facial-recognition technologies. Meanwhile, the European Union does not regulate the export of fingerprint technology, even though the dominant global suppliers are European. Export controls for biometric technologies could be improved relatively easily. The US Department of Commerce is currently considering revising regulations for emerging technologies6, such as Internet censorship and video surveillance, to try to reduce the likelihood of companies doing business with problematic buyers. Last month, it barred Xinjiang police forces and eight Chinese technology companies from buying US products or importing US technology because of their role in the repression of Uyghurs. Some regulatory initiatives are promising and could provide a deterrent if enforced. The 2017 EU directive on non-financial reporting (named 2014/95) has mandated that large companies listed on stock markets document their social and environmental impacts in their annual reports for shareholders and the public. Since 2017, France’s corporate ‘duty of vigilance’ law has required all French companies employing more than 5,000 people in the country to actively monitor their impacts on human rights, the environment and so on (see go.nature.com/2o8tcvn). In the United States, several human-rights lawyers have attempted to revive the Alien Tort Statute (28 U.S.C. § 1350) over the past 20 years. Produced in 1789 but never deployed, this law could enable a foreign individual to make a civil liability claim against a domestic corporation in US courts. A carefully crafted Alien Tort Statute could provide a way to hold companies to the same standards, whether they are operating at home or abroad. Ultimately, international laws must be established that clearly stipulate the human-rights responsibilities of corporations. For the past decade, a United Nations working group has been drafting a treaty to regulate the activities of transnational corporations with regards to human rights and the environment (see go.nature.com/35qnehe). If it is not crippled by lobbying, this could eventually become a powerful tool to promote ethical business practices. Yet companies are only part of the story when it comes to the potential misuse of DNA databases. The chain of technology development leads from fundamental to applied research to the products that enable the abuses. More academics working on biometric identification technology should reflect on the potential misuses of their inventions and engage with society. For instance they can contribute to mainstream media, participate in public debates or join ethics boards. Recent events indicate that publishers and scholars might be paying insufficient attention to the sources of biometric-identification research. For example, in August last year, after several Human Rights Watch and media reports about the surveillance abuses in Xinjiang, Springer Nature published the proceedings of a biometrics conference held in the province. (Springer Nature has been the publisher of the proceedings of the Chinese Conference on Biometric Recognition for nine years; Nature is editorially independent of its publisher.) One of the conference papers, on technologies for recognizing various languages in images, described how “Uyghur information” (referring to the Uyghur language script) could be detected in images that might be used to evade Internet censorship7. Another paper described how products from Thermo Fisher Scientific and the Chinese firms Hisign, Megvii and iFlytek are being used to build a population-scale database for DNA, fingerprint, face and voice information in a major Chinese city8. In July this year, researchers from Imperial College London announced the results of an open competition on facial recognition. (The winners presented their work at a conference in Seoul in October.) Before a reporter from the non-profit news platform Coda pointed it out, one of the sponsors of the conference had been a Chinese artificial-intelligence start-up called DeepGlint, which in 2018 set up a joint research laboratory with the Xinjiang police. The conference organizers removed DeepGlint as a sponsor in August. Over the past eight years, three leading forensic genetics journals — International Journal of Legal Medicine (published by Springer Nature), and Forensic Science International and Forensic Science International: Genetics Supplement Series (both published by Elsevier) — have published 40 articles co-authored by members of the Chinese police that describe the DNA profiling of Tibetans and Muslim minorities, including people from Xinjiang. I analysed 529 articles on forensic population genetics in Chinese populations, published between 2011 and 2018 in these journals and others. By my count, Uyghurs and Tibetans are 30–40 times more frequently studied than are people from Han communities, relative to the size of their populations (unpublished data). Half of the studies in my analysis had authors from the police force, military or judiciary. The involvement of such interests should raise red flags to reviewers and editors. In short, the scientific community in general — and publishers in particular — need to unequivocally affirm that the Declaration of Helsinki (a set of ethical principles regarding human experimentation, developed for the medical community) applies to all biometric identification research (see go.nature.com/34bypbf). Unethical work that has been published in this terrain must be retracted. DNA databases in local police forces are proliferating, even in countries that have democratic governments and well-established legal protections for citizens’ privacy9. By August this year, for instance, the Office of the Chief Medical Examiner of New York City held more than 82,000 genetic profiles. At the same time, there has been a growth in consumer and recreational genomic services, such as the US corporations 23andMe in Mountain View, California, and Ancestry in Lehi, Utah (see ‘DNA testing for all’). Medical DNA sequencing is also becoming routine10. Source: MIT Tech. Rev. 2019 (https://go.nature.com/2MHTJED) Currently, only some consumer-genomics companies have willingly shared people’s DNA data with law-enforcement agencies. And in many countries, patients’ data are confidential. But to deploy DNA surveillance across a group of people, you need profiles from only 2–5% of that population, because biological relationships can be inferred11,12. And as genealogy and medical databases mushroom, law enforcers and others are increasingly tempted to tap into them13. In 2017 in the Netherlands, the Ministry of Health drafted a bill that would have allowed police to obtain people’s DNA information from hospitals in some limited cases. It was abandoned following public outcry. And June saw what might be a game changer in the United States. The Orlando Police Department obtained a warrant that allowed it to search the entire DNA database of the GEDMatch genealogy website, based in Lake Worth, Florida. Because consumer-genomics companies already hold DNA data for an estimated 5% of the US population, unfettered access to these data by law-enforcement agencies would simply spell the end of genetic privacy in the United States. All of us must beware a world in which our behavioural, financial and biometric data, including our DNA profiles, or even entire genome sequences, are available to corporations — and so potentially to law enforcers and political parties. Without the changes outlined here, the use of DNA for state-level surveillance could become the norm in many countries. </body>
<date id = '93'>03 December 2019</date>
<url id = '94'>https://nature.com/articles/d41586-019-03595-0</url>
<title id = '94'>The growing threat of abrupt and irreversible climate changes must compel political and economic action on emissions.</title>
<body id = '94'>Politicians, economists and even some natural scientists have tended to assume that tipping points1 in the Earth system — such as the loss of the Amazon rainforest or the West Antarctic ice sheet — are of low probability and little understood. Yet evidence is mounting that these events could be more likely than was thought, have high impacts and are interconnected across different biophysical systems, potentially committing the world to long-term irreversible changes. Here we summarize evidence on the threat of exceeding tipping points, identify knowledge gaps and suggest how these should be plugged. We explore the effects of such large-scale changes, how quickly they might unfold and whether we still have any control over them. In our view, the consideration of tipping points helps to define that we are in a climate emergency and strengthens this year’s chorus of calls for urgent climate action — from schoolchildren to scientists, cities and countries. The Intergovernmental Panel on Climate Change (IPCC) introduced the idea of tipping points two decades ago. At that time, these ‘large-scale discontinuities’ in the climate system were considered likely only if global warming exceeded 5 °C above pre-industrial levels. Information summarized in the two most recent IPCC Special Reports (published in 2018 and in September this year)2,3 suggests that tipping points could be exceeded even between 1 and 2 °C of warming (see ‘Too close for comfort’). Source: IPCC and J. B. Smith et al. Proc. Natl Acad. Sci. USA 106, 4133-4137 (2009) If current national pledges to reduce greenhouse-gas emissions are implemented — and that’s a big ‘if’ — they are likely to result in at least 3 °C of global warming. This is despite the goal of the 2015 Paris agreement to limit warming to well below 2 °C. Some economists, assuming that climate tipping points are of very low probability (even if they would be catastrophic), have suggested that 3 °C warming is optimal from a cost–benefit perspective. However, if tipping points are looking more likely, then the ‘optimal policy’ recommendation of simple cost–benefit climate-economy models4 aligns with those of the recent IPCC report2. In other words, warming must be limited to 1.5 °C. This requires an emergency response. We think that several cryosphere tipping points are dangerously close, but mitigating greenhouse-gas emissions could still slow down the inevitable accumulation of impacts and help us to adapt. Research in the past decade has shown that the Amundsen Sea embayment of West Antarctica might have passed a tipping point3: the ‘grounding line’ where ice, ocean and bedrock meet is retreating irreversibly. A model study shows5 that when this sector collapses, it could destabilize the rest of the West Antarctic ice sheet like toppling dominoes — leading to about 3 metres of sea-level rise on a timescale of centuries to millennia. Palaeo-evidence shows that such widespread collapse of the West Antarctic ice sheet has occurred repeatedly in the past. The latest data show that part of the East Antarctic ice sheet — the Wilkes Basin — might be similarly unstable3. Modelling work suggests that it could add another 3–4 m to sea level on timescales beyond a century. The Greenland ice sheet is melting at an accelerating rate3. It could add a further 7 m to sea level over thousands of years if it passes a particular threshold. Beyond that, as the elevation of the ice sheet lowers, it melts further, exposing the surface to ever-warmer air. Models suggest that the Greenland ice sheet could be doomed at 1.5 °C of warming3, which could happen as soon as 2030. Thus, we might already have committed future generations to living with sea-level rises of around 10 m over thousands of years3. But that timescale is still under our control. The rate of melting depends on the magnitude of warming above the tipping point. At 1.5 °C, it could take 10,000 years to unfold3; above 2 °C it could take less than 1,000 years6. Researchers need more observational data to establish whether ice sheets are reaching a tipping point, and require better models constrained by past and present data to resolve how soon and how fast the ice sheets could collapse. Whatever those data show, action must be taken to slow sea-level rise. This will aid adaptation, including the eventual resettling of large, low-lying population centres. A further key impetus to limit warming to 1.5 °C is that other tipping points could be triggered at low levels of global warming. The latest IPCC models projected a cluster of abrupt shifts7 between 1.5 °C and 2 °C, several of which involve sea ice. This ice is already shrinking rapidly in the Arctic, indicating that, at 2 °C of warming, the region has a 10–35% chance3 of becoming largely ice-free in summer. Climate change and other human activities risk triggering biosphere tipping points across a range of ecosystems and scales (see ‘Raising the alarm’). Source: T. M. Lenton et al. Ocean heatwaves have led to mass coral bleaching and to the loss of half of the shallow-water corals on Australia’s Great Barrier Reef. A staggering 99% of tropical corals are projected2 to be lost if global average temperature rises by 2 °C, owing to interactions between warming, ocean acidification and pollution. This would represent a profound loss of marine biodiversity and human livelihoods. As well as undermining our life-support system, biosphere tipping points can trigger abrupt carbon release back to the atmosphere. This can amplify climate change and reduce remaining emission budgets. Deforestation and climate change are destabilizing the Amazon — the world’s largest rainforest, which is home to one in ten known species. Estimates of where an Amazon tipping point could lie range from 40% deforestation to just 20% forest-cover loss8. About 17% has been lost since 1970. The rate of deforestation varies with changes in policy. Finding the tipping point requires models that include deforestation and climate change as interacting drivers, and that incorporate fire and climate feedbacks as interacting tipping mechanisms across scales. With the Arctic warming at least twice as quickly as the global average, the boreal forest in the subarctic is increasingly vulnerable. Already, warming has triggered large-scale insect disturbances and an increase in fires that have led to dieback of North American boreal forests, potentially turning some regions from a carbon sink to a carbon source9. Permafrost across the Arctic is beginning to irreversibly thaw and release carbon dioxide and methane — a greenhouse gas that is around 30 times more potent than CO2 over a 100-year period. Researchers need to improve their understanding of these observed changes in major ecosystems, as well as where future tipping points might lie. Existing carbon stores and potential releases of CO2 and methane need better quantification. The world’s remaining emissions budget for a 50:50 chance of staying within 1.5 °C of warming is only about 500 gigatonnes (Gt) of CO2. Permafrost emissions could take an estimated 20% (100 Gt CO2) off this budget10, and that’s without including methane from deep permafrost or undersea hydrates. If forests are close to tipping points, Amazon dieback could release another 90 Gt CO2 and boreal forests a further 110 Gt CO211. With global total CO2 emissions still at more than 40 Gt per year, the remaining budget could be all but erased already. Bleached corals on a reef near the island of Moorea in French Polynesia in the South Pacific.Credit: Alexis Rosenfeld/Getty In our view, the clearest emergency would be if we were approaching a global cascade of tipping points that led to a new, less habitable, ‘hothouse’ climate state11. Interactions could happen through ocean and atmospheric circulation or through feedbacks that increase greenhouse-gas levels and global temperature. Alternatively, strong cloud feedbacks could cause a global tipping point12,13. We argue that cascading effects might be common. Research last year14 analysed 30 types of regime shift spanning physical climate and ecological systems, from collapse of the West Antarctic ice sheet to a switch from rainforest to savanna. This indicated that exceeding tipping points in one system can increase the risk of crossing them in others. Such links were found for 45% of possible interactions14. In our view, examples are starting to be observed. For example, Arctic sea-ice loss is amplifying regional warming, and Arctic warming and Greenland melting are driving an influx of fresh water into the North Atlantic. This could have contributed to a 15% slowdown15 since the mid-twentieth century of the Atlantic Meridional Overturning Circulation (AMOC) , a key part of global heat and salt transport by the ocean3. Rapid melting of the Greenland ice sheet and further slowdown of the AMOC could destabilize the West African monsoon, triggering drought in Africa’s Sahel region. A slowdown in the AMOC could also dry the Amazon, disrupt the East Asian monsoon and cause heat to build up in the Southern Ocean, which could accelerate Antarctic ice loss. The palaeo-record shows global tipping, such as the entry into ice-age cycles 2.6 million years ago and their switch in amplitude and frequency around one million years ago, which models are only just capable of simulating. Regional tipping occurred repeatedly within and at the end of the last ice age, between 80,000 and 10,000 years ago (the Dansgaard–Oeschger and Heinrich events). Although this is not directly applicable to the present interglacial period, it highlights that the Earth system has been unstable across multiple timescales before, under relatively weak forcing caused by changes in Earth’s orbit. Now we are strongly forcing the system, with atmospheric CO2 concentration and global temperature increasing at rates that are an order of magnitude higher than those during the most recent deglaciation. Atmospheric CO2 is already at levels last seen around four million years ago, in the Pliocene epoch. It is rapidly heading towards levels last seen some 50 million years ago — in the Eocene — when temperatures were up to 14 °C higher than they were in pre-industrial times. It is challenging for climate models to simulate such past ‘hothouse’ Earth states. One possible explanation is that the models have been missing a key tipping point: a cloud-resolving model published this year suggests that the abrupt break-up of stratocumulus cloud above about 1,200 parts per million of CO2 could have resulted in roughly 8 °C of global warming12. Some early results from the latest climate models — run for the IPCC’s sixth assessment report, due in 2021 — indicate a much larger climate sensitivity (defined as the temperature response to doubling of atmospheric CO2) than in previous models. Many more results are pending and further investigation is required, but to us, these preliminary results hint that a global tipping point is possible. To address these issues, we need models that capture a richer suite of couplings and feedbacks in the Earth system, and we need more data — present and past — and better ways to use them. Improving the ability of models to capture known past abrupt climate changes and ‘hothouse’ climate states should increase confidence in their ability to forecast these. Some scientists counter that the possibility of global tipping remains highly speculative. It is our position that, given its huge impact and irreversible nature, any serious risk assessment must consider the evidence, however limited our understanding might still be. To err on the side of danger is not a responsible option. If damaging tipping cascades can occur and a global tipping point cannot be ruled out, then this is an existential threat to civilization. No amount of economic cost–benefit analysis is going to help us. We need to change our approach to the climate problem. In our view, the evidence from tipping points alone suggests that we are in a state of planetary emergency: both the risk and urgency of the situation are acute (see ‘Emergency: do the maths’). We define emergency (E) as the product of risk and urgency. Risk (R) is defined by insurers as probability (p) multiplied by damage (D). Urgency (U) is defined in emergency situations as reaction time to an alert (τ) divided by the intervention time left to avoid a bad outcome (T). Thus: E = R × U = p × D × τ / T The situation is an emergency if both risk and urgency are high. If reaction time is longer than the intervention time left (τ / T > 1), we have lost control. We argue that the intervention time left to prevent tipping could already have shrunk towards zero, whereas the reaction time to achieve net zero emissions is 30 years at best. Hence we might already have lost control of whether tipping happens. A saving grace is that the rate at which damage accumulates from tipping — and hence the risk posed — could still be under our control to some extent. The stability and resilience of our planet is in peril. International action — not just words — must reflect this. </body>
<date id = '94'>27 November 2019</date>
<url id = '95'>https://nature.com/articles/d41586-019-03613-1</url>
<title id = '95'>A swift increase in scientific productivity has outstripped the country’s ability to promote rigour and curb academic misconduct; it is time to seize solutions.</title>
<body id = '95'>How researchers in China behave has an impact on the global scientific community. With more than four million researchers, China has more science and technology personnel than any other nation. In 2008, it overtook the United Kingdom in the number of articles indexed in the Web of Science, and now ranks second in the world. In 2018, China published 412,000 papers. But China also produces a disproportionate number of faked peer reviews and plagiarized or fraudulent publications. Its share of retracted papers is around three times that expected from its scientific output (see ‘Outsized retractions’). The past few years have witnessed high-profile cases of faked peer reviews, image manipulations and authorships for sale, some involving prominent Chinese scientists. In May last year, China asked two groups to foster research integrity and manage misconduct cases: its Ministry of Science and Technology (MOST) and the Chinese Academy of Social Sciences (CASS) . In November 2018, 41 national government agencies endorsed a set of 43 penalties for major academic misconduct. These range from terminating grants to restricting academic promotion and revoking business licences. This year, the government issued a foundational document to promote the scientific enterprise and foster a culture of academic integrity1. China’s strides towards reform have been well received domestically and abroad, but effecting lasting change is hard2. To better characterize the situation, my team has studied global retraction data alongside national grants and applications that were revoked. We also surveyed researchers online and interviewed major stakeholders in China3,4. These included experts on university ethics committees, programmes for research-integrity training and plagiarism detection, as well as funding-programme managers, journal editors and academics. Here, I outline major challenges in research integrity, and potential strategies and solutions to buttress it. Source: L. Tang/Web of Science Align norms. What counts as misconduct rather than acceptable practice differs across cultural settings and disciplines. The lack of consensus over what misconduct means is a thorny challenge for an emerging scientific powerhouse. One of our interviewees noted that senior academics even disagreed over what constitutes an allegation. Any discussion about misconduct and penalties is buffeted by conflicting norms: historical versus the present, national versus international. For example, the reuse of text without proper citation is, to some degree, accepted in textbook publishing in China. Until 1999, duplicate submissions or even dual publication in Chinese and English were not considered particularly inappropriate. More than 20% of our survey respondents felt that duplicate submission and self-plagiarism were common in their domain. These are deemed misconduct in international scientific communities. That presents Chinese scientific leaders with a dilemma: if wrongdoing is not punished, the scientific community could become more tolerant, and there might be more misconduct and recidivism. That would waste public money, erode trust in science and tarnish the country’s reputation. Already, Chinese academics can find it difficult to maintain or expand international collaborations, and universities and funding agencies outside China have ethical concerns about forming partnerships. But requiring strict compliance with international norms would target a broad spectrum of misbehaviours that are common practice. And high standards with unworkable rules could legitimize non-compliance5. Either scenario could stymie reform. Optimize approaches. Research misbehaviour needs to be policed. Strategies can be classed as ‘patrols’ or ‘fire alarms’6. Like other countries, China deploys both. On the patrol side, China National Knowledge Infrastructure (CNKI), a Chinese version of the Web of Science database, provides a plagiarism-checking service to Chinese journals and universities. These have deployed CNKI software to detect plagiarized texts, including those saved as manipulated images. Since 2010, grant proposals have been checked for possible plagiarism at the National Natural Science Foundation of China (NSFC). Similarly, the National Social Science Fund of China (NSSFC) instigates systematic clean-ups for its funded projects, halting those that are left unfinished after the completion deadline (typically six years after receiving the grant). This put an end to 302 of 5,035 grants funded from 2002 to 2005. Terminated projects increased from 60 in 2002 to 99 in 2005, but have plummeted since checks were implemented and publicized in 2012 (ref. 3; see ‘Checks changed behaviour’). Source: NSSFC Patrol deters certain types of misconduct, particularly before a grant or degree is awarded or a paper accepted. But patrols require dedicated software and infrastructure, so are costly to enforce. Every May (just before graduation), college students, university faculty members and support staff spend hours checking theses for plagiarism. Perhaps that is why a fire-alarm tactic is dominant. China’s science agencies and universities often wait to act until contacted by the media, wronged parties or whistle-blowers, and they focus most on cases that grab headlines. This can be effective in the short term: in 2017, after 107 articles by Chinese authors were retracted by the journal Tumor Biology for faked peer reviews, investigations were completed within 4 months. More than 100 people were penalized and some 40 NSFC grants revoked. But the fire-alarm tactic leads to selective investigations and uncertainty. It punishes past offences, but does little to deter future ones. Empower enforcement. The burden of policing misconduct is too much for national agencies in any country, China included. That power is delegated to the universities and institutes where researchers work. But these organizations, concerned about soiling reputations and losing grant funds, are often unwilling to investigate alleged misconduct. They tend to respond only when whistles are blown. That depends on whistle-blowers who shoulder great professional and personal risk, especially in Chinese society, which values collectivism and interdependence over individualism and independence. In a 2017 survey of Chinese scholars, more than half of respondents who observed misconduct in the past three years said that they did nothing about it (unpublished results; see also Supplementary Information). Assign responsibility. Perhaps the most difficult challenge in China, as elsewhere, is whether and to what extent to hold team members accountable for misconduct in joint work. Increasing specialization and globalization has made collaborations larger and more essential. That complicates how to allocate blame as well as credit. Should each listed author be held accountable for the full work, or just for their own? Should the corresponding author take most of the responsibility for fraud and errors others committed? Although more journals are requiring detailed descriptions of authors’ contributions, discerning who should be responsible for a collaborative piece of work is difficult. This is particularly true when older articles are retracted as a result of proven fraud — often, author contributions have not been specified. The supervisor–student relationship poses a particular dilemma. In China, when PhD students are found guilty of misconduct, their supervisors are also punished. In recent scandals, plagiarists were stripped of their doctoral degrees, and their supervisors were demoted and barred from taking on PhD candidates. Alternatively, junior scientists might be punished, while senior ones responsible for misconduct retain status and position. Some argue that holding members of a research team accountable by association will improve enforcement and prevent scapegoating; others say that this shift in responsibility is unfair and burdensome. Cultivate integrity. China’s rapid research development must be brought into sync with a culture of integrity. Like other countries, it has seen that tying publication requirements to degree requirements, promotion or monetary rewards can lure researchers into inappropriate behaviour7. What is the best way to implement these strategies? I propose that working on several fronts will make each easier to accomplish. Forgive, then be tough. China’s scientific community first needs to agree on a common code of academic integrity that defines misconduct and undesirable research practices and sets out sanctions. China has a greater diversity of funders and a more mobile scientific workforce than ever before, so all stakeholders — including researchers, managers, journal editors and funding officers — must be in accord. Penalties should focus on the most egregious acts, which are universally recognized: falsification, fabrication, plagiarism and fake reviews. Researchers should be admonished for past fraud but face harsher penalties for incidents that occur once the code is in place. Less serious questionable practices that were historically accepted should be subject to a statute of limitations. Institutionalize. Integrity must be built into scientific institutions, with MOST and CASS taking the lead. CASS should set up departments to oversee misconduct cases, as MOST has. Both agencies should facilitate communication between all stakeholders and coordinate input from research societies to formulate workable rules that are compatible with international norms. Transparency will help. Funding agencies should, for example, publicize the claimed achievements and promised research outputs of award recipients in prestigious talent programmes. This accountability will deter fraud and false advertising. China’s General Administration of Press and Publications can help by urging Chinese publishers and database providers to take a proactive stance. For instance, Chinese journals often simply remove retracted articles from their collections and the CNKI database. Instead, journals should explicitly mark articles as retracted, as many Western journals do8. They should also share their ‘blacklists’ of authors who have repeatedly been found guilty of duplicate submissions. With the right support, universities and research institutes can be best placed to initiate misconduct investigations. MOST and CASS should help them set up procedures. These should include appointing an independent ombudsperson to protect whistle-blowers and those accused of misconduct, for example by developing strategies to prevent cyberbullying and smear campaigns. In addition, each university should employ a professional chief integrity officer — not a faculty ‘volunteer’ — who reports directly to a vice-president. Incentivize. Administrative agencies must explicitly link support for a university to whether it vigorously investigates misconduct allegations and promotes integrity education, including putting dedicated professionals in place. Agencies can also set up open, regular communication about reform with junior and senior researchers — for real-world input and to allow institutions to learn from each other. Educate. A healthy academic atmosphere cannot be built on penalties for misbehaviour alone. Universities could set up research-integrity help desks and hotlines, making contact information and investigation procedures accessible. The Chinese university code of academic integrity should be linked from every course syllabus. Teachers should have access to plagiarism-checking software and to training so they can understand its shortcomings. More broadly, universities must work out how to provide effective integrity education. Training upstream is always better than disciplining transgressors after the fact (see also go.nature.com/2rpdhkv). Many Chinese universities now require graduate students to take responsible-conduct courses. Around three-quarters of our survey respondents said they had received training in research ethics and integrity. Those enrolling for a PhD at Fudan University in Shanghai, for example, must attend mandatory ethics modules. Only those who pass the ethics quiz can register for further coursework. Such training needs to be universal across Chinese institutions, and at all levels: for faculty members, technicians and non-scientific staff. Principal investigators who coordinate collaborations, as well as young researchers who collect, check and validate data, must know and accept their responsibilities4. ‘Trust and verify’ should be bywords for all. For example, at least two team members should collect and code raw data and record source links and detailed procedures. Pre-registration of analysis plans could also prevent tampering9. Study. Also needed is rigorous research on what kind of institutional structures and programmes foster integrity, which types of training effect the most lasting change, and how to apply best practice. Comparative studies could provide lessons from other countries that have experience in combating academic misconduct and cultivating integrity. For example, in 2014, Denmark adopted a new code of conduct for research integrity as a result of orchestrated efforts by researchers, funding agencies and other stakeholders. The Netherlands followed suit in 2018. Indian efforts against predatory publishing could be adapted for China, as could the long-established US emphasis on quality rather than quantity in research evaluation. To gather this knowledge, oversight agencies should have an open-door policy for stakeholders to express constructive and diverse opinions. Proceedings of misconduct investigations should be made public, not be shrouded in secrecy10. Funding agencies need to earmark money for research-integrity studies to attract bright minds to the field. This year, the NSFC issued an open call for proposals on research integrity and ethics; it is unclear whether such funds will be available in future. China must curb misconduct and foster integrity if it is to realize the central government’s ambition of “world-class universities, world-class disciplines”. It is still too early to anticipate all the changes reforms will bring, but the government has signalled its determination to act. We might see more investigations of misconduct because of closer scrutiny in the next couple of years. Improving the research practices of Chinese scholars will boost innovation and development everywhere. </body>
<date id = '95'>26 November 2019</date>
<url id = '96'>https://nature.com/articles/d41586-019-03534-z</url>
<title id = '96'>Study the effects of earthquakes, floods and other natural hazards with sensitivity to ethical dilemmas and power imbalances.</title>
<body id = '96'>A magnitude-7.0 earthquake rocked Anchorage, Alaska, in late November 2018. Roads buckled and chimneys tumbled from rooftops. Business operations were disrupted. Schools were damaged across the district. This was the largest earthquake to shake the region in a generation, and there was much to learn. What was the state of the infrastructure? Might further quakes occur? How did people respond? Teams of scientists and engineers from across the United States mobilized to conduct field reconnaissance in partnership with local researchers and practitioners. These efforts were coordinated through the clearing house set up by the Earthquake Engineering Research Institute in Oakland, California, which provided daily in-person and online briefings, as well as a web portal for sharing data. But researchers are not always so welcome in disaster zones. After the deadly Indian Ocean earthquake and tsunami on 26 December 2004, hundreds of academics from countries including Japan, Russia, France and the United States rushed to the region to collect perishable data. This influx of foreign scientists angered and fatigued some locals; many declined researchers’ requests for interviews. The former governor of Aceh province, Indonesia, where more than 128,000 people died, described foreign researchers as “guerrillas applying hit-and-run tactics”1. Yet research on tsunami propagation and people’s response to the event has led to improved warnings and emergency-response plans. When, on 28 September 2018, an earthquake and tsunami hit the Indonesian island of Sulawesi, dozens of researchers found themselves unable to enter the country2. Indonesian law now requires foreign scientists to obtain a special visa before they can begin research. Data-collection protocols must be submitted to the government in advance and projects must have an Indonesian partner. Violators could face criminal charges and even prison. This incident has inflamed a smouldering debate among disaster researchers. Some scholars argue that stringent administrative protocols violate researchers’ rights and prevent the collection of crucial, potentially life-saving, data3. Others counter that such procedures protect survivors and preserve the integrity of local scientific efforts. For instance, concerns over studies placing undue burdens on overwhelmed groups — including grieving schoolchildren — led New Zealand to impose a moratorium on social-science research after the 2011 Christchurch earthquake4. Here we argue that disaster research needs a culture shift. As in other branches of study involving human participants, ethical concerns should have the same primacy as research questions5. We call on the United Nations Office for Disaster Risk Reduction (UNDRR) to put forward a researcher-driven ethical code of conduct. This should advance disaster research, making it scientifically rigorous as well as locally and culturally grounded. After all, the UNDRR has a mandate “to ensure synergies among … regional organizations and activities in socio-economic and humanitarian fields”. Researchers working in disaster zones, with people whose culture might be different from their own, need to know how to interact with survivors as well as local officials and scholars, without adding to those people’s problems. There is no universal definition of ethical behaviour, and only a handful of countries have ethically informed guidelines for post-disaster research. In New Zealand, guiding principles from the Natural Hazards Research Platform advise that researchers must “avoid creating unnecessary anxiety by speculating to locals”. The Philippines allows research on the trauma caused by disasters only in exceptional cases, such as when affected people want to share their feelings as a way to process the event. Brazil, like Indonesia, requires all researchers working in the country to have a special visa and an established local connection. University ethics committees and national ethical review boards are unable to fill the gap. They tend to focus on studies in medicine and social sciences that involve human participants. They have little to say on how to investigate a collapsed building or a compromised coastal landscape. Yet studies by engineers or natural scientists have participants, too: local residents, scholars, guides and interpreters. Tsunami researchers might need to ask coastal dwellers about the height of waves; structural engineers assessing a collapsed stairwell might question the building’s occupants about how they escaped. Researchers equipped with an ‘ethical toolkit’ are better able to help affected populations6 without causing harm. Following the earthquake that struck Luzon island in the Philippines in April this year, research was coordinated by academics based in nearby Manila. They provided support deemed appropriate by those affected. A code of conduct could build on such successes and should consider the following three principles. Have a clear purpose. Researchers should collectively identify knowledge gaps that future studies will fill. They should partner with affected people to establish emergent research priorities in dealing with a disaster. Such collaborative engagement can help to clarify where and when researchers will go into the field, what they will study, and who should be on the team. For example, psychologists and anthropologists might study and support local coping mechanisms; historians and civil engineers might collaborate to examine and promote resilient traditional architectural features when rebuilding homes in cyclone-affected areas. Representatives of the Digital Archaeology Foundation talk to a local woman near Kathmandu in 2016. The foundation was set up after the Nepal earthquake in 2015 to document temples and other heritage sites using 3D technology.Credit: Tom Van Cakenberghe/Getty The needs of local people should be central7. Too often, research is driven by media coverage and politics. Disasters in heavily populated areas receive the most attention, but the cumulative impacts of smaller events can be just as devastating. For example, after the massive Nepal earthquake in April 2015, the impacts on infrastructure and the quality of shelters were widely studied, and aid donors gave millions of dollars to rebuild parts of Kathmandu. Yet in rural western Nepal, hundreds of villages cope with floods and landslides each year, unnoticed by the outside world. A researcher code can help to redress the balance. For example, the Philippines requires that post-disaster projects demonstrate how they will meet the priorities of affected communities. New Zealand encourages researchers to defer collecting data unless the information will support responders. More relevant research could provide the evidence to inform and direct recovery funding to where needs really lie. Respect local voices. Wealthy countries account for most disaster scholarship and funding. For example, more than 90% of articles published following Hurricane Katrina, which hit the southern United States in 2005, were by US researchers8. By contrast, fewer than 5% of publications on the 2010 Haiti earthquake were led by authors based in the country (see ‘Unequal partners’). Source: Scopus/Adapted from ref. 8 Similarly, 84% of articles published between 1977 and 2017 in Disasters, the flagship journal in the field, were led by authors based in countries of the Organisation for Economic Co-operation and Development (OECD). Yet 93% of the people killed by large disasters over the same period lived in non-OECD countries, according to the EM-DAT disaster database9. Outside researchers — who have not had their lives disrupted by disaster — are positioned to seek funding and might overlook local work and partners. After Hurricane Katrina in 2005, local experts in urban poverty, affordable housing and coastal land loss were passed over for grants10. And local and external priorities might differ. In 2011, following the Joplin tornado in Missouri, outside academics assessed damage to infrastructure. By contrast, locally based researchers were eager to learn how to support emotional health after witnessing a rise in post-traumatic stress in children and adults11. Both are important topics, but funding streams do not always follow local desires. An understanding of local languages, policies and practices is essential and can improve response and speed recovery. After Katrina, ‘culture brokers’ helped survivors to make sense of government documents so that they could access aid quickly12. Nonetheless, much disaster research is still framed by narrow world views. Concepts such as vulnerability and resilience do not necessarily translate well13. Even where equivalent terms exist, they might be felt to be irrelevant, because natural phenomena such as cyclones and floods are not always seen as hazards. In some religious traditions, volcanic eruptions are thought to reflect the emotions of deities, for instance. A lack of recognition of this nuance can affect the outcomes of risk-perception research as well as early-warning processes. More discussions between disaster researchers inside and outside affected areas would shed light on these issues and could inform a more holistic research agenda. The Geotechnical Extreme Events Reconnaissance Association’s ethics protocol might serve as a starting point. It encourages engineers to adhere to “high standards of professionalism” and to be “respectful of local customs, traditions, privacy, and rights of affected individuals” (see go.nature.com/32kptno). Government agencies, companies and non-governmental organizations should also be involved in such conversations, given that they are increasingly engaged in post-disaster data collection14. Volunteers plant mangrove seedlings in Palu Bay on the Indonesian island of Sulawesi to help restore coastline that was damaged by the September 2018 tsunami and earthquake.Credit: Basri Marzuki/NurPhoto/Getty Coordinate locals and outsiders. Projects that are uncoordinated can become irrelevant or redundant, and might overwhelm local people and responders. In 2013, survivors of Typhoon Yolanda (also known as Haiyan) in Tacloban in the Philippines were deluged with questionnaires, when their immediate concerns were to secure housing, food, clothing and education. After Hurricane Harvey in the United States in 2017, officials at emergency operations centres struggled to decipher the credentials of dozens of researchers who descended on Houston, Texas, requesting access. Emergency managers also had to spend precious time revising researchers’ survey questions to put them in a local context. Foreign scientists sometimes approach local researchers to serve as translators or assistants. These locals have little power to direct the research strategy, even though their insights are valuable. They might feel unable to be critical even when they know the questions are wrong-headed. Even when they make substantial contributions, they might still be relegated to co-authorship — or no authorship — rather than being listed as the primary author. Incoherent data and findings might confuse authorities and delay decisions. Volcanologists still argue about exactly when local communities should be evacuated. To help, the International Association of Volcanology and Chemistry of the Earth’s Interior has produced guidelines on the roles and responsibilities of local and outside scientists, local authorities and the media. Local researchers need to be identified quickly in a crisis. As a start, the Social Science Extreme Events Research (SSEER) network has produced a global map of social scientists who study hazards and disasters (see go.nature.com/2qfwezc). Regional SSEER councils ensure that those researchers remain involved after the event. Discussions regarding a shared code of conduct could start through collaborative disaster-research initiatives that are under way worldwide. These have established strong coordinating structures and forums for information sharing, and include those in Latin America, Africa, the European Union and the Asia-Pacific region. They could also build on disaster-response initiatives from the medical sciences15,16. The US National Science Foundation (NSF) now supports several extreme-events reconnaissance and research networks. These advance coordination and set scientific agendas in geotechnical and structural engineering, social sciences, near-shore systems, operations and systems engineering, and interdisciplinary science. The NSF-funded CONVERGE initiative (of which L.P. is the principal investigator) brings together leaders from these networks and major NSF facilities to support the development of guidance and data-sharing by hazards and disaster researchers. Other resources, including a set of free online training modules, are also available. These NSF initiatives are open to researchers globally, but they are led by researchers at US institutions. Most countries do not provide ethical guidance for researchers, and universities have widely varying standards for the protection of study participants. The UNDRR is a trusted convener of scientists and practitioners globally. It could serve as a focal point for the development and implementation of an ethical code of conduct for researchers in disaster zones. As disasters unfold around the globe, the need for such a code of conduct becomes ever more urgent. </body>
<date id = '96'>20 November 2019</date>
<url id = '97'>https://nature.com/articles/d41586-019-03445-z</url>
<title id = '97'>Governments worldwide must invest in girls’ education, family planning, agriculture and security in this vulnerable region.</title>
<body id = '97'>Women in Niger and other parts of the Sahel are among the least empowered globally, but various efforts are starting to provide solutions.Credit: Issouf Sanogo/AFP via Getty World leaders have a disappointing record when it comes to crises that take decades to unfold. Much greater investment in the prevention of HIV/AIDS in the 1980s could have saved millions of lives and billions of dollars 20 years later, for instance. The western Sahel region of Africa lies between the Sahara Desert to the north and the Sudanian Savannah to the south. By 2050, the region’s population is expected to more than double, to 450 million1, and temperatures there are expected to rise to about 3 °C above their 1950 level. Already, hunger and malnutrition are widespread in the Sahel. As droughts and other weather extremes make it even harder for farmers to produce the crops and livestock needed to sustain the growing population, conflict and terrorism will increase2. As conditions worsen, millions of people could die in famines, and there is likely to be unprecedented levels of migration, including to Europe3. Our analyses of population projections and the probable impacts of climate change on food security in this ecologically vulnerable zone indicate that four steps are needed to head off these effects. We call on governments worldwide, together with those of seven countries spanning the Sahel (see ‘Defining the Sahel’), to invest in girls’ education; expand people’s access to family-planning information and services; increase agricultural production; and increase security using local police forces as well as national and international military services. Neglect just one of these actions, and political or economic systems could fail.  Over the past eight years, we have tracked links between demography and food security in six countries — Senegal, Mauritania, Mali, Burkina Faso, Niger and Chad — and the northern states of Nigeria, where Hausa–Fulani people are in the majority. We selected these mainly Muslim countries and states (which we call the Sahel study region, SSR) because they have similar climates and histories of colonization, and because the people living there have similar livelihoods, as well as some cultural connections. Throughout this region, people are already living on the edge. Population growth is outstripping food supply in the Sahel4. Relief efforts, such as those provided by the United Nations’ World Food Programme, are struggling to meet the demand for emergency food assistance. Data from the World Bank show that stunting (in which a child’s height is more than two standard deviations below the World Health Organization’s median for that age group) currently affects 17% of children under five in Senegal, and 42% in Niger. Because of the lack of clean water and sanitation services, the SSR has the world’s highest mortality from waterborne diseases: around half a million people here die from diarrhoeal disease and cholera each year. Women in the SSR are among the least empowered in the world. Many don’t have a say in their own basic life choices, such as when and whom to marry, or whether to work outside the home. They also lack the freedom to seek health care as needed. Niger and Chad have the world’s lowest median age of marriage among girls (around 16 years old) and — together with Mali — among the highest rates of adolescent births. In these three countries, 36–40% of women give birth before they are 18 years old. Such women are more likely than older mothers to deliver a preterm or low-birth-weight baby and to experience complications during childbirth5. Rapid population growth is undermining the quality of both primary and secondary education. It means that fewer resources are available per child6. In Niger and northern Nigeria, it is increasingly common for schools to teach half the children in the morning and half in the afternoon, because there are too many students and not enough teachers. Violence in the Sahel is rising rapidly, too. As population growth forces producers of crops such as sorghum and millet to expand their fields, herders and farmers are increasingly clashing. Women in Niger have on average 7.6 children, and in some northern Nigerian states, more than 8. These large families are relying on ever-smaller plots of land on which to grow food and graze their animals, mainly cattle and goats. According to the World Bank, Niger’s arable land per capita shrank from 1.4 hectares in 1996 to 0.8 in 2016. Investing in education for girls, such as at this school in Niger, has been shown to improve their health and that of their future families.Credit: Tara Todras-Whitehill/Polaris/eyevine Meanwhile, violence linked to militant Islamist groups is escalating. In Chad and Mauritania, average military spending as a proportion of gross domestic product is approaching that of the United States — at around 3%. In 2016, the same figure for Niger was around 6.4%. Even so, governments are struggling to keep people safe outside of large cities. According to the Africa Center for Strategic Studies, the number of violent incidents attributed to militant Islamist groups roughly doubled from 90 in 2016 to 194 in 2017, and more than doubled again to 465 in 2018 (see go.nature.com/2c7tkeq). Yet all of these challenges could pale in comparison to what lies ahead. Farmers in the Sahel are highly dependent on rain-fed agriculture, which is hugely vulnerable to climatic fluctuations. The record droughts of the 1970s and 1980s (which have been linked to climate change7) killed more than 100,000 people and left some 750,000 dependent on food aid, while affecting most of the 50 million people in the Sahel region. Future climate change is expected to have even greater impacts. Today, 30–60% of the SSR’s population is unemployed. These figures will worsen over the next decade, as more than half the population — currently under 18 — become adults. As a 2004 report8 by the 9/11 Commission, set up to investigate the 2001 terrorist attacks on the United States, observed: “A large, steadily growing population of young men without any reasonable expectation of suitable or steady employment [is] a sure prescription for social turbulence”. Deteriorating food and economic security in the SSR could lead to failed states. As well as being disastrous for the people living there, countries that are unable to perform fundamental functions, such as the provision of health care, education and food to their citizens, can prove extremely costly to the global economy. Somalia’s collapse in the 1990s increased pirating in the Gulf of Aden and the Indian Ocean, for instance. In turn, that cost the shipping industry an estimated US$18 billion annually, because cargo ships were forced to take alternative routes. West Africa is the origin of more migrants than any other region of Africa9. A 2017 study predicts that millions of people could be forced to leave the Sahel by the end of this century3. As time passes, adapting to the interactions between climate change and population growth will become ever harder. Acting now is imperative. Here are our proposals. Invest in girls’ education; reconsider marriage laws. Numerous studies over at least three decades have shown that investing in a girl’s education is a powerful way to bring about all sorts of other improvements, for instance in her and her family’s health, and in her children’s education10. Women in SSR countries who have completed secondary school have two fewer children, on average, than do those with no schooling. These women are more likely to marry later and overcome barriers to accessing family planning. In fact, demographers estimate that population growth in countries with fertility rates comparable to those in the SSR could be slowed by 15–20% if women delayed marriage and childbearing by five years11. A French soldier in a market in Mali.Credit: Pascal Maitre/Panos Pictures It is not necessarily enough to ensure that more girls go to school. The Centre for Girls Education (CGE) in Abuja, a non-governmental organization that is working to help girls in the Sahel to transition to secondary school, provides a good model for governments, non-governmental organizations and other groups to follow. (Two of us, A.G. and A.M.N., have partnered with the CGE, and co-signatory D. P. is a co-founder of and adviser to the organization. See Supplementary information for a full list of co-signatories’ affiliations.) Through ethnographic studies conducted in Nigeria in 2007, researchers from the University of California, Berkeley, and the CGE first established that poor-quality education is the biggest factor causing girls to drop out of school in rural communities. (Parents are not motivated to send their children to secondary school when they can’t read or write after six years of primary school.) The researchers then created ‘safe-space clubs’ in which mentored groups of up to 18 girls attending their final year in primary school or first year in secondary school could build a support network and get help with reading and writing, as well as with self-expression, negotiation, reproductive health and other crucial life skills. After participating in the programme for two years, girls in Kaduna state in northern Nigeria were 20 times more likely to complete secondary school than were other girls in their communities. They also got married around 2.5 years later than non-participating girls12. We estimate that a similar ‘safe-space’ programme could be rolled out for all girls aged 14 across the entire SSR for $641 million per year. This represents about 12% of the total annual aid received by the SSR countries (excluding northern Nigeria). As well as investing in girls’ education, countries should adopt and enforce laws establishing a minimum age for marriage or change current minimum ages. Although culturally sensitive, this recommendation is in accordance with the African Charter on the Rights and Welfare of the Child, which all SSR countries have ratified. This sets the minimum age of marriage at 18. Currently, across the SSR, the minimum legal age of marriage for girls is 15 or 16. In northern Nigeria, it is as low as 12. Globally, countries with consistent laws setting the age of marriage at or above 18 have 40% fewer child marriages and 25% fewer teenagers having babies than do countries without consistent laws13. A teacher at a mentored ‘safe-space club’ for girls in northern Nigeria. Such clubs have been shown to help girls finish secondary school and delay the age at which they married.Credit: Malala Fund Expand access to family planning. The use of contraception is increasing globally. Various studies show that when people are given good information and services relating to contraceptives, they want to use them14. Our analysis shows that, with the exception of Senegal, there are more women in the SSR who want to stop or delay childbearing but are not using contraception than there are current contraception users. (In Senegal, political will has helped to double the contraceptive prevalence in the past 7 years.) We estimate that meeting these women’s needs would lower the total fertility rate by 1–1.5 children per woman15. All sorts of approaches to improve access to contraceptives have been successful in other mainly Muslim countries. The total fertility rate in Iran fell from 6 births per woman in 1986 to 2.2 in 2000 following a national family-planning programme that included the integration of high-quality services into the primary health-care system and the provision of free contraceptives. This is a faster decline than has occurred in any other country. Given the dearth of doctors and nurses, national guidelines are needed to make it easier for community health workers to provide family-planning services, especially in rural areas. Legislation passed in Niger earlier this year, for instance, now permits community health workers to give people injectable contraceptives. (The World Health Organization approved this practice more than 20 years ago.) Ethiopia is a leader on this. Thanks to the country’s investments in rural health services, and the steps taken to enable community health workers to provide contraceptives and related services, Ethiopia might achieve the UN’s medium population projection of 191 million by 2050, as opposed to the high projection of 212 million. This could result in around one million fewer malnourished children by mid-century16. Increase agricultural production. Current crop yields are at least 50% below what they could be were farmers to use the best available seeds, as well as the optimal irrigation and fertilizer schemes that are feasible for the region. Today, only 4% of SSR cropland is equipped with any kind of irrigation infrastructure, and the average application rate for fertilizer is 10 kilograms per hectare — one-tenth of the global average. With investments in irrigation, we calculate that, under current climate conditions, it would be possible to feed 140 million more people in the SSR, without damaging freshwater ecosystems or depleting groundwater stocks17. The Sahel is already prone to unpredictable rainfall, soil degradation and erosion — all of which are expected to worsen with global warming. Farming approaches that enable crop growers to retain enough rainwater to bridge the dry season (by collecting run-off, reducing evaporation or improving infiltration) must be scaled up. Burkina Faso, for instance, has increased its yields of millet and sorghum by up to 500% by adopting the Zaï technique. Here, pits are dug in the soil to concentrate water and nutrients, and seeds are placed in the pits rather than scattered over the soil. Other water-harvesting techniques include terracing, mulching (the use of bark and other plant matter to reduce evaporation and enrich the soil), no-till farming (whereby farmers use hoes or other tools to remove weeds instead of ploughs) and contour stone bunds (the building of rock walls that prevent the soil from being washed away). Agroforestry, combining agriculture and forestry, can also increase food production and resilience to climate change. In Niger and Mali, trees grown at the edge of fields or among crops increase the fertility and moisture of the soil by producing leaf litter and shading, among other benefits. The development of new varieties of cereal (within limits) that are locally adapted and drought-tolerant could help to improve agricultural production as the effects of climate change intensify (see ‘Perfect storm’). Sources: Temperature: RCP Database/RCP 8.5, version 2.0.5; Niger cereal/population: Natl Statistics Inst. Niger. Improve security. US military spending in the Sahel soared from zero in 2008 to more than $400 million in 2016. Spending by European Union institutions more than doubled in that time, to greater than $800 million18. The French counter-terrorist military intervention in Mali alone is costing $2.2 million a day. This kind of external support is crucial. Yet, over the longer term, foreign boots on the ground rarely, if ever, build stability. Between 2012 and 2018, Burkina Faso increased its per-capita spending on security from $8.9 to $15.8; Mali’s increased from $9.3 to $25.9 over the same period. This has almost certainly meant a reduction in the already insufficient spending on health and social services — the upstream investments that are imperative for ensuring a country’s long-term stability. In Mali, spending on health per capita fell by nearly 30% between 2013 and 2016 — from $42 to $3019. To avoid a vicious cycle of reduced spending on basic services, escalating violence and yet more resources being shunted towards security, there must be greater cooperation between the security and humanitarian communities. Although people in humanitarian organizations have vastly different approaches and cultural norms from local police forces and national and international military organizations, these two communities invariably work side by side and rely on each other. Building more trust and aligning operations better would help to achieve the same end. At least three actions are needed to encourage the international community to increase its spending on security in the SSR. First, receiving governments must be fully transparent about where aid money goes. Second, if full transparency can be achieved, spending on security should be included in the definitions of official development assistance, along with spending on clean water, education, governance assistance and so on. Third, the World Bank, the International Monetary Fund and European donors, such as the French Development Agency AFD or the UK Department for International Development, should commit to funding deficits induced by spending more on security18. The humanitarian community tends to develop short-term plans and budgets spanning 3–5 years. Often this is because organizations are constrained by limited resources and are struggling to address immediate problems, such as refugees fleeing conflict or ecological disasters. Yet acting to prevent crises before they become acute is much more cost-effective in the long term. What’s more, expert recommendations for food security and other development goals almost always sidestep the possibility of slowing population growth through investing in girls and women. This is partly out of fear of being perceived as coercive following abhorrent abuses of reproductive rights — as occurred in India in the 1970s, when the government used various measures from propaganda to bribes to increase male and female sterilization19. Yet there have been many more cases of coercive pregnancy than coercive contraception. In fact, women being denied information and contraceptives is itself a human-rights violation. In countries that are reliant on US family-planning assistance, modern contraceptive use declined by 14% and abortion rates rose by 40% in 2001 and 2008 when the US government implemented its Mexico City Policy. This blocks federal funding for foreign non-governmental organizations that provide abortion counselling or referrals, advocate to decriminalize abortion or expand abortion services. The current version of the policy is more restrictive and is probably associated with even higher numbers of unintended pregnancies. The humanitarian implications of the emerging crisis in the Sahel must be brought to the table at high-level meetings of the UN, African Union and EU. The OASIS Initiative (Organizing to Advance Solutions in the Sahel), which was co-founded by A.G. and co-signatory M.P., plans to hold a conference with Sahel and European partners next year to increase donor investments in girls’ education and family planning. Over the past three decades, we estimate from UNAIDS data that HIV/AIDS has killed around 22 million people in sub-Saharan Africa. Over the coming three decades, population growth and climate change in the Sahel could threaten the well-being of hundreds of millions — unless the four recommendations we lay out here are enacted now. </body>
<date id = '97'>13 November 2019</date>
<url id = '98'>https://nature.com/articles/d41586-019-03308-7</url>
<title id = '98'>A scientific paper today is inspired by more disciplines than ever before, shows a new analysis marking the journal’s 150th anniversary.</title>
<body id = '98'>The co-citation network for Nature. More than 88,000 papers published by the journal since 1900 are each represented by a dot, coloured by discipline. Papers are linked if another scientific paper (of those indexed in the Web of Science) cites both; the dot size reflects the number of these co-citation links.Credit: A. J. Gates et al. How knowledge informs and alters disciplines is itself an enlightening, and vibrant field1. This type of meta research into new findings, insights, conceptual frameworks and techniques is important, among other things, for policymakers who fund research in the hope of tackling society’s most pressing challenges, which inevitably span disciplines. Since its founding in 1869, Nature has offered a venue for publishing major advances from many fields. To mark its anniversary, we track here how papers cite and are cited across disciplines, using data on tens of millions of scientific articles indexed in Clarivate Analytics’ Web of Science (WoS), a bibliometric database that encompasses many thousands of research journals starting from 1900. We pay particular attention to articles that appeared in Nature. In our view, this snapshot, for all its idiosyncrasies, reveals how scientific work is ever more becoming a mixture of disciplines. Several caveats are important. The volatility of our metrics in the early twentieth century can be attributed, at least in part, to the fact that articles then typically had many fewer references and citations. Until the mid-1920s, Nature articles typically listed no references; today, they can have up to 50. Another caveat is that the number of disciplines recognized by WoS grew from 57 in 1900 to 251 in 1993, but this is only one factor contributing to the disciplinary trends we found.   Many scholars have developed methods and metrics to gauge how scientific publishing contributes to knowledge, and to assess influence. For more detailed explanations of our choices, along with essential qualifications, see Supplementary Information (SI). Across the scientific literature overall, our analysis hints that articles are drawing from and influencing more disciplines than they did 100 years ago, although some disciplines have broader influence than others. As a journal, Nature publishes mostly specialized, or deeply disciplinary, papers; these tend to reference a narrower range of disciplines than does the average paper. Usually, however, Nature papers are cited by a broader range of disciplines than average. We extracted references for papers contained in the WoS publication database from 1900 to 2017, capturing close to 700 million citation relationships. We pinned subsequent analysis to the approximately 19 million articles that had at least one reference and one citation and that were published before 2010 (to give time for citations to accumulate). The resulting corpus integrated the discipline information for 38 million articles. To identify disciplines, we relied on relatively broad categorizations from WoS. These are necessarily imperfect, but cumulatively reveal patterns of scholarship. Most journals are disciplinary, and so WoS assigns each article to one or more disciplines on the basis of the journal in which it is published. For instance, articles in the Journal of Bacteriology are categorized as microbiology.   We traced the conceptual journeys to each paper by identifying the inspiration for articles by their references: the works authors credited for their concepts, methods, techniques and insight. Similarly, we identified the impact of each publication by the citations it received in the corpus. Caution is required when using citation-based measures to assess the importance of individual papers or authors; still, the accessibility and quantity of such data provide one view — among many — of how scientific knowledge accumulates1. We explored how the 88,637 Nature articles in our data set mediate the metabolism of ideas using the broadest WoS disciplinary categories. A Nature article with references mainly from biomedical research will typically collect the largest proportion of its citations from other biomedical-research papers (see ‘Knowledge flows’). About half of the papers that cite it will be spread across the other categories. By contrast, a paper with references mainly from engineering and technology is much more likely to be cited by papers in other fields (72%) than by other papers in the same field (28%). Engineering and technology papers also make up a very small proportion of the papers Nature opts to publish; those that are selected might be chosen for their broad appeal. At the other extreme, papers in Earth and space science are much more likely to be cited by papers in their own field (72%) than by other disciplines (28%). Source data: Web of Science. Analysis by A. J. Gates et al. Another way to reveal intrinsic communities in and across disciplines is through co-citation analysis2. In this approach, each paper is represented by a node, shown as a dot. Two papers are linked if another paper cites both of them; the node size reflects the number of co-citations. Our visualization algorithm treats each link as a spring and arranges the nodes to make links as short as possible. This produces clusters of Nature papers that vary in their level of interdisciplinary connections (see go.nature.com/n150int). Source data: Web of Science. Analysis by A. J. Gates et al. The overall network structure echoes scientific perceptions of how publications relate to each other. Articles tend to bunch together according to age and topic, because authors usually reference recent articles related to their paper’s subject3. Over its recent history (see ‘Timely subjects’), more than half of Nature’s papers have come from the life sciences. Consequently, clusters of biomedical-research papers appear throughout the network. Since 1930 (when it became reliable to use references to assign papers to disciplines), the proportion of physics papers has shrunk and Earth and space science has grown. Certain papers — such as the discovery of the first exoplanet orbiting a Sun-like star4 — are deeply embedded in a cluster of papers in the same field. By contrast, the discovery of the ozone hole5 is in a region where articles of many disciplines — chemistry, social sciences, Earth sciences — are found (see ‘Co-citation network’). Our analysis shows that this paper’s references are more diverse than 95% of Nature papers, and its citations are more diverse than 99% of Nature papers. Source data: Web of Science. Analysis by A. J. Gates et al. An analysis of the co-citation network from any more-specialized journal would probably look different. Still, distinct episodes from the history of science are apparent in the 3D view of Nature’s co-citation network (see go.nature.com/2patums). These include the study of radioactive elements in the 1930s, and how studies of superconducting materials flirted with diverse applications and then were intensely characterized deep within the physical sciences in the late 1980s and 1990s. The numbers of papers in every discipline grew exponentially over the past century1. Exact rates differ over time, although since about the 1960s, 48% of papers were in the life sciences (with 42% from ‘hard’ sciences and 10% from behavioural science). Scholars define and measure influences across disciplines in various ways. Multidisciplinarity usually refers to separate disciplines coming together yet remaining distinct: we define it for journals as the breadth of disciplines that are either inspiring or being impacted by the journal’s articles. Interdisciplinarity refers to integration: we define it as the diversity in inspiration in an article’s references, and the diversity in how an article’s impact diffuses across disciplines. Although it is difficult to assess integration across an article’s citations, this measure can capture how the knowledge communicated by the article had diverse impact6. This analysis indicates the extent of interactions across disciplines, but does not reveal the specific details of how those disciplines interact. First, we explored the breadth of disciplines reflected in the references and citations across a journal, capturing the journal’s multidisciplinarity (see ‘Inspiration and impact’). We labelled each paper in a journal with the primary discipline assigned to its references (inspiration) or citations (impact), and measured multidisciplinarity on a scale of zero to one. Zero meant that all of an article’s references or citations were in the same discipline; one meant that they were balanced evenly across all disciplines, using the normalized entropy measure (see SI). We found that this measure does not depend on the number of articles each journal published (see SI). It probably reflects other qualities of a journal, such as the pool of articles submitted and the editors’ selection criteria. Source data: Web of Science. Analysis by A. J. Gates et al. For most journals, the breadth of impact and inspiration are highly correlated. This holds true for specialist journals such as Cell and Physical Review Letters. A typical journal today publishes articles inspired by and impacting about six disciplines. The general-science journals Nature and Science both have a greater breadth of impact (citations) and inspiration (references) than 99.7% of other journals. The multidisciplinarity of Nature peaked in the 1960s and has remained relatively high since then, probably reflecting a combination of papers selected by Nature that are expected to have broad appeal, and the papers’ greater visibility to the scientific community. Second, we explored the interdisciplinarity of individual articles by measuring the diversity of disciplines in the references and citations7–10. Many measures have been proposed to assess interdisciplinarity, and can have inconsistent results (see, for example, refs 11,12). Scholars agree, however, that simply counting the number of disciplines that occur in references and citations is inadequate. For example, a paper that largely references biology and clinical science draws on less diversity than one inspired by biology and physics. We quantify this characteristic on a scale of zero to one using the Rao–Stirling diversity index, which captures the number of disciplines represented, how evenly they are distributed and their degree of difference13. Our analysis shows that the diversity of disciplines in articles’ references and citations is increasing. Roughly speaking, a typical article is inspired by and impacts three times more disciplines this decade than it did 50 years ago. Whereas a typical article published today references articles from the equivalent of 11 disciplines, a Nature publication references the equivalent of only 9 (SI, Fig. S5). This is in line with previous analyses suggesting that highly influential work tends to be grounded in deep expertise14. By contrast, the disciplinary diversity for the citations of articles in general-science journals has consistently been higher than for articles published elsewhere, suggesting that content in these journals reaches a broader swathe of the scientific community than it drew from. This observation makes sense, considering that these journals aim to reach a broader readership and to publish major advances. Sometimes, the fields that inspire a paper differ markedly from those on which it has an impact. For example, ‘The Digital Code of DNA’, a 2003 Nature essay by systems biologists Leroy Hood and David Galas15, takes most of its inspiration from molecular biology, yet is cited across computer science, clinical medicine and social science. We quantify cross-disciplinarity on a scale from zero to one. In this case, zero implies all disciplines that inspired an article and all those it impacts are identical; a score of one implies these lists differ completely (using the Jensen–Shannon divergence, a measure of the similarity between two probability distributions; see SI). What we see is that in recent decades cross-disciplinarity has declined, with that of the general-science journals falling faster than the scientific literature overall. Perhaps this is because articles that bridge disciplines influence multiple fields, including those from which they arose. As works draw on a broader set of disciplines, there is less scope to influence a set of completely different disciplines. Assessment of scientific work generally works best when contextualized within its specific discipline. For example, citation counts are more effective when comparing biomedical papers to other biomedical papers rather than to physics papers. But if interactions between disciplines are increasing, then a stringent, coherent assignment makes less sense. We speculate that considering how disciplines intermix within individual articles might allow better comparisons across disciplines or improve assessment of a paper’s impact. What’s more, strictly structured research departments and funding programmes make less sense if boundaries between disciplines are becoming less distinct. As network scientists, we relish the idea that science is becoming less siloed. The increase we observe in interdisciplinary thinking is seen across disciplines (see SI) and shows no signs of slowing. With the population of researchers, scientific literature and knowledge ever growing, the scientific endeavour increasingly integrates across boundaries. Research institutions and funding bodies would do well to realize that interdisciplinarity is becoming the norm.   The co-citation network for Nature is displayed on the front cover of the journal’s 150th anniversary issue. Download a PDF of the image or visit the interactive to explore the full network. Download the PDF Also shown on the cover, and viewable in a 3D interactive version, are ‘reference trees’ for selected articles, which reveal the diversity of disciplines that inspired the articles and that were impacted by them. Download a PDF of reference trees on the cover or visit the interactive to examine them in 3D. Download the PDF </body>
<date id = '98'>06 November 2019</date>
<url id = '99'>https://nature.com/articles/d41586-019-03307-8</url>
<title id = '99'>Research cannot fulfil its social contract and reach new horizons by advancing on the same footing into the future, argues Philip Ball in the last essay of a series on how the past 150 years have shaped today’s science system, to mark Nature’s anniversary.</title>
<body id = '99'>In 1866, three years before the first issue of Nature was published, a transatlantic telegraph cable established light-speed communication between Great Britain and North America. The triumph won William Thomson (later Lord Kelvin) a knighthood for the scientific advice he had given to the project. Yet Thomson had also advised on a disastrous earlier attempt in 1858 that barely worked from the outset and deteriorated within weeks. It was partly in response to that costly debacle that the Cavendish Laboratory was established at the University of Cambridge, UK, in the early 1870s, to provide the nation’s future engineers with a better grounding in physics. The first director was James Clerk Maxwell, whose electromagnetic theory of the mid-1860s led to the discovery of radio waves in 1887 — which soon enabled ‘wireless’ telecommunication and rendered the telegraph obsolete. In such ways, the distinctly Western and specifically British world into which Nature was launched regarded fundamental scientific research as the engine of socially transformative industrial innovation. Emanating from London, Norman Lockyer’s journal showcased those developments from the perspective of a British Empire that grew to encompass about one-fifth of the world’s population by the century’s end. The benefits of research laboratories and the systematic institutionalization of science, in both academia and industry, were beyond doubt for Nature’s target audience.   Eight decades later, this model motivated Vannevar Bush’s 1945 report to US president Franklin D. Roosevelt. Science — The Endless Frontier made the case for governmental support of basic science research to promote national security, public health and welfare. It led to the establishment of the US National Science Foundation, and it appealed to the optimistic and simplistic vision of science as a quest that, motivated by curiosity and guaranteed freedom of enquiry, would serve the interests of the nation and of humankind. Science — whether it is Maxwell’s electromagnetism, the Manhattan Project that inspired Bush, or the Human Genome Project — has indeed been so socially transformative that its intellectual and technological machinery has gained seemingly irresistible momentum. Is this not how progress is made, and is that not, on balance, a good thing? Even to ask the question invites familiar and polarized arguments. Some commentators question the wisdom of unfettered scientific development, pointing to the problems of climate change and environmental despoliation, nuclear weapons and antibiotic resistance, along with the ambivalent influence of artificial intelligence and robotics, information technologies and genetic engineering. Others point out that quality-of-life indicators — lifespan and infant mortality, say — have improved steadily (if unevenly, geographically and temporally) during the era of modern science that roughly coincides with the span of Nature’s existence. But Manichean views and tropes of ‘dual use’ miss the point. Some of the key questions that confront science today are about whether its methods, practices and ethos, pursued with very little real change since Maxwell’s day, are fit for purpose in the light of the challenges — conceptual and practical — we now face. Can science continue to fulfil its social contract and to reach new horizons by advancing on the same footing into the future? Or does something need to shift? Three years before the launch of Nature, the laying of transatlantic telegraph cable established light-speed communication between Great Britain and North America.Credit: Heritage Image Partnership Ltd/Alamy Let’s consider where we stand. The convention of the past century or so has tended to place the frontiers of knowledge at the scales of the very large and very small. Today we might be inclined to add the very complex — which typically pertains to the intermediate scales of direct human experience. It’s now clear that challenges at the two extreme scales — fundamental particles and cosmology — are related. As the island of knowledge grows, so does the perimeter of the horizon where knowledge ends, says Marcelo Gleiser, a particle cosmologist at Dartmouth College in Hanover, New Hampshire. “The more we know, the more exposed we are to our ignorance, and the more we know to ask”, he writes1. We have known for only several decades that dark matter outweighs all visible matter by a factor of five, yet we are no closer to knowing what it consists of. And scarcely two decades have passed since the mysterious entity dubbed dark energy, which causes the Universe’s expansion to accelerate, has been recognized to comprise more than two-thirds of the total cosmic energy density. Never before has our knowledge of the Universe seemed so deficient. Plugging these gaps at the largest scales will depend on elucidating the physical world at the smallest. Here the prospects are currently dim enough to cause desperation and even rancour. The world’s largest particle accelerator, the Large Hadron Collider at CERN near Geneva, Switzerland, has so far failed to offer any hint of how to proceed beyond known physics. Elegant ideas look moribund in the face of an ugly lack of facts. In the meantime, models are being forced towards ideas, such as the multitude of universes now permitted by the inflationary model of the Big Bang, that seem to some critics to abandon the empirical basis of science itself. Yet even as our view of the Universe becomes increasingly perplexing, it is being fleshed out as never before. In the 1860s, it was almost casually assumed that life would be common on other worlds. H. G. Wells’s 1897 novel The War of the Worlds (informed by his reading of Nature) seemed all the more chilling because of the widespread belief — which persisted for another half-century — that there was indeed life on Mars. Seasonal changes of surface colour were interpreted as vegetation growth, and striations described by astronomer Giovanni Schiaparelli were notoriously ascribed by others to artificial waterways. But the barren, sterile Martian landscape that the Viking landers revealed in 1976 confirmed a growing sense — stoked by the Apollo Moon landings and reflected in physicist Enrico Fermi’s famous question about the apparent absence of alien visitations — that we are a lonely outpost in a bleak, lifeless cosmos. Well, no longer. Since the first discovery of an extrasolar planet orbiting a Sun-like star was reported in this journal2 in 1995, around 4,000 sightings of such planets have now accumulated (and a 2019 Nobel prize). It seems that planetary systems are the norm for other stars, and Earth-like planets far from uncommon. Already we know a little about the atmospheres of some of these worlds. With the launch of NASA’s Transiting Exoplanet Survey Satellite last year, and the James Webb Space Telescope scheduled to launch in 2021, we will soon know much more. Researchers are now speaking plausibly about deducing within a lifetime if there is life elsewhere. Where does all this leave us? The cosmological perspective could seem to perpetuate the sense of an unfolding Copernican revolution in making humankind even more peripheral. Not just an insignificant dot in a vast Universe, we’re possibly an insignificant universe in a potentially infinite multiverse. It’s hard to imagine a demotion more extreme. There is another view that is anything but Copernican. Here, habitable worlds are ubiquitous and we remain uncomfortably, almost absurdly, at the centre of things. In the inflationary multiverse, our presence is the explanation for the fundamental constants of nature. They might have different values in other universes, but the conditions necessary for our existence guarantee that we will see the ones we do. The first six primary mirror segments for the James Webb Space Telescope.Credit: NASA/Marshall Space Flight Center/David Higginbotham The foundations of quantum mechanics (a topic once disreputable that now verges on fashionable) muddy the picture too. The ‘many worlds’ interpretation is more popular today than when US physicist Hugh Everett proposed it in the 1950s. It multiplies universes (in a manner distinct from the cosmological multiverse) and it multiplies each of ‘us’ beyond measure. Meanwhile, US theoretical physicist John Wheeler’s ‘participatory universe’ and new interpretations such as QBism3 insist that quantum theory requires the observer’s presence — rather than being the abstract and objective framework that science usually supplies. These ideas remain speculative. But they challenge the Newtonian promise of an impersonal mechanics. In other words, it’s still unclear when or whether we can exclude ourselves from the scientific frame. This would have been no surprise to Maxwell. His conception of physical reality was predicated (no less than was Newton’s) on a religious position that awarded humanity a special place. This, of course, is where Charles Darwin also enters the frame. His ideas on evolution by natural selection, published in On the Origin of Species (1859) were still causing shock waves when Nature was founded. Two years after that, he delivered the final bombshell in The Descent of Man (1871). The significance of his ideas was not as an explosive charge placed underneath the church but as the opening salvo to a century and a half of debate about what it means to be human. If there was a struggle, it was not about which book to consult but about who had the most decisive authority. Within science, first evolutionary theory, then psychoanalysis, and now genetics and neuroscience, have all staked their claims. On Nature’s centenary, you might have placed your bets with the latter disciplines. Half a century later, it is less clear that they can offer the last word. Powerful new techniques applied to rapidly growing data sets, such as genome-wide association studies4, have disclosed a clear and sometimes strong genetic component to almost every human behavioural trait we choose to study, as well as influencing health and disease. But a mechanistic understanding of genetic effects often remains remote. And for traits in which many — perhaps even several thousand — genes are implicated, it is not even clear if this is the right level at which to ascribe causes for what we can see and measure. The emerging picture of development and tissue function at the level of single-cell transcription (and perhaps soon of translation) adds a new layer of complexity5. Apparently identical cells in the same tissue can show a wide range of dynamic states of gene expression. It might be that the genome tells us no more about how an organism builds and sustains itself than a dictionary does about how a story unfolds. New methods, rather than finally answering old questions, could merely beggar them, shifting the goalposts entirely — as genomics itself has done for notions of race. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times Neuroscience, like genetics, has been restricted in the questions it can ask by the data it can gather. Functional magnetic resonance imaging remains a blunt tool, showing where things are happening in the brain (at rather coarse-grained resolution), but not what transpires. The idea that the human brain might be understood by exhaustive documentation and perhaps simulation of neuronal connections and firing patterns was challenged as soon as it was mooted (by the ill-fated European Human Brain Project6). Here we arrive at one stretch of the ‘complexity’ frontier. If history is any guide, we should expect that understanding these complex systems will not emerge by drawing analogies with the latest cutting-edge technologies. Just as the brain is not (as was thought in the early nineteenth century) a battery, neither is it a computer; nor is the genome a digital list of parts. And more data, although extremely valuable as a resource, will not help us without new ideas. These are in short supply. As neurobiologist and historian Matthew Cobb at the University of Manchester, UK, writes, “no major conceptual innovation has been made in our overall understanding of how the brain works for over half a century”7. It’s no surprise, then, that the ‘hard problem’ of consciousness is barely articulated, let alone understood. We are still at the stage where serious thinkers on the topic embrace the gamut of positions, from regarding it as an illusion to considering it the only valid starting point for a theory of human experience. That latter view harks back to how US psychologist William James ignored “the traditional antithesis between reality and appearance”, as Nature put it in 19158. As for claims that neuroscience has banished free will (for example, because decisions can be predicted from brain scans in advance of their conscious manifestation), saying that “your brain decides before you do” merely returns us to British philosopher Gilbert Ryle’s famous regression of mental homunculi9. Among the ways in which science has changed over the past century and a half, three loom large. First, it is no longer driven by lone figures labouring in their laboratories, but has become a team effort that spans labs, departments, disciplines, institutions and continents. Second, it often relies now on data sets so vast that human brains cannot hope to hold or parse them all. Third, it increasingly confronts issues of global reach and even existential urgency — from climate heating and the need for a carbon-neutral economy, to epidemics and water security. Yet these changing demands are not reflected in incentives, funding mechanisms, awards or popular narratives. Systemic biases — for example, in barriers to the entry and advancement of women and people from minorities, or in the demographic coverage of medical databases, or the prejudices that algorithms inherit from their makers — remain entrenched. Even science’s internationalism is threatened by current political trends. To regard what biologist Thomas Henry Huxley in Nature’s first issue called the “progress of Science” as an inexorable, triumphant forward march, today seems dangerously complacent. It is time to ask whether such problems are not imperfections of the system but consequences of it. Science might be hindered by channelling its practitioners into a single mode of thinking. There is hubris in the assumption that the traditions, conventions, training, disciplinary boundaries, methods, responsibilities and social contract that crystallized in the nineteenth century from a highly restricted demographic must still be the best way of working. To say as much is not to submit to some trendy caricature of postmodernism. Rather, it is to acknowledge that there are assumptions embedded, often invisibly, in the way we develop models, deploy metaphors, apportion priorities, recognize and reward achievement, and recruit participants that must be questioned. The canonical scientific article, with its unified and passive voice, its closed and self-contained narrative, its seductively confident diagrams and standardized format, and its eventual metric quantification of impact, is not the only or the best vehicle for translating and disseminating today’s research: for posing and then answering questions. There’s scope for more variety in who does this, and how. Who would have guessed, for example, that what was needed to finally put climate science firmly on the public agenda was the candour and courage of a schoolgirl who is on the autistic spectrum? The history of science tells us that some of the toughest questions will be addressed not by being answered but by being replaced with better questions. Among those haunting us today that might deserve this fate are: what is life? What is consciousness? What makes individuals who they are? Why does our Universe seem fine-tuned for our existence? How did it all begin? It will take creative and diverse thinking to improve on them — for the view over the horizon might not be the one we anticipated. </body>
<date id = '99'>05 November 2019</date>
<url id = '100'>https://nature.com/articles/d41586-019-03226-8</url>
<title id = '100'>Tackling mental disorders before they arise in pregnant women and new mothers is an approach that could be scaled up online — and would aid the overall health of populations.</title>
<body id = '100'>I have been convinced of the importance of prevention in addressing mental-health problems since the early 1970s, when I began my doctorate in clinical psychology. But only now is there sufficient evidence from clinical trials of the effectiveness of preventive interventions, using approaches derived from interpersonal and cognitive behavioural therapy, to justify deploying them. And only now are the tools available to make such interventions available to people worldwide. Two recent reports underline this conclusion. In February, the US Preventive Services Task Force, an independent panel of experts in evidence-based medicine, urged clinicians to “provide or refer pregnant and postpartum persons who are at increased risk of perinatal depression to counseling interventions”1. And last month, the US National Academies of Sciences, Engineering, and Medicine (NASEM) released a report2 calling on various stakeholders, from educators to policymakers, to prevent mental-health disorders and to promote healthy mental, emotional and behavioural development in the under 25s. (I was a member of the committees that prepared this document and two previous NASEM reports in 1994 and 2009 on preventive interventions3,4.) The latest NASEM call to action2 is so all-encompassing, it is hard to know where to begin. I propose that initial efforts focus on preventing depression in pregnant women or in women who have recently given birth (perinatal depression). There is substantial evidence for the effectiveness of providing such women with basic skills in mood management5. These interventions could have an impact across generations, because better maternal mental health is linked to babies’ healthier development2. And if researchers and health-care systems were to monitor and compare the epidemiology of depression in thousands of mothers and their children in areas that have or have not deployed preventive interventions, stakeholders could measure their effect on entire communities. Ultimately, massive open online interventions will need to be created (similar to the massive open online courses that are delivered on the Internet for free). These would allow anyone to obtain information and tools to help them stave off depression, at times and places that are convenient to them. In the United States, nearly 15% of men and around 26% of women experience a major depressive episode at some point6. People are diagnosed with this if they report experiencing five of nine symptoms over at least two weeks. These must include either feeling depressed or being unable to feel interest or pleasure, as well as problems sleeping, changes in appetite, fatigue or having suicidal thoughts. Numerous psychological, pharmacological and physical treatments are effective, such as cognitive behavioural therapy, antidepressant drugs and electroconvulsive therapy. But many people who are depressed are not receiving treatment7 because they fear stigma, can’t get to clinics or afford treatment, or because there aren’t enough psychologists and psychiatrists to meet their needs. Given these challenges — and especially given the scale of the problem — societies worldwide need to take steps to stop depression from taking hold in the first place. The number of randomized controlled trials testing preventive interventions has greatly increased since 1995 (see ‘Mounting data’). Two approaches have been studied the most: cognitive behavioural therapy and interpersonal therapy. The first involves teaching people how to use the natural relationship between thoughts, behaviours and mood to increase those thoughts and behaviours that lead to healthy mood states — and to reduce or modify those that elicit sadness, helplessness and hopelessness. (People might be asked, for instance, to predict how their mood would change if they undertook certain activities, such as seeing a friend — and then to record how their mood actually changed following the activity.) The second approach, interpersonal therapy, helps people to communicate better with others, and so to obtain more support from friends and family. Sources: Refs 5, 9 & 10; https://go.nature.com/342TJWV In the early 2000s in California, for example, my colleagues and I at San Francisco General Hospital (now the Zuckerberg San Francisco General Hospital and Trauma Center) conducted a pilot study funded by a US National Institute of Mental Health grant. The study involved 41 Spanish- and English-speaking women, most of whom were in their 16th week of pregnancy. These women were not clinically depressed but were deemed to be at high risk because they scored 16 or more on a depression scale, or had a history of major depressive episodes. The preventive intervention we used in this case involved psychologists teaching a cognitive behavioural ‘Mothers and Babies/Mamás y Bebés’ course in 2-hour sessions once a week for 12 weeks. Only 14% of the women taking the course had a depressive episode in the following year, compared to 25% in the control group8. A meta-analysis of 32 studies in 2014 showed that, in all sorts of groups that are at risk — from expectant and new mothers to individuals who’d experienced a stroke — such preventive interventions reduce the onset of major depressive episodes by 21%, on average9. In the same year, my colleagues and I found that 15 of 42 randomized trials reported reductions of 50% or more in the incidence of depression10. Then, this year, the US Preventive Services Task Force reviewed 50 randomized controlled trials testing preventive interventions specifically for perinatal depression. This has shown that, on average, such interventions reduce the incidence of major depressive episodes by 39%. However, one interpersonal approach, called ROSE, reduces the incidence of episodes by 50%, and the Mothers and Babies intervention reduces the incidence of episodes by 53%5. In short, the data suggest that if we implement interventions that seem to be the most effective in clinical trials, we could halve the new cases of major depression. So why focus on expectant or new mothers? I propose an initial focus on perinatal depression for four reasons. The evidence is strong. The window of risk is clear (during pregnancy and for a year after giving birth). Education and mood-management skills could be wrapped into the prenatal classes or home visits many pregnant women already receive, lowering cost and stigma — as was done in a 2010 study involving more than 2,000 women in the Trent area of England11. Most importantly, interventions could benefit multiple generations. A mother’s depression is associated with lower than average birthweight and preterm deliveries, as well as problems in children such as impaired cognitive development2. Conversely, the healthy development of babies and children could result in their having healthier, planned pregnancies when they themselves reach childbearing age. Of course, rolling out evidence-based preventive interventions to millions of women at risk of perinatal depression all over a nation, or the world (many of whom don’t have access to prenatal or postnatal care), as well as to other high-risk groups such as adolescents, is a daunting proposition. The number of therapists or health workers available to provide cognitive behavioural courses or interventions based on interpersonal therapy is vastly inadequate, even when it comes to treatment. In 2013, for example, an estimated 43.8 million adults in the United States had experienced a mental illness in the past year, yet only 19.6 million received mental-health services. The World Health Organization estimates that, worldwide, more than 300 million people of all ages experience depression. Most do not receive treatment. A different strategy is required. In 1998, with support from the Tobacco-Related Disease Research Program in Oakland, California, my colleagues and I began building an online resource to help people to quit smoking. We then conducted a randomized controlled trial in Spanish and English to determine whether people’s use of the site could generate quit rates comparable to those obtained from current smoking-cessation aids, such as the nicotine patch. (After six months, the quit rates of people using nicotine patches in the United States are 14–22%12,13.) After registering on the site, people were able to access a guide on how to stop smoking. They could submit their ‘quit date’ and would then receive e-mails advising them on what steps to take as the date drew near. They were given instructions on how to manage their moods, and encouraged to keep diaries as part of the mood-management training. They also became part of an online community that offered support and information. Our sample consisted of 1,000 smokers from 68 countries, 69% of whom provided follow-up data after one year. (If we didn’t get a response to our e-mails or phone calls, we assumed that person had resumed smoking.) In our study, 20% of Spanish speakers and 21% of English speakers quit14. In other words, we ‘matched the patch’. Baby yoga in São Paulo, Brazil, which some new mothers find helps them to avoid depression.Credit: Nacho Doce/Reuters Instead of shutting down the website at the end of the grant period, we continued to run the trial with a donation from the Brin Wojcicki Foundation in San Francisco. Over the following 6 years, data from more than 34,000 smokers from 168 countries generated similar results15,16. I realized then that our open online interventions were very similar to the now-popular massive open online courses. We had, in fact, carried out a proof-of-concept study of a massive open online intervention, or MOOI16. Various online interventions have already helped to reduce symptoms of depression17. Australia’s moodgym programme, launched in 2001, is one of the oldest global online interventions for depression. With more than one million registered users, it could well be the most widely used computerized cognitive-behavioural programme in the world18. And this month, the UK National Health Service launched an online campaign called Every Mind Matters to help people maintain their mental health. The interventions I’m calling for would be similar to these, but would need to be built such that their effectiveness could be evaluated on an ongoing basis. Also, effectiveness would need to be made transparent through continually updated ‘box scores’ on home pages; these could indicate, for instance, that the intervention resulted in a substantial improvement for 20% of 1,000 individuals. People would learn to look for the websites or apps that display effectiveness data, just as they look for services or films that have high ratings. MOOIs (websites, apps, text-based interventions, and so on) could be provided at no charge to every expectant or new mother in the world — as well as to other groups at risk, such as adolescents, people who have lost a loved one or those who are experiencing physical-health problems. In communities where few people have access to the Internet, health clinics could provide resource rooms where people could access MOOIs. And in remote locations where there are no clinics, local providers could use tablets, laptops or mobile phones to share MOOIs with the people they serve. In fact, the Mothers and Babies course is already being implemented in Tanzania and Kenya.   Some might argue that large-scale implementation is premature if the risk of developing clinical depression can be cut by only 50%. But such rates of reduction are comparable to those for reducing the risk of influenza through vaccination (40–60%; see go.nature.com/2wjkr93). Few online and smartphone apps for mental health have been rigorously tested. And some in the field might be concerned that MOOIs could be useless — or worse, harmful. All MOOIs would need to be evidence-based. Another potential concern is that, once MOOIs become available, insurance companies could refuse to reimburse people for in-person sessions with counsellors. And there is the worry that MOOIs might exacerbate inequities, with wealthy people receiving cognitive behavioural therapy from therapists and poorer people having access only to online resources. But risks must be weighed against benefits for every intervention. So what about the other 50%, whose depression is harder to head off? As all three NASEM reports2–4 point out, genetics, other biological factors — such as virus infections or hormone disorders — and people’s social and physical environments interact to have a major impact on mental health. Epigenetics, the study of heritable alterations of the genome structure that are environmentally induced and don’t involve changes to the DNA sequence itself, is revealing how life events affect gene expression and the development of mental disorders. Much of the variation in DNA methylation that occurs during the first month of an infant’s life, as well as their weight at birth and even some childhood behaviours, have been associated with prenatal environmental factors, such as the mother’s smoking habits, mental health and body weight2. The impact of the social environment on development has also been documented in detail. There is growing evidence that nurturing environments (achieved by rewarding good performance at school rather than punishing poor performance, for instance) have a major impact on children’s healthy development19. To reach the other 50%, we need to expand beyond individually focused interventions. The 2019 NASEM report2 recommends that clinical researchers, health-care providers and policymakers should systematically study and implement more-ambitious interventions that focus on children’s social and physical environments. Soon after arriving at the University of Oregon in Eugene in 1972 to start my doctorate in clinical psychology, I attended a talk at the local community mental-health centre. The speaker chided the professionals in the room, saying, in essence, “We therapists sit in our offices waiting for people to suffer enough to come to see us, or to be brought in by their family or the police because they are being disruptive. We should be going out into the community and sharing what we have learnt so that people can prevent the mental, emotional and behavioural problems that bring them to our offices.” That evening, I decided to devote much of my professional work to the prevention of mental disorders. Forty-seven years later, we have the knowledge and the tools to create a world in which fewer people ever experience clinical depression and other mental disorders. Let’s start creating it. </body>
<date id = '100'>30 October 2019</date>
<url id = '101'>https://nature.com/articles/d41586-019-03270-4</url>
<title id = '101'>From all too scarce, to professionalized, the ethics of research is now everybody’s business, argues Sarah Franklin in the sixth essay in a series on how the past 150 years have shaped science, marking Nature’s anniversary.</title>
<body id = '101'>In the autumn of 1869, Charles Darwin was hard at work revising the fifth edition of On The Origin of Species and drafting his next book, The Descent of Man, to be published in 1871. As he finished chapters, Darwin sent them to his daughter, Henrietta, to edit — hoping she could help to head off the hostile responses to his debut, including objections to the implication that morality and ethics could have no basis in nature, because nature had no purpose. That same year, Darwin’s cousin Francis Galton published Hereditary Genius, a book that recast natural selection as a question of social planning1. Galton argued that human abilities were differentially inherited, and introduced a statistical methodology to aid “improvement of the race”. Later, he coined the term ‘eugenics’ to advocate selective reproduction through application of the breeder’s guiding hand. Darwin’s transformative theory inspired modern biology; Galton’s attempt to equate selection and social reform spawned eugenics. The ethical dilemmas engendered by these two late-nineteenth-century visions of biological control proliferate still. And, as older quandaries die out, they are replaced by more vigorous descendants. That there has never been a border between ethics and biology remains as apparent today as it was 150 years ago. The difference is that many of the issues, such as the remodelling of future generations or the surveillance of personal data, have become as everyday as they are vast in their implications. To work out how to move forward, it is worth looking at how we got here. In the late nineteenth century, like today, society was in upheaval and science was on a roll. With Darwin’s bold hypotheses set before them, Victorian breeders, microscopists, collectors, astronomers, geologists and anatomists sought to discover the laws interconnecting life’s core processes — often by using ingenious experimental designs. To probe the formative effects of gestation on heredity in mammals, the gentleman naturalist Walter Heap, a laboratory demonstrator at the University of Cambridge, UK, conducted the first experiments in transferring embryos from one variety of rabbit to another at his home in Prestwich in the 1890s. His methods typified a new era of disrupt-and-learn biology. By the early part of the twentieth century, what had come to dominate was “the biological gaze”, to quote historian Evelyn Fox Keller at the Massachusetts Institute of Technology in Cambridge2. Rather than simply observing life, experimenters began to manipulate its component parts to test the limits of the system, mix up ingredients and turn biology inside out.   In 1903, the embryologist Hans Spemann conducted his famous experiments with amphibians. Using one of his infant daughter’s fine, elastic hairs, he tied a loop around a fertilized salamander egg to create an animal with two heads and one tail. That same decade, in the United States, physiologist Jacques Loeb pursued a new ‘engineering biology’, trying out all sort of chemicals and conditions to prompt development in model organisms such as sea urchins3. Ian Wilmut, who led the team that created Dolly the cloned sheep in the 1990s (at the Roslin Institute near Edinburgh, UK), once stated that it was Dolly’s birth that ushered in “the age of biological control” — and made obsolete the expression “biologically impossible”4. In fact, this view of life was born at least a century earlier. And as confident experimentation turned ever more closely and deliberately towards humans, the relationships between research, industry and governments became a tangled ethical bank, and have remained so ever since. Embryologist Ian Wilmut encounters his brainchild Dolly the cloned sheep, which is now stuffed and went on show at an exhibition in 2015.Credit: Will Latham/eyevine Eugenics, never without its trenchant opponents, became an increasingly crucial part of a new world order over the course of the twentieth century. It is particularly associated with the mass-sterilization campaigns that began after Indiana’s 1907 act, and with the Nazi racial-hygiene programme that reached its nadir in the Holocaust. Another legacy of the eugenics movement is the management of populations using techniques such as demography, racial classification and statistical modelling. These, combined with family planning, became synonymous with modernity and progress. From Latin America and Scandinavia to India, China and the Soviet Union, eugenics took root in projects to ‘improve the population’ throughout the twentieth century. Eugenic presumptions about the differential fitness of native and immigrant populations were central to colonial administrations across the British Empire. Census-takers created ‘races’ and ‘tribes’ where none existed, for the purpose of managing populations more ‘scientifically’. These categorizations got inked into emerging nations across Africa and southeast Asia, and continue to shape definitions of race in countries including Malaysia and Singapore today. The logic of the modern nation state is in no small part provided by eugenic techniques of classifying and controlling citizens, as pointed out by historians Alison Bashford, now at the University of New South Wales in Sydney, Australia, and Philippa Levine, at the University of Texas at Austin5. This typological approach to administration was normalized through what has been called the “prism of heritability” by the sociologist Troy Duster, now at the University of California, Berkeley6. That had the effect of linking together the pathologization of mental illness, homosexuality, criminality, poverty, ethnicity and race into a discourse of ‘rational’ management that became mainstream. In other words, the principles of the eugenics movement are part of contemporary society’s DNA. Across national and global policies affecting everything from health care, fertility and incarceration to border control, education and regional development, the goal of shaping the population through selective pressures — such as creating a “hostile environment” for immigrants — is alive and well. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times The birth of bioethics in the 1970s was in no small part a response to harmful research projects undertaken within this context — on vulnerable groups such as immigrants, prisoners and psychiatric patients — and without meaningful consent. The field emerged largely in the United States, partly driven by the international outrage at the exposure in 1972 of the covert US Public Health Service research project at Tuskegee University — in which more than 400 black US men, mostly poor share-croppers from Alabama, were subjected to untreated syphilis between 1932 and 1972. As many as half of them died, and 60 of their wives and children contracted the disease. In 1974, the US government passed the National Research Act and established the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. Two years later, the commission drafted a report outlining in detail the “basic ethical principles and guidelines that should assist in resolving the ethical problems that surround the conduct of research with human subjects”. In 1978, this was published as The Belmont Report7 in the US Federal Register, establishing guidance for national research and the three pillars of modern bioethics. These were: respect for persons, beneficence (‘doing good’) and justice. The report also clarified the basis for informed consent of study participants, and helped to enforce mandatory policies for ethical oversight of research. The three principles were largely aimed at preventing the mistreatment of vulnerable individuals and communities. Under Belmont’s influence, research ethics became a central principle of modern science. Bioethics flourished throughout the 1980s, expanding to include equity in public health and access to medical care. The field became increasingly central to medical and scientific training, as well as to research funding. That focus was intensified by the ‘too little, too late’ critiques of government responses to the HIV crisis that emerged in the mid-1980s. Louise Brown, the world’s first baby born as a result of in vitro fertilization, pictured in 1981.Credit: Michel Artault/Gamma-Rapho/Getty Bioethics gathered momentum at this time by offering guidance on controversial biomedical applications such as organ transplants and in vitro fertilization (IVF). In the first encyclopaedia of bioethics, published in 1978, theologian Warren T. Reich drew attention to a key shift in the practice of medicine: one that moved away from a commitment to preserving life8. In the past, he argued, medicine was guided by the absolute principle to ‘do no harm’. However, a different ethical dilemma arose out of a heart transplant, a procedure that could significantly improve an individual’s quality of life but which also had the potential to kill them. In contrast to the iron-clad medical ethics of old, the absolute value of human life became relativized. In the world’s most advanced medical facilities, a higher quality of life could now be worth dying for. Once again, ethical debate was reignited. It was in the 1990s that professionalized bioethics reached its high-water mark. The Human Genome Project (HGP) — the leviathan of publicly funded DNA sequencing — promised to unleash a combination of Darwin’s and Galton’s visions as the century drew to a close. Ethics claimed the largest share of HGP funds set aside for the analysis of “Ethical, Legal, and Social Implications” (ELSI) of genome mapping. In the United States alone, around US$3 billion (5% of the HGP budget) was spent to create “the world’s largest bioethics program”. Armies of ethicists combed over the philosophical principles of altering genetic material in ways that might or might not be passed on to future generations, and the perils of designer babies. Then, as the century neared its end, something else took centre stage: new techniques derived from reproductive and developmental biology, such as cloning and research into stem cells and embryos. As the prospects of quick bench-to-bedside applications from the HGP faded, so did the allure of bioethics. The discipline lost its most significant source of funding as ELSI programmes ceased. To return to 1978, there was another turning point for bioethics in the year of The Belmont Report: the birth of Louise Brown, the first baby conceived through IVF. Some of the most controversial research and applications over the past half-century have concerned reproductive and developmental biology. But while bioethicists were recruited en masse to contemplate the impact of the HGP, the fertility industry mushroomed, generating an impressive set of acronyms (but no ELSI). For a while, global public opinion became more sharply divided over cloned dogs and genetically modified (GM) maize (corn) than GM babies. That concern would come later. In retrospect, many of the forces that propelled late-twentieth-century bioethics into the limelight — such as the focus on speculative genomic futures — eventually left it unmoored. In the past two decades, bioethics has drifted into uncharted waters. Today, amid a panoply of ethical quagmires ranging from gene-edited babies and neurotechnology to dish-grown organoids and nanobots, the fraught relationship between society and research is once again front and centre. Genetically modified foods have caused controversy in many nations.Credit: Renee C. Byer/Sacramento Bee/zReportage/eyevine Just as the ramifications of the birth of modern biology were hard to delineate in the late nineteenth century, so there is a sense of ethical bewilderment today. The feeling of being overwhelmed is exacerbated by a lack of regulatory infrastructure or adequate policy precedents. Bioethics, once a beacon of principled pathways to policy, is increasingly lost, like Simba, in a sea of thundering wildebeest. Many of the ethical challenges arising from today’s turbocharged research culture involve rapidly evolving fields that are pursued by globally competitive projects and teams, spanning disparate national regulatory systems and cultural norms. The unknown unknowns grow by the day. The bar for proper scrutiny has not so much been lowered as sawn to pieces: dispersed, area-specific ethical oversight now exists in a range of forms for every acronym from AI (artificial intelligence) to GM organisms. A single, Belmont-style umbrella no longer seems likely, or even feasible. Much basic science is privately funded and therefore secretive. And the mergers between machine learning and biological synthesis raise additional concerns. Instances of enduring and successful international regulation are rare. The stereotype of bureaucratic, box-ticking ethical compliance is no longer fit for purpose in a world of CRISPR twins, synthetic neurons and self-driving cars. Bioethics evolves, as does any other branch of knowledge. The post-millennial trend has been to become more global, less canonical and more reflexive. The field no longer relies on philosophically derived mandates codified into textbook formulas. Instead, it functions as a dashboard of pragmatic instruments, and is less expert-driven, more interdisciplinary, less multipurpose and more bespoke. In the wake of the ‘turn to dialogue’ in science, bioethics often looks more like public engagement — and vice versa. Policymakers, polling companies and government quangos tasked with organizing ethical consultations on questions such as mitochondrial donation (‘three-parent embryos’, as the media would have it) now perform the evaluations formerly assigned to bioethicists. Journal editors, funding bodies, grant-review boards and policymakers are increasingly the new ethical adjudicators. These shifts have been a long time coming and have many different sources, including the driving influence of practical ethicists such as the British philosopher Mary Warnock — often to the consternation of the wider bioethics community. After Warnock’s Report of the Committee of Inquiry into Human Fertilisation and Embryology was published for the UK government in 1984, John Harris, a medical ethicist at the University of Manchester, UK, complained that “the crucial questions are fudged, or rather are never addressed”9. He argued that Warnock’s approach was over-reliant on “primitive feelings”, resulting in recommendations that were “false” and “dangerous”. The Warnock committee, in his view, had evaded the single most important question they faced concerning the moral status of the human embryo, in favour of a sentimental concession to expedient policy. As we now know, Warnock was prescient in her attention to the strength of public feeling in relation to human-embryo research. Her reliance on several overlapping types of argument to justify strict limits on the introduction of new reproductive technologies has enabled the United Kingdom to establish a licensing system that is more flexible and that has proved more long-lasting than in any other country. Her committee, unusually comprising a majority of non-scientists, reached its consensus based on a pragmatic and principled proposal: that approval for the study of controversial therapeutic and experimental procedures would be subject to a strict and comprehensive code of practice upheld by Parliament. The law itself, Warnock argued, would act as both a guarantor and a symbol of public morality; it would in its combination of permissive scope and legislative precision express “the moral idea of society”. This was a new template for ethical reasoning. When the UK government decided, in the wake of the passage of the Human Fertilisation and Embryology Act in 1990, that it would not establish a counterpart to the US National Bioethics Advisory Commission, it was following Warnock’s lead, and that of Anne McLaren. This developmental biologist and Warnock committee member took a populist and practical approach to public trust in science that has been highly influential. Today, interdisciplinary expertise plus extensive and creative public consultation increasingly define a new approach to ethical science. This trend has been reinforced by organizations such as the Nuffield Council on Bioethics, which advises the UK government by mobilizing a broad spectrum of knowledge, far beyond that of bioethicists and philosophers. Since 1993, the council has commissioned and published nearly 30 specialist reports on controversial biomedical issues, ranging from genetic screening to xenotransplantation. Few of the panels have been chaired by bioethicists. Many of the reports have widened the idea of what counts as an ethical issue — for example, the exploration of cultures of UK scientific research, chaired by the University of Cambridge plant scientist Ottoline Leyser10. In a similar vein, the International Society for Stem Cell Research has released a series of global guidelines that prioritize oversight, communication and research integrity as well as patient welfare, social justice and respect for study participants over fixed principles of ethical conduct. In a social-media-saturated age wary of fake news, the new holy grail is the ability to create trustworthy systems for governing controversial research such as chimeric embryos and face-recognition algorithms. The pursuit of a more ethical science has come to be associated with building trust by creating transparent processes, inclusive participation and openness to uncertainty, as opposed to distinguishing between ‘is’ and ‘ought’. In short, expert knowledge and reliable data are essential but never enough to enable enduring, humane governance to emerge. So there is now more emphasis on continuous communication and outreach, and on long-term strategies to ensure collective participation and feedback at all stages of scientific inquiry. The result is less reliance on specialized ethical expertise and more attention to diversity of representation. Amid the perils and promises of applications, from replacement heart and liver cells or driving malarial resistance through the mosquito population to ending Huntingdon’s disease, a new legacy to Darwin and Galton has emerged. It turns out that what we have in common is less a single biological essence — or the ability to alter it — than a shared responsibility for human and non-human futures. The implication of this new model is that the most ethical science is the most sociable one, and thus that scientific excellence depends on greater inclusivity. We are better together — we must all be ethicists now. </body>
<date id = '101'>29 October 2019</date>
<url id = '102'>https://nature.com/articles/d41586-019-03172-5</url>
<title id = '102'>Historian Paul Lucier traces the explosion and fragmentation of industrial research in the fifth essay in a series on how the past 150 years have shaped today’s science system, marking Nature’s anniversary.</title>
<body id = '102'>Four years after the first issue of Nature was published, the US National Academy of Sciences (NAS) faced an existential crisis. In October 1873, one of its original members demanded the expulsion of another member for swindling. Josiah Whitney, California’s state geologist, accused Benjamin Silliman Jr, professor of applied chemistry at Yale University in New Haven, Connecticut, of accepting large sums from California oil companies in return for favourable, possibly fraudulent, science. Silliman responded forcefully that company funding for science was evidence of responsibility, not misconduct: companies needed objective “technical opinions”. Without science, swindling would be more common, he argued. NAS president Joseph Henry, secretary of the Smithsonian Institution and a former consultant to Samuel F. B. Morse, inventor of the telegraph, had to agree. If the NAS expelled every member who had ever consulted for a private company, it would not survive. Henry rejected the efforts to remove Silliman. More importantly, he resolved to expand the NAS membership; new members were to be judged on the basis of their research, not on the source of their income1. By the 1870s, it was already clear that industry relied on science. The Silliman–Whitney controversy marked a watershed in the relationship between science and industry. For US scientists, as well as many in Britain and Europe, private companies had become valuable patrons, supplying both funds for research and problems to be researched, and were gainful employers who provided short-term commissions. Likewise, companies regarded scientists and their findings as profitable to the development of their respective industries. Over the next 150 years, relations between science and industry continued to evolve — in four significant stages. Scientists moved from part-time consultants to full-time corporate researchers, and then to academic entrepreneurs. Industry grew from a scattering of local businesses to a concentration of large companies, and on to multinational corporations with global reach. Although these transformations might seem symbiotic, and even inevitable, the very fact that US scientists and industries emerged as leaders and exemplars (in terms of employment, funding, publishing, patenting and innovating) serves as a cautionary reminder of the contingent nature of such developments. At the heart of the NAS crisis was an essential tension in the relations between science and industry: can the pursuit of knowledge be corrupted by the pursuit of profit? To Whitney and his allies, the answer was obviously yes. Their ‘pure’ science needed to be practised in places protected from the profit motive, such as government agencies or well-endowed universities. Silliman and supporters of ‘applied’ science, by contrast, believed the interactions between science and industry to be mutually advantageous. Indeed, the emergence of a distinct kind of endeavour called applied science characterized a new era in which research would address more and more industrial concerns, and private enterprise would, ideally, become a steady supporter of that work2. The profession of scientific consulting goes back to the early nineteenth century, when individuals or groups of capitalists occasionally commissioned scientists to examine prospects in farming, mining, transportation (canals and railroads) and manufacturing. These fee-for-expertise engagements were short term and advisory. By the 1870s, changes in US commercial law (similar to those in British and European law) allowed the formation of limited-liability, joint-stock companies. These businesses, with their large pools of funds and numerous shareholders looking for investment assurances, regularly consulted scientists. As the engagements became both more routine (continuous testing and analysing of existing products and processes) and more investigative, scientists began to receive lucrative contracts and retainers1. A United States Geological Survey team in the Wasatch mountains in Utah in 1869. US geologists were among the most active consultants during the Gilded Age, a period of rapid economic growth from the 1870s to the 1890s.Credit: Timothy H. O'Sullivan/George Eastman Museum/Getty In the United States, geologists were among the most active consultants during the Gilded Age, a period of rapid economic growth from the 1870s to the 1890s, especially in precious-metal mining in the area west of the Mississippi River. In Britain and Germany, the most prolific consultants were chemists, because of their essential expertise in new products such as acids, soaps, paints and especially synthetic dyes, including mauve and alizarin. Consulting chemists also found themselves in prominent public roles as expert witnesses in sensational patent cases. Witness-box quarrelling among chemists made good newspaper copy, and it highlighted profound developments in the chemical industries. Changes in patent law in the United States, Britain and Germany allowed inventors to claim those new chemical products and processes as their intellectual property (IP) instead of judging them to be scientific discoveries, which were, by definition, unpatentable. At the turn of the twentieth century, the independent consulting scientist was replaced by the salaried researcher in new industrial laboratories. These labs represented the incorporation of applied science; that is, the creation of a separate place within the organization for ‘research and development’ — a phrase that entered the lexicon at this time. In Germany, the largest dye companies, such as Bayer, Hoechst and BASF, were the first to establish dedicated labs for chemical research. These were connected to production departments, also staffed by university-trained chemists, and to specialized legal departments, from which the new products and processes were submitted for patenting. This type of industrialized invention, with close connections between German academic chemistry and company labs, was firmly established before the First World War3.   In the United States, the prototype for the industrial research lab appeared in the electrical industry, when inventor Thomas Edison set up an ‘invention factory’ in Menlo Park, New Jersey, in 1876. Edison wanted to replace what had been an unpredictable act of creative genius with a regular and reliable system. He recruited machinists, mechanics, chemists, physicists and mathematicians to work on technical problems connected to telegraphy and electric lighting. Although their efforts were collaborative, only the ‘Wizard of Menlo Park’ (the singular inventor) was listed on more than 1,000 US patents, including those for the phonograph (1878) and electric light bulb (1880)4. The looming expiration of that original light-bulb patent and the threat from other lighting companies impelled General Electric (GE), the corporation that took over Edison’s Electric Light Company and all his patents, to establish the aptly named Research Laboratory in 1900 in Schenectady, New York. This proved profitable within a decade — commercially, with the invention of a new light bulb that restored GE to its dominant market position, and professionally, with the recruitment of more than 250 engineers and scientists. A few other large US corporations followed suit and pioneered their own formal research and development (R&D) labs — DuPont (1903), Westinghouse Electric (1904), American Telephone and Telegraph (AT&T, 1909) and Eastman Kodak (1912). It was the First World War and the embargo on all German products, especially chemicals, that was the catalyst to the golden age of ‘industrial research’, a neologism of the 1920s. Between 1919 and 1936, US corporations established more than 1,100 labs in nearly all industries — petroleum, pharmaceuticals, cars, steel — thereby dominating the world’s industrial research. In 1921, these employed roughly 3,000 engineers and scientists; by 1940, there were more than 27,000 researchers. At the end of the Second World War, the figure was nearly 46,0005. This remarkable proliferation reflected the massive scale of vertically integrated corporations that controlled nearly all areas of their respective industries, from natural resources through R&D to mass production and mass marketing. Industrial research was also fuelled by radical changes in US patent law that allowed these behemoths to claim the IP of their employees. The inventor was now the corporation. During the Great Depression, critics singled out modern big business for its ruinous consequences to society — unemployment, overproduction and bankruptcy. Having research in thrall to industry raised the alarm, again, that capitalism corrupted science. So corporate captains and R&D directors marshalled the cornucopia of wondrous consumer products (‘technology’ in the new parlance) created by their science-based industries. In this story, science in industry was good; it guaranteed efficacy, efficiency and safety. In words that nineteenth-century consulting scientists would have understood, consumers could trust these modern technologies (and their corporations) because of the R&D. At the World’s Fair in New York City in 1939, industry paraded the fruits of its science. The Radio Corporation of America (RCA) introduced consumers to the television. International Business Machines (IBM) showed off its electric typewriter. GE exhibited its new electrical refrigeration system, and DuPont, under its banner “Better Things for Better Living through Chemistry”, showcased a synthetic fibre called nylon6. US firms paraded the fruits of their industrial research at the 1939 World’s Fair in New York City.Credit: Bettmann Fears of corporate corruption of science were put to rest by awards of the Nobel prize. In 1931, two Germans, Carl Bosch and Friedrich Bergius, became the first industrial researchers to win in chemistry. The next year, GE’s Irving Langmuir won the chemistry prize, and in 1937, Clinton J. Davisson of Bell Telephone Laboratories (Bell Labs) won a share of the Nobel Prize in Physics. The largest research facility in the United States was Bell Labs, established in 1925 in New York City to consolidate the R&D arm of AT&T and Western Electric, its telephone-manufacturing arm. The labs had around 3,600 staff members and a budget in excess of US$12 million. (GE allocated less than $2 million to its Research Laboratory.) The first president of Bell Labs was the physicist Frank Jewett. In 1939, he became the first industrial scientist to be president of the NAS7. In short, national standing and international acclaim seemed to confirm that science done under the auspices of industry was equal to science in universities or governments. Still, industrial labs of the 1920s and 1930s were not simply universities without students. As institutions of applied science, they always needed to show corporate headquarters their value in terms of profitable products and processes. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times By the time the New York World’s Fair closed in October 1940, Europe was already at war. The United States entered in December 1941, and the Second World War transformed the relationship between science and industry, along with the very terms — and even the history — of those relations. The prime mover in all those changes was the US military and the unprecedented amounts of money it allocated — through new forms of contracting and subcontracting — to scientific research. During the war, the Office of Scientific Research and Development, under its director Vannevar Bush, signed more than 2,300 research contracts, worth roughly $350 million, with more than 140 academic institutions and 320 companies. About two-thirds of that funding went to universities; the Massachusetts Institute of Technology (MIT) in Cambridge, for example, received more than $200 million for its Radiation Laboratory for research on radar. Corporate R&D also received unrivalled amounts: AT&T was allocated $16 million, GE $8 million and RCA, DuPont and Westinghouse between $5 million and $6 million each8. But by far the most prodigious investments in R&D flowed from the War Department ($800 million) and the Navy Department ($400 million). The largest portion of that went to private industry ($800 million), much of it directed towards emergent industries with compelling national-security interests — for example, aerospace, electronics, computing and nuclear technology8. The US military had not intended to become the commander-in-chief of US science, but by the end of the war it was apparent, at least to Bush, that the federal government needed a plan. In his 1945 report to US president Franklin D. Roosevelt, Science — The Endless Frontier, Bush presented a vision for US science policy that would guide and define both university science and corporate R&D throughout the cold war. The endless frontier was ‘basic’ research, the kind performed “without thought of practical ends”, a direct throwback to the nineteenth-century idea of pure science. The US military would fund this to boost industrial research because, the reasoning went, basic research was “the pacemaker of technological progress”. Here, then, was a new argument. As many commentators at the time and since have pointed out, it did not reflect either the experience of the war years (during which multifunctional teams worked on military projects such as the atomic bomb or radar) or of the previous decades (in which multifunctional teams worked in R&D labs on corporate projects such as the light bulb). Science — The Endless Frontier thus propounded a different idea for developing new technologies, both military and commercial. In time, this became known as the linear model of innovation9. The theory posits a conveyor belt, beginning with basic science and moving smoothly along to development, then to manufacturing and production, and culminating with technology or innovation. Increase the amount of basic science and the (alleged) result would be more technology, innovation and overall economic growth. Theoretically, basic research was to be centred in universities (and military funding did transform US universities and their science departments accordingly). But corporate R&D labs were also contracting with the military, as they had been during the war. With these military contracts, as well as enlarged funding from corporate headquarters (business leaders also bought into the linear model), industrial labs were redirected away from applied science and towards basic research10. Such faith in endless scientific innovation combined with prodigious financial resources led to the creation of central corporate research labs. These functioned more or less independently, which nicely suited the new organizational structure of multinationals. In place of vertical integration, sprawling conglomerates adopted horizontal organizational structures comprising multiple divisions (the M-form organization), in which each division, including the central research lab, operated on its own. Leading research labs relocated to the countryside, far removed from headquarters and any connection to manufacturing. RCA Laboratories Division, for example, expanded its campus near Princeton, New Jersey, after 1945 and started work on colour TV and semiconductors. In 1956, Westinghouse built up its research labs in Churchill outside Pittsburgh, Pennsylvania, for nuclear research. IBM set up its Thomas J. Watson Research Center, designed by the modernist architect Eero Saarinen, in Yorktown Heights near New York City in 1961, to work on lasers, semiconductors and other computer-related physics. And Bell Labs moved its research headquarters to Murray Hill, New Jersey. At its height (before 2001), Bell Labs conducted world-class research in many fields (physics, mathematics, radio astronomy) at numerous sites. Its largest campus at Naperville near Chicago, Illinois, employed 11,000 people. The 191-hectare flagship campus at Holmdel, New Jersey, some 30 kilometres south of New York City, included a magnificent mirrored-glass building also designed by Saarinen in 1962. These ‘industrial Versailles’ did research without much development; they had indeed been converted into universities without students11. As industrial ivory towers, they hoovered up university faculty members and PhD scientists and engineers, promising them time and resources to pursue their own agendas, and offering them open publication policies that allowed their results to appear in the most prestigious journals. By the mid-1950s at RCA in Princeton, half of the staff were theoretical scientists and more than 75% of the contracts were with the military. DuPont, likewise, increased its scientific staff by 150% in the decade after the war, with the greatest growth in fundamental chemistry being at its Experimental Station near Wilmington, Delaware. By the early 1960s, the number of engineers and scientists employed in US industrial research topped 300,00012. John Bardeen, William Shockley and Walter Brattain invented the transistor at Bell Labs in 1947, and were awarded the 1956 Nobel Prize in Physics for their work.Credit: Science History Images/Alamy These leading corporate laboratories — Bell Labs, IBM, Westinghouse, DuPont, RCA (Princeton), Xerox Palo Alto Research Center (PARC, 1970) — became powerhouses of basic science. Between 1956 and 1987, 12 corporate scientists won Nobel prizes. Bell Labs alone has collected eight in physics and one in chemistry since the Second World War, including one for its most famous technology, the transistor, in 1956. In the early 1960s, corporate researchers authored 70% of papers appearing in Physics Abstracts. By 1980, Xerox PARC matched the world’s leading universities on citation impact6,8. With its emphasis on basic science as the necessary prerequisite to any future technological progress, the linear model was a break with the past. It prompted a new interpretation of the historical relations of science and industry. In the 1950s and 1960s, economists, historians and other scholars began to re-examine the latter half of the nineteenth century, and claimed to have discovered a ‘Second Industrial Revolution’. Characterized by the chemical and electrical industries, this revolution involved replacing the old trial-and-error methods of invention used in the dirty industries of the ‘First Industrial Revolution’ (textile factories, coal mines and iron foundries) with science-based methods. In this revisionist history, glamorous synthetic dyes and bright electric bulbs sprang directly from the pure science of organic chemistry and electromagnetic physics. History thus seemed to provide definitive evidence for the necessity of continued funding of basic science, as well as a ready explanation for why US and Western European corporations had dominated the world’s economy for more than a century13. It was not to last. Corporate investment in basic science had been sustained by dominant positions in international markets. AT&T, DuPont, IBM, Kodak and Xerox held more than 80% market shares in their respective core businesses. Then the oil shocks of the 1970s, combined with widespread stagflation (high inflation, slow growth), weakened the US and European economies. Global competition increased, especially from Japanese and South Korean firms. In the early 1980s, growing free trade squeezed profit margins even further. In response, US corporations began to restructure and downsize. Business leaders and shareholders decided that the multi-division conglomerate had become too unwieldy to compete. A new, leaner corporation was required. One way to restructure was outsourcing, replacing internal suppliers with external ones. Corporations began to relocate their manufacturing, once the backbone of the industrial economy, to plants in lower-cost and less-regulated countries. (The pace has only accelerated, especially after 2001, when China joined the World Trade Organization.) Bell Labs in the 1990s: a researcher testing data transmission through fibre-optic cable.Credit: Ovak Arslanian/The LIFE Images Collection via Getty Another way to downsize was divestiture, selling off subsidiaries unrelated to the core business. To shareholders seeking quick profits, long-term corporate research looked like a financial liability. The central laboratory became a prime target. In 1988, RCA sold off its Princeton lab as an independent business, Sarnoff Corporation. In 1993, IBM slashed $1 billion — roughly 20% — from its R&D budget. The German corporation Siemens bought Westinghouse’s Churchill laboratory in 1997, and in 2002, PARC, the former division of Xerox, became an independent company. In 1996, AT&T, following the break-up of its phone monopoly, spun off the vaunted Bell Labs as a separate company, Lucent Technologies (in 2016 this was taken over by Nokia, the Finnish telecommunications company). The Holmdel campus closed in 2007. Within a year, just four scientists remained at Murray Hill doing fundamental physics research. It was the end of an era14. Accompanying globalized competitive markets, liberalized free trade and shareholder short-termism, the US military began to cut back funding for basic science at corporate labs. With the exception of a few years in the early 1980s (US president Ronald Reagan’s Strategic Defense Initiative, the ‘Star Wars’ programme), the US government steadily reallocated research funds to universities and other non-profit organizations, particularly towards medical schools and research hospitals through the National Institutes of Health (NIH). With continuous funding, new fields (molecular biology, biochemistry and biotechnology, for instance) surged past the diminished physical sciences. By 1988, only about 10% of basic research articles in physics were authored by industrial scientists; by 2005, the number had plummeted to less than 3%15. The demise of the corporate research lab heralded the death of the linear-model idea. Many scholars concluded that it was too simplistic. The pathway from science to technology was neither straight nor singular, and perhaps not even one way (technological advances can also lead to scientific discoveries). For corporate executives, investment in basic science did not seem to pay off. DuPont discovered no new nylons; Kodak failed to produce a revolution in photography; RCA lost its edge in consumer electronics; IBM ignored the personal computer; and Xerox PARC let slip the graphical user interface. In the late 1960s and 1970s, small firms such as Intel, Microsoft, Apple, Sun Microsystems and Cisco Systems did commercialize the basic research being done at the larger corporations. Without establishing traditional research labs of their own, these players came to dominate the new information technology (IT) industry. In 1991, for example, when Microsoft created Microsoft Research — one of the largest industrial labs of its generation — its declared mission was not basic science, but innovation. In a more extreme case, Apple co-founder Steve Jobs shut down a fledgling research lab in 1998 in the belief that innovation would not require any investment in R&D. A device at the Google Quantum Computing lab in Goleta, California, in 2017. Researchers at Google AI are aiming to build quantum processors that speed up computational tasks in machine learning.Credit: Greg Kendall-Ball/Nature Until 2010 and the emergence of machine learning, artificial intelligence (AI) and the Internet of Things, most technology companies ignored basic research. In 2012, following Jobs’s death, Apple began investing in R&D again, particularly in AI. Likewise, Amazon, Google, Facebook and Uber began to recruit AI researchers from academia. This brain drain has become so serious that universities have begun to worry about their ability to train future AI researchers. Twenty-first-century corporations value science (particularly, patentable discoveries) and still think that basic research can lead to invention and innovation. They would just prefer that someone else do it (and pay for it). In business terms, they optimize their ‘supply-chain management’, a phrase that gained currency in the 1990s, by replacing stable in-house labs (warehouses of scientists and engineers) with flexible contract research. Their ability to do so was greatly facilitated by the US government and the loosening of antitrust enforcement. The settlement of the monopoly case against Microsoft in 2001, for example, stands in stark contrast to the forced break-up of AT&T in 1984. Moreover, the US government now permitted innovative start-ups to acquire new technologies, patents and licences from other companies and independent non-profit organizations such as Sarnoff and PARC, and to engage in extensive collaborative research with institutes and universities. Microsoft Research, for instance, now has labs around the globe (New York City, Beijing, Bangalore) and on several university campuses (MIT, the University of California, Santa Barbara, and Cambridge, UK), which account for 20% of patents in AI worldwide. Google, by contrast, mostly underwrites academic research through grants, fellowships, internships and visiting positions. Universities have traditionally been the home of basic science. In the twenty-first century they have also become the source of innovation and entrepreneurship, in part because of sweeping changes in US patent law. In 1980, the US Supreme Court (in Diamond v. Chakrabarty) significantly expanded what could be patented to include new life forms. That same year, the US Congress passed the Bayh–Dole Act, permitting universities to patent the results of research funded by the NIH or other federal agencies and conducted on their campuses by faculty members, students and employees. Universities started filing for patents at an increasing rate — from 2,266 in 1996 to 5,990 in 2014. The university is now an inventor16. The most prominent industry that has been transformed by these legal and policy changes has been biotechnology. In 1976, a university biochemist and a venture capitalist founded Genentech, the first biotech firm. Genentech focused, as did other biotech start-ups (Amgen in 1980 and Genzyme in 1981), on translating basic science done in universities and, subsequently, in-house into patents and other forms of profitable IP. They facilitated that linear movement from research to development. Further commercialization towards the manufacture and distribution of drugs and therapies was taken up by traditional big pharmaceutical corporations. Eli Lilly (founded in 1876), for example, guided Genentech’s first drug (synthetic human insulin) through clinical trials and brought it to market17. The emergence of biotech represented both a new business plan (entrepreneurial scientists partnering with venture capitalists to sell their research) and a new model of innovation. Here, industry shifted from a single internal or closed source of research to multiple external or open sources18. In this model, academic entrepreneurs, commercialized universities, globalized contract-research institutes and numerous small research start-ups supply the science and the IP. Larger, more established firms then develop and commercialize these into new products and processes. According to some economists and business scholars, open innovation characterizes a ‘Third Industrial Revolution’19. From their perspective, the university professor seeking to patent the results of federally funded research to form a start-up, with seed money from venture capitalists, is the direct descendant of the consulting chemist of the nineteenth century. In this ecosystem, a population of nimble researchers and small firms has displaced a pack of lumbering corporate labs20. To critics and less-sanguine academics, the twenty-first-century relations of science and industry illustrate the commodification of university research and the corruption of the pursuit of knowledge by the profit motive21. Today, a complex innovation web has replaced the old conveyor belt. This is another new model — global commercialization. Supply-chain science is premised on the belief that research is a fungible commodity to be bought on demand and sold by the lowest-cost lab. In some ways, twenty-first-century contract research is reminiscent of nineteenth-century consulting science. In both cases, the question remains: is marketplace science trustworthy? </body>
<date id = '102'>22 October 2019</date>
<url id = '103'>https://nature.com/articles/d41586-019-03062-w</url>
<title id = '103'>How did data get so big? Through political, social and economic interests, shows Sabina Leonelli, in the fourth essay on how the past 150 years have shaped the science system, marking Nature’s anniversary.</title>
<body id = '103'>Data. The confusingly plural cornerstone of research. The grounding for a scientific understanding of the world. Lightning rods for the negotiation of political, social and economic interests. Over the past 150 years, ideas have shifted drastically as to what counts as data, which data are reliable and who owns them. Once regarded as stable objects whose significance was determined by a handful of professional interpreters, data are now reusable goods. Their mettle depends on the extent to which they are mobilized across contexts and aggregated with others. Growing in volume, variety and value, data have come to drive the very process of discovery. This explicit designation as assets has become possible only through a complex web of institutional, technological and economic developments. The history and consequences of how this web has been woven have repeatedly transformed research and its role in society. Until the start of nineteenth century, efforts to collect facts and objects of study were spearheaded by visionary individuals, typically backed by wealthy patrons. Naturalists roamed the globe in search of biological specimens that were new to science. Court astronomers devised tools to observe new parts of the cosmos. The large quantities of data accumulated were systematized and analysed through simple and powerful models (think Kepler’s laws) and classification systems (such as that developed by botanist Carl Linnaeus). Thus was born the myth of the heroic theoretician, mining order from the chaos of observations. This individualistic view was tied to an understanding of data as fundamentally private — their scientific value residing in conceptual interpretation. The nineteenth century marked a shift. Data, as we now recognize them, became institutionalized as social commodities. Their intellectual, financial and political worth arose from investments, requiring regulation and oversight. The botanical wonder cabinet that was Paris’s natural-history museum was reorganized as a world-leading, publicly accessible repository of objects of potential scientific value. By the 1850s, the natural-history museums of Berlin, London and New York City followed suit.   The centralization of food markets spawned standardized approaches to the valuation and trade of organisms — such as the crop measures devised by the Chicago Board of Trade in Illinois. Cholera epidemics in Europe spurred large-scale collection of information on the spread and targets of disease. New methods of visualization and analysis emerged, such as physician John Snow’s famous maps of how contaminated water spread cholera in central London. National weather services started to build links between data collected regionally. The 1853 Brussels Convention on naval meteorology coordinated ships’ logbooks into the first quasi-global data records for climate science. In Berlin, the first real bureau of standards, the Physikalische-Technische Reichsanstalt, was inaugurated in 1887 with physicist Hermann von Helmholtz as its founding director and a mandate to generate data needed for society as a whole. In the meantime, the US Army tasked the Library of the Surgeon-General’s Office with collecting as many disease case reports as possible. Within 30 years, it had become the largest medical library in the world. By the turn of the twentieth century, the rise of nation states and the increasing demands of international trade drove initiatives to measure nature and society in a more systematic, objective way. National information infrastructures helped regions to share data, marking the start of a new informational globalism1. International entities, such as the League of Nations and the International Monetary Fund, yearned to globalize data collection and analysis for many purposes and across all scientific domains. For example, the League of Nations Health Organization created the Permanent Commission on Biological Standardisation to monitor drug tests and biological assays from 1924. Well before the Second World War, there was increasing momentum to share information on employment, unemployment, wages and migration; from 1947, these data were amassed by the new International Statistical Commission. Such initiatives were fostered by an ever-expanding cadre of researchers, administrators, merchants and politicians. All this fuelled the development of sophisticated approaches to quantification. Statistics emerged as a separate discipline — the main source of information for emerging insurance practices and public-health monitoring systems2,3. Techniques were developed to match the complexity of social exercises such as the census4. Population-level thinking gripped the life sciences, too — for good (genetics) and ill (eugenics). A new type of data collection focused on genetic mutants of a single model species5,6, such as the fruit fly. Microscopy slides used in the first detailed UK report of a link between lung cancer and asbestos.Credit: SSPL/Getty The two world wars severely disrupted data collection and sharing in the short term. But from the 1940s, the huge military investment in intelligence and information technologies kick-started the drive towards mechanized computing. The space race was perhaps the most notable cold-war contribution to globalized data systems and practices, particularly satellite technology. This produced the first global view of the planet and spurred the inauguration of the Intelsat system for worldwide civil-communications networks in the 1960s. The World Meteorological Organization was founded in 1950 to oversee the international linkage of regional weather services, for instance in the Global Atmospheric Research Program. The International Geophysical Year of 1957–58 marked a step change in the commitment of Earth sciences to global data exchange, and was a diplomatic achievement in the middle of the cold war7. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times From the 1970s, almost every scientific field was building global, digitalized infrastructures for data sharing. The United Nations consolidated its global environmental monitoring system just as the World Health Organization systematized its efforts to map the spread of infectious diseases. The holy grail became the development of tools, such as computer models, that could crunch numbers at a previously unimaginable scale. Increasingly, data were seen as sharable assets for repurposing, the value of which could change depending on their use. This view owed much to the cybernetics movement, with its emphasis on modularity and complexity8. Once again, the shifting role of data was also informed by the growth of international trade and the rising recognition of research as an engine of economic growth, military power and international relations. Also in the 1970s, big science such as studies of particle collisions at Los Alamos National Laboratory in New Mexico and at CERN, Europe’s particle-physics lab near Geneva, Switzerland, took centre stage. Here, the production and trade of data were no longer the responsibility of individual researchers. Rather, they were the output of large investment and collective efforts performed in centralized experimental facilities. Such centralization was unfeasible in many fields, for instance in environmental, biological and climate sciences, which work with observational rather than experimental data. Yet even those disciplines were focused on building networks for sharing information so it could be fed into new computational tools. A Hollerith data machine at a steel works in Sheffield, UK, in 1963. The electromechanical device helped workers to tabulate statistics stored on punch cards.Credit: Paul Walters Worldwide Photography Ltd/Heritage Images/Getty Since the 1980s, portable computers, modelling and simulations have shaped data collection, manipulation and archiving. Climate scientists have developed ways to use legacy records to reconstruct a history of the atmosphere at the global level. This effort drove the pooling of international data, culminating in 1992 in the Global Climate Observing System. In biology, the quest to map moved to the molecular level with big genetic sequencing projects, first in model organisms such as the nematode worm Caenorhabditis elegans, then through the Human Genome Project9. Sequencing databases were reimagined as playgrounds for discovery to facilitate immediate sharing, visualization and analysis online at a low cost, transforming the massive investment in genomic data production into useful knowledge. As global data infrastructures and related institutions burgeoned, the resources needed to maintain them have mushroomed, and in ways that do not fit contemporary regimes of funding, credit and communication. For example, the curators of biological databases do essential work. But they do not routinely publish in top-ranking journals and might not be recognized or rewarded as high-level researchers. Similarly, keeping digital platforms robust and fit for purpose requires serious investment. The more data move around and are repurposed, the more vulnerable they are to unwarranted and even misleading forms of manipulation. Over the past few decades, the Open Science movement has called for widespread data sharing as fundamental to better research. This has prompted several changes. One is the birth of journals devoted largely to the publication of data sets. Another is ambitious investment in data infrastructures, exemplified by the European Open Science Cloud. And the FAIR guidelines were crafted for how data should be labelled and managed to make them reusable10. There have also been calls to improve rewards for data stewards (such as technicians, archivists and curators), to raise their professional status from support workers to knowledge creators11.   These reforms are temporary solutions to a large-scale crisis of the contemporary research system, rooted in the inability to reconcile the diverse social and scientific aspects of data. The crisis recalls how the twentieth century reconfigured research data as political and economic assets. Their ownership can confer and signal power, and their release can constitute a security threat — as in the cold-war efforts to contain geological data that could have signalled nuclear testing. Now, new technologies are intersecting with emerging regimes of data ownership and trade. Starting from the 2000s, a handful of corporations has created — and wielded control over — new kinds of data left by billions of people as they meet, work, play, shop and interact online. (Think Amazon and Google.) As algorithms become ever more opaque, the transparency and accountability of techniques and tools used to interpret data are declining. Whereas data curators remain the Cinderellas of academia, those who understand and control data management have climbed company ranks. And concerns are growing around data property rights, especially in the wake of misuses of personal data by the likes of Facebook and the UK company Cambridge Analytica. Such tensions between data as public goods and private commodities have long shaped practices and technologies. Consider, for instance, the acrimonious debate over the ownership and dissemination of genomic data in the 1990s. On that occasion, free sharing won out through the establishment of the Bermuda Rules — an agreement among publicly funded researchers to deposit their sequences in public databases as soon as possible12. Wildly successful, this paved the way for open-data practices in other fields. Yet it also emphasized the financial advantages of owning genomic data13,14 — a lesson swiftly learnt by companies that sequence and claim to interpret clients’ genomes, which typically retain and use such data. Another example is the vast number of patents being filed for synthetic organisms by the chemical industries. Banks of servers at one of Google’s US data centres.Credit: Connie Zhou/Google/ZUMA Press The use of big data as input for artificial-intelligence systems relies on the promise of global, comprehensive, easily available data riches. In principle, the marriage of powerful analytical tools with big biological data can support personalized medicine and precision agriculture. Similarly, social data hoovered up from Internet platforms and social-media services can inform evidence-based policy, business strategies and education. Yet history shows that moving research data around is not so simple. Underpinning technical questions around integration and use are thorny social, ethical and semantic issues. How can different research cultures be encouraged to communicate effectively? What is the best way to collect, share and interpret data generated by the state, industry or social media? Which experts and stakeholders should have a say in data management and analysis? Who should have access to what, when and how? Addressing these issues requires effective administration and monitoring, and a long-term vision of the research domain at hand15,16. It also demands a repertoire of skills, methods and institutions geared to the study of specific research objects17. In summary, data generation, processing and analysis are unavoidably value-laden. The scientific legitimacy of these activities depends on the extent to which such values are held up for public scrutiny. Indeed, the best examples of data-intensive research to this day include strategies and methods to explicitly account for the choices made during data collection, storage, dissemination and analysis. Model-organism databases such as PomBase (for the fission yeast Schizosaccharomyces pombe) and FlyBase (for Drosophila), for instance, clearly signal the provenance of what they store, including information about who created the data, for what purpose and under which experimental circumstances. Users can then assess the quality and significance of data18. Similarly, the Catalogue of Somatic Mutations in Cancer (COSMIC) captures the provenance of its holdings and the interpretive decisions taken by its curators while processing them. This helps clinicians to reassess the value of the information19. The more such assumptions and judgement are filtered by large digital infrastructures, the easier it becomes to hide or lose them, making it impossible for future generations to situate the data adequately. Data are cultural artefacts whose significance is clear only once their provenance — and subsequent processing — is known. Technological development, particularly digitization, has revolutionized the production, methods, dissemination, aims, players and role of science. Just as important, however, are the broad shifts in the processes, rules and institutions that have determined who does what, under which conditions and why. Governance, in a word. Data emerge from this reading of history as relational objects, the very identity of which as sources of evidence — let alone their significance and interpretation — depends on the interests, goals and motives of the people involved, and their institutional and financial context. Extracting knowledge from data is not a neutral act. Building robust records of the judgements baked into data systems, supplemented by explicit reflections on whom they represent, include or exclude will enhance the accountability of future uses of data. It also helps to bring questions of value to the heart of research, rather than pretending that they are external to the scientific process, as has arguably happened in bioethics20. This is a crucial step towards making big-data sciences into reliable allies for tackling the grave social and environmental challenges of the twenty-first century. </body>
<date id = '103'>15 October 2019</date>
<url id = '104'>https://nature.com/articles/d41586-019-03014-4</url>
<title id = '104'>Biological advances have repeatedly changed who we think we are, writes Nathaniel Comfort, in the third essay of a series marking Nature’s anniversary on how the past 150 years have shaped science today.</title>
<body id = '104'>In the iconic frontispiece to Thomas Henry Huxley’s Evidence as to Man’s Place in Nature (1863), primate skeletons march across the page and, presumably, into the future: “Gibbon, Orang, Chimpanzee, Gorilla, Man.” Fresh evidence from anatomy and palaeontology had made humans’ place on the scala naturae scientifically irrefutable. We were unequivocally with the animals — albeit at the head of the line. Nicolaus Copernicus had displaced us from the centre of the Universe; now Charles Darwin had displaced us from the centre of the living world. Regardless of how one took this demotion (Huxley wasn’t troubled; Darwin was), there was no doubting Huxley’s larger message: science alone can answer what he called the ‘question of questions’: “Man’s place in nature and his relations to the Universe of things.” Huxley’s question had a prominent place in the early issues of Nature magazine. Witty and provocative, ‘Darwin’s bulldog’ was among the most in-demand essayists of the day. Norman Lockyer, the magazine’s founding editor, scored a coup when he persuaded his friend to become a regular contributor. And Huxley knew a soapbox when he saw one. He hopped up and used Nature’s pages to make his case for Darwinism and the public utility of science.   It was in the seventh issue — 16 December 1869 — that Huxley advanced a scheme for what he called ‘practical Darwinism’ and we call eugenics. Convinced that continued dominance of the British Empire would depend on the “energetic enterprising” English character, he mused about selecting for a can-do attitude among Britons1. Acknowledging that the law, not to mention ethics, might get in the way, he nevertheless wrote: “it may be possible, indirectly, to influence the character and prosperity of our descendants.” Francis Galton — Darwin’s cousin and an outer planet of Huxley’s solar system — was already writing about similar ideas and would come to be known as the father of eugenics. When this magazine appeared, then, the idea of ‘improving’ human heredity was on many people’s minds — not least as a potent tool of empire. Huxley’s sunny view — of infinite human progress and triumph, brought about by the inexorable march of science — epitomizes a problem with so-called Enlightenment values. The precept that society should be based on reason, facts and universal truths has been a guiding theme of modern times. Which in many ways is a splendid thing (lately I’ve seen enough governance without facts for one lifetime). Yet Occam’s razor is double edged. Enlightenment values have accommodated screechingly discordant beliefs, such as that all men are created equal, that aristocrats should be decapitated and that people can be traded as chattel. I want to suggest that many of the worst chapters of this history result from scientism: the ideology that science is the only valid way to understand the world and solve social problems. Where science has often expanded and liberated our sense of self, scientism has constrained it. Across the arc of the past 150 years, we can see both science and scientism shaping human identity in many ways. Developmental psychology zeroed in on the intellect, leading to the transformation of IQ (intelligence quotient) from an educational tool into a weapon of social control. Immunology redefined the ‘self’ in terms of ‘non-self’. Information theory provided fresh metaphors that recast identity as residing in a text or a wiring diagram. More recently, cell and molecular studies have relaxed the borders of the self. Reproductive technology, genetic engineering and synthetic biology have made human nature more malleable, epigenetics and microbiology complicate notions of individuality and autonomy, and biotechnology and information technology suggest a world where the self is distributed, dispersed, atomized. Individual identities, rooted in biology, have perhaps never played a larger part in social life, even as their bounds and parameters grow ever fuzzier. Frontispiece to Thomas Henry Huxley’s Evidence as to Man’s Place in Nature (1863).Credit: Paul D. Stewart/SPL “Methods of scientific precision must be introduced into all educational work, to carry everywhere good sense and light,” wrote the French psychologist Alfred Binet in 1907 (English translation published in 1914 (ref. 2)). A decade earlier, Binet and Théodore Simon developed a series of tests for French schoolchildren to measure what they called ‘mental age’. If a child’s mental age was less than her chronological age, she could receive extra help to catch up. The German psychologist William Stern took the ratio of mental to chronological age, giving what he called the IQ and, theoretically, making it comparable across groups. Meanwhile, Charles Spearman, a British statistician and eugenicist of the Galton school, found a correlation between a child’s performance on different tests. To explain the correlations, he theorized an innate, fixed, underlying quality he called ‘g’, for ‘general intelligence’. Then the American psychologist Henry Goddard, with the eugenicist Charles Davenport whispering in his ear, claimed that low IQ was a simple Mendelian trait. Thus, step by scientistic step, IQ was converted from a measure of a given child’s past performance to a predictor of any child’s future performance. IQ became a measure not of what you do, but of who you are — a score for one’s inherent worth as a person. In the Progressive era, eugenicists became obsessed with low intelligence, believing it to be the root of crime, poverty, promiscuity and disease. By the time Adolf Hitler expanded eugenics to cover entire ethnic and cultural groups, tens of thousands of people worldwide had already been yanked from the gene pool, sterilized, institutionalized, or both. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times Immunologists took another approach, They located identity in the body, defining it in relational rather than absolute terms: self and non-self. Tissue-graft rejection, allergies and autoimmune reactions could be understood not as a war but as an identity crisis. This was pretty philosophical territory. Indeed, the historian Warwick Anderson has suggested that3 in immunology, biological and social thought have been “mixing promiscuously in a common tropical setting, under the palm trees”. The immunological Plato was the Australian immunologist Frank MacFarlane Burnet. Burnet’s fashioning of immunology as the science of the self was a direct response to his reading of the philosopher Alfred North Whitehead. Tit for tat, social theorists from Jacques Derrida to Bruno Latour and Donna Haraway have leaned on immunological imagery and concepts in theorizing the self in society. The point is that scientific and social thought are deeply entangled, resonant, co-constructed. You can’t fully understand one without the other.   Later, Burnet was drawn to new metaphors taken from cybernetics and information theory. “It is in the spirit of the times,” he wrote in 19544, to believe there would soon be “a ‘communications theory’ of the living organism.” Indeed there was. In the same period, molecular biologists also became enamoured of information metaphors. After the 1953 solution of the DNA double helix, as the problem of the genetic code took shape, molecular biologists found analogies with information, text and communication irresistible, borrowing words such as ‘transcription’, ‘translation’, ‘messengers’, ‘transfers’ and ‘signalling’. The genome ‘spells’ in an ‘alphabet’ of four letters, and is almost invariably discussed as a text, whether it is a book, manual or parts list. Not coincidentally, these fields grew up alongside computer science and the computing industry. The postwar self became a cipher to be decoded. DNA sequences could be digitized. Its messages could, at least in theory, be intercepted, decoded and programmed. Soon it became hard not to think of human nature in terms of information. By the 1960s, DNA was becoming known as the ‘secret of life’. In the late 1960s and 1970s, critics (including a number of scientists) grew concerned that the new biology could alter what it means to be human. The ethical and social issues raised were “far too important to be left solely in the hands of the scientific and medical communities”, wrote James Watson (of DNA fame and later infamy) in 1971. In 1978, Patrick Steptoe and Robert Edwards succeeded with human in vitro fertilization, leading to the birth of Louise Brown, the first ‘test-tube baby’. By 1996, human cloning seemed to be around the corner, with the cloning of a sheep that Ian Wilmut and his team named Dolly. Cloning and genetic engineering have prompted much soul-searching but little soul-finding. There has long been something both terrible and fascinating about the idea of a human-made, perhaps not-quite-person. Would a cloned individual have the same rights as the naturally born? Would a baby conceived or engineered to be a tissue donor be somehow dehumanized? Do we have a right to alter the genes of the unborn? Or, as provocateurs have argued, do we have an obligation to do so? The recent development of potent gene-editing tools such as CRISPR has only made widening participation in such decision-making more urgent. A macaque undergoing a liver transplant from a pig in China in 2013.Credit: VCG/Getty Arguments, both pro and con, around engineering humans often lean on an overly deterministic understanding of genetic identity. Scientism can cut both ways. A deep reductionism located human nature inside the cell nucleus. In 1902, the English physician Archibald Garrod had written5 of genetically based “chemical individuality”. In the 1990s, as the first tsunamis of genomic sequence data began to wash up on the shores of basic science, it became obvious that human genetic variation was much more extensive than we had realized. Garrod has become a totem of the genome age. By the end of the century, visionaries had begun to tout the coming of ‘personalized medicine’ based on your genome. No more ‘one size fits all’, went the slogan. Instead, diagnostics and therapy would be tailored to you — that is, to your DNA. After the Human Genome Project, the cost of DNA sequencing nosedived, making ‘getting your genome done’ part of mass culture. Today, tech-forward colleges offer genome profiles to all incoming first-years. Hip companies purport to use your genome to compose personalized wine lists, nutritional supplements, skin cream, smoothies or lip balm. The sequence has become the self. As it says on the DNA testing kit from sequencing company 23andMe, “Welcome to you.” But you are not all you — not by a long shot. The DNA-as-blueprint model is outdated, almost quaint. For starters, all of the cells in a body do not have the same chromosomes. Cisgender women are mosaics: the random inactivation of one X chromosome in each cell means that half a woman’s cells express her mother’s X and half express her father’s. Mothers are also chimaeras, thanks to the exchange of cells with a fetus through the placenta. Chimaerism can cross the species boundary, too. Human–chimpanzee embryos have been made in the laboratory, and researchers are hard at work trying to grow immune-tolerant human organs in pigs. Genes, proteins and microorganisms stream continuously among almost any life forms living cheek by jowl. John Lennon was right: “I am he as you are he as you are me and we are all together.” Even in strictly scientific terms, ‘you’ are more than the contents of your chromosomes. The human body contains at least as many non-human cells (mostly bacteria, archaea and fungi) as human ones6. Tens of thousands of microbial species crowd and jostle over and through the body, with profound effects on digestion, complexion, disease resistance, vision and mood. Without them, you don’t feel like you; in fact, you aren’t really you. The biological self has been reframed as a cluster of communities, all in communication with each other. These, too, cavort promiscuously beneath the palms. Scientists found that they could use a person’s microbiome to identify their sexual partner 86% of the time7. The communities of greatest similarity in cohabiting couples, they found, are on the feet. The thigh microbiome, by contrast, is more closely correlated with your biological sex than with the identity of your partner. A body part, a cesspool, a subway car, a classroom — any place with a characteristic community — can be understood as having a genetic identity. In such a community, genetic information passes within and between individual organisms, through sex, predation, infection and horizontal gene transfer. In the past year, studies have shown that the communities of symbiotic microbes in deep-sea mussels become genetically isolated over time, like species. In fungi, genes called Spok (spore-killer) ebb and flow and recombine across species by ‘meiotic drive’, a kind of genomic fast-forward button that permits heritable genetic change to occur fast enough to respond to a rapidly changing environment. The genome, as the geneticist Barbara McClintock said long ago, is a sensitive organ of the cell. Epigenetics dissolves the boundaries of the self even further. Messages coded in the DNA can be modified in many ways — by mixing and matching DNA modules, by capping or hiding bits so that they can’t be read, or by changing the message after it’s been read, its meaning altered in translation. DNA was once taught as a sacred text handed faithfully down the generations. Now, increasing evidence points to the nuclear genome as more of a grab bag of suggestions, tourist phrases, syllables and gibberish that you use and modify as needed. The genome now seems less like the seat of the self and more of a toolkit for fashioning the self. So who is doing the fashioning? Brain implants, human–machine interfaces and other neurotechnical devices extend the self into the domain of the ‘universe of things’. Elon Musk’s company Neuralink in San Francisco, California, seeks to make the seamless mind–machine interface — that sci-fi trope — a (virtual) reality. Natural intelligence and artificial intelligence already meet; it’s not far-fetched for them to somehow, someday, meld. Can the self become not merely extended but distributed? The writer and former Nature editor Philip Ball let researchers sample his skin cells, turn them back into stem cells (with the potential to become any organ) and then culture them into a ‘mini-brain’, neural tissue in a dish that developed electrical firing patterns typical of regions of the brain. Other sci-fi staples, such as growing whole brains in Petri dishes or culturing human organs in farm animals, remain a long way off, but active efforts to achieve them are under way. Yet there is a fruit fly in the ointment. Most of these Age-of-Reason notions of identity, and the dominant sci-fi scenarios of post-human futures, have been developed by university-educated men who were not disabled, and who hailed from the middle and upper classes of wealthy nations of the global north. Their ideas reflect not only the findings but also the values of those who have for too long commanded the science system: positivist, reductionist and focused on dominating nature. Those who control the means of sequence production get to write the story. That has begun to change. Although there is far to go, greater attention to equity, inclusion and diversity has already profoundly shaped thinking about disease, health and what it means to be human. It matters that Henrietta Lacks, whose tumour cells are used in labs all over the world, cultured and distributed without her consent, was a poor African American woman. Her story has stimulated countless conversations about inequities and biases in biomedicine, and changed practices at the United States’ largest biomedical funder, the National Institutes of Health. Considering genomic genealogy from an African American perspective, the sociologist Alondra Nelson has revealed complex, emotionally charged efforts to recover family histories lost to the Middle Passage. In the Native American community, creation of a genetic Native identity was a co-production of Western science and Indigenous culture, as the historian Kim TallBear has shown. DNA-based conceptions of ethnicity are far from unproblematic. But the impulse to make the technologies of the self more accessible, more democratic — more about self-determination and less about social control — is, at its basis, liberatory. Nowhere is this clearer than for people living with disabilities and using assistive technologies. They might gain or regain modes of perception, might be able to communicate and express themselves in new ways, and gain new relationships to the universe of things. The artist Lisa Park plays with these ideas. She uses biofeedback and sensor technologies derived from neuroscience to create what she calls audiovisual representations of the self. A tree of light blooms and dazzles as viewers hold hands; pools of water resonate harmonically in response to Park’s electroencephalogram waves; an ‘orchestra’ of cyborg musicians wearing heart and brain sensors make eerily beautiful music by reacting and interacting in different ways as Park, the conductor, instructs them to remove blindfolds, gaze at one another, wink, laugh, touch or kiss. Yet even this artistic, subjective and interactive sense of self is tied to an identity bounded by biology. Since the Enlightenment, we have tended to define human identity and worth in terms of the values of science itself, as if it alone could tell us who we are. That is an odd and blinkered notion. In the face of colonialism, slavery, opioid epidemics, environmental degradation and climate change, the idea that Western science and technology are the only reliable sources of self-knowledge is no longer tenable. This isn’t to lay all human misery at science’s feet — far from it. The problem is scientism. Defining the self only in biological terms tends to obscure other forms of identity, such as one’s labour or social role. Maybe the answer to Huxley’s ‘question of questions’ isn’t a number, after all. </body>
<date id = '104'>08 October 2019</date>
<url id = '105'>https://nature.com/articles/d41586-019-02937-2</url>
<title id = '105'>Shellen Wu traces the rise of the dominant force in science, in the second of a series of essays on the ways in which the past 150 years have shaped today’s research system, marking Nature’s anniversary.</title>
<body id = '105'>The opening ceremony of the 2008 Olympic Games in Beijing featured ancient China’s four great inventions: the compass, printing press, paper and gunpowder. The lesson on display, as taught in classrooms across the country that today publishes the most research papers, is that Chinese innovation in science and technology changed the world. Yet less than a hundred years before, the Chinese philosopher Feng Youlan wrote the provocative essay ‘Why China Has No Science’1. The scholar — trained at Columbia University in New York City — argued that from antiquity, the nation’s philosophical traditions and unique understanding of the human relationship to nature had prevented the spirit of scientific inquiry from taking root. Feng, like many others at the time and since, urged that science was the only salvation for a nation in precipitous decline.   Placing the efforts to change the perceived lack of science in the context of China’s turbulent modern history is key to understanding how the nation arrived at its current superpower state. The red thread that runs through China’s past 150 years is its unwavering belief in science as the path to wealth and power. The entangled relationship between research and nationalism in China has obscured how this belief grew from a combination of foreign influence and Chinese adaptation2,3. Particularly in the 1960s and 1970s, the Chinese government tried to focus on home-grown science, and succeeded in areas such as agriculture and medicine. But in the longer view, the periods of greatest advancement were those when China opened to outside influence. It’s a salutary lesson as we brace for the challenges of the next 150 years, including climate change, resource depletion and space exploration. These require a broad engagement with the world. Catastrophes created the conditions for the development of science and technology in China. The last imperial era, the Qing dynasty (1644–1912), faced a series of humiliating defeats to foreign powers in the nineteenth century, starting with the First Opium War in 1839. These, and the subsequent opium crisis, led to one of the largest ever domestic uprisings. The Taiping Rebellion (1850–64) laid waste to the wealthiest region in the middle of the country, and resulted in as many as 50 million deaths. In 1868, the year before Nature was founded, the first textbook of Western science was published in Chinese, Introduction to Natural Philosophy (Gewu Rumen). It was intended for students at the Interpreters’ College, a school opened by reformers who sought to adapt the empire for a changing world by teaching aspiring officials foreign languages and knowledge from the West. The American who translated the book, William Martin, had no background in science, but understood its importance for improving the fortunes of a country beset by disasters. The book contained illustrations of microscopes and trains, and basic explanations of an idiosyncratic assortment of concepts in chemistry, electricity and physics. Martin and other Protestant missionaries who headed to China in the nineteenth century saw the country as the next frontier in spiritual salvation. The introduction of science through Martin’s textbook and other translated works provided an opening and a way to improve the material well-being of the vast population of an impoverished country. The Chinese people who worked on the translations were less interested in spiritual salvation, but recognized the importance of science as the foundation of the West’s growing military and economic might. They saw its lack as the reason for China’s state of backwardness. Liu Yang became China’s first female astronaut in 2012.Credit: Jason Lee/Reuters By 1863, mathematicians Xu Shou and Hua Hengfang built China’s first steamship, using illustrations from a missionary magazine as a guide. They then helped to establish a translation bureau that introduced numerous scientific works to China. By the end of the nineteenth century, many more Chinese people were convinced that what made the West rich and powerful was science and technology. Thousands of students ventured abroad to study, many to Japan. Seeing science as the way to alleviate their country’s woes, they returned home eager to establish their fields. As the dynasty collapsed in slow motion, missionaries and other representatives of foreign powers became increasingly assertive in the interior. In the hot, dry summer of 1900, simmering tensions burst into the open. Rebels, aiming their ire at foreigners, laid siege to the diplomatic quarters in Beijing. In the first international news sensation of the new century, troops from eight countries, including Britain, the United States and Japan, rescued the trapped diplomats. In the frenzy of destruction and looting that followed, French and German soldiers claimed the observatory on the outskirts of the old city that contained astronomical instruments made for the court by Jesuit priests in the seventeenth and eighteenth centuries. The Germans shipped home a haul of astrolabes and sextants, elaborately decorated with dragons and other royal motifs. (These were displayed in the grounds of a palace at Potsdam outside Berlin until 1919, when the Treaty of Versailles stipulated their return). Worse, the eight invading countries imposed sizeable indemnity payments. These bankrupted the Qing state and hastened its demise. With part of their spoils, the Americans established a scholarship fund — money that a generation of the best Chinese students used to study in the United States. In January 1914, a group of them established the Science Society of China at Cornell University in Ithaca, New York. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times So it was that the flagship organization of Chinese science in the first half of the twentieth century came to be abroad. The founders and subsequent students mostly returned to China and became leaders of their fields, at a time when political instability and lack of centralized funding made research a herculean task. Trained in disciplines that many viewed as essential for the building of a modern country, they set to work on agricultural science, genetics, biology, chemistry and more4. For example, a group of mostly foreign-trained geologists persistently called on the government to sponsor a national resource survey. One, Ding Wenjiang, who co-founded the China Geological Survey in Beijing in 1915, became a prominent public intellectual, engaging in widely publicized debates and calling for increased state funding for the sciences5. His tireless promotion helped geology to become the most cohesive and internationally respected Chinese science in the first half of the twentieth century. Yuan Longping helped to create hybrid rice that gave rise to China’s green revolution.Credit: Adrian Bradshaw/EPA/Shutterstock Meanwhile, Xu Chongqing and Li Fangbai, two Chinese physicists educated in Japan, introduced Einstein’s theory of relativity6. Rockefeller fellows Li Ruqi and Tan Jiazhen returned from the United States to head leading biology and genetics departments. The biologists Hu Xiansu and Bing Zhi advocated for taxonomic study of Chinese flora and fauna7,8. In the decades that followed, these scientists grew increasingly wary of basing their research agenda on foreign models, seeking instead to build a specifically Chinese science. Around this time, the phrase ‘saving China through science’ (‘kexue jiuguo’) appeared frequently in popular writings. Poverty and political turmoil haunted the overseas students. Learning plant physiology and genetics at Cornell, Jin Shanbao was sent spoilt food as a prank by US students, who teased him that it was for his starving countrymen. Deeply upset and eager to alleviate China’s suffering, Jin returned home before finishing his graduate degree. He went on to develop high-yield varieties of wheat, writing “food is the first necessity of people, agriculture is the foundation of the country”9. The belief that science would save the nation reached its height during the Japanese invasion, starting in 1937. Facing vastly superior forces, the Nationalist government retreated far west to the mountainous province of Sichuan. Many scientists willingly followed. Geologists, for example, continued their work from a farm house outside the wartime capital of Chongqing. Photos taken by the British biologist Joseph Needham when he visited in 1943 capture the meagre facilities and the spirit of patriotism among the scientists he encountered. Entranced, he started to study the history of science in China. (Needham went on to publish a monumental book series called Science and Civilisation in China that helped to popularize the idea of the four great ancient inventions inside the country, as well as around the world.)   In sum, an eagerness to solve national problems through science prevailed even before 1949, when Marxist ideology prioritized the applied over the theoretical. Throughout the twentieth century, the country’s biggest challenges were to feed and improve the living standards of a vast and growing population. Whatever their political affiliations, most of the leading Chinese scientists of each era devoted themselves to meeting these challenges. This year — 2019 — is a year of anniversaries. The May Fourth Movement of 1919, a response to the allies’ betrayal of China at Versailles, defined a generation of Chinese intellectuals. Student protests in Tiananmen Square in 1989 built on this tradition, and became a turning point in China’s era of ‘Reform and Opening’. This month marks the seventieth anniversary of the founding of the People’s Republic. In hagiographic accounts of an era replete with horrors, 1949 was the beginning of a ‘New’ China. Such misleading histories gloss over the continuities in the sciences across the regime change. When the Communists crushed the Nationalists in the civil war that followed Japan’s defeat in 1945, most scientists stayed to help rebuild. The new regime continued efforts to develop science that had begun in the previous era. Chinese revolutionary poster promoting the study of science in 1980.Credit: Pictures From History/akg-images Although the ranks of Chinese researchers remained largely the same, in the first decade of the Communist regime, the rhetoric changed dramatically. Science was now explicitly defined as an endeavour of and for the people. Einstein and relativity were out, surveys and mass-health campaigns were (even more) in. At the peak of Sino–Soviet cooperation in the 1950s, 10,000 Soviet advisers worked across China to provide technical and scientific aid to the country’s industrial development. The Communist Party oversaw a complete restructuring of the country’s universities and research institutions to remove US and European influence and model them after those in the Soviet Union. Academia Sinica, China’s premier research institution, established by the Republican government in 1928, was reorganized into the Chinese Academy of Sciences (CAS). Soviet specialists helped to set the first five-year agenda of CAS to focus on resource extraction and other practical applications. In fact, this agenda did not differ radically from the war-time research focus of many Chinese scientists10. Nor did the Chinese scientific community capitulate entirely to Soviet influence. For example, the pseudo-genetics of Lysenkoism, so disastrous for agriculture elsewhere, never became the official position owing to strong resistance from prominent Chinese biologists, despite intense political pressure. The material constraints of conducting science in a poor country shaped this generation of Chinese researchers. Those who continued to make significant progress downplayed their Western training and expertise, emphasizing instead their empathy for the masses. After receiving his PhD at the University of Minnesota in Minneapolis in 1949, entomologist Pu Zhelong returned to mainland China and called for the use of insects over expensive chemical pesticides (which turned out to be the more environmentally sustainable route). In the 1970s, agricultural scientist Yuan Longping and others created hybrid rice, leading to China’s own green revolution. Yuan is said to have learnt from his interactions with farmers in the fields11. The Maoist era also diversified the scientific workforce. Women, peasants and young people were encouraged to challenge the social hierarchy in their villages and workplaces and extolled for their contributions to science. For women, in particular, the 1950s and 1960s dramatically opened up horizons and allowed them to participate in science to an unprecedented degree. Tu Youyou, for example, who won a Nobel Prize in Medicine, did the bulk of her research on the antimalarial qualities of artemisinin during these years. (The transformation was temporary. In the past four decades, gender biases have returned along with market reforms.) But scientists who had longed for a robust state and support for research were quickly disappointed. The Cultural Revolution starting in 1966 shut down the CAS and all universities. Overseas educations became a liability, and the same researchers who had stayed in China out of patriotism in earlier decades found themselves the targets of attacks against elitism. Revolutionary credentials were seen as more important than specialist knowledge. The engineering of dams and other large-scale projects to build socialism by overcoming nature sometimes proceeded against expert advice12,13. Projects important to national defence, including nuclear, rocket and satellite research, designated the ‘Two Bombs and One Satellite’ programme, continued to receive much state support and were protected from political intervention. Led by Chinese scientists, most of whom had trained in Europe and the United States, China became a nuclear power in 1964, and had its first successful satellite launch in 1970.   Fifty years ago, the outlook for science in China more generally was bleak. Many fields ground to a halt as the very institutional structure supporting the advancement of science shut down during the decade of the Cultural Revolution. Elders of science spent years labouring on remote farms and in reform camps. In the personal diaries of CAS vice-president Zhu Kezhen, a meteorologist trained at Harvard University in Cambridge, Massachusetts, stretches of days in the 1960s featured little but “sweeping”. Many fared much worse, some even died. But the idea that science and technology formed the bedrock of modern society never completely disappeared. When the United States and China re-established relations in 1972, US scientists were quick to visit14. Most did not realize the extent of the political suppression their colleagues had faced and were excited by the prospect of exploring socialist science. They noted the stagnant state of theoretical research; fields such as particle physics were decades behind the West. The visitors were impressed by some advances made given the straitened circumstances. In addition to its green revolution, the nation had made significant progress in public health: mass campaigns helped to wipe out schistosomiasis, an infectious diseases that was killing around 400,000 people per year. Many of the scientists involved had spent years in remote fieldwork sites without seeing their families. After the death of Chairman Mao in 1976, the emphasis on science and technology bounced back. In 1978, Deng Xiaoping formally launched a policy known as ‘Four Modernizations’, which placed a renewed focus on agriculture, industry, national defence, and science and technology. By then, the universities and CAS had reopened, and their leaderships were eager to get cracking. In the decades since, the Chinese economy has outwardly come to resemble that of a capitalist country. But the top-down approach forged in the Mao years is still clear. It created a centralized educational and institutional infrastructure for science, which has made it easy to direct strategic investment quickly. The robotics industry, for example, one of the key components of China’s plan to move into high-tech manufacturing by 2025, is based in the northeast of the country because of the proximity to the CAS robotics research centre in Shenyang. Other areas of strength, such as materials science and engineering, also built on the previous era’s interests in overcoming resource shortages and environmental challenges. During this period of reform and opening, scientists who had trained abroad in the 1930s and 1940s and had survived the turmoil of the previous decades found their international networks had value again. A second wave of Chinese students embarked on overseas studies — 5.86 million between 1978 and 2018. Big government investments in the past few years have attracted that talent to return. Over the past century and a half, the belief that science and technology can improve the nation has become deeply embedded in Chinese culture, visible in slogans painted on walls and posters from cities to the countryside. Unacknowledged in these displays is the connection between science and an openness to influences and ideas from abroad.   To walk around Beijing today is to see traces of the history of science everywhere. On the east side, on the Second Ring Road, are the Jesuit astronomical instruments, which rode the turbulent geopolitical tides of the twentieth century. On the other side of town, in a quiet corner of Beijing Zoo, a small sign indicates the place where, in the waning days of the Qing dynasty in 1906, the Ministry of Agriculture, Industry and Commerce opened the first station for agricultural experiments on 70 hectares of land. In the northwest corner of the city, the Interpreters’ College has become the elite Peking University; down the road, another college with historical ties to the United States is today’s Tsinghua University, the leading science and technology school in China. Posters and public displays celebrate scientific development. In book shops, science fiction is the trendiest genre. In well-funded laboratories and state of the art field stations, China is rushing forwards with a firm belief in its status as a scientific superpower. There is another story along this route, that of the engagements with the outside world that transformed the country. In the peaks and valleys of that story is a message — the future requires the internationalism that propelled China’s rise in the past 150 years. </body>
<date id = '105'>01 October 2019</date>
<url id = '106'>https://nature.com/articles/d41586-019-02938-1</url>
<title id = '106'>People can get a false sense of security from flimsy gauze, and linger too long outdoors in toxic air, argue Wei Huang and Lidia Morawska.</title>
<body id = '106'>A woman wears a protective face mask while walking in Beijing.Credit: Nicolas Asfouri/AFP/Getty Across Asia, and increasingly elsewhere, people are wearing medical masks in the street. Young and old, people are tying gauze squares over their noses and mouths when they step outside. In China, the habit began in 2003, when health authorities recommended wearing medical masks to slow the spread of severe acute respiratory syndrome (SARS). Today, many Chinese citizens wear masks regularly, in a range of fabrics and styles, to lower the risk of catching or transmitting colds or influenza — we, too, wear them for this purpose. But masks are increasingly donned for another reason — air pollution. Although cloth can filter out large grains of dust, pollen and sand, it does not block the finer particles that reach the lungs, arteries and veins. These include particulate matter that is less than 2.5 micrometres in size (PM2.5) and ultrafine particles (less than 0.1 µm) as well as toxic vapours emitted by cars and industry. PM2.5 is associated with more than 4 million deaths every year worldwide. People want ways to protect themselves. They are aware of the health risks of dirty air, and social media is full of images of people wearing masks on smoggy days. But public-health bodies such as the World Health Organization (WHO), the American Heart Association and the European Society of Cardiology have no recommendations on the use of masks or portable purifiers against air pollution. Hardly any clinical studies have tested how effective medical masks are against air pollution, or how people use them. It is hard to predict individual risks because people’s exposures and health statuses vary widely. We worry that wearing masks could even make the problem worse. They have the potential to lull people into a false sense of security, encouraging them to spend more time outside in dirty air.   By contrast, there are clear standards for the specialist respirators that professionals working in dirty environments must use, including builders, pavers and traffic police. These are certified for specific situations, and include gas masks and other devices that limit dust inhalation. None is suitable for everyday wear on the streets. Governments and scientists need to educate the public and health workers about the correct ways to avoid risks from polluted air. Researchers need to establish what protections might be valuable in some circumstances. But the only long-term solution is to clean the air. Until then, the message is the same: stay indoors as much as possible when pollution levels are high. Medical staff and patients wear disposable cloth masks to cut the likelihood of contracting or passing on infections transmitted through liquid droplets. For the same reason, some members of the public wear masks to prevent them from spreading viruses when they sneeze or cough. Typically, medical masks are made from three layers of dense cotton or similar materials. These capture the large droplets that carry bacteria and viruses when people exhale. These droplets are typically the size of pollen grains or dust specks (from a few to about 100 μm). But for small particles in the air, and toxic gases (such as nitrogen dioxide, ozone and volatile organics), medical masks, and even the best fabric masks, offer no protection.   Good masks also have pleats or folds to cover the nose and chin fully. They fit some face shapes better than others; they cannot fit tightly over a beard for example. Physicians are trained to put them on correctly1. The public is not. Air and pollutants can flow through gaps between the mask and face to reach the nose and mouth. Behaviours are also important when advising a population. Face masks of any kind are uncomfortable to wear for a long time2. It can be difficult to breathe in one, especially on a hot day. Carbon dioxide can build up and cause drowsiness3. The mask must be taken on and off to talk, eat and drink. The filter can become wet, altering its performance. People sometimes wear the same one many times to save money; once clogged, masks are worse than useless. Many outdoor workers are advised and trained in how to use specialist equipment. These devices comply with standards developed by the US National Institute for Occupational Safety and Health. Simple cotton masks block dust and asbestos on construction sites and in workshops. Others contain active adsorbent materials that filter chemicals and gases more thoroughly. Some respirators have splash-proof face guards and their own air supplies. They’re nothing like a small bit of cloth, in other words. Medical masks have been well researched in clinical settings, and shown to do a good job against the spread of infections. Hardly anything is known about how effective they are against polluted air4,5. The performance of any face mask is inherently hard to quantify. Many factors need to be considered, including the sizes and sources of particles, the type of mask and the face shape and behaviour of the wearer. A child in India wears a face mask on a smoggy day.Credit: Sunil Ghosh/Hindistan Times/Getty Some lab studies of the filtration properties of mask materials have been done. But only a handful of human studies has examined the efficacy of wearing a face mask under real-world conditions. Most of these studies were done in Beijing and Shanghai in China and in the United States. They typically followed people just for a few hours, and most focused on effects on the heart6–9. Few studies have examined the impacts on respiratory health. A few trends have been reported; none were statistically significant. For example, one study found that people who regularly wear masks in polluted conditions have lower blood pressure and more regular heart rates than those that don’t6. Healthy adults might benefit more than people who already have heart conditions6,7. But studies often disagree on everything from the direction and magnitude to the timing and types of response. Rapid changes in pollution levels also make it difficult to assess health impacts. For example, concentrations of gases and ultrafine particles can be 100 times higher at a busy intersection or in a road tunnel than in a back street. And people’s age, gender, health status, medication and activity patterns complicate matters. For example, fit and active people often spend more time outdoors exercising, increasing their exposure. Men are typically more exposed to outdoor pollution; women to pollutants indoors. There are no rigorous studies quantifying how people change their behaviour when wearing face masks. People need to know when, why and how to wear a mask. Experts and authorities need to gather evidence to advise on the situations in which short-term wearing of a mask could be beneficial, such as during a dust storm, or in certain cities in Africa where desert dust is a big component of air pollution. Researchers should collect evidence on the efficacy of mask use against air pollution. Clinical trials should use larger sample sizes and follow-up the long-term impacts in high-risk populations. The WHO and other public-health bodies should educate the public on the best ways to protect themselves. Until better evidence is available, they should recommend wearing a well-fitting mask only for preventing infection, not for protecting against air pollution. Staying indoors when pollution levels are high is safest, as long as indoor pollutants such as tobacco smoke are avoided. Outdoors, people should stay away from heavy traffic when walking and exercising. Cyclists should find routes away from busy roads. Drivers should shut car windows. When pollution is high, it is probably better not to cycle, rather than cycle and wear a mask. People who must work outside for long hours, such as construction workers, should be given professional-quality respirators and training. And the top priority remains preventing air pollution in the first place. </body>
<date id = '106'>01 October 2019</date>
<url id = '107'>https://nature.com/articles/d41586-019-02849-1</url>
<title id = '107'>Virtual models boost smart manufacturing by simulating decisions and optimization, from design to operations, explain Fei Tao and Qinglin Qi.</title>
<body id = '107'>Digital twins — precise, virtual copies of machines or systems — are revolutionizing industry. Driven by data collected from sensors in real time, these sophisticated computer models mirror almost every facet of a product, process or service. Many major companies already use digital twins to spot problems and increase efficiency1. Half of all corporations might be using them by 2021, one analyst predicts2. For instance, NASA uses digital copies to monitor the status of its spacecraft. Energy companies General Electric (GE) and Chevron use them to track the operations of wind turbines. Singapore is developing a digital copy of the entire city to monitor and improve utilities. Machine intelligence and cloud computing will boost such models’ power. There is much to be done to realize the potential of digital twins. Each model is built from scratch: there are no common methods, standards or norms. It can be difficult to aggregate data from thousands of sensors that track vibration, temperature, force, speed and power, for example. And data can be spread among many owners and be held in various formats. For example, the designers of a particular car might hold information on its materials and structure, while the manufacturers keep data on how the vehicle is produced and garages retain information on sales and maintenance. The result? Confusion. A digital twin can fail to echo what is going on in the real world and lead managers to make poor decisions. Here we set out the main problems and call for closer collaboration between industry and academia to solve them. The first step is to decide what types of data to collect3. It is not always obvious. To model a wind turbine, for example, might require monitoring of vibrations from the gearbox, generator, blades, shafts and tower, as well as of voltages from the control system. Torques and rotation rates, temperatures of components and the state of the lubricating oil must also be tracked, together with environmental conditions (wind speed, wind direction, temperature, humidity and pressure). Missing or erroneous data can distort results and obscure faults. The wobbling of a wind turbine, say, would be missed if vibration sensors fail. Beijing-based power company BKC Technology struggled to work out that an oil leak was causing a steam turbine to overheat. It turned out that lubricant levels were missing from its digital twin.   The optimal number and placing of sensors must be determined. Too few, and predictions will be inaccurate; too many, and the user will be mired in detail. The rate of data collection also matters. Engineers might monitor vibrations from a turbine gearbox every minute, meaning they would miss shorter glitches. But sampling every second could yield way too much data, leading to transmission bottlenecks. To illustrate: Google’s self-driving car could produce 1 gigabyte of data each second, according to some estimates. But today’s bluetooth connections can handle only 0.03% of that rate. Disparate data types are hard to merge, too. Vibrations can be recorded as lengths of time or as frequencies; temperatures can be in celsius or fahrenheit; and videos or images might not be to the same scale. Timings can get out of step, especially when data are sampled at different rates. For example, aircraft communication systems send signals every few nanoseconds, while navigation systems record the position of a plane every second. Averaging fine data doesn’t help because detail is lost. Scattered ownership of data is another barrier. For instance, Boeing aircraft include parts from more than 500 suppliers in 70 countries, each with different data interfaces, formats and software. Companies often don’t want to share commercially sensitive information. Nor do countries: Japan restricts the export of some computer chips to competitors in South Korea, and the United States bans the sale of chips and other technology to Chinese company Huawei. To build a digital twin of an object or system, researchers must model its parts. The German manufacturing company Siemens uses many mathematical models and virtual representations of its products and production lines. These include 3D geometric models and finite-element analysis, the latter for tracking temperatures, stresses and strains. Fault diagnosis and life cycles are treated separately. Other errors could arise when software written for different purposes is patched together by hand. And without standards and guidelines, it is hard to verify the accuracy of the resulting models. Many digital twins might need to be combined. For example, a virtual aircraft might incorporate a 3D model of the fuselage with one of a fault-diagnosis system and one monitoring the air conditioning and pressurization. Even the definition of a digital twin is not settled. Some people think any 3D model or simulation counts. More ambitiously, others envisage a set of integrated models or software that pairs the digital world with physical assets, with or without live information from sensors. Each approach has its own norms, with little crossover. A close-knit team of specialists spanning disciplines is thus essential to building a precise digital twin. No one person can know every detail. Materials scientists, metallurgists and mechanics might need to work with engineers, computer scientists and manufacturing experts. The range of disciplines needed will widen as applications diversify. Most digital twins are found in large companies such as GE or Siemens, because it is difficult and expensive to assemble the teams required. With commercial pressures dissuading businesses from sharing models, smaller firms lose out. There is a lack of common space — physical and virtual — in which experts can communicate and share knowledge and software. And there are few connections between industry and academia, in part because of commercial secrecy. Most academic research focuses on improving modelling techniques rather than on optimizing data and implementing digital twins. The following steps would make research and development of digital twins more coherent. Unify data and model standards. Manufacturing data should be standardized and delivered in common formats such as XML (Extensible Markup Language), which is used in areas from electronic commerce to supply-chain software. Other data standards should be adopted where they exist. For example, the electricity sector uses COMTRADE (‘common format for transient data exchange’), a standard overseen by the Institute of Electrical and Electronics Engineers; the construction industry uses Industry Foundation Classes; and international health-care organizations require data to conform to HL7 (Health Level 7) standards.   A universal design and development platform for digital twins should also be developed on which all models can run. One step in the right direction is a virtual shared workspace, the Global Collaborative Environment, created by aircraft maker Boeing to align practices among its corporate partners. Corporations, foundations, universities and governments should set up and fund an association to oversee a broader one. It could emulate the chip industry’s non-profit research consortium founded in 1982. Called the Semiconductor Research Corporation, this is based in Durham, North Carolina. Share data and models. A public database for sharing digital twins should be created, to be managed by government funding agencies or by a coalition of universities and enterprises. Issues of data ownership and openness will need to be addressed. One such example is the openVertebrate platform, funded by the US National Science Foundation. It allows researchers to freely share data and models of vertebrate anatomies. Digital images and 3D mesh files can be explored, downloaded and 3D-printed on MorphoSource, an open-access online database. Curators can oversee ‘virtual loans’ of their specimen data and receive updates on their use. Platforms of this kind would allow researchers from industry to purchase digital twin data and models, or to lease them to others to conduct research and develop business applications. Innovate on services. Companies should develop products and services to help digital twins become easier to build and use. For example, Siemens’ NX software combines design, simulation and manufacturing tools in one package. Canadian company LlamaZOO has developed a virtual-reality/augmented-reality application that enables mining supervisors to monitor their vehicles. The virtual forest developed by Metsä Group, Tieto and CTRL Reality, all based in Finland, simulates different forest-management methods and their impacts on income and the landscape. Establish forums. Practitioners and researchers need an online space where they can discuss, develop and publish specifications. That is why, in 2017, we set up a social media group on digital twins on the Chinese social-media platform WeChat. Foundations, universities and companies should offer similar forums. Physical ‘innovation hubs’ should also be set up in mutually accessible locations to connect industry, data scientists, cybersecurity experts and engineering and business strategists. One example is the Smart Innovation Hub on the campus of Keele University, UK, alongside Keele Business School. And business consultants Booz Allen Hamilton run several such hubs in Washington DC, near federal government agencies. </body>
<date id = '107'>25 September 2019</date>
<url id = '108'>https://nature.com/articles/d41586-019-02848-2</url>
<title id = '108'>David Kaiser traces the roots of government support for science, in the first of a series of essays on how the past 150 years have shaped the research system, marking Nature’s 150th anniversary.</title>
<body id = '108'>Late in August 1609, the Italian astronomer Galileo Galilei wrote excitedly to his brother-in-law, relating the fast-moving events of that summer. A few weeks earlier, Galileo had heard rumours that a spyglass had been invented in Flanders (now part of Belgium). He quickly produced an improved version, setting off a new wave of rumours. Soon, the Venetian senate called on him to demonstrate his device. Galileo boasted to his family about the “numerous gentlemen and senators” who had “scaled the stairs in the highest campaniles in Venice to observe at sea sails and vessels so far away that … two hours or more were required before they could be seen without my spyglass.” The senate voted immediately to grant Galileo an appointment for life at the University of Padua in Italy, with an annual salary of 1,000 florins — back when 1,000 florins really meant something1. Galileo was only getting started. Turning his new telescope towards the heavens, he discovered (among other things) four moons orbiting Jupiter. Craftily, he named them the Medicean Stars in honour of Cosimo II de’ Medici, the grand duke of Tuscany. The gambit worked: within a year of that letter about his Venetian success, Galileo had landed an even larger salary (and shed his teaching duties) as the official natural philosopher of the Medici court in Florence2.   Galileo had a knack for convincing government officials and courtly patrons to support his research. Tracing his exploits, as he darted from one benefactor to the next, we might recognize glimmers of today’s enterprising scientists. A good 250 years after Galileo’s time, however, a rather different relationship between government and science began to take hold. Just when astronomer Norman Lockyer was founding Nature in 1869, major shifts in the government–science nexus were unfolding across many parts of the world. During the middle decades of the nineteenth century, the British Empire swelled to include about one-quarter of Earth’s land and to hold dominion over nearly one-quarter of its population. At this time, several prominent British politicians — including former and future prime ministers — sought to boost the fortunes of science and technology. In the 1840s, Robert Peel, Benjamin Disraeli, William Gladstone and others donated funds from their own coffers to help found the Royal College of Chemistry, convinced that focused research in this field would benefit the nation and its imperial ambitions. By the 1860s, many researchers were hard at work formalizing such arrangements. Construction began on a spate of laboratories at universities throughout the United Kingdom, each built on the promise that precision measurements of physical quantities could advance fundamental scientific understanding and spur industrial development. Electrification, telegraphy, the expansion of railways and large-scale production of steel were the signature developments of an era often called the second industrial revolution, which began around 1870. Each demanded standard units and measures. New synergies emerged as leading researchers, including James Clerk Maxwell and William Thomson (later Lord Kelvin), plied their understanding of electromagnetism and thermodynamics as members of high-level government commissions, aiming to tackle the challenges of transatlantic communications, electrical standards, ocean navigation and steam power3.   In some ways, the British were playing catch-up. Since the mid-nineteenth century, local universities throughout the German-speaking states had been recruiting academic talent in contests for prestige — latter-day Galileos were snatched up by government-funded institutions. The pattern escalated rapidly after the Prussian defeat of France and the establishment of a unified Germany early in 1871. Under a centralized education ministry, and with even grander ambitions for rapid industrialization, the German government invested heavily in academic research across the natural sciences4. Even amid such support, however, leading industrialists such as Werner von Siemens feared that Germany was losing its edge. Concerted lobbying led to the establishment of a new government-funded institution in 1887: the Physikalisch-Technische Reichsanstalt in Berlin. Headed by the physicist Hermann von Helmholtz, its mandate was to accelerate work at the intersection of basic science, applied research and industrial development. Within a few years, pioneering efforts there to evaluate competing proposals for large-scale street lighting — which required careful measurements of the radiation output from various devices — yielded such precise recordings of the spectrum of blackbody radiation that prevailing physical theories could no longer accommodate the data. Inspired, physicist Max Planck reluctantly broke with Maxwell’s electromagnetic theory and took his first, tentative steps towards quantum theory5. Meanwhile, a different war with Prussia triggered significant changes in government and science to the east, when the Austro-Hungarian empire formed in 1867. Very quickly, the imperial authorities launched epic efforts in meteorology and climatology. The aim was to create extended institutional networks that might foster a new, common sense of purpose across the hotchpotch of local legal, religious and linguistic traditions. Universities, museums and other government-supported institutions began to collect and standardize weather recordings, with a goal of understanding how local patterns related to larger-scale phenomena. The imperative to unify the far-flung empire catalysed cutting-edge research on such modern-sounding concepts as regional interactions and interdependencies across scales from microclimates to continents6. By that time, Tsar Alexander II in Russia was busy pursuing a modernization project of his own. Beginning in 1861, he issued a series of proclamations that came to be known as the Great Reforms. Emancipating the serfs was followed quickly by overhaul of the state-run universities, as well as changes to regional governments and the judicial system. The vast bureaucracy that was created meant new opportunities for ambitious intellectuals, including chemist Dmitrii Mendeleev. After two years of study in Heidelberg, Germany, Mendeleev returned to his native St Petersburg in 1861 to teach chemistry at the local university. He published his now-famous version of the periodic table of the elements in 1869, the same year that Nature was launched.   The next steps in Mendeleev’s remarkable career are emblematic of the expanded roles of science and technology in the era. Before long, he was consulting for the Ministry of Finance and the Russian Navy, ultimately serving as director of the country’s Chief Bureau of Weights and Measures, in which capacity he helped to introduce the metric system in Russia. Much like Otto von Bismarck and other nation-builders in Germany, Tsar Alexander II was eager to bolster industrial development throughout his country. Central to those efforts was investing heavily in precision metrology; the tsar found eager and skilful natural scientists such as Mendeleev to help7. In the same decade, Japan underwent enormous changes, too. The Meiji Restoration of 1868 marked a period of opening up for the formerly isolated country. The emperor’s Charter Oath proclaimed that: “Knowledge shall be sought all over the world, and thereby the foundations of imperial rule shall be strengthened.” The government began investing in manufacturing and other industrial reforms. It instituted new public schools and funded fellowships to send students abroad to study advances in science. The central government brought senior scientists from other countries — such as Britain and the United States — to Japan to build up training in state-funded facilities. Here, too, leaders began to prioritize government-sponsored research institutions as part of the modern state-building effort8. Read more of this collection published to mark Nature’s 150th anniversary, in which leading historians explore how the past century and a half has forged some of the defining features of today’s scientific system. We ignore the past at our peril Government: Discovery is always political China: How science made a superpower Identity: How advances have repeatedly changed who we think we are Data: From objects to assets Can marketplace science be trusted? Ethical research: the long and bumpy road from shirked to shared Science must move with the times The United States remained a stubborn outlier. The timing was far from promising for new investment. The bloodiest conflict in US history sputtered to an end in 1865, punctuated by the assassination of President Abraham Lincoln. (More US soldiers died during the civil war of 1861–65 than during the First and Second World Wars and the wars in Korea, Vietnam, Afghanistan and Iraq combined.) Support for scientific research and institutions at the federal level remained scarce until the end of the nineteenth century. Indeed, several leading policymakers were scandalized by the nation’s comparative lack of scientific and technical preparation during the First World War. Efforts by reformers in the United States to shore up government support for research was stymied by the long-standing US tradition that education should remain the province of state and local authorities, rather than the federal government. Across the United States, individual colleges and universities gradually placed greater emphasis on original research and built up infrastructure for laboratories. But the impact remained uneven at best. As late as 1927, when the young physicist Isidor Rabi travelled to Germany to study quantum theory, he found that university libraries tended to order one full year’s worth of US journal the Physical Review at a time. There seemed to be no reason to receive copies with any greater frequency, given their undistinguished contents9. Science was even largely ignored in the grips of the Great Depression of the 1930s, when the federal government centralized so many other things under President Franklin D. Roosevelt’s New Deal. US students protest in 1969 over links between university scientists and the military.Credit: Joyce Dopkeen/The Boston Globe/Getty Only in the early 1940s, amid emergency wartime mobilization, did the US federal government undertake large-scale support for research and development. Radar, nuclear weapons, the proximity fuse and dozens of other military projects required billions of dollars and close coordination between abstract studies and practical development. The effectiveness of the wartime arrangements impressed politicians, military planners and university administrators alike. When peace came, they scrambled to build a new infrastructure that could maintain the war-forged relationships. Budgets across the physical sciences and engineering in the United States continued to rise thereafter, sourced almost entirely from the federal government. In 1949, 96% of all funding in the United States for basic research in the physical sciences came from defence-related federal agencies. By 1954 — four years after the founding of the civilian US National Science Foundation — that proportion had risen to 98%10. Thereafter, policymakers in the United States found new reasons to support research: it helped to meet domestic goals for industrial development and military defence, and was a key element in international relations. Federal investment in scientific institutions across war-ravaged Europe — so the thinking went — might fend off scientists’ flirtations with communism in countries such as France, Italy and Greece. Major reforms of the Japanese university system under US occupation after the Second World War likewise helped to spread the US model. Spending on science became an investment in hearts and minds11,12.   In the United States, the steady federal investment drove an unprecedented growth in scientific research and infrastructure. More young people were trained in the natural sciences during the 25 years after the end of the Second World War than had been trained in total throughout all of previous human history. The US government developed a national laboratory system and supported a broad spectrum of research at universities, most of it with little direct connection to military projects. The expenditures were often justified in terms of broader ‘preparedness’: creating a large pool of trained personnel who would be available to work on focused military projects should the cold war ever turn hot13. In the meantime, enterprising scientists made use of opportunities that came from close ties to military sponsors. US Navy concerns about submarine warfare drove intense exploration of the ocean floor. Geoscientists, capitalizing on new data and instruments, found compelling evidence for plate tectonics14. Similarly, physicists consulting on classified missile-defence projects spurred the development of new areas of study, such as non-linear optics15. That ‘new normal’ held for about a quarter of a century. Just as Nature marked its 100th anniversary in 1969, military auditors in the United States released a lengthy analysis, dubbed Project Hindsight. It argued that the federal defence agencies had received a poor return on their investment in open-ended science. That year, Democratic Senator Michael Mansfield (Montana) — who would soon become the longest-serving majority leader of the senate in US history — introduced a last-minute amendment to the federal Military Authorization Act of 1970. It stipulated that no funds from the Department of Defense could be used “to carry out any research project or study” that did not have “a direct and apparent relationship to a specific military function”. On university campuses across the country, debate over the government’s role in supporting scientific research became even more raucous. Amid the escalation of the Vietnam War, scientists and students grappled with the proper place of defence spending in higher education. At Columbia University in New York City and the University of Wisconsin–Madison, radicals targeted military-funded research laboratories with explosives. On many other campuses, police resorted to tear gas and billy clubs to disperse angry protesters16.   During the 1970s and 1980s, scientists forged partnerships with private industries as well as philanthropies. These relationships were accelerated by steep cuts in federal spending on defence and education in the United States and in many other parts of the world. Biotechnology and nanotechnology emerged in those years, buoyed by systems of support that were different from the government spending that had underwritten research in nuclear physics after the Second World War17. Recent, hybrid patterns of support still depend heavily on central-government funding — just consider how closely scientists follow each year’s appropriation cycle in the US Congress and elsewhere. But support for research today is rarely sustained by the kind of saturation model that had seemed so natural early in the nuclear age. Fewer than 20 countries currently invest more than 2% of their gross domestic product in research and development, according to data from the Organisation for Economic Co-operation and Development and the World Bank. In several of those countries, meanwhile, the nature of government support has shifted, often prioritizing projects with short-term goals and practical applications over longer-scale inquiries. When Lockyer was sending the first issue of Nature off to press, many elements of the modern scientific enterprise were being forged across Britain, the European continent and parts of Asia. But to fully grasp the range of monetary relationships that scientists now navigate — scouring today’s equivalents of the Venetian senate for funds, while courting private donors in Kavli Institutes and Simons Foundation centres that are no less sparkling than a Medici palace — we would do well to keep Galileo in mind. </body>
<date id = '108'>24 September 2019</date>
<url id = '109'>https://nature.com/articles/d41586-019-02737-8</url>
<title id = '109'>An approach that tackles the underlying causes of coral-reef decline could be applied to other habitats, argue Tiffany H. Morrison, Terry P. Hughes and colleagues.</title>
<body id = '109'>All the coral reefs in the world could be gone by 2070 if global heating continues on its current path1,2. Since 1998, heatwaves have bleached or killed corals in more than 90% of reefs listed as World Heritage sites worldwide (including in the Galapagos Islands, Hawaii and Australia)2 (see ‘Under pressure’). In the Great Barrier Reef, the world’s largest reef system, half of the corals died in 2016 and 2017 alone3. Coral reefs cover only 0.5% of the ocean floor, but they support almost 30% of the world’s marine fish species. Their loss has huge implications for biodiversity and for the roughly 400 million people who depend on them for work, food and protection from waves, storms and floods in more than 100 countries across Australasia, southeast Asia, the Indo-Pacific, the Middle East, the Caribbean and the tropical Americas. We think a change in approach is urgently needed from the slew of groups striving to safeguard reefs: ecologists, conservationists, non-governmental organizations, national and regional policymakers, and philanthropists. Such groups must address the causes of reef ecosystem decline — not just focus on biodiversity, or on trying to restore a particular reef or region to some idealized ‘prior state’, for instance by establishing marine parks. Policymakers in Australia, say, should try to change land use in the 425,000-square-kilometre catchment of the Great Barrier Reef. Currently, they funnel some US$14 million each year into local-scale approaches such as coral gardening to restore damaged reefs. Instead, they should replace coal-fired power with renewable energy sources, develop land-based aquaculture (which avoids the release of animal waste and antibiotics into the sea), and restore or rehabilitate terrestrial vegetation, wetlands, mangroves and seagrass. All of these actions would simultaneously reduce emissions, capture carbon, curb agricultural runoff onto coastal reefs and enhance people’s livelihoods and food security. Thus, the benefits would extend far beyond the preservation of coral reefs. Source: see Supplementary information There is enormous interest in coral reefs worldwide, and growing concern about the pace of their decline. Done right, efforts to save reefs could protect other ecosystems by safeguarding coastal catchment areas and providing a model for similar approaches that could be applied to diverse systems. Indeed, the plight of coral reefs could finally help to push nations past a societal and political tipping point, where the protection of ecosystems, with their multiple services and functions, is seen as socially and politically necessary. Raised sea temperatures during heatwaves can kill sensitive corals or prompt them to expel the beneficial microscopic algae (Symbiodinium and related genera) living in their tissues, resulting in mass bleaching. Over one or two decades, most coral populations depleted by bleaching can recover. But the gap between consecutive bleachings has shrunk drastically — from an average of 25 years in the 1980s to just 6 years since 20104. Already, people in the 22 small-island nations and territories of the southwest Pacific have increased their reliance on imported foods, including canned meats and packaged products, in part because of depleted fish stocks. Food imports to countries such as Samoa and Tonga now exceed total exports5. What’s more, deaths in the Pacific from preventable diseases, such as diabetes, cardiovascular disease and cancer, have risen in part because of the dietary and lifestyle changes that have accompanied people’s increased reliance on imports. (Six years ago, these diseases caused 80% of all deaths; in 2017, they caused 86%6.) The decline of reefs, especially in the past five years, has prompted scientists, policymakers, non-governmental organizations and philanthropists to undertake increasingly desperate attempts to save or restore targeted coral species. (The most recent 2014–17 global bleaching event was the longest ever recorded, and caused around 70% of the world’s reefs to bleach once or more in consecutive, record-breaking hot years7.) Biologists track corals in nurseries amid renewed bleaching in French Polynesia.Credit: Alexis Rosenfeld/Getty In Australia, engineers are using a robot to disperse coral larvae to degraded sites and experimenting with underwater fans to create cooling, artificial upwellings. In Florida, surviving corals are being transferred from degraded reefs to aquaria to rescue them from disease. In many countries, artificial reefs are being built to promote biodiversity, or corals are being reared in the laboratory or in underwater ‘nurseries’ and later transferred to reefs. Coral sperm are even being frozen in the hope that populations could be restored in the future8. Such interventions are difficult, expensive and labour-intensive. Replanting coral fragments grown in a nursery costs between $1 million and $4 million per hectare9. Thus, even without factoring in ongoing maintenance, restoring 10 km2 of reef would cost in the region of $1 billion9. None of these approaches will restore the ecological functions of reefs at a meaningful scale. (Indeed, given the current levels of human influence on the environment, restorations to past ecological conditions and past levels of biodiversity are no longer possible10.) What’s more, small-scale attempts at coral gardening, aquarium breeding and cryopreservation can convey a misleading message: that the decline of coral reefs is solvable without rapid, coordinated action on climate change caused by human activity. We think that a bolder approach to the governance of coral reefs should be inspired in part by ecological theory on synergistic effects11. Over the past decade, ecologists have found that the response of reefs to any one pressure, such as overfishing or pollution, is typically non-linear. Numerous feedbacks and multiple drivers reinforcing each other can lead to new stable states11. For example, agricultural runoff can promote the growth of algae that compete with corals for space — a problem that is exacerbated by the overfishing of herbivores. Under many pressures, the capacity of corals to reassemble after disturbances might be so reduced that a new stable ecosystem results, often consisting mainly of mats of algae or cyanobacteria. The sustained protection of coral reefs similarly requires making many changes at once: reducing greenhouse-gas emissions, rebuilding fish stocks and improving water quality. This requires policymakers to work with a much broader range of social actors, including commercial and recreational fishers, farmers, the tourism industry, mining companies, energy providers, property developers and individual citizens. The importance of such broader-scale, synergistic policy interventions to protect ecosystems is starting to be recognized in multilateral policy agreements; by major environmental non-governmental organizations and international aid agencies; and even by some countries. A tethered robot — LarvalBot — disperses coral larvae on 3 hectares of the Great Barrier Reef.Credit: Gary Cranitch, Queensland Museum/Great Barrier Reef Foundation The Ramsar Convention on Wetlands of International Importance, for example, now highlights how the conservation of wetlands can help to achieve multiple goals. They can conserve biodiversity, act as carbon sinks and provide ecosystem services such as protection against flooding. (In 1971, when the convention was established, it focused much more on the conservation of species than on the broader benefits of wetlands.) Likewise, non-governmental organizations are beginning to combine ecological, economic and social interventions to help sustain ecosystems. For instance, environmental charity the Nature Conservancy in Arlington, Virginia, is considering funding the installation of sewage-treatment plants and other infrastructure that would improve public health as well as prevent sewage from reaching coral reefs12. And agricultural research and development agencies, such as CGIAR in Montpellier, France, are supporting integrated national policies for agriculture and the environment in Pacific island nations to address the declining supply of reef fish and the altered movement of pelagic stocks due to climate change. (Historically, agricultural and environmental ministries in most countries have operated in silos, or even in opposition13.) Meanwhile, the Philippines, Indonesia, Malaysia, Papua New Guinea, the Solomon Islands and Timor-Leste have formed the Coral Triangle Initiative on Coral Reefs, Fisheries and Food Security. Established formally in 2014, this transnational partnership initially focused on the conservation of marine biodiversity, megafauna and coastal resources. Since August 2017, the six governments have also begun to address the management of mangroves, seagrass meadows and tidal marshes to help sequester carbon. A similar initiative called the Global Environmental Facility Pacific Ridge to Reef Programme is seeking to promote sustainable energy and food production, while reducing global greenhouse-gas emissions and pollutant runoff into coastal waters, in 14 Pacific island nations14. So, on many fronts, the approach we endorse is gaining traction. The major challenge now is establishing the governance, organizations, mechanisms and funding to realize broad, synergistic goals at scale. Conventional governance — for instance, of fisheries or agriculture — is typically uncoordinated, competitive and short-sighted15. We think that catchment-based agencies could better integrate environmental, social and economic planning. A good example is the 86-year-old Tennessee Valley Authority in the United States. This agency led innovations in education, agriculture and energy use in the 1930s to help lift the region out of the Great Depression15. Also, various nascent and underused funding schemes could complement conventional, popular ones. These underused schemes include green economic stimuli, such as public–private partnerships to facilitate the development of renewable-energy systems16; and ‘debt-for-change’ schemes, whereby organizations such as the Nature Conservancy help to pay off a country’s debt in exchange for the nation reducing its environmental impact. More-controversial industrial and philanthropic payments for ecosystem services and ecosystem insurance schemes, such as the coral reef insurance programme deployed in Mexico in the past year17, could also help. Already, key agencies are extending the responsibility for safeguarding coral reefs beyond local reef governments, managers and users. For example, in a major departure from the past (see Supplementary information), the United Nations Educational, Scientific and Cultural Organization (UNESCO) World Heritage Centre is beginning to focus on the long-term and distant drivers of reef decline2, not just on the symptoms of reef degradation. So, too, are the US National Oceanic and Atmospheric Administration and the Great Barrier Reef Marine Park Authority. In July, the latter called for immediate reductions in global greenhouse-gas emissions and acknowledged that local actions on water pollution and fishing pressure cannot reverse the ongoing decline of reefs. That same month, the Climate Vulnerability Index (cvi-heritage.org) was presented at the World Heritage Committee’s annual meeting in Baku. The index assesses the risk to individual World Heritage sites (29 of which are coral reefs) from extreme climate events and changes in climate over the next 30 years. Some environmentalists are suggesting that this type of index could also be used to track accountability. Specifically, such tools could compare the vulnerability of a country’s World Heritage site with its national policy on carbon emissions and its progress on achieving Paris climate agreement commitments. Conceivably, a World Heritage site at high risk in a country reneging on its promises could be classified as ‘in danger’. Such a development would represent a radical expansion of UNESCO’s oversight, and would highlight for the first time the responsibilities of individual nations for protecting World Heritage sites from climate change. Ultimately, coral reefs will be lost unless global carbon emissions are slashed to 45% of 2010 levels by 20301. Yet a bolder, scaled-up approach to the stewardship of land and sea — focused initially on coral reefs — could itself help society to meet this goal. International attempts to address climate change have repeatedly derailed1. At a national level, vested interests, entrenched priorities and social inertia are enormous barriers to change. Australia’s lock-in to coal is a case in point18. But individual countries such as Bhutan and Costa Rica, states such as California and Tasmania, and even certain cities such as Copenhagen and Canberra are setting powerful examples for the rest of the world with mitigation and adaptation initiatives. California, the world’s fifth-largest economy, has set a target date of 2045 for carbon neutrality. In August, the state brokered a deal with four leading carmakers to reduce air pollution, despite strong criticism from US President Donald Trump. Powerful efforts to protect coral reefs could similarly set an example for the world. Reefs are revered worldwide. Their loss has even inspired ‘last-chance tourism’19 and what psychologists and others are labelling ‘ecological grief’20. The plight of these charismatic and stunningly beautiful systems, and of the people who depend on them, is rapidly galvanizing a broad spectrum of support — from the United Nations to film stars, youth movements and industry barons. In 2017, the Leonardo DiCaprio Foundation in Beverly Hills, California, began to install a self-funding system of renewable energy in coral-reef communities across Fiji, in partnership with the Fijian government and others. In 2018, the charity Bloomberg Philanthropies in New York City announced that it would provide $86 million to help build the resilience of coral-reef and fishing communities in ten countries, including Australia, Fiji, Indonesia and the United States. And in May this year, when students in more than 1,600 cities across 125 countries walked out of school to encourage more climate action, coral reefs were the centrepiece of many of the protests. We urge scientists, policymakers, non-governmental organizations and philanthropists to tap into this energy and develop a bold strategy to protect reefs, other ecosystems and people in our warming world. </body>
<date id = '109'>18 September 2019</date>
<url id = '110'>https://nature.com/articles/d41586-019-02736-9</url>
<title id = '110'>Lawyer Farhana Yamin explains what drove her to civil disobedience after three decades of environmental advocacy for the IPCC, the United Nations and more.</title>
<body id = '110'>On 16 April this year, I superglued my hands to the pavement outside the headquarters of the oil company Shell in London, surrounded by dozens of policemen. Once unstuck, I was arrested for causing criminal damage. I have been a lead author for the Intergovernmental Panel on Climate Change (IPCC) for three of its five assessment reports, and an adviser in the United Nations climate negotiations for almost 30 years. Why did I, an international environmental lawyer, break the law? Having spent three decades failing to get governments to pay attention to the climate crisis through advocacy at the highest levels, I felt that activism was now crucial. I wanted to show how ridiculous it is that a law-abiding (indeed, law-making) mother of four should be handcuffed while the world’s major polluters remain unaccountable for ecocide.   My arrest was part of a wave of peaceful protests against the UK government in April 2019, organized by the global movement Extinction Rebellion, or XR. It uses non-violent civil disobedience to demand radical action to tackle what many of us now refer to as the climate emergency. Until June this year, I coordinated XR’s political strategy team. My role was to find ways to build momentum across the party spectrum and organize negotiations with government. I’ve now returned to my profession: helping governments of developing and developed countries to implement commitments under treaties such as the 2015 Paris climate agreement, and through national laws that have created carbon markets. I will focus on providing legal assistance to vulnerable communities, advising them on how they can enhance ambition in the run-up to the next round of Paris agreement negotiations in Glasgow, UK, in December 2020. Such treaties and laws provide a crucial framework for action. But sadly, weak legislation and tweaks to ‘business as usual’ practices have not prevented environmental devastation. The current form of capitalism is toxic for life on Earth. It is based on the never-ending extraction of nature and an unjust appropriation of resources that belong to historically marginalized communities. In their current forms, green taxes and tradeable carbon permits let polluters pay to play the same old games. The global economy must be fundamentally reconfigured into a circular system that uses fewer resources and is based on renewable technologies. The time for half measures has run out — as made plain by the 2018 IPCC special report on the impacts of a 1.5 °C rise in global average temperatures. That’s why I chose to get arrested.   By now you might have labelled me an extremist, here to boast about her mid-life flirtation with the barricades. Talk of injustice, devastation, emergency and the need for radical change is far removed from the neutral vocabulary used by the scientific community in journals such as Nature. But these seemingly emotional terms now fit the facts — and they effect change. I’d rather be labelled ideological than mislead the public into complacency. Many of my climate colleagues were surprised when I became an activist. But since my arrest, they have applauded what I, and thousands of fellow rebels, did in shifting the political discourse (see ‘Advice for potential activists’). Many others still question whether disruptive, mass civil disobedience is really necessary. I believe it was, and remains so. In large part, this is because it is producing the sorts of positive rapid result I could only dream of in my years of committee-sitting and draft-wrangling. Representatives of UK political parties on all sides congratulated XR for its festival-like actions that shut down large parts of central London for ten days in April. In a few months, XR put the need for global system change on the political map at the highest levels, confounding its detractors. In the United Kingdom, where XR was founded and is strongest, public support for climate action is now at record levels. XR’s political strategy team met separately with the UK government, the Mayor of London and the opposition Labour Party. On 1 May, Parliament passed a non-legally binding emergency motion that recognized the climate crisis. A month later, it legislated a legally binding target of net zero greenhouse-gas emissions by 2050, making the United Kingdom one of the first countries to do so. The date is nowhere near soon enough, but this fast-tracking would never have happened without XR’s disruptive protests and the global student strikes on which they built, led by campaigner Greta Thunberg. We need to value scientists and negotiators for the work they do. But we also need sustained, widespread, peaceful disruption and direct action. Collectively, governments are way off their Paris commitments to keep temperatures well below 2 °C. We need to try a diversity of new tactics. The old forms of campaigning and advocacy aren’t working fast enough. Earth Day began in 1970, when millions of Americans took over streets and campuses across the United States. The IPCC’s First Assessment Report was published in 1990; governments adopted the UN Framework Convention on Climate Change in 1992 and the Kyoto Protocol in 1997. It took four more IPCC assessments to get to the Paris agreement. Is it any wonder that frustration is mounting? Students are leading the charge, calling young people and adults to join a global climate strike. This is planned for 20 September, a few days ahead of a crucial UN summit in New York City, convened by UN secretary-general António Guterres. He wants leaders to bring new climate commitments to close the vast ambition gap. The strike is expected to draw millions of people worldwide. I hope that scientists will give up their valuable work for at least half a day. Climate activist Greta Thunberg arriving by boat in New York City, greeted by yachts bearing the 17 UN Sustainable Development Goals on their sails.Credit: Johannes Eisele/AFP/Getty XR is planning a second International Rebellion in key cities worldwide, starting on 7 October. The UK government is not on track to meet its current legal obligations to cut emissions under the 2008 Climate Change Act. (It still subsidizes fossil-fuel production and supports carbon-intensive investments in infrastructure, such as for a third runway at Heathrow airport.) In the United States, the global Green New Deal (GND) movement is gaining traction. It is supported by US senators Alexandria Ocasio-Cortez (Democrat, New York) and Bernie Sanders (Democrat, Vermont), as well as the youth movements Zero Hour and Sunrise, which share XR’s demand for a break with current politics. The GND seeks an approach to rapid decarbonization that is based on social justice, through the use of renewable energy, clean-air technologies, community resilience and prioritization of historically marginalized communities. These campaigns can only succeed if more people join in — including professionals, such as scientists. It is harder to dismiss protests that have a broad base of support. Long-sought change can come about unexpectedly quickly under the right conditions, as exemplified by the downfall of apartheid and communism, or by more mundane shifts as happened with norms over public smoking in several countries. Understandably, many professionals are wary of endorsing campaigns, let alone taking direct action. I still share some of their trepidation. Being an activist can be emotionally and physically draining, requiring long meetings and careful coordination of strategies, tactics and systems of support. But the same can be said of working on UN negotiations: I’ve lost count of the number of all-night meetings I’ve attended, with some negotiations turning into 48-hour marathons. Plus, activists can risk their lives, as so many do in highly illiberal nations. And being an activist can threaten livelihoods: in law, as in science, a person’s credibility rests on perceived impartiality built through offering knowledge and advice in the form of books, peer-reviewed articles, policy reports and expert testimony. Not glue and placards. For all these reasons, I anguished for a long time before taking the plunge from academia. The trigger for my leap into activism was the release in October 2018 of the IPCC’s grim special report comparing the impacts of a 1.5 °C change in global average temperatures with higher rises. It landed during a time of personal, political and professional despair, brought about by bereavement, burnout, Brexit, Trump’s withdrawal from the Paris agreement, and more. For so long, I’d trusted that government actions are essentially evidence-based, and that our ‘normal’ electoral cycles are messy but ultimately safeguard long-term national and planetary interests. Like every other scholar, I’d churned out papers and policy reports in the hope that these would be used by campaigners and heeded by politicians. On behalf of the small island states, I had worked since 2008 to get the UN climate negotiations to acknowledge that a 2 °C rise was too dangerous, and that it needed to enshrine the 1.5 °C threshold demanded by the world’s most vulnerable countries and ecosystems. Still emissions rise; still the rhetoric is “well below 2 °C”? Galvanized, I could no longer ignore what all my legal writing and advising was not changing. The few politicians who want to alter the status quo are blocked because systems are dominated by vested interests and inertia. For example, the top five oil companies have together spent US$1 billion on influencing public and political opinion since the Paris agreement, according to a report by InfluenceMap. Just 3% of their combined $115 billion of capital investments in 2019 will go to low-carbon options (see go.nature.com/2m8pja3). What we need is not system change or personal change — it’s both. Not street circus or government and industrial overhaul, but both. Not reform through revolution or the ballot box. Both. The climate emergency we face now requires every one of us to question how we compartmentalize our professional, personal and political choices. That means acting differently in all three spheres and rethinking how to become audacious leaders in all aspects of our lives. Climate devastation demands us to be upstanders, not bystanders. The era when we limited our jobs to researching, writing, presenting and throwing our reports over the ‘policy fence’, leaving it to campaigners and activists to implement their conclusions, is over. Is working in silos and factions and fretting only about tenure, citations and the next research grant really the best we can do? Professionalism and impartiality must not require us to be indifferent to the fate of the world. The two biggest global injustices in human history are unfolding in our lifetimes. One is how we treat poor people who are on the front line of ecological destruction, such as those living in the Amazon. The other is how we treat our young people. In both cases, a privileged few are leaving a massive burden of irreversible change, destabilization and possible collapse of food and agricultural systems with attendant social and psychological breakdown. Looking back at my career, I realize I was mesmerized by the appeal of using traditional forms of power to change those very same forms. As an immigrant, I wanted to get to the top of the system to fit in: both to make my parents proud of their sacrifices and because, as a feminist lawyer qualifying in the early 1990s, I yearned to smash the many glass ceilings that held back working mothers and people of colour like me. Exhausted by long hours and having endured numerous professional battles to be taken seriously, I felt entitled to a carbon-intensive, meat-eating lifestyle that included flying off to a couple of nice holidays a year. I didn’t connect my lifestyle, career and strategy for changing things with an economic system that was so blatantly out of kilter with my politics. As a young lawyer and feminist seeking equality in a toxic system, what I aspired to is part of the problem. To students today, I say: scholarship alone isn’t going to create a just and pleasant society, so along the way be an activist in your community and question the status quo. There are tough times ahead, so learn how to stay the course: surround yourself with friends and family who will support your journey, both personal and professional. F.Y. It is easier to say all these things now that I am 54 years old with considerable capital — economic, social and reputational. These give me the freedom to speak out, as a lawyer, an activist and a mother. Like all parents, I’ll do whatever it takes to keep my children safe. Right now, that means rebelling against a way of being that is destroying their future and by supporting activists, especially global youth strikers, to intensify their movement. Having power and status in the current system and refusing to challenge the rules hampers the co-creation of a better world. Just as John Keats called on hope to “keep that fiend Despondence far aloof”, let’s use our failures as stepping stones to success. Building regenerative political communities — in which humans and nature co-exist — needs committed, courageous people to stand up for what they believe in, repeatedly, for a long time to come. Join us. </body>
<date id = '110'>17 September 2019</date>
<url id = '111'>https://nature.com/articles/d41586-019-02675-5</url>
<title id = '111'>The race to cash in is draining universities of talent, fracturing the field and closing off avenues of enquiry, warn Jacob D. Biamonte, Pavel Dorozhkin and Igor Zacharov.</title>
<body id = '111'>In just a few years, the field of quantum computing has moved swiftly from an academic backwater to a subject of vast public and private interest. The ultimate goal of a ‘universal’ quantum computer — capable of performing any calculation while correcting for noise, faults and disruptions — remains decades away. But billions of dollars are being ploughed into commercializing the first fruits1. The US technology company IBM and Canadian firm D-Wave Systems are already selling access to quantum-enhanced calculators. Google, Microsoft and Intel plan to do so in three to five years. These early devices should perform certain tasks faster than a conventional computer. They are, however, less versatile and less powerful than a universal quantum computer, and are still subject to errors and noise. Areas such as machine learning and optimization could benefit — if technical challenges can be overcome2,3. But the race to cash in is fracturing the field. Companies are rushing to build large teams of researchers, draining universities of talent4,5. Hundreds of start-ups are patenting the products of publicly funded research, closing off avenues of enquiry.   Public funding for quantum computing is also booming. But it is uneven and skewed towards hardware. North American institutions dominate, and drive the field in directions that suit them. They focus on superconductor technologies, for example. Researchers lacking huge labs and infrastructure find it hard to compete. Geopolitical walls are also rising as national security and commercial interests heat up. All of this is happening at a crucial time. Moore’s law — which states that the number of transistors in integrated microchip circuits doubles about every two years — is stalling. And machine learning is opening its doors to entirely new industries. We cannot wait decades for ‘quantum advantage’: the point at which a quantum processor solves a problem impossible for any existing classical computer to solve. This contrasts with other fields, such as biotechnology, in which the revolutionary technique of CRISPR gene editing emerged 20 years after the field seemed to have peaked in the 1990s. We appeal to academic and industrial scientists to develop quantum applications in an open scientific spirit. Basic research must not be done in isolation or steered by political agendas. These huge investments and the devices stemming from them should serve all of humankind, like science itself. Most of the concepts underpinning quantum computers have come from publicly funded research. Now that quantum computing is potentially valuable, many governments are ramping up support. A handful of nations — the United States, the United Kingdom, Japan, Sweden, Singapore, Canada and China — are leading the way. Each committed between US$100 million and $300 million per year to quantum computing research in 2017; the United Kingdom’s total public and private investment since 2014 now exceeds $1 billion. In 2018, the United States and the European Union both launched billion-dollar behemoths: the five-year US National Quantum Initiative and the ten-year EU Quantum Technologies Flagship programme. China aims to open the world’s largest quantum-research laboratory in 2020 at a cost of $10 billion.   Other countries are following suit. India and South Korea each intend to invest tens of millions of US dollars per year. And Russia includes quantum technology in its top-ten list of national technological initiatives. A number of leading centres are being formed there to coordinate private and government research and development. Large projects should receive up to $300 million in the first phase. But many of the results flowing from all this investment are being hived off. Corporate interests and a cooling of the international political climate are making it harder for scientists to collaborate and share knowledge6,7. Closing off areas leads to pointless replication and time wasted pursuing dead ends. For example, quantum computing’s implications for national security were highlighted in a 2019 report by the US National Academy of Sciences8, sponsored by the Office of the Director of National Intelligence. Scientists funded by the US Department of Energy now face a ban on collaborating with researchers from some 30 countries, including China and Russia6. Such shadows are affecting our research at Moscow’s Skolkovo Institute of Science and Technology — an English-speaking advanced research university that was established in 2011 in partnership with the Massachusetts Institute of Technology in Cambridge. We chose to work at Skoltech because of its international, world-class and collaborative atmosphere. Take our origins: J.D.B. is American, P.D. is Russian and I.Z. is a Dutch citizen of Russian descent. Although our close collaborators continue to interact with us, old research agreements between Russia and the EU are fizzling out, with no discussion of renewal. Partnerships between participants in the EU quantum flagship and scientists in the United States, Russia or China now require special negotiations. Quantum technology risks becoming another ‘Moonshot’ race in which the winner takes all. North America has built up an insurmountable lead in quantum hardware. Companies such as Google in California and D-Wave in Canada still grant access to their machines without borders in mind — but only through the cloud. No one expects to touch the processors. European politicians fear they have missed the technology boat. The region has only one large computer-hardware manufacturer (France-based Atos/Bull) with a quantum-technology programme. The EU quantum flagship promotes the development of hardware similar to that being created in the United States, and is investigating some approaches not being widely explored by US efforts. But the EU Commission failed to allocate much funding to quantum applications or algorithm research in the first round — a major omission, in our view. Those behind the flagship effort have indicated that they will invest more in quantum software in subsequent funding rounds.   Many researchers hope that companies will fill the applications gap. But businesses have little interest in addressing the basic theory of quantum information processing, and they often operate in isolation. This means that the proliferation of start-ups and commercialization of quantum software at this early stage could hinder the development of theoretical methods and quantum software tools. Outdated conventions and assumptions are also holding back the field. The range of applications for quantum-enhanced technology is limited and has not been mapped out. Even known uses, such as the benefits for machine learning, are poorly understood. The devices might not deliver anticipated improvements. Textbook algorithms (such as Shor’s quantum factoring algorithm or Grover’s search algorithm) seem not to work on non-ideal machines without error correction. Can programmers write better codes that can work on realistic devices subject to noise? Academic researchers must map out the space of quantum-computing concepts and applications more fully. This is a fruitful moment for solving difficult problems that industry and start-ups will not be able to address. How far can these devices be pushed in the presence of noise? Will some developments stall, causing investment to dry up? Governments should direct more funding to quantum software. Experimentalists are happy to pitch for large sums of money to build a quantum processor that might lead to a Nobel prize. Quantum programmers should similarly state their grand challenges confidently. Sponsors, too, need to understand that this is a long game that requires diverse approaches. Even when the hardware scales up, we might still be unsure what to do with it. Industry, particularly start-up firms, should work more closely with universities. Companies could fund small theory projects and invest in developing the fundamentals of the field. And international collaborations should be protected. Governments should work harder to keep science agreements intact despite political disagreements. Scientists and funders must respect the fact that research is truly global, international and open. Researchers relocate all over the globe. There are many random factors behind the jobs we end up with. It is not in anyone’s interests to cut off relationships just because one person crosses a border. Mid-sized quantum processors will appear soon (although they will still be noisy). Will the quantum software be ready? Or will companies kick themselves for failing to invest in the algorithms and ideas that will drive the devices? </body>
<date id = '111'>11 September 2019</date>
<url id = '112'>https://nature.com/articles/d41586-019-02674-6</url>
<title id = '112'>Reviewing and accepting study plans before results are known can counter perverse incentives. Chris Chambers sets out three ways to improve the approach.</title>
<body id = '112'>What part of a research study — hypotheses, methods, results, or discussion — should remain beyond a scientist’s control? The answer, of course, is the results: the part that matters most for publishing in prestigious journals and advancing careers. This paradox means that the careful scepticism required to avoid massaging data or skewing analysis is pitted against the drive to identify eye-catching outcomes. Unbiased, negative and complicated findings lose out to cherry-picked highlights that can bring prominent articles, grant funding, promotion and esteem. The ‘results paradox’ is a chief cause of unreliable science. Negative, or null, results go unpublished, leading other researchers into unwittingly redundant studies. Ambiguous or otherwise ‘unattractive’ results are airbrushed (consciously or not) into publishable false positives, spurring follow-up research and theories that are bound to collapse. Clearly, we need to change how we evaluate and publish research. For the past six years, I have championed Registered Reports (RRs), a type of research article that is radically different from conventional papers. The 30 or so journals that were early adopters have together published some 200 RRs, and more than 200 journals are now accepting submissions in this format (see ‘Rapid rise’). When it launched in 2017, Nature Human Behaviour became the first of the Nature journals to join this group. In July, it published its first two such reports1. With RRs on the rise, now is a good time to take stock of their potential and limitations. Source: C. Chambers The Registered Report format splits conventional peer review in half. First, authors write an explanation of how they will probe an important question. This ‘Stage 1’ manuscript includes an overview of the background literature, preliminary work, theory, hypotheses and proposed methods, including the study procedures and analysis plan. Before researchers do the studies, peer reviewers assess the value and validity of the research question, the rationale of the hypotheses and the rigour of the proposed methods. They might reject the Stage 1 manuscript, accept it or accept it pending revisions to the study design and rationale. This ‘in-principle acceptance’ means that the research will be published whatever the outcome, as long as the authors adhere closely to their protocol and interpret the results according to the evidence.   After the Stage 1 manuscript is accepted, the authors formally preregister it in a recognized repository such as the Open Science Framework, either publicly or under a temporary embargo. They then collect and analyse data and submit a completed ‘Stage 2’ manuscript that includes results and a discussion. They are free to conduct further exploratory analyses, provided these are clearly identified as post hoc — having been done after planned analyses were completed. The Stage 2 submission is sent back to the original reviewers, who cannot question the study rationale or design now that the results are known. Whether the results are judged by reviewers to be new, groundbreaking or exciting is irrelevant to acceptance. At the journal Cortex, where I serve as an editor, the acceptance rate for Stage 1 RRs that enter in-depth review is about 90%: more than double that of conventional articles. The publication rate at Stage 2 is currently 100%, with no withdrawals by authors. This assured acceptance means that authors are free to present results as they are, without having to shoehorn them into a clean, compelling narrative. And the outcome is striking. An analysis this year2 suggests that RRs are more likely to report null findings than are conventional articles: 66% of RRs for replication studies did not support initial hypotheses; for RRs of novel studies, the figure was 55%. Estimates for conventional papers range from 5 to 20%2. It is possible that researchers opt for this format when they think that null findings are likely. Nonetheless, these disparities suggest that RRs are a powerful way to counter publication bias (see ‘A brief history of Registered Reports’). And the research community cares: preliminary evidence finds that RRs are cited at levels that are comparable to or slighter higher than those for conventional articles3. The potential of protocol registration to prevent publication bias and increase rigour has been recognized for decades in clinical-trials research. A format similar to Registered Reports (RRs) was also piloted at the now-defunct European Journal for Parapsychology in the 1970s to help ensure publication of negative results. In 1997, The Lancet launched an article type similar to Stage 1 of RRs, which reviewed protocols of proposed research. Almost 150 were published before the article type was discontinued in 2015, ostensibly because other outlets served the same purpose6. I began lobbying the editorial board of Cortex to consider RRs almost as soon as I joined as an editor. It gave the green light in November 2012, and by March 2013 it had adopted the full RR format. At around the same time, a separate group launched a variant focusing on replications at Perspectives on Psychological Science7. The same year, psychologists Brian Nosek and Daniël Lakens announced that a special issue of Social Psychology would use the format to publish replications of important results8. From 2014, more journals in neuroscience and psychology began adopting and publishing RRs, and the format has now expanded across the life and social sciences. No specialized physical-science journals yet offer them. Some multidisciplinary journals — including Royal Society Open Science — have launched the format across all subjects in science, technology, engineering and mathematics. I hope RRs will become an option in all mainstream life- and social-science journals within ten years. One of the most striking characteristics of RRs is that reviewers can help authors to improve the protocol or rationale while it is still possible to make changes. I have overseen numerous cases in which reviewers have intervened to prevent a serious flaw in a study design — adding crucial controls, ensuring the sample size is sufficient or explaining why the hypotheses or planned statistical analyses cannot really answer the research question. Even when a proposed design is sound, the review process often adds clarity and focus. In my experience, the reviewers find the process rewarding. One comment from a reviewer is typical of the informal feedback I receive: “If the authors can incorporate many of the suggestions from all of us reviewers, they will have a far better study than what they originally planned, which is really valuable and exciting.” As RRs have grown, I have come to spend as much time advocating, optimizing and getting feedback on the format as I do on my own research. I chair the Registered Reports committee supported by the Center for Open Science, and serve as a Registered Reports editor at BMJ Open Science, Collabra: Psychology, the European Journal of Neuroscience, NeuroImage, PLoS Biology and Royal Society Open Science. I am often asked whether all research publications should be RRs. No! Work that is purely exploratory and not driven by a hypothesis is usually not suitable for the format. For example, an RR might be a poor fit for the discovery of a new disease mechanism or potential drug molecule without a clear set of predictions. Often, the same goes for work to develop new experimental methods. RRs are not designed to supplant publications that announce this kind of research; they are intended only to strengthen the rigour and transparency of studies that test hypotheses. Another common question is whether RRs are suitable for sequential experiments in which the results of one study determine the design of the next. In principle, yes: many journals now offer ‘incremental registrations’ in which authors can re-enter Stage 1 review after the results are in, and then add protocols for one or more further studies. In practice, authors rarely take up this option, probably because of the time associated with multiple rounds of Stage 1 review. More common is for authors to perform a series of experiments and report these in the Stage 1 manuscript. These can then be used to design one or more extra experiments to ‘seal the deal’. The final article describes all of the experiments and is badged as an RR. Another option is for authors to preregister multiple experiments at the beginning, as in one recent study. Over eight experiments, it asked whether light in the range typically used in optogenetics studies can influence neuronal physiology in mice4.   There are also times when hypothesis-driven research itself is not suitable for RRs. Studies seeking to capture the effects of unpredictable events (such as solar flares, flash floods, mass violence or stroke-induced brain injury) must start collecting data as soon as is feasible. They cannot wait two to four months for a Stage 1 manuscript to complete peer review. (Ideally, researchers would still take a few minutes to self-register their protocol in a recognized repository.) Similarly, undergraduate students who must finish a summer project in a short time might not be able to wait for reviewer feedback, although some teaching programmes have had success by dividing up research-project design and execution in creative ways (see, for example, K. Button Nature 561, 287; 2018). By contrast, RRs have distinct advantages for longer-term students. The in-principle acceptance at Stage 1 allows them to list a publication much sooner than they could for a conventional manuscript, and with more certainty. There is emerging evidence that RRs are popular with early-career researchers. For example, at Cortex, 78% of RR first authors (n = 82) are PhD students or postdocs, compared with 67% in a control sample (n = 57) of conventional articles. Although RRs require researchers to wait for review before starting experiments, I suspect that the time to publication probably declines overall. A conventional article might be rejected on the basis of results or because of methodological problems that can no longer be fixed, leaving authors to submit their work to journal after journal, or to perform extra experiments. Over the past six years, dozens of authors have told me —and written publicly — that they appreciate the more-predictable timeline of RRs (see, for example, go.nature.com/2kwnjuj). Decreased flexibility is an oft-expressed concern over the format. One early critic said it would “put science in chains”. The fear is that peer-reviewed preregistration dampens the creativity and serendipity that could come from free-wheeling data exploration. But preregistration imposes no such limit: it merely requires that exploratory analyses are labelled transparently as post hoc and do not dominate conclusions. Exploration is alive and well. Stage 2 submissions almost always include further analyses. The difference is that researchers cannot fool themselves or their readers by presenting only the most interesting analyses or imply that these were intended from the outset. RRs are a plan, not a prison. A related misgiving is that researchers will find themselves locked into a suboptimal protocol once experiments begin. In my experience, the opposite is more likely: reviewers can prevent researchers from running less-informative experiments. And reviewers of Stage 2 manuscripts generally understand reasonable changes. It is not flexibility that is lost, but the ability to airbrush both reasonable and questionable changes out of the picture. RRs are not a panacea — the format needs constant refinement. It currently sits rather awkwardly between the old world of scientific publishing and the new. Innovations over the next few years should make this format even more powerful, and stimulate wider reforms. Transparency. When RRs first launched, some journals published Stage 2 manuscripts but not those for Stage 1, making it impossible for readers to see whether the completed protocol matched the planned one. In 2018, the Center for Open Science launched a simple tool that places submitted Stage 1 manuscripts in a public registry (see go.nature.com/2kb5s7v). This is now used by many journals, including Cortex and Animal Behavior and Cognition. The publisher Wiley has opted to publish accepted protocols. And venues such as F1000Research offer the option to post Stage 1 articles before peer review, with reviews and revisions made public as they become available. A badging system shows that the Stage 2 article adhered to the criteria and can be labelled as a RR. Standardization. Improving the standardization of submitted protocols promises to improve computational reproducibility. Currently, submitted manuscripts are often prepared in word-processing software and contain insufficient methodological detail or linking between predictions and analyses. The next generation of RRs — ‘Registered Reports 2.0’ — is likely to be template-based and could integrate tools such as Code Ocean (see https://codeocean.com/researchers). This would ensure that analyses are immutable within a stable, self-contained software environment. With standardized metadata and badging, RRs will become useful for systematic reviews and meta-analyses. Efficiency. The review process can be extended even further back in the research life cycle. Under the emerging RR grant model, reviewers award funding and signal in-principle acceptance of a research publication simultaneously or in rapid succession. The Children’s Tumor Foundation and PLoS ONE have pioneered such a partnership (see go.nature.com/2kpjzat), as have Cancer Research UK and the journal Nicotine & Tobacco Research5. More are in the works. The lesson of RRs speaks to all areas of science reform. Instead of forcing quality to compete with success, partner them up. Instead of pitting what is best for the individual against what is best for all, create a model that benefits everyone — the scientist, their community and the taxpayer — and the rest will come naturally. </body>
<date id = '112'>10 September 2019</date>
<url id = '113'>https://nature.com/articles/d41586-019-02594-5</url>
<title id = '113'>Tear gas and pepper spray put lives at risk. Examine their effects and regulate their deployment, urges Dan Kaszeta.</title>
<body id = '113'>In the past few months, tear gas and pepper spray have been deployed to break up street protests in Hong Kong, Gaza, Paris and on the US–Mexico border. Police forces use these riot-control chemicals to clear crowds or to stop fighting. In theory, exposure should be minimal — a group should disperse within minutes to avoid the gas. The line between civilian and military applications of these chemical agents is a fine one. Rules governing their use are confused. Reference books and training materials continue to cite toxicology studies from the 1950s. And those were done on animals and soldiers, not the public. The chemicals involved are mainly CS (2-chlorobenzalmalononitrile, the primary component of tear gas) and OC (oleoresin capsicum, a chilli-pepper extract used in pepper spray). Tear gases were developed to harass the enemy or to clear bunkers and tunnels in conflicts such as the Vietnam War, as alternatives to deadly force. Pepper sprays came into use in the 1980s for police and self-defence use after being developed as an animal repellent in the 1960s. Effects vary from person to person. They range from irritation to permanent injury and death. Occasionally, people are exposed to dangerous levels. Authorities might miscalculate how much agent to use. Someone with an injury or disability, or who is under restraint, might be unable to flee. This variation is not being considered when agents are used in the street. For example, a 2016 review of the medical literature1 by Physicians for Human Rights identified 5,131 people from 31 studies who had been injured by riot-control chemicals. Two of these were killed: one from respiratory problems caused by CS exposure; the other after being hit on the head by a tear-gas projectile. Another 70 people (1.7%) developed permanent disabilities, including blindness. To minimize deaths and serious injuries, researchers and policymakers must work together to expand the evidence base and design guidelines for when and how to use riot-control agents. Stricter reporting and restrictions on use would allow these valuable tools to be deployed as they are intended: to fight violence and crime while keeping the public safe. What are these chemicals? Most ‘tear gases’ are solid powders, delivered in sprays or bursts from canisters. CS is often combined with a flammable filler and spreads as smoke from burning grenades. Sometimes, it is dissolved in a solvent such as methylene chloride. Clouds of acrid fog or smoke drive people to leave buildings or large areas. CS irritates the skin and eyes and has a distinct peppery smell. It is commonly also used to prepare soldiers for potential exposure to more-dangerous nuclear, biological and chemical agents. It is possible to develop a tolerance to the effects of CS. The OC in pepper spray comes from the chilli pepper (Capsicum annuum). Strengths vary greatly, so it is usually diluted and sprayed directly at a person or group. It can be extremely irritating, to the point of incapacitation, through intense burning sensations on contact with skin, eyes and the respiratory tract. Tear gas is used to disperse an environmental protest in Istanbul’s Taksim Square in 2013.Credit: Osman Orsal/Reuters A handful of other sprays used in the 1950s, such as CN (phenacyl chloride) and CR (dibenzoxazepine), are still deployed today, although they are widely considered obsolete. CN is marketed (often as ‘mace’) in the United States as a self-defence spray. Egyptian security forces allegedly used CR against anti-government protesters in 2011. Adamsite (diphenylaminechlorarsine) and DC (diphenylcyanoarsine) were also once used in munitions. And in Venezuelan riots in 2013, local police allegedly used old stocks of adamsite that were perhaps left over from US military aid delivered in the 1960s. These alternatives have a narrower margin of safety than do CS and OC. Also, old munitions degrade into toxic by-products that are unpredictable and poorly studied. Security forces might use more grenades or shells if the munitions don’t seem to be working as expected, increasing risks. CS and the older agents (such as CN and adamsite) are said to have low toxicity; it takes a very high dose to kill a person. But these estimates are largely extrapolated from tests in the 1950s and 1960s on laboratory animals2. The toxicity of CS was described in this journal nearly 50 years ago3. Human studies tended to be limited to healthy male volunteers. This demographic might have been broadly true then for street-gang members, bank robbers and military personnel, but it is not typical of protesters today. A crowd is likely to contain people with many different vulnerabilities, such as heart conditions or asthma and other respiratory problems. A cloud of CS could drift the wrong way and affect people who are not the target. People unrelated to a disturbance can be caught up in it by mistake. Nobody knows how much CS or OC is lethal to someone with asthma or the lung disease emphysema, or to a person who needs therapeutic oxygen. It is likely that respiratory conditions will be aggravated, although the research literature is equivocal. In training environments, people who wear contact lenses have told me that these heighten pain when they get OC droplets or CS particles in their eyes. The effects on children, pregnant women and older people are poorly studied. An analysis after the 2013 Gezi Park civil disturbances in Turkey found women to be more seriously affected by the respiratory effects of CS than men were4. No mechanism was established. US Navy recruits test their protective equipment before entering a gas chamber as part of their training.Credit: PO Camilo Fernan/DVIDS And there are many more safety issues to consider. Projectiles containing agents can cause head injuries5. The hot, burning canisters in which CS is often dispensed can burn the skin and start fires. Rioters might be exposed to solvents and toxic by-products from burning munitions6. Contaminants might be found in poorly made agents, sometimes encountered in the developing world, or in those that are well past their use-by date. None of these factors has been adequately explored. The parlous state of research in this area stems from many factors. Lab-based testing of people, particularly vulnerable individuals, is likely to be unsafe and unethical. Old data involving prisoners might be questionable. Much field data potentially exist, but gathering them would require identifying people who were involved in civil disturbances. Such individuals might not make themselves available for study, particularly in countries with oppressive regimes or ongoing unrest. The use of riot-control agents for enforcing domestic law is explicitly permitted by the 1997 Chemical Weapons Convention. Applications in warfare are not. There is no ban on producing or stockpiling these agents. Signatories to the treaty (comprising most countries in the world) are required to declare stocks. There is no international framework for controlling export and trade in these substances. Nor is there a reporting mechanism for instances of use.   The use of these agents is thus a matter for national laws. But governments specify few legal prohibitions, other than through generic laws on police misconduct. Major exporters of the chemicals, such as the United States, tend to have few legal restrictions. The United Kingdom prohibits import, possession or use for personal defence by private citizens. Unsurprisingly, countries with few safeguards on policing, such as China and Turkey, have seen more-widespread uses of tear gas in recent years. Riot-control agents are often misrepresented in the media. Reports sometimes refer to CS incorrectly as a nerve agent or confuse it with mustard gas or Agent Orange. Some commentators are quick to point out that riot-control agents are banned by a treaty or represent ‘war crimes’. They are not, and do not. This misrepresentation can escalate violence. People who think that nerve agents are being used against them might think that they need to resort to lethal force. Governments need to reconsider how they use riot-control agents. Members of the public deserve protection from injury. When personnel who have had effective training deploy these substances, they can be a valid alternative to batons, tasers or guns in one-on-one situations. Batons often cause fractures7; tasers can cause cardiac arrests. And tear gas and pepper spray might have some utility in a prison yard. But more-widespread use, such as lobbing them indiscriminately into demonstrations, needs careful examination. The risks of death, injury or property damage cannot be ignored. The effects of any chemical weapon are inherently unpredictable; history is full of examples of ‘friendly’ casualties caused by agents carried on shifting winds. Vast clouds of gas can also be counterproductive. Causing death or injury often escalates, rather than diffuses, a situation. Researchers should gather more data about the toxicity of riot-control agents on people other than healthy young men, including women and those with underlying health conditions. Efforts must be made to engage with and study victims of exposure in ways that do not compromise their safety. A better understanding of risks could provide potent evidence to justify increased regulation. Policymakers and law enforcers should prepare guidelines for best practices and uses of these agents. A good start would be a code of conduct. This should be backed up by legal frameworks that require those wishing to purchase the agents to adhere to the code. Hard projectiles and canisters that burn could be banned or at least restricted to particular situations, such as prison riots or cases in which lethal force has already been used. Training should be increased. The Chemical Weapons Convention could be amended to include stronger provisions for riot-control agents. The Organisation for the Prohibition of Chemical Weapons has the framework and infrastructure for such work because it polices other chemical weapons. Riot-control agents should be on the agenda at the organization’s meeting in November. </body>
<date id = '113'>03 September 2019</date>
<url id = '114'>https://nature.com/articles/d41586-019-02516-5</url>
<title id = '114'>Researchers rushing to apply powerful sequencing techniques to ancient-human remains must think harder about safeguarding, urge Keolu Fox and John Hawks.</title>
<body id = '114'>The study of ancient-human populations and our now-extinct close relatives has thrived over the past decade, as genetic material is examined with cheaper and more sophisticated sequencing technologies. Only nine years ago, the partial sequencing of a Neanderthal genome was a major scientific achievement1. Today, researchers are pursuing what many have termed a factory-like approach to analysing ancient DNA2, with the processing of hundreds of samples. As a result, we have a much better understanding of (among other things) which human populations interbred with Neanderthals, and which didn’t3; how people dispersed across Europe during the Bronze Age4; and how pastoralism developed in Africa5. But such progress comes at a price. Extracting the best-quality DNA from ancient remains requires the partial destruction of those specimens. And once bones, teeth, hair and so on are ground into dust, future opportunities for using them to understand our past are lost. We recognize the enormous potential of ancient DNA to help reveal human history. In fact, as long as interested parties give their consent, we are hoping to apply genomics to the remains of Hawaiian men and women who lived hundreds to thousands of years ago. (Our aim is to understand how the introduction of leprosy, smallpox, syphilis and other diseases from European colonialists in the eighteenth century have shaped the genomes of Native Hawaiians today.) We also recognize that some leading labs are taking steps to reduce the destructiveness of sampling, for instance by developing techniques that allow ancient-DNA sequences and radiocarbon dates to be obtained from the same sample instead of from multiple ones6.   Yet we are becoming increasingly concerned. To our knowledge, no one currently has a full list of all the samples from ancient humans and closely related species examined so far (meaning samples ranging from hundreds to tens of thousands of years old). No one is tracking the success rate of data recovery across laboratories and samples. And no one knows how many specimens are left. With such a rapid scale up in analytical capacity, the diverse stakeholders involved (archaeologists, molecular biologists and bioinformaticians; editors and journalists; museum curators; and the descendants of the populations being studied) must talk. They need to establish how to balance discovery now with the need to safeguard cultural remains in the long term. Unless some ground rules are established, future scientists, armed with better, potentially less-invasive methods for extracting DNA from ancient samples7 could well look back on this era as a time of heedless destruction, fuelled by the relentless pressure to publish — or what one anthropologist has described as an “impetuous anxiety for discovery”8. Over the past ten years, there have been tremendous successes in education and engagement efforts that aim to bring a broader range of people (including those with interests and responsibilities as descendants of particular ancient communities) into consultations about genetic research. For instance, since 2011, a growing consortium of genomicists, now in North America, Hawaii, Finland, New Zealand and Australia, have helped to guide summer training programmes for Indigenous people. These educate students about the potential uses and misuses of genomics, including ancient genomics, as well as how to sequence DNA. Yet irrevocable decisions continue to be made about the sampling of ancient specimens, guided by the immediate research interests of a few. As an example, many researchers focus their sampling effort on the petrous bone, the hard portion of the temporal bone at the base of the skull, which houses the intricate structures of the inner ear. This dense bone contains a high concentration of endogenous DNA. Last year, a team looking at the morphology of the inner ear noted that researchers were breaking open bony labyrinths and drilling into hundreds of petrous bones for DNA without first taking photographs, or using scanning techniques such as micro computed tomography (microCT) to make morphological records9. Petrous bone could contain uniquely high concentrations of other potentially informative biomolecules, such as protein or lipid biomarkers10. Also, because it contains the structures of the inner ear, including the semicircular canals and cochlea, intact bone could reveal insights about an individual’s balance or hearing. Some laboratories have used microCT scanning, both to preserve data from petrous bone, and to guide their drilling to minimize destruction of the specimen11. Unfortunately, such methods have not been adopted as a standard, partly because individual groups tend to focus on their own research agenda rather than on the bigger picture. The petrous part of the temporal bone is used for radiocarbon dating.Credit: James King-Holmes/SPL Destruction of fragments of ancient bones or teeth is key to many techniques used in palaeoanthropology — including ancient proteomics, radiocarbon analysis, electron-spin resonance dating, stable-isotope sampling, dental-calculus sampling to assess what food people ate, and the sectioning of teeth for studies of growth. But so far, investigators and commentators have begun to routinely apply the terms ‘DNA factory’ or ‘industrial-scale’ only to ancient genomics (whether in publications, at conferences or on social media). Most of these other techniques are applied to tens of samples in any one study, occasionally to a single sample. Ancient genomics stands apart because the decreased cost of sequencing and the rapid acceleration of technologies have enabled some laboratories to pursue projects involving hundreds of samples. The publication of such large-scale studies has put pressure on others to use similarly impressive sample sizes. What’s more, analysing the movement and evolution of ancient populations requires researchers to compare the genome of any one sample with those of as many of the individual’s ancient contemporaries as possible. Thus, studies involving bigger sample sizes provide more reference data for other investigators to draw on, creating a feedback loop. In our view, two changes need to be implemented in ancient genomics research. Give diverse stakeholders a say. Currently, a patchwork of regulations and institutions determines whether destructive research on ancient human remains can proceed. In some jurisdictions, Indigenous communities are formally involved in decision-making for research that involves the bones of their ancestors. In others, the decision could rest in the hands of a single curator. But on its current trajectory (see ‘Bone bonanza’), genomic research on ancient-human populations, or on close extinct relatives, could hit a ceiling within decades because of the scarcity of ancient remains. It is therefore urgent that, rather than sequencing an ancient genome in the hope that something interesting will emerge, researchers state up front what question they are seeking to answer — and that people with diverse perspectives evaluate their goals. Because human remains have intrinsic value and a role in the beliefs and cultures of many peoples of the world, as well as scientific value, decisions about whether or how to use them for research should be governed by a broad group, from researchers to the descendants of the populations being studied. For instance, if only three samples of a given ancient human population exist in the world, how many is it reasonable to destroy to answer a specific question about human migration? Sources: E. Callaway Nature 555, 573–576 (2018); David Reich This ‘question-led’ approach would enable people to consider the trade-off between collecting ancient DNA data today and waiting for future sequencing methods, which could potentially yield more information less expensively and less destructively7. (Sequencing DNA from ancient samples was much more hit and miss before the emergence in the mid- to late 2000s of targeted-capture next-generation sequencing, which enables researchers to separate endogenous from contaminant DNA, and then amplify it.) Also, greater engagement from more diverse stakeholders on how to handle scarce ancient remains as new technologies emerge will inspire conversations that bridge disciplines, lead to more accurate models and hypotheses and help form lasting partnerships. In our view, such an approach is crucial for fostering trust in a field in which, historically, the decisions of archaeologists and geneticists have led to deep distrust in many communities12. Create accountability. Just as timber and minerals are meticulously tracked at truck weighing stations and other venues to discourage the illegal acquisition of resources, curators, researchers and others must openly document the passage of ancient remains from one institution to another — and everything that happens to those remains along the way. With such a record, all ancient remains would be audited and people would know which specimens were ground into dust, but did not generate useful data, and which efforts generated data but did not result in a publication, and so on13. In the United States, the National Science Foundation (NSF) could take the lead on establishing such a database. Or grass-roots initiatives at museums, such as the Smithsonian Museum of Natural History in Washington DC or the Bernice Pauahi Bishop Museum in Honolulu, Hawaii, could help to shift practice. Buy-in from the research community could easily be obtained if referees and grantors required declaration of all sampling information. Importantly, such a decentralized approach would help to ensure that knowledge about ancient samples is not limited to a few groups13. Many of the great archaeological sites of prehistory are now empty thanks to early archaeologists — sometimes little more than treasure-hunters — commanding armies of unskilled workers to scoop up the contents of caves, tombs and burial grounds. When so little was known, the bar was low; any discovery was interesting, and little or nothing was left for future generations. In fact, even as late as the 1990s, large sections of ancient human skeletons were destroyed for radiocarbon and other analyses that can now be accomplished using much smaller portions of bone. Rather than repeat the mistakes of the past, future generations of scientists — from all countries of the world and from all sectors of society — must be given the opportunity to interpret our shared history. </body>
<date id = '114'>28 August 2019</date>
<url id = '115'>https://nature.com/articles/d41586-019-02474-y</url>
<title id = '115'>Better regulation, flight control, batteries and software would improve the range of craft and data quality, argue Nicholas C. Coops, Tristan R. H. Goodbody and Lin Cao.</title>
<body id = '115'>Drones are revolutionizing environmental research. Using small remote-controlled aerial vehicles, scientists can photograph far-flung places more quickly, easily and cheaply than by conventional surveys done on foot, by car or from balloons, satellites or aircraft. Drones are becoming essential for monitoring forests1, rivers2, farms3 and wildlife4. They can track the regeneration of woodland and tell farmers where to apply fertilizer or pesticide. They can fly swiftly into disaster zones. For example, in Mozambique in March, they revealed flooding, damage and survivors in the wake of Cyclone Idai. The technology is advancing and costs are falling (see Nature 498, 156–158; 2013). Carbon-fibre frames are strong yet lightweight. Electronics and batteries are getting smaller and using less power. On-board GPS systems and flight-planning software make manoeuvring simple and safe. Myriad sensors can be deployed, from three-colour cameras such as those on smartphones to sophisticated laser altimeters and thermal sensors. Processing algorithms can turn sequences of pictures into 3D imagery and data sets. But drones could do much more for scientific research. They can currently only be flown for up to an hour before needing to be recharged, and regulations, data standards and processing algorithms all fall short. Scientists need to work with regulators and software developers to improve laws and data products. Here we propose four steps towards improvement. Legal frameworks controlling the civilian uses of drones vary widely around the world (see https://uavcoach.com/drone-laws). One-quarter of countries, including most European Union nations, Canada and the United States, have strict laws detailing how, when, where and for what purpose drones can be used. Pilot proficiency, hardware registration, flight specifications and insurance are commonly also required. Another one-third of nations lack legislation. Some countries, including Cuba, Iran and Morocco, ban drones altogether. Others, such as India, are opening up markets slowly by regulating only a few uses. Such regulatory confusion deters many scientists from conducting research with drones, especially in cities or areas that have heavy air traffic. Contractors are often hired to do the surveying. But third parties cannot respond as well as scientists to real-time changes in data needs. Such inflexibility limits drone applications, increases costs and hampers innovation. Some regulations hinder scientific uses. For example, requiring that the drone is in view of the operator at all times protects pedestrians on New York City’s streets from crashes and intrusion. But it also prevents researchers from monitoring urban green spaces from the air or, in other regions, from detecting illegal logging in dense forests. These laws also stop scientists using drones far away from populated areas, to study scrub encroachment in the Amazon basin or snow melt in the Arctic or the Himalayas5.   Misuse of drones is perhaps the greatest threat to widening their use. Disapproval from the public or funders can lead to tighter regulations or bans. Concerns have been raised over their impacts on wildlife. For example, flying drones over black bears in the United States quadrupled the animals’ heart rates6. Drone flights have been illegal in all US National Parks since August 2014, in part because of their effects on wildlife. Earlier that year, for instance, an illegal drone in Utah’s Zion National Park scattered herds of bighorn sheep (Ovis canadensis) and separated several young from adults. In 2018, the grounding of commercial air traffic by rogue drone operators in the United Kingdom led the government to extend no-fly zones around airports, limit flight heights and demand more training for drone pilots. Regulation moves more slowly than technology. A prime example is the capacity for many drones to fly together and collaborate as a ‘swarm’. Drone swarms for military uses such as surveillance are being developed by the US Defense Advanced Research Projects Agency. Drone swarms hold promise for science, too, and would speed up data collection over large areas. But current US regulations require drones to be in the line of sight of a single operator. This effectively prevents swarms from being flown, because it is impossible to ensure that all drones in a swarm can be seen by one operator. Standardizing drone laws across regions and countries would simplify enforcement and promote consistency. There is progress: in November 2018, the International Organization for Standardization (ISO) released a draft of the First International Drone Standards for a short period of public comment. The standards focus on: classification, design, manufacture, operation (including maintenance) and safety management. When the standards are finally released, scientists must provide feedback on them. Researchers must collaborate with policymakers to ensure that drone regulations are fit for research as well as other purposes. For example, special licences could be set up. Some regulations, such as line-of-sight requirements, could have exemptions for scientific studies away from populated areas. Systems for directing drone flights need improving to protect public safety, privacy and to avoid collisions. Regulatory restrictions should be programmed in and updated as legislation evolves. Collision-avoidance systems are increasingly installed on drones. These use on-board sensors to detect nearby obstacles or craft and then warn users to adjust the vehicle’s course, or the drone can do so automatically. Simple forms of air-traffic tracking are being integrated. For example, China’s civil aviation administration uses two cloud-computing systems for registering and monitoring drone ownership and flights — U-cloud and U-care. A drone equipped with a control module can access the systems, which warn operators about flight zones and speed limits. Drones can survey hostile and difficult-to-reach terrain.Credit: Arctic Images/Alamy Drone developers, such as the world’s largest, DJI, in Shenzhen, China, are beginning to show regulatory and safety hazards on maps in the control software. Derived from local aviation maps and regulations, these use GPS and other navigational satellite signals to restrict the heights at which the craft can fly and to block its access to no-fly zones. Cooperation between manufacturers and legislative bodies is needed to improve the availability of these data. Permission to bypass these ‘geo-fences’ is granted on a case-by-case basis; users must demonstrate in advance that their activity minimizes risks to the public, property and wildlife. For example, researchers wanting to study birds near an airport would need to provide flight plans to local aircraft controllers and ensure that the drone’s altitude, speed and trajectories will not affect local air traffic or trouble people or animals. Researchers must also tell controllers when, where and what drone activities are taking place, and receive permission for them. As the technological capacity and spatial coverage of these systems improve, regulations should be harmonized across regions. Quality-control testing must be ongoing to ensure that the regulations perform as intended and as advertised. Regulatory bodies will need to consider data privacy and anonymity. Efficient solid-state lithium-ion cells are being developed. But drones are still predominantly powered by conventional batteries. These limit flight times to 15–60 minutes, and thus also cap flight distances. Drones are typically used for small-scale studies, such as imaging a stand of trees or counting animals in a valley. Scientists looking to monitor larger areas, greater than 100 hectares, for example, are forced to swap in multiple battery sets. Length of flight is also influenced by the type of air frame — fixed wings have less air resistance than do rotors. Weight, the power draw of on-board electronics and sensors, hardware aerodynamics, and wind direction and speed also have an influence7. Shrinking batteries while improving the efficiency of on-board electronics would help to reduce weight and lengthen flight times. Hybrid electric- and hydrocarbon-based fuel systems can also extend flight times. Drones fitted with photovoltaic cells are becoming a reality. Also promising is the development of in-flight networks that can recharge batteries from nearby ground stations through Wi-Fi and ultrasound signals. Drones are often flown repeatedly along transects to gather data over an area or to track changes. When such craft are flown below 100 metres — the typical ceiling for unregulated use — the resolution of images can be as fine as 3 centimetres, enough to identify the species of individual trees, shrubs and animals. Challenges remain for collecting and processing data. Airframes flex as they fly, and in-flight roll, pitch and yaw movements affect the speed and orientation of the craft as well as the consistency and quality of images. Researchers must use fast shutter speeds and image-matching software to recreate terrain in 3D or to view particular projections8. Software is available for performing these tasks, but the algorithms are not standardized. There is no consensus on optimal parameters, such as elevation, image dimensions and flight speed, or methods for reconstructing views or processing structural and spectral information. Although open-source software is being developed, most packages are proprietary black boxes. Users cannot know what the underlying algorithms are doing, or test or adjust them. Researchers need to work with software developers to develop algorithms that suit the many goals of scientific data gathering from drones. For instance, commercial software for processing digital images is often tailored to reconstructing simple geometrical shapes for architectural uses. Distortions and omissions are common when such software is applied to more complex features like vegetation. We call for more dialogue and collaboration between legislators, users, developers and manufacturers of drones to expand their use for science and to democratize remote sensing. Upcoming meetings of the ISO technical committee in London in October and Nanjing in November, as well as remote-sensing conferences such as those of the IEEE Geoscience and Remote Sensing Society, offer opportunities to begin this conversation. </body>
<date id = '115'>20 August 2019</date>
<url id = '116'>https://nature.com/articles/d41586-019-02407-9</url>
<title id = '116'>Teach people to think critically about claims and comparisons using these concepts, urge Andrew D. Oxman and an alliance of 24 researchers — they will make better decisions.</title>
<body id = '116'>Everyone makes claims about what works. Politicians claim that stop-and-search policing will reduce violent crime; friends might assert that vaccines cause autism; advertisers declare that natural food is healthy. A group of scientists describes giving all schoolchildren deworming pills in some areas as one of the most potent anti-poverty interventions of our time. Another group counters that it does not improve children’s health or performance at school. Unfortunately, people often fail to think critically about the trustworthiness of claims, including policymakers who weigh up those made by scientists. Schools do not do enough to prepare young people to think critically1. So many people struggle to assess evidence. As a consequence, they might make poor choices. To address this deficit, we present here a set of principles for assessing the trustworthiness of claims about what works, and for making informed choices (see ‘Key Concepts for Informed Choices’). We hope that scientists and professionals in all fields will evaluate, use and comment on it. The resources were adapted, drawing on the expertise of two dozen researchers, from a framework developed for health care2 (see ‘Randomized trial’). Ideally, these concepts should be embedded in education for citizens of all ages. This should be done using learning resources and teaching strategies that have been evaluated and shown to be effective. Claims about effects should be supported by evidence from fair comparisons. Other claims are not necessarily wrong, but there is an insufficient basis for believing them. • Interventions can cause harm as well as benefits. • Large, dramatic effects are rare. • We can rarely, if ever, be certain about the effects of interventions. • Beliefs alone about how interventions work are not reliable predictors of the presence or size of effects. • An outcome may be associated with an intervention but not caused by it. • More data are not necessarily better data. • The results of one study considered in isolation can be misleading. • Widely used interventions or those that have been used for decades are not necessarily beneficial or safe. • Interventions that are new or technologically impressive might not be better than available alternatives. • Increasing the amount of an intervention does not necessarily increase its benefits and might cause harm. • Competing interests can result in misleading claims. • Personal experiences or anecdotes alone are an unreliable basis for most claims. • Opinions of experts, authorities, celebrities or other respected individuals are not solely a reliable basis for claims. • Peer review and publication by a journal do not guarantee that comparisons have been fair. Studies should make fair comparisons, designed to minimize the risk of systematic errors (biases) and random errors (the play of chance). • Comparison groups and conditions should be as similar as possible. • Indirect comparisons of interventions across different studies can be misleading. • The people, groups or conditions being compared should be treated similarly, apart from the interventions being studied. • Outcomes should be assessed in the same way in the groups or conditions being compared. • Outcomes should be assessed using methods that have been shown to be reliable. • It is important to assess outcomes in all (or nearly all) the people or subjects in a study. • When random allocation is used, people’s or subjects’ outcomes should be counted in the group to which they were allocated. • Reviews of studies comparing interventions should use systematic methods. • Failure to consider unpublished results of fair comparisons can bias estimates of effects. • Comparisons of interventions might be sensitive to underlying assumptions. • Verbal descriptions of the size of effects alone can be misleading. • Small studies might be misleading. • Confidence intervals should be reported for estimates of effects. • Deeming results to be ‘statistically significant’ or ‘non-significant’ can be misleading. • Lack of evidence for a difference is not the same as evidence of no difference. What to do depends on judgements about the problem, the relevance (applicability or transferability) of evidence available and the balance of expected benefits, harm and costs. • The problem should be diagnosed or described correctly. • The goals and options should be acceptable and feasible. • Attention should focus on important, not surrogate, outcomes of interventions. • There should not be important differences between the people in studies and those to whom the study results will be applied. • The interventions compared should be similar to those of interest. • The circumstances in which the interventions were compared should be similar to those of interest. • Weigh the benefits and savings against the harm and costs of acting or not. • Consider how these are valued, their certainty and how they are distributed. • Important uncertainties about the effects of interventions should be reduced by further fair comparisons. People are flooded with information. Simply giving them more is unlikely to be helpful, unless its value is understood. A 2016 survey in the United Kingdom showed that only about one-third of the public trusts evidence from medical research; about two-thirds trust the experiences of friends and family3. Not all evidence is created equal. Yet people often don’t appreciate which claims are more trustworthy than others; what sort of comparisons are needed to evaluate different proposals fairly; or what other information needs to be considered to inform good choices. For example, many people don’t grasp that two things can be associated without one necessarily causing the other. The media sometimes perpetuates this problem by using language suggesting that cause and effect has been established when it has not4 — for instance, statements such as ‘coffee can kill you’ or ‘drinking one glass of beer a day can make you live longer’. Worse, exaggerated causal claims often pepper press releases from universities and journals5. Pupils at a school in Uganda.Credit: Mikkel Ostergaard/Panos The Informed Health Choices (IHC) Project was initially developed between 2012 and 2017 by a collaboration including some of the co-authors of this article (A.D.O., A.D., I.C. and M.O.). The project includes its own set of key concepts2, learning resources and a database of multiple-choice questions to assess how well users can apply the concepts. In 2016, a randomized trial involving 120 schools and more than 10,000 schoolchildren in Uganda showed that these resources improved the ability of 10–12-year-old children to apply 12 of the key concepts7. These concepts included, for example, recognizing that personal experiences alone are an insufficient basis for claims about effects, and that small studies can be misleading. In this trial, 69% of schoolchildren who were taught the key concepts passed a multiple-choice test of their ability to think critically about health claims. By comparison, just 27% of children who were not told about the concepts passed the same test. Studies that make fair comparisons are crucial, yet people often don’t know how to appraise the validity of research. Systematic reviews that synthesize well-designed studies that are relevant to clearly defined questions are more trustworthy than haphazard observations. This is because they are less susceptible to biases (systematic distortions) and the play of chance (random errors). Yet results from single studies are often reported in isolation, as facts. Hence the familiar flip-flopping headlines such as ‘chocolate is good for you’, followed the next week by ‘chocolate is bad for you’. To make good choices, other types of information are needed too — for example, about costs and feasibility. Judgements must also be made about the relevance of information from research (how applicable or transferable it is), and about the balance between the likely desirable and undesirable effects of a drug, therapy or regulation. When it comes to carbon taxes, for example, policymakers need to consider evidence about the environmental and economic effects of such taxes, judge how comparable their context is with that of the studies and weigh how onerous the administrative difficulties are. They also need to model how tax burdens will be distributed across socio-economic groups and think about whether the taxes will be accepted in their jurisdictions. Individuals and organizations across many fields are working to enable people to make informed decisions. These efforts include synthesizing the best available evidence in systematic reviews; making that information more accessible, such as through plain-language summaries or open access; and teaching people how to use such resources. Examples of such review organizations are Cochrane (previously called the Cochrane Collaboration), which focuses on health care; the Campbell Collaboration, which looks at the effects of social policies; the Collaboration for Environmental Evidence; and the International Society for Evidence-Based Health Care. Others include the Center for Evidence-Based Management, the Africa Centre for Evidence, the International Initiative for Impact Evaluation (known as 3ie) and Britain’s What Works Centres. Unfortunately, academics tend to work in silos and can miss opportunities to learn from others. The expertise of the authors of this article spans 14 fields: agriculture, economics, education, environmental management, international development, health care, informal learning, management, nutrition, planetary health, policing, speech and language therapy, social welfare, and veterinary medicine. We have identified many concepts that apply across these fields (see ‘Key Concepts for Informed Choices’ and ‘Key concepts in action’). Some further concepts are more relevant in some fields than in others. For example, it is often important to consider potential placebo effects when assessing claims about medical treatments and nutrition; these are rarely relevant to interventions in the environment. A maternity ward in Dar es Salaam, Tanzania.Credit: Gary Carlton/Alamy Beliefs alone about how interventions work are not reliable predictors of the presence or size of effects. Most people feel that it is hard to influence parents’ engagement with their children’s education. The assumption is therefore that more intensive (and more costly) interventions would be more likely to be effective. However, studies of intensive interventions have often failed to show effects on pupils’ attainment, as measured using standard tests (see go.nature.com/2gfy8io). Meanwhile, a recent evaluation of the effects of simply text-messaging parents weekly with updates about their child’s schooling had positive effects on children’s attendance, homework submission and mathematics attainment (see go.nature.com/2t7ormy). These effects were small, but the cost was very low. This illustrates that — contrary to our hunches — inexpensive interventions can be helpful, and expensive ones can fail. Conditions should be as similar as possible. ‘Scared Straight’ programmes take young offenders on prison visits on the assumption that this experience and listening to inmates’ descriptions of life inside will deter juvenile delinquency. Some studies have found that such prison visits were followed by large reductions in delinquent behaviour. But a lot can change in a group of youngsters over time, including their becoming older and more mature. How can anyone know that the prison visits caused the reduction? Fairer experiments were done in which youths were randomly assigned to visit prison or not, creating groups that were more comparable. Comparisons between these groups showed more delinquency in the youngsters who had been exposed to prisons than among those who had not8,9. When there are important uncertainties about the effects of interventions, those should be reduced by fair comparisons. In the health sector, financing schemes in which funds are released only if a specific action is taken or performance target is met have become popular. Billions of dollars have been invested in promoting these schemes in low- and middle-income countries, with the aim of achieving international development goals10. For example, health providers have been offered cash rewards for increasing the percentage of births in clinics (rather than at home), with the intention of improving maternal and newborn health and survival. But performance-based financing schemes can have unintended adverse effects, such as encouraging health-care workers to falsify records or to neglect other activities. In Tanzania, some health facilities threatened new mothers with fines or denial of vaccinations for their children10. For interventions in which there is much uncertainty about the pros and cons, further fair comparisons should be done before or while rolling out such schemes. Our collaboration has already prompted many of us to develop frameworks for specific fields and to suggest improvements to the original Informed Health Choices framework2. There is power in identifying an issue that resonates across different domains; it provides momentum to align efforts. The Key Concepts for Informed Choices is not a checklist. It is a starting point. Although we have organized the ideas into three groups (claims, comparisons and choices), they can be used to develop learning resources that include any combination of these, presented in any order. We hope that the concepts will prove useful to people who help others to think critically about what evidence to trust and what to do, including those who teach critical thinking and those responsible for communicating research findings. Evidence-informed practice is now taught to professionals in many different fields, and these efforts must grow. It is also crucial that schoolchildren learn these key concepts, rather than delaying acquisition of these skills until adulthood. Young people who have been explicitly taught critical thinking make better judgements than those who have not6. Educating people about such concepts at a young age sets an important foundation for future learning. An important part of the work of encouraging critical thinking is learning and sharing strategies that promote healthy scepticism, but which avoid unintended adverse consequences. These include inducing nihilism (extreme scepticism); allowing for disingenuous claims that uncertainty is a defensible argument against action (on climate change, for example); or encouraging false beliefs — such as that all research is untrustworthy because of competing interests among those who promote particular interventions. Competing interests take various forms in different fields, but the challenges and remedies are similar: recognition of potential conflicts, transparency and independent evaluations. Achieving these depends on improved public understanding of the need for independent evaluation, and public demand for investment in it, as well as unbiased communication of findings. Further development and specialization of the Key Concepts for Informed Choices is needed, and we welcome suggestions. For example, more consideration needs to be given to how these concepts can be applied to actions to address system-wide changes, taking into account complex, dynamic interactions and feedback loops, such as in climate-change mitigation or adaptation strategies. We have therefore created a website (www.thatsaclaim.org) on which our key concepts can be adapted to different fields and target users, translated into other languages and linked to learning resources. </body>
<date id = '116'>12 August 2019</date>
<url id = '117'>https://nature.com/articles/d41586-019-02378-x</url>
<title id = '117'>Researchers need guidance on how to handle published work whose ethics have been questioned, argue Graeme D. Ruxton and Tom Mulder.</title>
<body id = '117'>In 2014, an article in the Proceedings of the National Academy of Sciences (PNAS) described an experiment investigating whether human emotional states can be transferred to others by “emotional contagion”1. Researchers altered the news feeds of almost 700,000 Facebook users to investigate whether the percentage of positive or negative posts they view affects the tone of those they write. According to Facebook, all users consent to this kind of manipulation when they agree to the company’s terms of service. Following widespread debate about the ethics of this research2–4, PNAS issued an editorial expression of concern, noting that the collection of data “may have involved practices that were not fully consistent with the principles of obtaining informed consent and allowing participants to opt out”5. Facebook issued an apologetic post6. That social-media data raise challenges that conventional research-ethics frameworks might not be fit for is an area of ongoing debate7. Even so, it is concerning to us that, when we read 120 of the more than 1,500 publications citing the PNAS article (according to Google Scholar), we found that only 11 mentioned ethical concerns. Data-collection protocols routinely go through several rounds of ethical scrutiny — by funders, reviewers and journal editors. But if ethically questionable work makes it through these procedures and is published, no formal safeguards are in place to ensure that such research is handled appropriately. We think that this needs to change. In our view, the need for clarification on how researchers and others should handle published work that is potentially unethical is increasing for four reasons. First, as illustrated by the Facebook study, conventional ethics frameworks and guidance keep being outpaced by new technological developments8. Second, the number of countries with a strong science base is growing (see go.nature.com/2jk5cty), which inevitably leads to greater heterogeneity in cultural norms with respect to research ethics, and to more diversity in the approaches of regulatory authorities. Third, science is being conducted by a shifting cast of commercial entities — either independently from, or in collaboration with, traditional research institutions9. Such organizations might not have the same ethics culture and systems of regulation as traditional universities. Also, the involvement of multiple institutions might make it easier for oversight to fall between the cracks. The emotional-contagion research was conducted by Facebook in collaboration with researchers at Cornell University in Ithaca, New York. Cornell’s institutional review board apparently decided that the study did not need approval by institutional ethical committees because the data had been collected by an outside agency5. A doctor takes a blood sample during the infamous ‘Tuskegee Study of Untreated Syphilis in the African American’, which the US government ran between 1932 and 1972.Credit: National Archives at Atlanta/eyevine Finally, over the past few decades, awareness of the importance of research ethics has grown. Many studies conducted in respected universities in the twentieth century would never be approved under current legislation — such as the experiments performed in the 1960s by the US social psychologist Stanley Milgram, in which people were tricked into believing that they were giving others potentially lethal electric shocks. So what should be done about ethically problematic published studies, be they present-day ones that would be illegal or unethical, or both, in most places, or historical ones that would now be deemed unethical in many jurisdictions? We already have ethics committees making judgements about research approaches before publication. It should be possible to implement systems to help filter out, or flag, ethically problematic research after publication. Researchers from different fields, institutions and countries will have different biases and norms when it comes to research ethics. So, ultimately, universal standards are needed, with agreed-on protocols enshrined in scientific codes of practice similar to those laid out in the International Society for Stem Cell Research guidelines.   Retraction Watch, a blog that aims to enhance the transparency of the retraction process, in June announced plans to provide a tool that will allow its users to be alerted to retractions of any papers in their personal libraries. Could something similar one day be used to flag ethically problematic work? Such a tool is needed because not all such work is retracted. Also, it can be years before a study is withdrawn. A Lancet article that claimed to link the MMR vaccination to autism attracted controversy as soon as it was published. It was shown to be fatally flawed, but wasn’t retracted until 12 years after publication10. At least in the short term, one possibility is for journal editors to ask authors to confirm — whenever they submit a manuscript — that they have no concerns about the ethics of the methodologies used in any of the publications that they cite. Editors could also ask reviewers to note whether they think that the methodologies used in any of the papers cited in a manuscript might today be criticized on ethical grounds, and perhaps seek further consultation with ethics experts if concerns are raised. If nothing else, this would help to raise awareness of the issue. And it could help to reduce the number of times researchers cite ethically problematic work without even flagging the potential problem. Rightly or wrongly, the citation of articles is still the main yardstick by which importance and quality in science is measured. So, every citation adds to the perceived quality of an article, and of the associated authors, institutions and journal. There is no straightforward solution to this problem. Certainly, within medical research, the question of how to handle unethical works has been the subject of heated debate for decades11–13. For instance, some argue that citing the notorious experiments carried out by Nazi scientists on concentration-camp inmates legitimizes that research. Others contend that the use of the data and insights from such studies provide a way to honour those who died14. Essentially, we are calling for this discussion to be broadened to include any research that could potentially cause human or animal suffering. Taking steps to address this problem, and making the ethical problems raised by some scientific studies more transparent, will benefit all of science. </body>
<date id = '117'>05 August 2019</date>
<url id = '118'>https://nature.com/articles/d41586-019-02307-y</url>
<title id = '118'>As scientists from myriad fields rush to perform algorithmic analyses, Google’s Patrick Riley calls for clear standards in research and reporting.</title>
<body id = '118'>Researchers at TAE Technologies in California and at Google are using machine learning to optimize equipment that produces a high-energy plasma.Credit: Liz Kuball Machine learning is driving discovery across the sciences. Its powerful pattern finding and prediction tools are helping researchers in all fields — from finding new ways to make molecules and spotting subtle signals in assays, to improving medical diagnoses and revealing fundamental particles. Yet, machine-learning tools can also turn up fool’s gold — false positives, blind alleys and mistakes. Many of the algorithms are so complicated that it is impossible to inspect all the parameters or to reason about exactly how the inputs have been manipulated. As these algorithms begin to be applied ever more widely, risks of misinterpretations, erroneous conclusions and wasted scientific effort will spiral. These problems are not new. The machine-learning field has chastened itself for decades with the ‘tank problem’. The original study seems to have arisen in the 1960s (ref. 1 is the earliest plausible reference known for this study; with thanks to software engineer Jeff Kaufman) and is obscured by the mists of time, but the story goes like this. Researchers wrote an algorithm to spot tanks in photographs provided by the military. The model found the tanks successfully in test images. But it failed later with future real photos in the field. Why? The details vary in the retelling, but the pictures it was trained on contained other patterns — tanks emerging in the morning light, or under clouds. So, it was other factors such as these that drove the algorithm, not the presence of tanks. Similar confusions are causing soul-searching today2. Many machine-learning papers fail to perform an adequate set of experiments. Standards for review are inconsistent. And competition is encouraging some researchers to cut corners and skip checks once they think they have the answer they want. We cannot predict all the difficulties that will arise with each analysis. But, as a minimum, researchers bringing machine learning to their fields should familiarize themselves with the common pitfalls and the practices they can use to detect and avoid them. To illustrate, I highlight three problems in machine-learning analyses that we have faced and overcome in the Google Accelerated Science team. Splitting data inappropriately. When building models, machine-learning practitioners typically break data into training and test sets. The training set teaches the model, and the model’s performance is evaluated by how well it describes the test set. Researchers typically split the data at random. But data in real life are rarely random. They might contain trends in time — such as from changes in the way the data were gathered, or from varying choices over what information to collect. Such historical patterns are buried in data sets on molecules, for example, which are being screened virtually by machine-learning algorithms to find candidates for drugs. The challenge is to predict how effectively a hypothetical molecule will, for example, be absorbed into the body or decrease inflammation. Screening starts with data on molecules that either do or do not have the desired effect. But the contexts in which the data were collected might be different from how the machine-learning model is to be used.   For example, a model might be built on a set of molecules that is publicly available, but then used on a different, proprietary set. And chemists’ gazes often switch from certain groups of molecules to others, when promising leads are examined and discarded. Thus, researchers often overestimate how well the model will do in practice3. This can lead to inflated expectations and it wastes time and money on poorly chosen molecules. Many model builders (myself included) have fallen into this trap. In other words, the question you want to answer should affect the way you split your data. For the model to predict the effect of adding a couple of atoms to a molecule, each molecule in the test set should have a partner in the training set that is a couple of atoms different. If you want to get good predictions on chemically diverse molecules, each molecule in the test set should be unlike everything in the training set. The ‘right’ way to split data might not be obvious, but careful consideration and trying several approaches will give more insight. Hidden variables. In an ideal experiment, the researcher changes only the variables of interest and fixes all others. This level of control is often impossible in the real world. The accuracy of equipment drifts over time, batches of reagents differ, one experimental condition is performed before another, and results can even be skewed by the weather. Such uncontrolled variables can be pernicious in machine-learning models. For example, my team at Google has been working with the nuclear-fusion start-up firm TAE Technologies in Foothill Ranch, California, to optimize an experiment for producing high-energy plasma4. We built models to try and understand the best equipment settings for the plasma machine. There were hundreds of control parameters, from when to energize electrodes to which voltage to set on the magnets. A range of measurements was recorded, including temperatures and spectra. We took data from thousands of runs of the plasma machine over many months. The settings varied as the device was tuned and modified and as components wore out and different ideas were tried. We were pleased when we arrived at a model that predicted well, for given settings, whether the plasma’s energy would be high. Soon, it became obvious that our predictions were not based on what we thought. An eye examination at Aravind hospital in Madurai, India, where staff and Google researchers are trying to automate diagnoses of blindness caused by diabetes.Credit: Atul Loke/The New York Times/Red/eyevine When we trained the model again, with the time of the experiment as the only input, rather than all the settings of the machine, we got similar predictive power. Why? We think that our first model locked on to time trends, rather than physical phenomena. As the experiments ran, there were periods of time when the machinery was functioning well and periods when it wasn’t. Therefore, the time at which the experiment was done gives you some information about whether the plasma produced was high energy or not. Furthermore, it’s possible to predict roughly when an experiment was done from the setting of the control parameters — there were time trends in how those were varied, too. Hidden variables can also stem from the layout of experiments. For example, we are working with many collaborators on interpreting microscope images, including the New York Stem Cell Foundation Research Institute in New York City. The images include arrays of biological experiments on plates — typically a grid of wells containing cells and liquids. The goal is to spot wells with certain characteristics, such as a change in appearance of the cells after a chemical treatment. But biological variation means that each plate will always look slightly different. And there can be variation across a single plate. The edges often look different from the centre, for example, if more liquid has evaporated in peripheral wells or if the plate was tilted. A machine-learning algorithm can easily pick up on these unintentional variations. For instance, the model might just identify which wells are on the edge of the plate. A simple way to check if this has happened is to ask the model to predict other things, such as the location on the plate, which plate it is and which batch the image is from. If it can do this, be suspicious of your results. The take-home lesson is: use multiple machine-learning models to detect unexpected and hidden variables. One model focuses on the question you care about — is the plasma high or low energy; are the cells healthy or sick? Other models flush out the confounders. If the latter result is strong, normalize your data, run further experiments or temper your conclusions. Mistaking the objective. Machine-learning algorithms require researchers to specify a ‘loss function’, which determines the severity of various errors — such as whether it is better to make two errors of 1% each, or a single error of 2%. Practitioners tend to use a small set of functions that can fail to capture what they really care about. For example, we have been using machine learning to assist in solving partial differential equations5. These formulae are common across the sciences, including in fluid dynamics, electromagnetism, materials science, astrophysics and economic modelling. Often, they must be solved numerically, and we trained models to provide better accuracy at limited resolution.   We started with an equation to describe how water waves propagate in one dimension. The algorithm was tasked with repeatedly predicting the next time step from the current one. We had two slightly different formulations and trained models on both. According to our loss functions, the two models were equally good. However, one produced nonsense while the other stayed close to the desired result. Why? The loss function controlling the learning was considering only the error of the next step, not the validity of the solution over many steps, which is what we really want. Diverging goals also cropped up in our work on machine screening for diabetic retinopathy6, a complication of diabetes and a leading cause of preventable blindness in the world. The condition can be treated effectively if it is detected early, from images of the back of the eye. As we gathered data and had ophthalmologists offer diagnoses based on the images, we asked our machine-learning tools to predict what the ophthalmologist would say. Two issues emerged. First, the ophthalmologists often disagreed on the diagnosis. Thus, we realized that we could not base our model on a single prediction. Nor could we use a majority vote, because, when it comes to medical accuracy, sometimes the minority opinion is the right one. Second, the diagnosis of a single disease was not actually the real objective. We should have been asking: ‘should this patient see a doctor?’ We therefore expanded our goal from the diagnosis of a single disease to multiple diseases. It is easy for machine-learning practitioners to become fixated on an ‘obvious’ objective in which the data and labels are clear. But they could be setting up the algorithm to solve the wrong problem. The overall aim must be kept in mind, or we will produce precise systems that answer the wrong questions. First, machine-learning experts need to hold themselves and their colleagues to higher standards. When a new piece of lab equipment arrives, we expect our lab mates to understand its functioning, how to calibrate it, how to detect errors and to know the limits of its capabilities. So, too, with machine learning. There is no magic involved, and the tools must be understood by those using them. Second, different disciplines need to develop clear standards for how to perform and report on machine learning in their areas. The appropriate controls, soundness checks and error measurements will vary from field to field, and these need to be spelt out clearly so that researchers, reviewers and editors can encourage good behaviour. Third, the education of scientists in machine learning needs to include these broader issues. Although some resources exist (such as http://ai.google/education), we need to do more. We often teach the algorithms and tools, but students need to learn more about how to apply their algorithms and question them appropriately. We are at an amazing point — computational power, data and algorithms are coming together to produce great opportunities for discoveries with the assistance of machine learning. It is our responsibility as a scientific community to ensure that we use this opportunity well. </body>
<date id = '118'>30 July 2019</date>
<url id = '119'>https://nature.com/articles/d41586-019-02232-0</url>
<title id = '119'>Governments that are considering compulsory immunizations must avoid stoking anti-vaccine sentiment, argue Saad B. Omer, Cornelia Betsch and Julie Leask.</title>
<body id = '119'>Children with measles in an overcrowded hospital ward in the Philippines, where an outbreak occurred in Manila and central Luzon in February 2019.Credit: Francis R Malasig/EPA-EFE/Shutterstock Thousands of people worldwide have been affected by recent measles outbreaks, even though there is a safe and effective vaccine. In the first four months of this year, the World Health Organization (WHO) reported about 226,000 measles cases — almost three times the count recorded in the same period last year (see go.nature.com/2jkq8d3). Already, the number of cases in the United States this year has exceeded the reported tally in any year since the country halted sustained transmission of the disease in 2000. Similarly, in Europe, the 2018 figures were the highest this decade (see ‘Measles on the rise’). Partly in response to these outbreaks, some governments are now considering making vaccination for measles and other diseases a legal requirement1. The state of New York signed legislation to that effect last month. Such mandates, which began with smallpox vaccination in nineteenth-century Europe, are in place for numerous vaccines in various countries. And several studies show that requiring vaccination can improve rates in high-income countries (see, for example, ref. 2), although there is limited evidence of the impact of such requirements in low- or middle-income nations. Source: WHO However, mandatory vaccination can worsen inequities in access to resources, because penalties for not complying can disproportionately affect disadvantaged groups. What’s more, the evidence suggests that there is no simple linear relationship between the forcefulness of a policy and its impact on the rate of vaccination. It is crucial that policies don’t inadvertently entrench inequity or fuel anti-vaccine activism. As specialists in vaccination policy and programmes, we lay out here what’s known, to help governments consider whether a mandate is the right fit for their situation. We also discuss what other changes should be made before introducing requirements (see ‘Best practice’). And we distil how mandates should be designed to ensure effectiveness. There has long been substantial variability in how governments and jurisdictions mandate vaccination — specifically, in what is actually required of people; the penalties imposed if requirements are not met; and the age groups and populations that are covered. In the United States, for instance, proof of immunization or exemption documentation is required before children can go to school. All 50 states and Washington DC allow exemptions for medical reasons, and 45 states allow philosophical or religious exemptions. In Australia, certain vaccines are a requirement for entry into preschool or childcare in some states, but not in others. In Uganda, parents who fail to vaccinate their children can be jailed for six months.   Studies conducted largely in the United States and Europe suggest that making vaccination a requirement for enrolment in childcare and school can help to increase rates (see, for example, ref. 2). For instance, a review of studies conducted mostly in the United States found that the need to provide documentation to access childcare or to attend school and college is associated with a median improvement of 18 percentage points in the rate of vaccination for diseases such as measles, hepatitis B and whooping cough (see go.nature.com/3tzrujo). When it comes to obtaining an exemption, having complex administrative procedures in place (such as those involving counselling with a physician) reduces the number of parents who refuse to have their children vaccinated. It also lowers the number of people who are affected by vaccine-preventable diseases2. In a 2012 study, non-medical exemption rates were more than twice as high in US states that had relatively easy exemption procedures, compared with states that had more complex ones3. Given such evidence, governments have sometimes removed non-medical exemptions altogether. In the past four years, the states of Maine, New York and California joined West Virginia and Mississippi in eliminating non-medical exemptions for all or some vaccines. And in response to a media and public campaign, Australia implemented legislation in 2016 that prevents parents from obtaining non-medical exemptions. Parents on a march protesting against mandatory vaccinations in Washington state earlier this year.Credit: Lindsey Wasson/Reuters Increases in vaccination rates have been associated with financial penalties. These take the form of either the withdrawal of family assistance payments (currently as much as Aus$26,000 (US$18,200) a year in Australia, by our calculations) or fines for parents who refuse to vaccinate their children. In a study evaluating mandatory vaccination in Europe, measles vaccine coverage was 0.8% higher and whooping-cough vaccine coverage was 1.1% higher for every €500 (US$560) increase in the penalty4. Vaccination requirements (tied to school and childcare access, or to monetary penalties) fare well in comparisons with other large-scale interventions, such as vaccination drives at schools, or communication campaigns involving pamphlets, billboards, television advertisements and so on. A 2017 review of interventions to increase vaccination found that in high-income countries, requirements to vaccinate are more likely to affect rates than are attempts to change how people think and feel about vaccination5. So, in many cases, requirements to vaccinate do seem to improve vaccination rates. But do rigid, punitive policies work better than flexible ones? In our view, not necessarily. In fact, the limited data that are available suggest that a middle-of-the-road approach might be more effective. These data come mainly from California, Washington state (which eliminated personal-belief exemptions to measles, mumps and rubella (MMR) vaccination this year) and Australia. In 2015, California became the third US state to eliminate all non-medical exemptions, and the first state to do so in more than three decades. This change in the law was preceded by a 2014 administrative initiative to reduce the misuse of a school admission process involving ‘conditional entrants’ — children who have started the required vaccination schedule but haven’t completed it6. (Since 1979, children in California have been allowed to attend school as conditional entrants — but before 2014, only some schools followed up with parents, and some children were never fully vaccinated6.) The proportion of children of kindergarten age who are not up to date on their vaccinations has decreased in California, from 9.8% in 2013 to 4.9% in 20177. However, this change seems to be mainly associated with the administrative crackdown on conditional entrants. Following the elimination of non-medical exemptions, many parents with strong objections to vaccination simply acquired medical exemptions instead, educated their children at home, enrolled them in independent study programmes that do not require classroom-based instruction, or found other loopholes6. In Australia, following policy changes in 1999, parents had to get their child vaccinated to get assistance payments. And they could obtain non-medical exemptions only after they had discussed the issue with a health-care provider. According to surveys, these policies helped to improve vaccination coverage from an estimated 80% to more than 90% in three years8. Then, in 2016, Australia implemented a ‘No Jab No Pay’ policy, which removed non-medical exemptions and applied the loss of payments more frequently. Overall immunization rates for five-year-olds have since increased nationally, from 92.6% in 2015 to 94.8% by March 2019 (see go.nature.com/2xmgtun). But this smaller improvement comes after the roll-out of several concurrent strategies designed to improve coverage — from schemes to remind parents to get their child vaccinated, to campaigns to improve public awareness. So the impact of the ‘No Jab No Pay’ policy alone is unclear. In 2017, one of us (J.L.) was involved in a study that interviewed 31 parents in Australia who were refusing vaccination for their child9. Of this group, 17 indicated that they planned to get more involved in protest action if additional such measures were implemented, because they felt that the government was coercing them. Interestingly, in an experimental study10, more people with a negative attitude towards vaccination chose to accept a hypothetical second vaccine when they had previously been told that they could choose to be vaccinated with a first vaccine. When these individuals were told that they had to be vaccinated with the first vaccine, 39% less people elected to receive the optional one10. In short, various findings suggest that the most effective approach when it comes to mandating vaccination could be to allow non-medical exemptions, but to make them hard to obtain. Removing the choice of opting out entirely might simply induce parents to seek loopholes, and, worse, fuel negative attitudes towards vaccination. If vaccination rates are low in a particular region or community, a government’s first step must be to find out why. Guidance from the WHO Regional Office for Europe, for example, lays out steps for targeting specific communities, such as by working with community leaders, health-care workers and service users to establish whether people are struggling to get to their local clinics or avoiding health-care providers for some other reason11. (J. L. was a reviewer of this guide, and all three of us have received funds from the WHO, which, as a UN agency, has no financial competing interest regarding this article.) Mandates are often inspired by the perception among politicians and the public that vaccine refusal by parents is the biggest problem. But poverty, social exclusion and difficulties over access also depress rates, and, in many settings, more so than refusal. In Germany, for example, barriers to access probably explain why children of migrant parents have a 10% lower immunization rate for booster doses (such as for tetanus or human papillomavirus) than do children who were born there12. A requirement to vaccinate when the vaccine or primary-care service is difficult or impossible for many people to reach is not justifiable or fair13. Thus, before even considering mandates, governments must ensure that people from all sectors of society can get vaccines easily and safely. This means making primary-care services flexible, welcoming and easy to reach, and ensuring a stable supply of vaccines. If governments then decide that mandates are appropriate, they should take the following five steps. Use multiple interventions. Ideally, requirements to vaccinate should be part of a suite of interventions. These could include: robust methods for recording immunization, such as in a registry; text-message or e-mail reminders to parents before a child’s vaccines are due; and a process to monitor and give feedback on how primary-care providers perform on vaccination rates5 (see also go.nature.com/3puzrga). All of these interventions should be in place whether or not mandates are implemented (see ‘Best practice’).  Ensure just procedures. Limited restrictions on personal autonomy are more likely to be workable in democracies. In these, societies are more able to express their collective will than in dictatorships, where such restrictions can be abused. Indeed, it is crucial that the process of developing mandates is itself democratic. Deliberative methods can ask what an informed citizenry would find an acceptable policy response and why. A good model is the community juries used for more than two decades, mostly in the United States, Australia and Canada, to address policy issues in other areas of health care, such as for cancer screening. In these, panels of citizens hear evidence, then debate the issue and give their verdicts14. Make penalties proportionate. In our view, incarceration is never justified for enforcing vaccination. Temporary quarantine or the use of child protection laws might be an appropriate action when the risk of a vaccine-preventable disease is very high (such as in a newborn whose mother tests positive for hepatitis B)15. Even with penalties such as fines, withheld benefits or blocked entry to childcare or schools, care must be taken to ensure that they do not exacerbate social or health inequities. Monitor safety and compensate for side effects. In the exceedingly rare instances in which required vaccines cause harm, those affected should be adequately compensated. (For instance, around 2.6 cases of the rare bleeding disorder thrombocytopenic purpura arise for every 100,000 doses of MMR vaccine that are given16.) Proactive surveillance systems that monitor side effects should be paired with a timely programme for compensation that minimizes administrative and legal burdens for those injured17. In the United States, people seeking compensation following vaccination have to demonstrate only that they (or their child) had an adverse event known to be associated with the vaccine. By contrast, people in Australia have to pursue compensation through the courts — a time-consuming and expensive process. Such programmes can be financially sustainable. In the United States, vaccine manufacturers are taxed on vaccines sold in the country to finance this (currently, 75 cents per antigen). Other financial models have been proposed, including for low and middle-income countries18. Avoid selective mandates. Governments should avoid making specific vaccines mandatory. In France in the 1960s, there was a policy shift. Older vaccines such as those for smallpox, diphtheria, tetanus, tuberculosis and polio remained mandatory; newer ones for diseases such as measles were only ‘recommended’19. For many years, there has been a difference in uptake of up to 20% between the two classes. Vaccines that were ‘only’ recommended were perceived as non-essential by French parents. (In 2018, the recommended vaccines became mandatory20.) And experimental evidence shows that making one vaccine mandatory might reduce people’s uptake of others10. In our view, Germany, which is currently considering a mandate for just measles, should rethink. In summary, making vaccination a legal requirement can be a powerful and effective tool if implemented with care and with regard to the context. Crucially, evidence for the effectiveness of mandates is largely limited to high-income countries. Overly strict mandates can result in parents finding ways to avoid the vaccine requirements, and selective mandates might damage the broader vaccination programme. Most importantly, vaccine policy — like other types of effective public policy — must be based on evidence, and not driven by political and ideological considerations. </body>
<date id = '119'>22 July 2019</date>
<url id = '120'>https://nature.com/articles/d41586-019-02143-0</url>
<title id = '120'>Thin, flexible, wireless monitoring systems could make medicine more predictive and personalized, argue Shuai Xu, Arun Jayaraman and John A. Rogers.</title>
<body id = '120'>Sensors soft enough to be used on a premature baby’s skin can monitor vital signs in the neonatal intensive care unit.Credit: J. Rogers/Northwestern Univ. Thin, soft electronic systems that stick onto skin are beginning to transform health care. Millions of early versions1 of sensors, computers and transmitters woven into flexible films, patches, bandages or tattoos are being deployed in dozens of trials in neurology applications alone2; and their numbers growing rapidly. Within a decade, many people will wear such sensors all the time. The data they collect will be fed into machine-learning algorithms to monitor vital signs, spot abnormalities and track treatments. Medical problems will be revealed earlier. Doctors will monitor their patients’ recovery remotely while the patient is at home, and intervene if their condition deteriorates. Epidemic spikes will be flagged quickly, allowing authorities to mobilize resources, identify vulnerable populations and monitor the safety and efficacy of drugs issued. All of this will make health care more predictive, safe and efficient. Where are we now? The first generation of biointegrated sensors can track biophysical signals, such as cardiac rhythms, breathing, temperature and motion3. More advanced systems are emerging that can track certain biomarkers (such as glucose) as well as actions such as swallowing and speech. Small companies are commercializing soft biosensor systems that measure clinical data continuously. These include Vital Connect in San Jose, California; iRhythm in San Francisco, California; MC10 in Lexington, Massachusetts; and Sibel Health in Evanston, Illinois. For example, iRhythm’s single-use Zio patch monitors electrical pulses from the heart for 14 days, and is more effective than intermittent hospital check-ups at detecting abnormal rhythms4. But it is bulky and temporary, and the data must be downloaded after use, rather than transmitted in real time.   More advanced sensors from our labs are undergoing clinical trials in Chicago, Illinois5. These include even smaller sensor networks for heart rate, respiration and temperature. They can transmit data wirelessly, and are soft enough to place on the chests of premature babies without damaging their fragile skin6. There is no need for nurses, doctors or parents to disconnect a forest of wires when they want to pick up a baby. Similar systems might sense pressure and temperature in people who have had limbs amputated, at the interface between a limb socket and prosthesis. Many challenges must be overcome to make wearable sensors fit for widespread use. Innovations in materials, devices and circuit designs must make soft biosensors even smaller, thinner, lighter and less power-hungry. The accuracy, precision and range of measurements must improve. And regulation, costs, usability and data security require attention. Here, we outline the priorities for action. Biomarkers. All the flexible sensor systems approved by the US Food and Drug Administration (FDA) so far collect biophysical signals. Biochemical signatures, such as glucose or hormone levels, are hard to glean without puncturing the skin with needles. Some emerging devices collect fluid by inserting a filament into the skin. And detecting chemicals in sweat is a promising alternative7. Sweat contains many indicators relating to cell health and organ function (such as electrolytes), the immune system (cytokines) and drug interactions (metabolites). Sweat sensors are being developed that capture chloride, glucose, lactate, urea, creatinine, alcohol, pH and even heavy metals. Quantifying protein and hormone levels in sweat would increase these sensors’ applicability further. Still, sensors need to be able to collect and analyse sweat without it becoming contaminated or degrading, and they will also require new chemical tests and types of assay. Tools. Imaging and spectroscopy capabilities would allow for real-time assessments of the body. Examples are optical coherence tomography, confocal microscopy, Raman spectroscopy and two-photon excitation microscopy. If such systems could be miniaturized, they could diagnose skin tumours without the need for a biopsy sample or surgery. They are currently still expensive, bulky and wired. Therapies. Interfaces that create skin sensations, such as vibrations, might enhance rehabilitation, notably with speech and motion therapies. Drugs could be delivered through skin patches, as they are already for motion sickness (scopolamine), pain (fentanyl), contraception (norelgestromin and ethinylestradiol) and high blood pressure (clonidine). The release could be triggered electrically, acoustically or thermally, for example, by applying heat to a polymer pocket. Sensors could also deliver electrical or thermal stimulation to treat neurological disorders or modulate pain. The single-use Zio patch from iRhythm in San Francisco, California, can monitor heart rate continuously for two weeks to detect irregularities.Credit: iRhythm Technologies Implants. Soft sensor systems could be used inside the body. A thin, flexible implant might be wrapped around the heart or spine to monitor and stimulate it. Demonstration versions of thin, flexible technologies that track the electrical activity of the brain have been tested in mice, cows and non-human primates. Practical challenges include developing biocompatible materials and manufacturing ultra-thin layers that protect the electronics for years or decades. Some patches might melt away harmlessly after they have done their job, just as a wound heals. Materials and design. There is work to be done to make devices less perceptible to wearers. Today’s patches typically include ultra-thin silicon electronics in a matrix of silicone elastomers. In future, organic polymers could be used to make biosensors that repair themselves. And the soft materials will have functions of their own, perhaps being antimicrobial or able to change colour if a biochemical is detected. Power could be harvested from body motions or changes in heat or blood flow rather than from batteries8. Data. Combinations of sensors need to be designed to suit certain conditions. For example, for Parkinson’s disease, a single sensor on the hand is enough to detect tremors9. But in people who have had a stroke, characterizing how hard their foot hits the ground when walking, how strongly they swallow or how soundly they sleep would require additional sensors and data outputs — from accelerometers, gyroscopes, microfluidic sensors, and electrocardiographs and electromyographs (which measure electrical activity in the heart and muscles, respectively). To improve data quality, these sensors should be sited on the best places on the body to collect information — for example, electrocardiogram signals should be recorded on the chest, not the wrist. Gait is better assessed with sensors on the ankles. Noise will need to be filtered out, and decisions will need to be made about whether it is better to stream all of the data to the cloud or process some of them on the chip and transmit only key parameters or insights extracted from the base data, in the form of warnings or notices. Interpretation. Digital dashboards need to be developed to allow physicians and patients to track outputs, log changes and make clinical decisions. Machine-learning models need improvement, for example to predict how long it will be until a patient is discharged from hospital or is able to walk or feed themselves safely without assistance. Long-term monitoring in the community would help physicians to assess the evolution of stroke recovery, Parkinson’s disease and other disorders. Behaviour. More needs to be learnt about how patients use biosensors in their everyday lives. If people are to wear the devices for weeks or months, the patches will need to look acceptable, and ideally attractive. They should be comfortable and maintain good contact with the skin during washing or exercising. Although some sensors are now small enough fit on a fingernail and thin enough not to show through clothing, they will need to become yet smaller and thinner. Bringing these technologies to patients will take action on three more fronts: validation, regulation and data protection. To speed up their entry into the clinic, soft biosensors must target unmet medical needs, such as mental-health monitoring in the home10. Changes in vital signs and in neuroendocrine, neurotropic and inflammatory biomarkers could yield insights that are unavailable to clinicians today. Signs of social isolation and loneliness might prompt a visit from a carer or a call from a loved one. Wireless health monitoring could also revolutionize health care in countries where infrastructure is lacking. We will trial our biosensors in maternity clinics in several African countries, including Zambia, Kenya and South Africa, later this year, in partnership with the non-profit organizations the Bill & Melinda Gates Foundation and Save the Children. The patches will track physiological data such as physical activity, blood pressure and respiratory rate in women and their babies during pregnancy, warning of complications such as fetal hypoxia or an impending haemorrhage. Regulatory approval is crucial, and challenging to obtain. Hardware is largely covered by existing frameworks; algorithms are not. But there are encouraging signs that software applications can be regulated. In the past few years, the FDA has approved machine-learning technology for the diagnosis of diabetic retinopathy, the first pill with an embedded sensor (Abilify MyCite) and an app to treat opioid-use disorder (reSET-O). The FDA’s pre-certification programme allows medical software from certain trusted developers to be deployed before formal evaluation. Regulations must adapt quickly, as the boundaries between devices, data, software and therapeutics continue to blur. Special attention should be paid to clinical areas of highest need and minimal risk — there are some such within rare diseases, paediatrics, women’s health and gerontology. Data security must be a top priority, particularly for patient information. The US Health Insurance Portability and Accountability Act established guidelines for the confidential handling of patient information in 1996. But this was well before the explosion in mobile devices and wearable sensors. New frameworks are needed. Patients must own their own data. And great care must be taken to ensure that companies do not exploit medical data for commercial gain without approval, or drive a division between those who can and cannot access this technology. Given the poor track record of private companies in protecting consumer privacy, leadership at both the national and international level is needed. Policies must prevent employers and insurers from discriminating against people with particular data profiles, much as the US Genetic Information Nondiscrimination Act of 2008 protects workers. Deviations should be met with serious financial and legal punishments11. It remains to be seen how these sensor systems will be paid for, and how doctors will be reimbursed for interpreting and acting on the data. Still, health-care funders should champion biointegrated sensor systems because they can potentially improve the quality of care and lower costs. This fits with the move towards value-based care in the United States, where health-insurance companies and government plans such as Medicare are selecting treatments on the basis of efficacy rather than simply reimbursing services. Technical progress will require close collaborations between materials and device engineers, data scientists and medical professionals. Users and carers need to be more closely involved. Interdisciplinary funding from government sources, corporate investments and charitable foundations will be essential for collecting proof-of-concept data before devices can be commercialized. For example, the Michael J. Fox Foundation in New York City has grant programmes focused on wearable technologies in global health. Companies need to improve manufacturing processes for devices that combine hard skeletal components and soft tissue-like materials. Yields and throughputs need to be improved to assure quality and lower costs. Automated tools are needed to design the layout and topology of circuits and mechanical components. The effort will be worth it: bio-integrated sensors have the potential to transform nearly every aspect of medicine. </body>
<date id = '120'>17 July 2019</date>
<url id = '121'>https://nature.com/articles/d41586-019-02088-4</url>
<title id = '121'>As today’s tensions mount, it is salutary to recall that cooperation was on the table during the cold war, writes Roger D. Launius.</title>
<body id = '121'>NASA astronaut Deke Slayton and Soviet cosmonaut Alexey Leonov in the Soyuz spacecraft.Credit: NASA The Apollo programme that took humans to the Moon is properly viewed as an outgrowth of the cold war between the United States and the Soviet Union. Today, as strained US relations with foreign powers threaten science once more, it is worth recalling how surprisingly close the Moonshot came to becoming a cooperative venture. On 25 May 1961, President John F. Kennedy announced the US commitment to astronauts reaching the Moon by the end of the decade, stoking Americans’ patriotism and pioneer spirit — a particularly fascinating period for me, a former chief historian of NASA. As we celebrate the 50th anniversary of Apollo 11’s successful Moon landing, few people are aware that almost immediately afterwards, Kennedy explored the possibility of bringing the Soviet Union — then the only other spacefaring nation — into the venture as a full partner. This would have reshaped the programme from one of competition into one that fostered international cooperation.   Kennedy proposed as much to the Soviet leader, Nikita Khrushchev, at their first and only summit in June 1961. Khrushchev insisted that such discussion should await the negotiation of a nuclear-test-ban treaty. Kennedy revisited the idea of cooperation repeatedly thereafter. By autumn 1963, his vision was to form an Apollo programme that would build bridges between the two superpowers, instead of heightening cold-war rivalries. But the timing never worked. A series of conflicts between the two nations in 1961 and 1962 — in the stand-off that led to the construction of the Berlin Wall, the Cuban missile crisis and so on — blunted efforts at genuine cooperation. Nonetheless, by September 1963, relations had improved and Kennedy invited the Soviet Union to work with the United States. Addressing the United Nations, he offered the vision of a joint lunar expedition. “Space offers no problems of sovereignty,” he said, and spoke of sending scientists to the Moon as representatives of all countries, not of a single nation. By that time, Khrushchev had started to think the idea had merit, but Kennedy’s assassination on 22 November 1963 scotched the plan. The Americans went on to win the space race, and landed Apollo astronauts on the Moon six times between July 1969 and December 1972. The Soviets tried and failed in their own landing programme, although their robotic sample-return missions were a success. Still, cooperation between the United States and the Soviet Union in space remained a constant subtext throughout the cold war. Often informal, this enjoyed varying degrees of support from national leaders on both sides. The most significant venture came as a result of the detente during the early 1970s, and led to the highly successful Apollo–Soyuz Test Project in 1975. This saw a US and a Soviet spacecraft dock in orbit, and got cosmonauts and astronauts working together on experiments. The conclusion of the cold war ushered in a new era of joint space endeavours. For the duration of the US Space Shuttle programme (between 1981 and 2011) and the International Space Station (ISS; continuously occupied since 2000), NASA has worked with several international partners. Since 1992, that has included the Russian space agency, Roscosmos. Then US President John F. Kennedy shakes hands with Soviet Premier Nikita Khrushchev in Vienna.Credit: Ron Case/Getty Bringing Russia into the international coalition that constructed the ISS in 1993 was a trailblazing achievement. It has had its difficulties — what long-standing relationship does not have rough patches? But, unquestionably, it has been a stunning success. When the significance of the ISS is reconsidered in the next century and beyond, I think that its greatest achievement will be seen as promoting peaceful cooperation among many nations. The United States and Russia have uniquely complementary capabilities, and they have been at each other’s side for more than 25 years as humans push back the final frontier. Through their efforts, low Earth orbit has become a normal realm of human activity. Russia’s Soyuz spacecraft transport system, the US Space Shuttle and the various space stations have made Earth orbit feel like a backyard. We are beginning to see the expansion of orbital commerce, all to the good, and perhaps the prospect of a return to the Moon through a broad international consortium, including the United States and Russia.   All that history puts recent scuffles in new light. One was the January decision by NASA administrator Jim Bridenstine to withdraw an invitation to the head of Roscosmos, Dmitry Rogozin, to visit the United States. It highlights geopolitical challenges that have long been present, especially with the Soviet Union and now Russia. Rogozin was serving as Russia’s deputy prime minister in 2014, and was outspoken when the United States imposed sanctions over Moscow’s annexation of the Crimean Peninsula that year. The Russian space agency warned that Bridenstine’s decision could lead to the termination of existing cooperative agreements and make future deals more difficult. Given decades of history, this is doubtful, but possible. The fact is, every aspect of space exploration and development advances through cooperative initiatives. At present, the human spaceflight programmes of all nations have been most successful when inextricably linked to each other. Disentangling them would be difficult, expensive, time-consuming and foolhardy. For decades, the United States and Russia have had terrestrial differences. These should remain on Earth. Cooperation in space continues to light the way, and should be encouraged by all sides. </body>
<date id = '121'>10 July 2019</date>
<url id = '122'>https://nature.com/articles/d41586-020-01499-y</url>
<title id = '122'>A long-anticipated recalibration of radiocarbon dating could shift the age of some prehistoric samples hundreds of years</title>
<body id = '122'>Researchers use data from tree rings, sediment layers and other samples to calibrate the process of carbon dating.Credit: Philippe Clement/Arterra/Universal Images Group/Getty Radiocarbon dating — a key tool used for determining the age of prehistoric samples — is about to get a major update. For the first time in seven years, the technique is due to be recalibrated using a slew of new data from around the world. The result could have implications for the estimated ages of many finds — such as Siberia’s oldest modern human fossils, which according to the latest calibrations are 1,000 years younger than previously thought. The work combines thousands of data points from tree rings, lake and ocean sediments, corals and stalagmites, among other features, and extends the time frame for radiocarbon dating back to 55,000 years ago — 5,000 years further than the last calibration update in 2013. Archaeologists are downright giddy. “Maybe I've been in lockdown too long,” tweeted Nicholas Sutton, an archaeologist at the University of Otago in New Zealand, “but … I'm really excited about it!”   Although the recalibration mostly results in subtle changes, even tiny tweaks can make a huge difference for archaeologists and paleo-ecologists aiming to pin events to a small window of time. A new calibration curve “is of key importance” for understanding prehistory, says Tom Higham, archaeological chronologist and director of the Oxford Radiocarbon Accelerator Unit, UK. The basis of radiocarbon dating is simple: all living things absorb carbon from the atmosphere and food sources around them, including a certain amount of natural, radioactive carbon-14. When the plant or animal dies, they stop absorbing, but the radioactive carbon that they’ve accumulated continues to decay. Measuring the amount left over gives an estimate as to how long something has been dead. But this basic calculation assumes that the amount of carbon-14 in the environment has been constant in time and space — which it hasn’t. In recent decades, the burning of fossil fuel and tests of nuclear bombs have radically altered the amount of carbon-14 in the air, and there are non-anthropogenic wobbles going much further back. During planetary magnetic-field reversals, for example, more solar radiation enters the atmosphere, producing more carbon-14. The oceans also suck up carbon — a little more so in the Southern Hemisphere, where there is more ocean — and circulate it for centuries, further complicating things. As a result, conversion tables are needed that match up calendar dates with radiocarbon dates in different regions. Scientists are releasing new curves for the Northern Hemisphere (IntCal20), Southern Hemisphere (SHCal20), and marine samples (MarineCal20). They will be published in the journal Radiocarbon in the next few months. Since the 1960s, researchers have mainly done this recalibration with trees, counting annual rings to get calendar dates and matching those with measured radiocarbon dates. The oldest single tree for which this has been done, a bristlecone pine from California, was about 5,000 years old. By matching up the relative widths of rings from one tree to another, including from bogs and historic buildings, the tree record has now been pushed back to 13,910 years ago.   Since 1998 there have been four official IntCal calibrations, adding in data from laminated lake and marine sediments, cave stalagmites and corals (which can be both radiocarbon dated and independently assessed using techniques such as radioactive thorium/uranium dating). In 2018, some stalagmites in Hulu Cave in China provided a datable record stretching back 54,000 years1. IntCal20 is based on 12,904 data points, nearly double the size of 2013’s data set. The results are far more satisfying, says Paula Reimer, who heads the IntCal working group and leads the radiocarbon-dating Chrono Centre at Queen’s University Belfast, UK. For a known, brief magnetic field reversal 40,000 years ago, for example, the 2013 curve’s carbon-14 peak was too low and too old by 500 years — an annoyance fixed by the new curve. Higham says the recalibration is fundamental for understanding the chronology of hominins living 40,000 years ago. “I am really excited about calibrating our latest data using this curve,” he says. IntCal20 revises the date for a Homo sapiens jawbone found in Romania called Oase 1, potentially making it hundreds of years older than previously thought2. Genetic analyses of Oase 1 have revealed that it had a Neanderthal ancestor just four to six generations back, says Higham, so the older the Oase 1 date, the further back Neanderthals were living in Europe. Meanwhile, the oldest H. sapiens fossil found in Eurasia — Ust’-Ishim, unearthed in Siberia — is almost 1,000 years younger according to the new conversion curves. “It changes the earliest date we can place on modern humans in central Siberia,” says Higham. He cautions, however, that there are more sources of error in such measurements than just radiocarbon calibration: “Contamination is the biggest influence for dating really old bones like these.”   Others will use the recalibration to assess environmental events. For example, researchers have been arguing for decades over the timing of the Minoan eruption at the Greek island of Santorini. Until now, radiocarbon results typically gave a best date in the low 1600s BC, about 100 years older than given by most archaeological assessments. IntCal20 improves the accuracy of dating but makes the debate more complicated: overall, it bumps the calendar dates for the radiocarbon result about 5–15 years younger, but — because the calibration curve wiggles around a lot — it also provides six potential time windows for the eruption, most likely in the low 1600s BC, but maybe in the high 1500s BC2. So the two groups still disagree, says Reimer, but less so, and with more complications. “Some of them are still arguing,” says Reimer. “There’s no hard answer.” Nevertheless, anyone looking at practically anything relating to human history from the past 50,000 years will be enthusiastic about the new calibration, says Higham: “This is a particularly exciting time to be working on the past.”   Cheng, H. et al. Science 362, 1293–1297 (2018). van der Plicht, J. et al. Radiocarbon https://doi.org/10.1017/RDC.2020.22 (2020). Download references Latest on: Archaeology Article 11 MAY 20 Research Highlight 23 APR 20 Research Highlight 16 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '122'>19 May 2020</date>
<url id = '123'>https://nature.com/articles/d41586-020-01092-3</url>
<title id = '123'>Scientists urge caution over hints of success emerging from small human and animal studies.</title>
<body id = '123'>Vaccines against the coronavirus are being tested in humans and animals.Credit: Juan Ignacio Roncoroni/EPA-EFE/Shutterstock As coronavirus vaccines hurtle through development, scientists are getting their first look at data that hint at how well different vaccines are likely to work. The picture, so far, is murky. On 18 May, US biotech firm Moderna revealed the first data from a human trial: its COVID-19 vaccine triggered an immune response in people, and protected mice from lung infections with the coronavirus SARS-CoV-2. The results — which the company, based in Cambridge, Massachusetts, announced in a press release — were widely interpreted as positive and sent stock prices surging. But some scientists say that because the data haven’t been published, they lack the details needed to properly evaluate those claims.   Tests of other fast-tracked vaccines show that they have prevented infections in the lungs of monkeys exposed to SARS-CoV-2 — but not in some other parts of the body. One — a vaccine being developed at the University of Oxford, UK, that is also in human trials — protected six monkeys from pneumonia, but the animals’ noses harboured as much virus as did those of unvaccinated monkeys, researchers reported1 last week in a bioRxiv preprint. A Chinese group reported similar caveats about its own vaccine’s early animal tests this month2. Despite uncertainties, all three teams are pressing ahead with clinical trials. These early studies are meant mainly to test safety, but larger clinical trials designed to determine whether the vaccines can actually protect humans from COVID-19 could report in the next few months. Still, the early data offer clues as to how coronavirus vaccines might generate a strong immune response. Scientists say that animal data will be crucial for understanding how coronavirus vaccines work, so that the most promising candidates can be identified quickly and then refined. “We might have vaccines in the clinic that are useful in people within 12 or 18 months,” says Dave O’Connor, a virologist at the University of Wisconsin–Madison. “But we’re going to need to improve on them to develop second- and third-generation vaccines.” Moderna’s vaccine, which is being co-developed with the US National Institute of Allergy and Infectious Diseases (NIAID) in Bethesda, Maryland, began safety testing in humans in March. The vaccine consists of mRNA instructions for building the coronavirus’s spike protein; it causes human cells to churn out the foreign protein, alerting the immune system. Although such RNA-based vaccines are easy to develop, none has yet been licensed anywhere in the world. In its press release, the company reported that 45 study participants who received one or two doses of the vaccine developed a strong immune response to the virus. Researchers measured virus-recognizing antibodies in 25 participants, and detected levels similar to or higher than those found in the blood of people who have recovered from COVID-19.   Tal Zaks, Moderna’s chief medical officer, said in a presentation to investors that these antibody levels bode well for the vaccine preventing infection. “If you get to the level of people who had disease, that should be enough,” Zaks said. But it’s not at all clear whether the responses are enough to protect people from infection, because Moderna hasn’t shared its data, says Peter Hotez, a vaccine scientist at Baylor College of Medicine in Houston, Texas. “I’m not convinced that this is really a positive result,” Hotez says. He points to a 15 May bioRxiv preprint3 that found that most people who have recovered from COVID-19 without hospitalization do not produce high levels of ‘neutralizing antibodies’, which block the virus from infecting cells. Moderna measured these potent antibodies in eight trial participants and found their levels to be similar to those in recovered patients. Hotez also has doubts about the Oxford team’s first results, which found that monkeys produced modest levels of neutralizing antibodies after receiving one dose of the vaccine (the same regime that is being tested in human trials). “It looks like those numbers need to be considerably higher to afford protection,” says Hotez. The vaccine is a made from a chimpanzee virus that has been genetically altered to produce a coronavirus protein. Hotez says that the vaccine being developed by Sinovac Biotech in Beijing seems to have elicited a more promising antibody response in macaque monkeys that received three doses, as reported2 in a 5 May paper in Science. That vaccine is comprised of chemically inactivated SARS-CoV-2 particles. No one yet knows the precise nature of the immune response that protects people from COVID-19, and the levels of neutralizing antibodies made by the monkeys in the Oxford Study might be enough to protect people from infection, says Michael Diamond, a viral immunologist at Washington University in St. Louis, Missouri, who is a member of Moderna’s scientific advisory board. If not, a second injection would probably boost levels appreciably. “What we don’t know is how long they’ll last,” he adds. Still more questions hover over experiments showing that vaccines can protect animals from infection. Moderna said its vaccine stopped the virus replicating in the lungs of mice. The rodents had been infected with a version of the virus that was genetically modified to let it attack mouse cells, which are not ordinarily susceptible to SARS-CoV-2, according to Zaks’s presentation. But the mutation affects the protein that most vaccines, including Moderna’s, use to stimulate the immune system, and this could change the animals’ response to infection4.   The Oxford monkeys were given an extremely high dose of virus after receiving the vaccine, says Sarah Gilbert, an Oxford vaccinologist who co-led the study with Vincent Munster, a virologist at NIAID’s laboratories in Hamilton, Montana. This could explain why the vaccinated animals had just as much SARS-CoV-2 genetic materials in their noses as control animals, even though the vaccinated monkeys didn't develop any signs of pneumonia. Administering high doses ensures that the animals are infected with the virus, but it might not replicate natural infections. The Oxford study did not measure whether the virus was still infectious, Diamond says, and the genetic material could represent virus particles inactivated by the monkeys’ immune response, or the viruses the researchers administered, rather than an ongoing infection. Rhesus macaques are one animal being used to test vaccines, but they show milder symptoms than humans.Credit: Magnus Lundgren/Wild Wonders of China/Nature Picture Library Still, the result is “a concern” that raises the possibility that vaccinated people could still spread the virus, says Douglas Reed, an aerobiologist at the University of Pittsburgh Center for Vaccine Research in Pennsylvania. “Ideally, you want a vaccine that would protect against disease and against transmission, so that we can kind of break the chain,” he says. One way to find out whether vaccines can prevent transmission would be to study them in animals that are naturally susceptible to the virus and seem capable of spreading it, such as ferrets and hamsters, says Reed. He and other researchers also point out that macaques display only mild symptoms of coronavirus infection, and they wonder whether vaccines should be trialled in animals that develop more severe disease. Although assessing vaccines’ potential efficacy is difficult, the latest data are clearer on safety, say researchers. The Moderna vaccine caused few severe and no lasting health problems in trial participants. The vaccinated Oxford and Sinovac monkeys did not develop an exacerbated disease after infection — a key fear, because an inactivated vaccine for the related coronavirus that causes SARS (severe acute respiratory syndrome) showed signs of this in macaques5. Stanley Perlman, a coronavirologist at the University of Iowa in Iowa City, says that the animal studies conducted so far can tell vaccine developers only so much. “People are doing as best they can,” he says. None of the data that he’s seen should dissuade developers from pressing on with trials in humans to determine whether the vaccines work, he says.   Moderna will soon begin a phase II trial involving 600 participants. It hopes to begin a phase III efficacy trial in July, to test whether the vaccine can prevent disease in high-risk groups, such as health-care workers and people with underlying medical problems. Zaks said that further animal studies, including some in monkeys, were under way, and that it wasn’t yet clear which animal would best predict whether and how the vaccine works. The Oxford team has already enrolled more than 1,000 people in its UK trial. Some volunteers have received a placebo, so the trial could allow researchers to determine whether the vaccine works in humans over the coming months. The lack of safety problems in the team’s monkey study was reassuring, Gilbert says. “We don’t really need any more data from animal trials to continue,” she says. “If we get human efficacy, we’ve got human efficacy, and that’s what matters.” van Doremalen, N. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.05.13.093195 (2020). Gao, Q. et al. Science https://doi.org/10.1126/science.abc1932 (2020). Robbiani, D. F. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.05.13.092619 (2020). Dinnon, K. H. III et al. Preprint at bioRxiv https://doi.org/10.1101/2020.05.06.081497 (2020). Tseng, C.-T. et al. PLoS ONE 7, e35421 (2012). Download references Latest on: Infection News 22 MAY 20 News 18 MAY 20 News 18 MAY 20 Vaccines News 13 MAY 20 Editorial 13 MAY 20 News 30 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '123'>19 May 2020</date>
<url id = '124'>https://nature.com/articles/d41586-020-01449-8</url>
<title id = '124'>Computational models, cell studies and animal experiments are being used to pinpoint the viral host that kicked off the pandemic.</title>
<body id = '124'>Animals, such as minks, that can be infected with the coronavirus could reveal clues about the virus’ origins.Credit: Jorma Luhta/Nature Picture Library As a growing number of countries push for an independent investigation into the origin of the COVID-19 pandemic, many scientists around the world are already trying to uncover when, where and how the new coronavirus got into people. Finding the source is important for preventing further reinfection, but scientists’ investigations — which include modelling, cell studies and animal experiments — are revealing how tricky pinpointing the source might be. “It is quite possible we won’t find it. In fact, it would be exceptionally lucky if we land on something,” says Lucy van Dorp, a geneticist from University College London (UCL). There is strong evidence that the virus originated in bats. The biggest mystery remains how it got from bats to people. Researchers overwhelmingly think that it’s a wild virus, which probably passed to people through an intermediate species. But no one has found the virus in the wild yet, so other explanations cannot be ruled out entirely.   US President Donald Trump has fuelled suggestions that the virus might have leaked from a laboratory in Wuhan, where the outbreak started. There is no evidence for that claim. Still, other world leaders have called for investigations into the outbreak's origin. The European Union and dozens of nations are supporting a draft proposal submitted to the World Health Assembly, the key decision making-body of the World Health Organisation, which is holding a virtual meeting with member states today and tomorrow. The proposal calls for “scientific and collaborative field missions” to “identify the zoonotic source of the virus and the route of introduction to the human population, including the possible role of intermediate hosts”. The only way to say with confidence which animal the virus came from is to find it in that species in the wild, says Arinjay Banerjee, a coronavirus researcher at McMaster University in Hamilton, Ontario. “Other approaches will only give you anecdotal evidence,” he says. But given that the virus has spread so widely among people, even detecting the virus in animals will not necessarily confirm their role as intermediate hosts as they might have been infected by people, says Li Xingguang, who studies viral evolution at Wuhan University of Bioengineering. “The situation is very complex now.” Researchers’ first started looking at the virus’s genome to see whether they could match it to pathogens found in other animals. In late January, a few weeks after researchers sequenced the SARS-CoV-2 genome, scientists at the Wuhan Institute of Virology posted online the entire sequence of a coronavirus that had been stored in their lab since being discovered in intermediate horseshoe bats (Rhinolophus affinis) in Yunnan province in 2013. That genome, named RATG13, was 96% identical to SARS-CoV-2, making it the closest known relative and strongly suggesting the new virus originated in bats. Many scientists think the new coronavirus originated in bats.Credit: Alex Hyde/Nature Picture Library Computational biologist Francois Balloux and his team at UCL, including colleague van Dorp, and other teams are searching genomic databases of animals looking for coronaviruses that are an even closer match.   Although the 4% difference between the genomes of SARS-CoV-2 and RATG13 still represents some 50 years since they last shared a common ancestor, says van Dorp. The divergence is another piece of evidence that suggests that SARS-CoV-2 could have passed to people through an intermediate species. Pangolins were among the first animals suspected of being the intermediate. Two teams in China reported that they’d found similarities between SARS-CoV-2 and coronaviruses isolated from tissue of Malayan pangolins (Manis javanica) that had been confiscated. Trading pangolins is illegal in China. Pangolins were one of the first animals suspected of being an intermediate host of the coronavirus.Credit: Suzi Eszterhas/Wild Wonders of China/Nature Picture Library The pangolin coronaviruses turned out to be too distant to be direct ancestors of SARS-CoV-2, but the fact that they are the only wild mammals besides bats known so far to be living with coronaviruses similar to SARS-CoV-2 suggests they can’t be ruled out as an intermediate source. Scientists are looking for similar coronaviruses in other animals, too. The ancestor of SARS-CoV-2 could be lurking in tissue samples that are stored in a lab, says Aaron Irving, an infectious-diseases researcher at Duke-NUS Medical School in Singapore. “Many labs have samples sitting in their freezers,” he says. Irving plans to collaborate with researchers at the Chinese Academy of Sciences (CAS) Xishuangbanna Tropical Botanical Garden in Yunnan to test tissue samples from wild mammals collected by wildlife surveillance programmes for coronaviruses that might be closely related to SARS-CoV-2. He is also about to start a new lab at the Zhejiang University-University of Edinburgh Institute in Haining, and plans to look for coronaviruses in bats, tree shrews, civets and other mammals, where permitted. But in February, China introduced a ban on wildlife farms and many are struggling to keep their civets alive, he says. “It may be too late when I get on the ground,” says Irving. Examining the SARS-CoV-2 genome could also reveal clues about possible intermediate hosts. Over time, viruses often start encoding their proteins using similar patterns of nucleotides to their host’s, which helps the virus adapt to their new environment. Researchers at UCL are using machine learning to tease apart patterns in the genetic code of SARS-CoV-2 that could predict which animals it might have adapted to.   But other researchers urge caution about this approach. In the early days of the pandemic, scientists at Peking University Health Science Center noted similarities between the protein-coding patterns of SARS-CoV-2 with those preferred by two snake species. The theory that a snake could be an intermediate host was quickly refuted by other researchers who said that the small sample size and limited data meant that the observed patterns were probably down to chance. Growing the virus in animal cells is one way to test whether the pathogen has adapted to a new host. Shi Yi, a microbiologist at the CAS Institute of Microbiology in Beijing, plans to introduce an inactivated version of RATG13, in various animals, such as bats, cats monkeys and pigs, and see whether the virus develops a similar pattern of mutations to SARS-CoV-2 over time. If similarities emerge, that could reveal which animals the virus adapted to before it jumped to people. Determining which animals SARS-CoV-2 can infect is another way to narrow down the possible intermediate sources. “Knowledge on the susceptibility of different species and potential routes of transmission between animals could give us clues about the likely candidate host or intermediate host in China,” says Bart Haagmans, a virologist at Erasmus MC in Rotterdam. Research so far suggests many species can be infected. In lab experiments, cats, fruit bats (Rousettus aegyptiacus), ferrets, rhesus macaques and hamsters have been shown to be susceptible to SARS-CoV-2. Outside the lab, animals including pet cats and dogs, tigers and lions at zoos, and farmed mink have also caught the virus — probably from people. Researchers are also using computational models and cell biology to investigate animal susceptibility. SARS-CoV-2 typically enters cells through a receptor protein called ACE2. One unreviewed study,1 led by Christine Orengo, a bioinformatician at UCL, modelled the structure of ACE2 from more than 215 vertebrates and found that the receptor in many mammals, including sheep, chimpanzees and gorillas, engages well with the spike protein on the surface of the virus, which suggests that these animals might be susceptible to infection. But modelling does not always correlate with the experimental evidence. For example, Orengo’s modelling suggests that horseshoe bats have a low risk of infection despite lab evidence that they can be infected. Another group, led by Yuen Kwok-yung, a microbiologist at the University of Hong Kong, has found2 that the virus replicates well in tiny organoids grown from intestinal stem cells of Chinese horseshoe bats (R. sinicus). It’s useful to know which animals are susceptible, to manage the risk that they might become virus reservoirs and possible sources of infection in people, says Michelle Baker, a comparative immunologist at the Commonwealth Scientific and Industrial Research Organisation in Geelong, Australia. But when trying to narrow down the culprit, it seems sensible to focus on those animals in close contact with bats, she says. Animals at wildlife farms in China are one of the first places to look, says Peter Daszak, president of the non-profit EcoHealth Alliance in New York City. Farms stock many captive-bred animals, from civets to raccoon dogs and coypu, a large rodent, often living close to livestock such as pigs, chickens and ducks. “These farms are usually wide open to bats, which feed at night above the pens, and some of which roost in the buildings. They are also usually linked to people’s houses so that whole families are potentially exposed,” says Daszak, who has visited many villages, wildlife markets, bat caves and farms in southern China over the past 15 years. “The opportunities for these viruses to spill over across a very active wildlife–livestock–human interface is clear and obvious,” he says. Lam, S. D. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.05.01.072371 (2020). Zhou, J. et al. Nature Med. https://doi.org/10.1038/s41591-020-0912-6 (2020). Liu, Y. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.04.22.046565 (2020). Download references Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 Machine learning News 21 MAY 20 World View 14 MAY 20 News 02 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '124'>18 May 2020</date>
<url id = '125'>https://nature.com/articles/d41586-020-01470-x</url>
<title id = '125'>US scientists say that better data, testing and hospital preparedness are key to erasing inequalities — and to defeating the pandemic overall.</title>
<body id = '125'>A woman gets her temperature taken at a mobile testing site in Compton, a city south of Los Angeles, California.Credit: Etienne Laurent/EPA-EFE/Shutterstock As figures emerge about the disproportionate toll that COVID-19 is taking on people of colour in the United States, scientists are suggesting measures to help mitigate the inequalities. They say that better data are needed on the incidence of the disease, that testing needs to be ramped up and that hospitals serving people at-risk need to better prepare. Researchers and some US lawmakers are now calling for a national commission devoted to identifying racial disparities in health that would act as a unified voice in trying to overcome them.   Researchers note that this will be important to stopping the disease’s overall spread. “It is a major health-disparities issue, but it's also a major health issue for all,” says Cato Laurencin, an orthopaedic surgeon and biomedical engineer at the University of Connecticut in Farmington, who led a roundtable discussion on diversity at the National Academies of Science, Engineering and Medicine. This is not solely a US problem — an analysis of UK national health records published in May1 showed that black residents and those of Asian descent were at a higher risk of dying from the virus than were white people — but the disparities are harshly felt in the United States, which currently has the highest number of COVID-19 infections and deaths. The US Centers for Disease Control and Prevention (CDC) started releasing death and infection rates broken down by race and ethnicity in late April, only after a public outcry from lawmakers, doctors and civil-rights groups. The breakdowns were available for just 35% of US deaths. But as these and other data have started to come in, they paint a stark picture of disproportionate disease burden.   As of early May, New York City reported more than twice as many deaths per 100,000 residents for African and Latino Americans than for white people2. The city’s highest rates of hospitalization and death occurred in the Bronx, the borough with the highest proportion of African American residents3. In Michigan, black people account for 32% of COVID-19 cases and 41% of deaths. They make up just 14% of the population. Many of the causes for these health disparities are systemic and well known. “We’re getting infected more because we are exposed more and less protected,” says Camara Phyllis Jones, an epidemiologist at the Rollins School of Public Health at Emory University in Atlanta, Georgia. Existing socio-economic and health disparities — caused by historical segregation and endemic racism in the United States — can at least partially explain why people of colour are getting sick and dying at disproportionate rates. People wait in line for coronavirus testing in the Bronx, one of the most affected areas in New York City.Credit: Alba Vigaray/EPA-EFE/Shutterstock In many parts of the United States, people of colour make up a higher proportion of some low-paid professions that have elevated risks of exposure to the virus — those who staff grocery stores, drive buses and work at food plants, for example. Also, COVID-19 is deadlier for people with chronic conditions, including diabetes, obesity and cardiovascular disease. These have a higher incidence in many minority ethnic and racial groups.   Some say a delayed and flawed federal public-health response is at fault. “That’s because our nation has abdicated its responsibility to do that kind of work, and ask those kinds of questions,” says Jones. At the earliest stages of the epidemic, aspects of the US response might have made things worse, says Enrique Neblett, a psychologist who studies race and health at the University of Michigan in Ann Arbor. With limited tests available, US authorities initially reserved them for people with symptoms who had a recent history of overseas travel. This could have excluded people from disadvantaged socio-economic backgrounds, including people of colour, says Neblett. “By having that as a criterion they’re automatically less likely to be tested.” Looking ahead, adapting the approaches to getting tests to the people and communities that are most at risk should be a priority, he says.   For example, in Louisiana — one of the first states to report data by race and ethnic group — testing teams went to poorer neighbourhoods to reach people without cars, who would have trouble making it to drive-through testing sites. Similarly, in May, New York announced a programme designed specifically to reach communities of colour. Outreach must extend beyond testing to all aspects of the response, says Evelynn Hammonds, a historian of medicine at Harvard University in Cambridge, Massachusetts. Clinical trials, for example, must actively work to recruit a diverse population, otherwise the treatments and vaccines might not be equally effective. “We already know there’s a real problem with making sure that populations that are enlisted into clinical trials need to be diverse,” Hammonds says. Communities of colour have historically been mistrustful of health systems, in part because they have been underserved or exploited. So, researchers must build relationships with these communities so that trials for COVID-19 treatments and vaccinations include people from all ethnic and racial groups, Hammonds says.   And hospitals in neighbourhoods that are likely to see a surge in cases because they serve more severely affected groups should be equipped sufficiently and in advance with protective equipment and ventilators, says Jones, who was president of the American Public Health Association in 2016. She calls for a plan to direct comparatively more resources into neighbourhoods where there are known to be higher incidences of chronic conditions. “If we know that these neighbourhoods are the ones being adversely impacted, then we need to move the ventilators and the health staff there,” Jones says. Health agencies and legislators around the world are beginning to grapple with the challenge. As of early May, most state departments and counties in the United States had begun to report race and ethnicity alongside infection and death rates. The UK government is putting out similar calls for data transparency, and it announced in mid-April that it would launch an inquiry into the reasons why the disparities exist.   Billions of federal dollars in the United States have been deployed towards tackling the pandemic, but because emergencies such as this one require a coordinated response across national agencies, some have argued for a central commission that will represent the needs of minority racial and ethnic groups. “The call is for a national coherent voice that starts to talk about these issues,” says Laurencin, who suggested such a commission in an editorial last month4. Hammonds says that such a group could be effective because local US leaders typically prescribe public-health guidance and decisions, and so far have differed in their response to the pandemic. She compared New York’s rapid yet measured response with that in Georgia, whose governor announced a gradual re-opening of the economy in mid-April — despite contrary advice from public-health experts. This pandemic could be an opportunity to deploy consistent attention towards the needs of underserved communities, she says. “If there is a positive to come out of this, that would be one of them.” The OpenSAFELY Collaborative et al. Preprint at medRxiv https://doi.org/10.1101/2020.05.06.20092999 (2020). Hooper, M. W. et al. J. Am. Med. Assoc. https://doi.org/10.1001/jama.2020.8598 (2020). Wadhera, R. K. et al. J. Am. Med. Assoc. https://doi.org/10.1001/jama.2020.7197 (2020). Laurencin, C. T. & McClinton, A. J. Racial Ethn. Health Disp. 7, 398–402 (2020). Download references Latest on: Epidemiology News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 Society Obituary 21 MAY 20 World View 19 MAY 20 Editorial 19 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '125'>18 May 2020</date>
<url id = '126'>https://nature.com/articles/d41586-020-01136-8</url>
<title id = '126'>Data and infographic updates on the COVID-19 pandemic.</title>
<body id = '126'>15 May — Online meetings find favour with scientists  More than 80% of respondents to a Nature reader poll said that they would be in favour of some scientific conferences remaining virtual even after the coronavirus pandemic ends. Many meetings have been pushed online since March as a result of the global COVID-19 outbreak — including large, flagship conferences that usually attract thousands of attendees. More than 40% of the roughly 500 survey respondents said that they had attended an online meeting. Readers lauded some aspects of virtual meetings — in particular improved accessibility, low costs and avoiding the hassle of travel. “I know colleagues around the world with limited budgets who’ve also suddenly been able to attend many more meetings,” says Tom Brown, who studies energy-system modelling at Karlsruhe Institute of Technology in Germany. Some respondents found presentations to be clearer, liked that they could rewatch recorded talks, and felt that it is easier to speak up as an audience member using digital tools than it is in a meeting hall. Downsides included clunky technology, connection issues and most notably, the lack of serendipitous encounters, human interaction and socializing. But many respondents thought virtual conferences would improve in time and that the digital experience could help to make in-person meetings better, for instance through improving technologies that allow attendees to exchange data, knowledge and opinions during presentations. Some advocated a hybrid model — a face-to-face meeting with increased virtual elements. “Some of the best parts of a conference, such as informal meetings, local flavour, and more time to talk to speakers after sessions were completely lost. A hybrid system might recover some of these benefits,” says Paul DeStefano, a physicist at Portland State University in Oregon. 11 May — Rapid peer review and blocking bad science Preprint servers — where researchers post manuscripts before peer review — have been flooded with studies since the coronavirus pandemic began, in a bid to rapidly share information that could lead to vaccines and treatments and shape policies. But these repositories have had to beef up their screening processes to weed out poorly conducted studies that could be harmful, or fuel conspiracy theories.  At the same time, journals are scrambling to rapidly review and publish studies. One analysis of titles found that the average time to publication had dropped from 117 to 60 days. Read more about science publishing during the pandemic here.  5 May — Paltry historical funding for coronavirus-related research Before the COVID-19 pandemic, funding for research related to coronaviruses constituted just 0.5% of global spending on infectious-disease studies by public and philanthropic organizations. From 2000 to the start of this year, these organizations spent about US$550 million on coronavirus work, according to an analysis by researchers at the University of Southampton, UK. By comparison, Ebola-related research received $1.2 billion (1.1% of global spending). Spending has risen to $985 million since the current outbreak began (see ‘Coronavirus cash’). About $275 million of COVID-19 research funding is focused on vaccine development, $40 million on therapeutics and $18 million on diagnostic tests. The researchers note that the spending has generally been reactive — explaining spikes in 2004 and 2015, after outbreaks of the coronaviruses that cause severe acute respiratory syndrome and Middle East respiratory syndrome, respectively. Source: Research Investments in Global Health study (RESIN), University of Southampton 5 May — How the coronavirus breaks into human cells Researchers are scrambling to uncover as much as possible about the biology of the latest coronavirus, named SARS-CoV-2 — and a profile of the killer is emerging. Scientists are learning that the virus has evolved an array of adaptations that make it much more lethal than the other coronaviruses humanity has met so far. Unlike its close relatives, SARS-CoV-2 can readily attack human cells at multiple points, with the lungs and the throat being the main targets (see ‘Deadly invader’). Read more about the complex biology of this killer virus here.  5 May — Which country had the strictest coronavirus response? Nations have responded in vastly different ways to the coronavirus pandemic, and researchers are now sifting through data to work out which strategies — from wearing face masks to enforcing lockdowns — worked best. Scientists with the Oxford Coronavirus Government Response Tracker project have developed a ‘stringency index’ that scores a nation’s strategy on the basis of how strict it is, and allows approaches to be compared directly (see ‘Pandemic protections’). Read more about efforts to track the most effective strategies against COVID-19 here. Source: Oxford Coronavirus Government Response Tracker (data); Nature (charts). Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '126'>18 May 2020</date>
<url id = '127'>https://nature.com/articles/d41586-020-01456-9</url>
<title id = '127'>First large-scale ancient genome analyses from China chart migrations of early farmers.</title>
<body id = '127'>An 8,400-year-old skeleton from the Qihe cave archaeological site in Fujian, China.Credit: Xiujie Wu/Institute of Vertebrate Paleontology and Paleoanthropology Ancient genomics is starting to unravel the history of East Asia. The first large-scale studies of ancient human genomes from the region suggest that many of its inhabitants descend from two once-distinct populations that began mixing after the development of agriculture some 10,000 years ago. The studies also reveal connections between ancient humans that extend from southern China to the South Pacific, and a link between coastal inhabitants that could offer clues about how humans came to settle in East Asia. The findings come from a 14 May study in Science that analysed the genomes of two dozen ancient Chinese people1, and a March preprint that looked at nearly 200 ancient genomes from across East Asia2.   Contemporary East Asians descend, by and large, from humans who left Africa 50,000–100,000 years ago. But researchers know little about the ancient population shifts that have since shaped the genomes of the region’s 1.7 billion current inhabitants. Only a handful of ancient-human genomes from East Asia have been published, and it is not clear how key events, such as the spread of farming — which drastically altered the genetic make-up of western Eurasians — affected this part of the world. A team led by Qiaomei Fu, a population geneticist at the Institute of Vertebrate Paleontology and Paleoanthropology in Beijing, analysed the genomes of 24 individuals who lived in East Asia, including what is now China, between 9,500 and 300 years ago. Most of the genomes came from archaeological sites in the Yellow River Basin in northeastern China, or more than 1,000 kilometres away, in southeast China’s Fujian province. During the early Neolithic period, around 10,000 to 6,000 years ago, people from these two geographic regions were genetically distinct, Fu’s team found. But over time, they began mixing: contemporary Chinese trace much of their ancestry to the northern groups, but are also related to the ancient Fujian people to varying degrees (those from southern China tend to be closest). Fu’s team doesn’t know exactly when the two groups started to interbreed, but it saw signs that the northern genetic signature had begun spreading into southeast China by the time of the late Neolithic 5,000–4,000 years ago. This suggests that farming in East Asia could have spread through mixing of farmers and hunter-gatherers, says Ling Qin, an archaeologist at Peking University in Beijing. That’s different from what ancient-genome studies have found in western Eurasia, where farmers with Middle Eastern ancestry largely replaced hunter-gatherers in Europe3. Farmers in the Yellow River Basin also moved west. A team led by David Reich, a population geneticist at Harvard Medical School in Boston, Massachusetts, who is a co-author of Fu’s study, analysed the genomes of 20 5,000-year-old individuals from this region and found connections with contemporary Tibetans. Their results are part of a study of 191 ancient individuals from East Asia that was posted to the bioRxiv preprint server on 25 March. The studies also revealed some surprising long-distance connections. Neolithic people who lived near China’s coast, whether in the northeast or southeast, shared some ancestry with ancient individuals from coastal sites in southeast Asia and Japan. “That means the entire coast of East Asia is a really important place for people to migrate,” says Fu. Reich and his team found a similar connection, which they say could be evidence that modern humans first settled in East Asia along a coastal route.   Ancestry from southeastern China stretched even farther afield. Neolithic individuals from Fujian and islands in the Taiwan Strait were closely related to ancient islanders from Vanuatu in remote Oceania, Fu’s team found. Previous ancient-genome studies had recorded the spread of this ancestry from East Asia to Oceania4, and Fu’s study suggests that this group originated in southern China. This conclusion makes sense, says Matthew Spriggs, an archaeologist at Australian National University in Canberra. Archaeological and genetic evidence has linked the South Pacific migration to ancient humans in Taiwan, whose Neolithic inhabitants probably came from southern parts of the mainland. “I am very pleased to see this paper,” he says of Fu’s study. Qin says that the ancient southern Chinese people in Fu’s study lived in an isolated pocket that might not be representative of the wider area. A priority should be sequencing DNA from early farmers from the Yangtze River Basin in southern China, a centre for rice domestication and a potential source of other migrations, she adds. The studies give researchers hope that ancient genomics will help them delve even deeper into East Asia’s early history. Pontus Skoglund, a population geneticist at the Francis Crick Institute in London, would love to know whether the first Homo sapiens to settle in the region interbred with Denisovans, an extinct group of hominins. “I think one of the most exciting open big questions of the region relates to the pre-Neolithic settlement — who were the earliest modern humans in the region?” adds Martin Sikora, a geneticist at the University of Copenhagen. M. A. Yang et al. Science https://doi.org/10.1126/science.aba0909 (2020). Wang, C.-C. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.03.25.004606 (2020). Malmström, H. et al. Curr. Biol. 19, 1758–1762 (2009). Skoglund, P. et al. Nature 538, 510–513 (2016). Download references Latest on: Evolution Article 20 MAY 20 News & Views 20 MAY 20 News 19 MAY 20 Genomics Research Highlight 07 MAY 20 Article 06 MAY 20 Article 06 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '127'>14 May 2020</date>
<url id = '128'>https://nature.com/articles/d41586-020-01430-5</url>
<title id = '128'>But there’s no evidence that dogs can pass the virus to people.</title>
<body id = '128'>A Pomeranian in Hong Kong was one of the first dogs to test positive for the coronavirus.Credit: Xinhua News Agency/Shutterstock The first two dogs reported to have coronavirus probably caught the infection from their owners, say researchers who studied the animals and members of the infected households in Hong Kong. An analysis of viral genetic sequences from the dogs showed them to be identical to those in the infected people. Researchers suspected that the infection had been passed from the owners to the dogs, and the direct genomic link strongly supports that, says Malik Peiris, a virologist at the University of Hong Kong who led the study, which is published today in Nature1. The study showed no evidence that dogs can pass the infection to other dogs or people, but it is impossible to be certain in which direction the virus traveled “so we have to keep an open mind”, says Peiris. Although the analysis confirms that people with COVID-19 can infect dogs, the probability of this happening is low, says Arjan Stegeman, a veterinary epidemiologist at Utrecht University in the Netherlands. In the study only 2 of the 15 dogs who lived with infected people caught the disease. But other scientists say the possibility that pets might spread the virus between each other, and to people, needs to be properly investigated as part of managing future outbreaks. Since the infections in the two canines in Hong Kong — a Pomeranian and a German shepherd — were reported, other pets have tested positive for the SARS-CoV-2 virus, including a cat in Hong Kong and another two in New York state. Four tigers and three lions at New York City’s Bronx Zoo also tested positive. Studies in cats have found that they can pass the virus to other felines without showing symptoms.   The Hong Kong study detected viral RNA and antibodies in both dogs, and live virus in one of them. Neither dog became noticeably sick. The findings support the results of an April study, in which researchers in China deliberately infected dogs with SARS-CoV-2, says Thomas Mettenleiter, a virologist who heads the Federal Research Institute for Animal Health in Riems, Germany. Dog owners who test positive for the coronavirus should be cautious when handling their pets, he says. The American Veterinary Medical Association recommends that people who have COVID-19 wear a mask when caring for their pets. It says sick people should also avoid petting, hugging or sharing food with animals, and should wash their hands before and after contact with them. Beyond protecting pets from the virus, there is an urgent need to test more animals that are in close contact with people, including working animals and livestock, to understand whether they have a role in spreading the virus, says Jürgen Richt, a veterinary virologist at Kansas State University in Manhattan. To do this, specialist diagnostic kits for testing animals will be required, he says. Richt would also like to see research into whether pets become sick or experience particular symptoms. A common COVID-19 symptom in people is loss of smell. If dogs experience similar symptoms, Richt says this might affect working detector dogs that sniff out drugs, explosives and other illicit items.   Stegeman plans to test cats living with people who have had COVID-19. Understanding the role of domestic and stray cats in the chain of transmission is increasingly important as infection rates between people fall, he says. Although the Hong Kong study found no evidence of dogs infecting people, Peiris says, their possible role — and that of cats — should be considered in attempts to understand how the virus passed to people in the first instance. Researchers think that SARS-CoV-2 probably originated in bats and passed to people through an intermediate animal, which remains unknown. A leading theory suggests that the intermediate species came into contact with people in a wild-animal market in Wuhan, China. Dogs, cats and other mammals are sold for meat in such markets, and stray cats and dogs roam around them freely, says Peiris. “The virus seems to have a fairly broad host range. Dogs, cats and other closely related mammalian species could be susceptible and form a bridge between bats and humans,” he says. But researchers also worry that reports of infected cats and dogs might lead people to abandon their pets. There were news reports of people deserting their pets during the outbreak in Wuhan, where the outbreak originated. “The danger we are facing is that people get nervous when they hear that companion animals could be virus carriers, and decide to get rid of them,” says Richt. Sit, T. H. C. et al. Nature https://doi.org/10.1038/s41586-020-2334-5 (2020). Download references Latest on: Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '128'>14 May 2020</date>
<url id = '129'>https://nature.com/articles/d41586-020-01367-9</url>
<title id = '129'>Drug manufacturers face supply-chain weaknesses and sourcing issues as they ramp up complex production processes to meet global demand.</title>
<body id = '129'>Production of remdesivir, an antiviral drug approved to treat COVID-19, is ramping up.Credit: Ulrich Perrey/POOL/AFP/Getty The world was waiting for any sign of hope in countering the COVID-19 pandemic when researchers released the first encouraging drips of data from a large clinical trial of the antiviral remdesivir last month. The drug, they said, reduced the time to recovery from COVID-19 by a few days — not enough to be branded a ‘cure’, but hopefully enough to relieve some pressure on overwhelmed health-care systems. The discovery of remdesivir’s potential focused attention on the next problem facing the development of COVID-19 therapeutics: ramping up complex manufacturing processes to address a global pandemic. It is likely to be one of the biggest drug-making challenges the world has ever faced. Some of the therapies being tested against COVID-19 are novel and difficult to produce. Others — even if they are relatively simple compounds that have been in use for decades — face complications such as supply-chain weaknesses as drug-makers try to scale up production.   “A major rate-limiting step is going to be manufacturing,” says Ezekiel Emanuel, a bioethicist at the University of Pennsylvania in Philadelphia. “Getting up to hundreds of millions of doses is hard.” Researchers are working furiously to test a wide variety of potential COVID-19 treatments. Those therapies span the gamut of complexity, from familiar generic medications such as the malaria drug hydroxychloroquine, to experimental small molecules like remdesivir, which was previously trialled against the Ebola virus. Scientists are also exploring antibody treatments that tamp down the body’s immune response when it becomes destructive, which happens in some critically ill coronavirus patients. And if the history of infectious disease is any guide, it will take a combination of drugs — each with a distinct, even if relatively minor, impact on the disease — to tame the novel coronavirus. Each treatment will face different challenges when scaling up production, says Stephen Chick, who studies health-care management at INSEAD in Fontainebleau, France. “If it’s successful and the technology is then adopted, you need to be prepared to deliver,” says Chick. “And if you’re not, you’re in trouble.” Remdesivir’s maker, Gilead Sciences in Foster City, California, has been working for months to scale up production of the compound, even before the latest data release. After the US Food and Drug Administration (FDA) authorized use of the drug for COVID-19 under emergency rules on 1 May, the company announced that it had reached out to drug manufacturers around the world to find ways of boosting production. By then, Gilead had already been streamlining its manufacturing process — reducing the time to produce large batches of the drug from 9–12 months to 6–8 months — and searching for alternative sources for the rare chemicals needed to make it. (Gilead said that it could not disclose the raw materials that go into remdesivir.) The company has projected that it could make enough remdesivir to treat one million people by the end of the year, and potentially twice as many if it finds that lower doses of the drug are sufficient to reduce recovery time from COVID-19.   But it also warned that production of remdesivir relies on a complex chemical synthesis — with individual steps that can take weeks to perform — and could be derailed by shortages of key ingredients. Remdesivir is a molecule that is similar to the nucleotide building blocks the virus uses to copy its RNA genome. By imitating those building blocks, remdesivir blocks the enzyme that the coronavirus uses to replicate itself. Gilead faces a particular challenge because it was not making large amounts of the drug when the pandemic started. But even for pharmaceuticals that are already produced in bulk — such as hydroxychloroquine and chloroquine — scaling up presents a significant problem, says David Simchi-Levi, an operations researcher at the Massachusetts Institute of Technology in Cambridge. Over the past two decades, manufacturers in many different industries have been shifting to a ‘lean’ manufacturing model that reduces the amount of raw materials and finished product they keep in stock. “This was successful in terms of reducing costs,” Simchi-Levi says. “But it increased exposure to risk.” A 20,000-litre bioreactor vat at the Swiss chemicals firm Lonza in which hamster cells are grown to make protein or antibody drugs.Credit: Lonza Ltd In addition, companies have been seeking low-cost suppliers of raw materials in countries such as China and India. When a crisis such as a pandemic strikes, those countries may clamp down on exports of pharmaceutical ingredients to ensure availability to their own people. Simchi-Levi and his colleagues’ research in the automotive industry shows that the riskiest links in the supply chain were providers of low-cost but crucial components that cost as little as US$0.10. The same could be true of the other industries including pharmaceuticals, he says, where there are already concerns about having enough glass vials to produce and distribute a vaccine, once one becomes available. “If supply of these components is disrupted you have to stop the production line,” Simchi-Levi says. “And many companies don’t have a good enough understanding of their own supply chains to know who are the suppliers of their suppliers.” For small-molecule drugs such as remdesivir or hydroxychloroquine, production broadly involves three stages. The first yields the active ingredient in the drug; the second modifies the drug to make it stable and readily absorbed by the body; and the third packages the drugs, for example into tablets or vials. All this takes place under the watchful eye of regulators, who periodically inspect facilities to ensure that quality and safety standards are maintained. Relatively few sites are approved by regulators to make drugs, meaning that when one site fails an inspection — or when more facilities are needed to crank out higher volumes of a particular drug — it can be difficult to find a replacement. “That can be pretty significant,” says Simchi-Levi. “There’s high dependency on only a few sites for manufacturing.”   Production can be even more delicate for more complex therapies, such as proteins or antibodies. Researchers are hopeful that antibodies that block certain immune-system processes will help against COVID-19, by restraining the out-of-control immune responses. Genentech in South San Francisco, California, makes one such antibody, called tocilizumab (Actemra), which blocks the activity of an immune-system regulator called IL-6. Tocilizumab is already approved for use against some forms of arthritis, but if found to be useful against COVID-19, production would need to be vastly scaled up. Antibody treatments such as tocilizumab are made in cells grown in culture, most often in Chinese hamster ovary cells . Antibodies are increasingly used to treat a range of diseases, from various forms of cancer to arthritis, and research has boosted production yields. About ten years ago, a manufacturer might expect to get less than 1 gram of antibody per litre of cell culture; now they typically extract 5 grams or more from the same volume, says Charles Christy, head of commercial solutions at the chemicals firm Lonza in Visp, Switzerland. A 2,000-litre culture might produce enough antibody to fuel an early clinical trial, but drug-makers can scale up to as much as 20,000 litres of culture grown in giant steel vats to handle larger trials and commercialization. Because antibody drugs are now such a large part of the pharmaceutical industry, there tend to be multiple suppliers for key reagents, Christy says. But you can always be blindsided, he says. “We and others are looking very hard at our supply chain.”   The industry found that out the hard way following the 2011 earthquake near Fukushima, Japan. Only a few factories in the world made the chemical polyethylene glycol for medicines, which can lengthen the time that some protein therapies remain stable in the blood. All were in Japan. Tocilizumab has not yet been shown to help patients against COVID-19, but Genentech says that it has already increased its supply by 50% and is working to further raise capacity. But even when companies work proactively to build supply, demand will almost certainly outstrip initial supplies of any compound found to be effective against COVID-19. That raises the spectre of determining who will be first to receive the treatments. Complaints have already surfaced about the allotment of remdesivir: Gilead donated its stocks of the drug to treat COVID-19, with about 40% — enough to treat 78,000 people — going to the United States. The US government has been distributing those vials to individual states, but some hospitals have complained about lack of access. The company also announced this week that it had entered into agreements with five makers of generic drugs. Those manufacturers may produce remdesivir for distribution in 127 countries that have limited access to healthcare, without paying royalties to Gilead. The agreement will remain until the global health emergency ends, or another treatment or vaccine is found for COVID-19. Concerns about access to pandemic medicines have arisen before, for example during the H1N1 outbreak in 2009, says Emanuel, when countries raced to stockpile the influenza drug Tamiflu. “It was a free-for-all,” he says.Those issues have never been fully addressed because the outbreak ended quickly. “People move on and no one stays around long enough to solve the problem,” Emmanuel says. “That will not happen here. We will be in this problem for a number of years.” </body>
<date id = '129'>14 May 2020</date>
<url id = '130'>https://nature.com/articles/d41586-020-01410-9</url>
<title id = '130'>Industry group aims to agree shared standards for software able to detect issues during peer review.</title>
<body id = '130'>Microscope images that have been altered or duplicated can appear in research papers.Credit: Universal Images Group via Getty The world’s largest science publishers are teaming up to discuss how to automatically flag altered or duplicated images in research papers. A new working group — the first formal cross-industry initiative to discuss the issue — aims to set standards for software that screens papers for problematic images during peer review. “The ultimate goal is to have an environment that helps us, in an automated way, to identify image alterations,” says IJsbrand Jan Aalbersberg, who is head of research integrity at the Dutch publishing giant Elsevier. Aalbersberg is chairing the cross-publisher working group, set up by the standards and technology committee of the STM, a global trade association for publishers, based in Oxford, UK. The group began meeting in April, and includes representatives from publishers including Elsevier, Wiley, Springer Nature and Taylor & Francis.   Journal editors have long been concerned about how best to spot altered and duplicated images in papers, which can be the result of honest mistakes, efforts to improve the appearance of images (such as altering contrast or colour balance), or fraud. In 2016, a manual analysis of more than 20,000 biomedical papers led by microbiologist Elisabeth Bik, now a consultant image-analyst in Sunnyvale, California, suggested that as many as 4% of them might contain problematic image duplications. So far, however, most journals haven’t employed image-checkers to screen manuscripts, saying that it is too expensive or time-consuming; and software that can screen papers on a large scale hasn't been available. The new group aims to lay out the minimal requirements for software that spots problems with images, and how publishers could use the technology across hundreds of thousands — or even millions — of papers. It also wants to classify the “types and severity of image-related issues” and to “propose guidelines on what types of image alteration is allowable under what conditions”, according to the STM’s description of the group. The latter phrase refers to what different disciplines find acceptable for the presentation of images, and how modifications — if permitted — should be transparently declared by authors, Aalbersberg says. He adds that, although many publishers are individually trialling software already, cross-industry discussions on common standards could take at least a year. In the past few years, some publishers have been trialling image-checking software. The firms LPIXEL in Tokyo and Proofig in Rehovot, Israel, both say that publishers or institutes can upload a research paper to their desktop or cloud software and, in 1–2 minutes, automatically have its images extracted and analysed for duplications or manipulations, including instances in which parts of images have been rotated, flipped, stretched or filtered. Neither firm would publicly name clients, but both said they had paying customers among science publishers and research institutes. Another firm that checks images using software is Resis, in Samone, Italy. And an academic group led by Daniel Acuna at Syracuse University in New York is also developing software to compare images across multiple papers, which he says is being trialled by institutions and a publisher.   Large publishers need software that can deal with a high throughput of papers, can plug directly into their peer-review processes, and, ideally, can compare a large number of images across many papers at once — a much more computationally intensive task than checking one paper. “Current technologies are not yet able to do this type of thing at a large scale,” Aalbersberg says. “Everybody realizes this is important. We are almost there but we are not there yet.” Catriona Fennell, who heads publishing services at Elsevier and is in the new working group, says Elsevier has been growing more concerned by indications of what she calls “industrialized cheating” in a small minority of papers — disturbing similarities between images and text in multiple papers from different groups that are potentially being churned out to order. This year, Bik and other image detectives flagged more than 400 papers, across a variety of journals and publishers, which they say contain so many similarities that they could have come from a single ‘paper mill’, a company that produces papers on demand.   Similarities between papers are difficult to flag during peer review, Fennell notes, not only because most peer reviewers aren’t looking for these kinds of problems, but also because many papers can be under review simultaneously — and confidentially — at different journals. Ultimately, publishers would need a shared database of images to check for re-use between papers. In 2010, publishers agreed to deposit the text of research papers into a central service called CrossCheck, so that journals could use software to check submitted manuscripts for plagiarism. “We are going to need the same collaboration for images,” says Fennell. Once the software is ready, “I’m convinced that is going to happen,” Aalbersberg says. Bik, who continues to find image problems in published papers, says software that could spot image problems in manuscripts during peer review would be a “wonderful” development. “Hopefully I will have less work to do,” she says. Latest on: Peer review News Feature 13 MAY 20 World View 11 MAY 20 News 03 APR 20 Publishing News 20 MAY 20 Career Column 19 MAY 20 Career Column 13 MAY 20 Research data News & Views 20 MAY 20 Career Column 19 MAY 20 Nature Index 29 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '130'>13 May 2020</date>
<url id = '131'>https://nature.com/articles/d41586-020-01423-4</url>
<title id = '131'>Studies of social networks show that opposition to vaccines is small but far-reaching — and growing.</title>
<body id = '131'>Anti-vaccine placards have appeared at US protests during the pandemic.Credit: Rich Pedroncelli/AP/Shutterstock As scientists work to create a vaccine against COVID-19, a small but fervent anti-vaccination movement is marshalling against it. Campaigners are seeding outlandish narratives: they falsely say that coronavirus vaccines will be used to implant microchips into people, for instance, and falsely claim that a woman who took part in a UK vaccine trial died. In April, some carried placards with anti-vaccine slogans at rallies in California to protest against the lockdown. Last week, a now-deleted YouTube video promoting wild conspiracy theories about the pandemic and asserting (without evidence) that vaccines would “kill millions” received more than 8 million views. It’s not known how many people would actually refuse a COVID-19 vaccine — and general support for vaccines remains high. But some researchers studying vaccine-opposition movements say they’re concerned that the messages could undermine efforts to establish herd immunity to the new coronavirus. Online opposition to vaccines has rapidly pivoted to talk of the pandemic, says Neil Johnson, a physicist at George Washington University in Washington DC, who is studying the campaigners’ tactics. “For a lot of these groups, it’s all about COVID now,” he says. Groups opposing vaccines are small in size, but their online-communications strategy is worryingly effective and far-reaching, a report from Johnson’s team suggests. Before the SARS-CoV-2 virus emerged, Johnson’s team began mapping out a network of views on vaccination, on Facebook. They investigated more than 1,300 pages, followed by about 85 million individuals. Their findings1, published on 13 May, suggest that anti-vaccination pages tend to have fewer followers but are more numerous than pro-vaccination ones, and are more often linked to in discussions on other Facebook pages — such as parent associations at schools — whose stance on vaccination is undecided. In contrast, pages that explain the benefits of and the scientific case for vaccination are linked in a network that is largely disconnected from this “main battlefield” for public sentiment, as Johnson puts it. During measles outbreaks in 2019, anti-vaccination pages grew more links than did pro-vaccination ones on Facebook, Johnson’s team adds. An extrapolation of current trends using computer simulations suggests that opposition to vaccines might dominate the network of views on vaccines within ten years, they write. Source: Ref 1. The work shows that “the pro-vaccine community are basically sticking to their narrative and talking to each other, and not reaching out and being responsive to the narratives that are out there among the undecided,” says Heidi Larson, who directs the Vaccine Confidence Project, a group that monitors public trust in vaccines, at the London School of Hygiene and Tropical Medicine. The issue isn’t confined to Facebook. On 1 April, Johnson’s team released a preprint of a separate study2 on online messaging about COVID-19. That report, which has not yet been peer-reviewed, suggests that links are growing across different social-media platforms between anti-vaccine groups debating COVID-19 and other interest groups, such as far-right extremists. Countering the spread of anti-vaccine sentiment will involve understanding not just the shape of the online map, but of how it got that way, says Bruce Gellin, president of global immunization at the Sabin Vaccine Institute in Washington DC. “We need to understand what it is about the conversations and content [around anti-vaccination] that compels people to listen and share it with others,” he says. Pro-vaccine groups have a simple message — vaccines work and save lives. Anti-vaccine narratives are numerous: from sowing worries about children’s health to advocating alternative medicines and linking immunizations to conspiracy theories. And the anti-vaccination messages are spread across many more Facebook clusters than are those from the larger pro-vaccine groups. Johnson says these features echo those his team has found in earlier studies of insurgency networks in conflict zones, where insurgents could often embed themselves deeply into existing social networks. Anti-vaccine campaigners tend to win converts with personalized, emotive messages, says Larson; these are built not necessarily on fear (“Vaccines will kill you.”), but on appeals to the heart (“Do you love your children?”). The public-health community, meanwhile, has simply been trying to get more people vaccinated, she says — which might lead to a feeling that they are just trying to get their numbers up. “The approach needs to be quite different with people who are undecided,” she says. Vaccine-advocacy organizations are “not listening to concerns and questions”. Overall, most people support vaccines, points out Gellin, and are likely to do so in this pandemic. Still, global vaccination rates have plateaued in the past two decades, Larson says. Both she and Gellin worry that another reason for public suspicion about a COVID-19 vaccine might be the speed of its development. “We should be very clear and transparent about the development process,” says Gellin. “Otherwise, when it shows up, people will ask ‘how can we be sure no shortcuts were taken?’” The messaging around a vaccine will also need to be carefully thought out. If there’s already fewer COVID-19 infections by then, it’s going to be a hard sell, says Larson. “The thing that’s going to change people’s minds is if the government says that if you have the vaccine, you can go to work”, she says. </body>
<date id = '131'>13 May 2020</date>
<url id = '132'>https://nature.com/articles/d41586-020-01285-w</url>
<title id = '132'>The men are reportedly doing well one year on, but there is no way to confirm that the unpublished treatment using ‘reprogrammed’ stem cells works.</title>
<body id = '132'>Heart muscle derived from induced pluripotent stem cells (pictured) is being trialled in people.Credit: Thomas Deerinck, NCMIR/SPL Two men in China were the first people in the world to receive an experimental treatment for heart disease based on ‘reprogrammed’ stem cells, and they have recovered successfully one year later, says the cardiac surgeon who performed the procedures. In May last year, the men were injected with heart muscle cells derived from induced pluripotent stem (iPS) cells, the surgeon told Nature — the first known clinical application of iPS-cell technology for treating damaged hearts. No results have yet been published, so researchers not involved in the work have cautioned that there is no way to confirm whether the treatment works, including whether the reported benefits are due to the iPS-derived cells or simply to the heart bypass that accompanied the treatment. But the surgeon, Wang Dongjin at Nanjing Drum Tower Hospital, spoke to Nature in detail about the procedure and about the patients’ conditions. And one of the men — Han Dayong, a 55-year-old electrician from Yangzhou in eastern China who received the treatment alongside a heart bypass — says he is very satisfied with the outcome. Before the surgery, Dayong remembers being tired and often out of breath. Now he can go for walks, climb stairs and sleep through the night. “It was beyond my expectations,” he says. The team behind the treatment plans to publish the results from the two patients later this year, says Wang Jiaxian, who heads the Nanjing-based biotechnology company HELP Therapeutics that supplied the heart muscle cells, known as cardiomyocytes, used in the study. The group also has approval to expand its study to include a further 20 patients, he says. The trial in China, which is listed on a global clinical trial registry, is not the only one that is ongoing. In January, a cardiac surgeon in Japan, Yoshiki Sawa, introduced iPS-derived cardiomyocytes designed to treat heart disease into a patient — which media reported at the time was a world first. His team is using an alternative approach in which sheets of cells are grafted onto the heart rather than injected into the organ. For decades, researchers have been trying to treat heart disease — a leading cause of death worldwide — using adult stem cells. They hoped that the cells would morph into muscle cells once inserted into the heart. But after trials in people proved inconclusive, researchers turned to iPS cells. These are created by inducing adult cells to revert to an embryonic-like state, from which they can develop into other cell types, such as cardiomyocytes. Work with iPS cells is less ethically fraught than that using embryonic stem cells, which can also differentiate into many other cell types. Evidence in rodents and monkeys suggests that introducing iPS-cell-derived cardiomyocytes directly into the heart does regenerate muscle tissue and improve the organ’s function1,2. Researchers hope that the first trials in people will reveal the same. “These are really exciting times,” says Wolfram-Hubertus Zimmermann, a pharmacologist at the University Medical Centre Göttingen in Germany. He says the first trial in people sounds promising, but because the results have not been published, there is no way to confirm whether the treatment works.   As well as the iPS-cell pilot study under way in Japan, several others are planned in France and the United States. Zimmermann is also planning one in Germany. But researchers are divided over how cardiomyocytes might repair the heart, and whether the best way to introduce them is by direct injection or a graft. “We are going to see a lot of interesting shots on goal in the next five years. We don’t know what the answer is going to be. It is time to let the patients teach us,” says Charles Murry, a pathologist at the University of Washington in Seattle who also plans to inject cells into people’s hearts. That there is a trial ongoing in China came as a surprise to many, who didn’t know that researchers there had overcome one of the field’s biggest challenges — the need to produce large numbers of iPS-cell-derived cardiomyocytes that are pure enough to be used in people. This takes a lot of time and effort to get right, so very few companies or research groups have successfully done it, says Murry. Wang Jiaxian says that his company has been developing the cells for almost four years. Wang Dongjin told Nature that he injected some 100 million cardiomyocytes, derived from iPS cells created using cells donated by a healthy person, around the damaged heart tissue of his two patients. At the same time, both men, who had severe heart disease, underwent a coronary-artery bypass operation, in which vessels from elsewhere in the body are transplanted onto the artery to improve blood flow. Wang says his goal was to assess the safety of the cell injections, and that he was encouraged when his patients’ heart function improved significantly after the operation. Neither recipient has developed tumours, he adds, which can be a risk of using pluripotent stem cells. To prevent the body from attacking the cardiomyocytes, Wang says, both patients took immunosuppressant drugs. One took them for a month; the other had to stop after a week, owing to side effects. Both men were discharged from hospital less than a month after surgery, he says. Wang also says that the procedure did not cause sustained dysfunction in cardiac rhythm. Zimmermann says that is a sign that it’s safe, although it needs to be trialled in more people to be sure. But the apparent safety of the treatment might also be explained by the low dose of cells the team reportedly used, he says. Murry adds that the health benefits the patients have reported cannot be attributed to the reprogrammed cells alone, because the men also received a coronary bypass. “If you do two things to somebody and they get better, you can’t say which one caused it,” he says. Researchers are divided on the best approach to introduce cardiomyocytes into the heart. Injecting them is typically less intrusive than grafting sheets of cells, because it doesn’t require surgery, although the Chinese patients did have surgery for the bypass. Proponents of injections also argue that in animals, the procedure allowed the tissue to better integrate into the heart and produce new muscle3. But Philippe Menasché, a cardiac surgeon at the University of Paris, says that the injections puncture the organ in multiple locations, which might damage the tissue. The approach is also not easily reproduced, because the cells are injected somewhat randomly, and sometimes cluster, which could contribute to irregular beating of the heart, he adds. Wang says his patients experienced short bouts of irregular heartbeat, but it was not sustained.   In January, Sawa, a cardiac surgeon at Osaka University in Japan, performed the first in-person trial of the alternative approach: grafting sheets of 100 million cardiac muscle cells derived from a healthy donor onto the diseased tissue of a patient’s heart. Sawa says the patient moved out of intensive care within a few days. He plans to conduct the procedure in eight more people. Work in animals shows that more cells tend to survive being transplanted in sheets or patches than survive being injected. But studies have also found that such grafted cells do not beat in synchrony with the heart3. “If you put a patch on the surface, it marches to its own drum. It doesn’t hook up and keep pace with the rest of the heart muscle,” says Murry. Researchers are also split on how the sheets work. Animal studies suggest that they might not create new muscle, but secrete growth factors that rejuvenate the existing heart tissue, a process known as the paracrine effect4. Menasché plans to test this in people by only introducing the proteins the cells secrete. Zimmermann plans to test whether patches of cells supported by extracellular scaffolding create new muscle cells. Zimmermann thinks that both injections and patches will be useful if they’re shown to work. Injection is more likely to help people who have recently experienced a heart attack and for whom surgery might not be appropriate, whereas patches could be used on those with chronic scarring in tissue where injected cells might not survive. “It will definitely not be case that one of the approaches will be the only one that is going to be useful,” he says. “There is no one size fits all approach.” </body>
<date id = '132'>13 May 2020</date>
<url id = '133'>https://nature.com/articles/d41586-020-01407-4</url>
<title id = '133'>Report warns that thousands of scientists will also have to abandon research projects.</title>
<body id = '133'>Science organizations warn that thousands of researchers could lose their jobs in the wake of coronavirus shutdowns.Credit: Carla Gottgens/Bloomberg/Getty Research in Australia will face drastic cuts in funding and jobs as a consequence of the COVID-19 pandemic and economic shut-downs, according to a report by organizations representing the country's researchers and academia. Universities in Australia are heavily reliant on international students as a source of revenue and research staff. Income from fee-paying international students makes up around one-quarter of institutions’ operating revenue, and contributes to the cost of research, staff salaries and research facilities. The report forecasts that this revenue will decline by at least Aus$3 billion (US$1.9 billion) this year as a result of the drop in international students because of travel bans and visa restrictions. As a result of this shortfall, an estimated 7,000 university researchers could lose their jobs over the next six months, the report warns. Universities employ almost half of Australia‘s 164,000 full-time researchers.   The report also estimates that more than 9,000 international research students will be forced to delay or abandon their research programmes this year because of travel bans. More than one-third of PhD students in Australia are international students — three-quarters of whom are doing science-related degrees — and many of them have returned to their country of origin because of the pandemic. “We’re seeing a significant impact on our capacity to support high-quality research teams,” says Susan Dodds, the deputy vice-chancellor of research and industry engagement at La Trobe University in Melbourne. Other countries around the world are likely to face similar impacts to research as Australia, says Kylie Walker, the chief executive of the Australian Academy of Technology and Engineering, which led the report’s production. One key difference is that Australia has a greater reliance on income from international students than many other nations similarly affected by the pandemic. But the longer-term implications could be even more profound than the repercussions expected in the next six months, she says. “There are lots of implications for this that we won’t even know until years down the track,” Walker says. The impact will be substantial on the careers of junior scientists, the loss of potential spin-off companies and industry collaborations, and the suspension of clinical research in vital non-pandemic health areas.   Latest on: Careers Career Column 22 MAY 20 Career Column 22 MAY 20 Career Column 21 MAY 20 Funding Nature Index 22 APR 20 Correspondence 07 APR 20 News Feature 01 APR 20 Institutions Nature Index 29 APR 20 Nature Index 29 APR 20 Nature Index 29 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '133'>12 May 2020</date>
<url id = '134'>https://nature.com/articles/d41586-020-01408-3</url>
<title id = '134'>A detailed study shows that young migrants’ risk of developing psychiatric disorders rises stepwise with the number of traumas experienced.</title>
<body id = '134'>People wait outside an immigration office in Berlin.Credit: Adam Berry/Getty Marteza Hasani fled Afghanistan in 2005 when he was six years old. During the war there, he found the beheaded body of his father, who had been killed by the Taliban, in front of his family home. That was the first of many unimaginable traumas he faced before arriving in Germany as a refugee in 2015. “I couldn’t get the image of my father out of my head,” he says.   Hasani is one of more than 100 refugees who have taken part in a study to examine how mental health can be damaged by such traumas — it is perhaps the largest and most detailed of young refugees’ psychological status carried out so far1. Young people who flee their countries are already at greater risk of developing mental-health problems than is the general population. Migration itself is known to be a factor in developing such disorders — but many refugees also experience violent and life-threatening events before and during their flight. The latest study is the first to try to quantify how these events affect psychiatric problems — and it finds that the risk of developing mental-health problems, and their severity, rises significantly with each accumulated trauma a person has experienced. “The data are very impressive,” says psychiatrist Andreas Meyer-Lindenberg, director of the Central Institute of Mental Health in Mannheim, Germany. It’s expected that refugees who have experienced more trauma would be at higher risk of developing mental-health issues, he says. “But the extent is remarkable.” The results reveal the need to change refugee policies, says Hannelore Ehrenreich, a neurologist and psychiatrist at the Max Planck Institute of Experimental Medicine in Göttingen, Germany, who led the study. For example, many countries — including Germany, where the work was conducted — forbid refugees from working or integrating into society until they have been granted asylum. “This waiting in fear of being sent back to the countries they fled piles on more stress — and so adds to their cumulative risk of mental-health problems,” she says. Nearly 71 million people worldwide are seeking asylum globally, according to the United Nations. Since 2014, Germany — which took a large share of the people arriving in the European Union in the 2015 migrant crisis — has received nearly 2 million asylum seekers. Many live for years in refugee centres before learning whether they have been granted asylum. Environmental stressors can increase the risk of young people developing psychiatric disorders, particularly if they already have a slight genetic predisposition. These range from experiencing or witnessing violence to migration and living in cities. In a 2014 study, Ehrenreich showed2 that even being the child of a migrant constitutes such a risk. In the latest research, her team recruited 133 apparently healthy young migrants from 9 refugee centres in Germany in 2018–19. Eighty per cent were male, and nearly one-third were unaccompanied minors at the time of their flight. Many were from Afghanistan, Syria and Iraq. Neurologist and psychiatrist Martin Begemann conducted detailed physical, psychological and cognitive examinations of each participant. He asked about their traumatic experiences, which often included torture, slavery and physical and sexual abuse. He found scars from gunshot wounds, stabbings, explosions, burns and electric shocks on 40% of the participants.   Begemann then conducted interviews to determine whether the participants showed signs of depression, psychosis or cognitive difficulties. He organized psychiatric treatment for those who needed it. The team counted the number of environmental risk factors to which each participant had been exposed in addition to migration. More than 40% had 3 or more extra risk factors — for example, having been raped or enslaved. Just 4.5% had no extra risk factors. The researchers used their assessments to quantify a person’s overall risk of mental-health problems, and found that this rose stepwise with the number of risk factors experienced. In addition, refugees’ ability to cope with daily life declined with each additional trauma. The team also found some factors that had been thought to be ‘psychologically protective’, such as fleeing with a family member or a friend, didn’t seem to mitigate the effects of the negative stressors. The authors note that stressors continue in host nations — for example, poor living conditions, multiple relocations, social exclusion and hostility towards refugees because they are foreigners. Stressful conditions such as these in many centres only add to the risk and the latest work emphasizes this, says Peter Ventevogel, senior mental-health officer at the UN Refugee Agency in Geneva, Switzerland. The situation in Europe is particularly critical now, he adds. One-third of the roughly 39,000 refugees in camps in the Greek Aegean islands are under 18. “These children are stuck in limbo, feeling helpless — and this is very bad for mental health,” he says. Hearing the young refugees’ stories of brutality took its toll on Begemann. “I had nightmares, and had to have psychiatric counselling and therapy for a while.” For participants such as Hasani, the study was beneficial because it led him into psychiatric therapy. “Now I am feeling better and can sleep,” he says. Begemann, M., Seidel, J., Poustka, L. & Ehrenreich, H. EClinicalMedicine https://doi.org/10.1016/j.eclinm.2020.100345 (2020). Stepniak, B. et al. Lancet Psychiatry https://doi.org/10.1016/S2215-0366(14)70379-7 (2014). Download references Latest on: Psychiatric disorders Career Column 13 MAY 20 Article 11 MAY 20 News Feature 05 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '134'>12 May 2020</date>
<url id = '135'>https://nature.com/articles/d41586-020-01402-9</url>
<title id = '135'>The kit has been granted approval under ‘emergency use’ provisions, and should help to ease testing backlogs in the country.</title>
<body id = '135'>More than 60 coronavirus tests have been approved for emergency use in an effort to keep pace with the pandemic.Credit: Al Seib/Los Angeles Times via Getty The US drug regulator has granted its first emergency-use approval for a new coronavirus test that takes advantage of the gene-editing technology CRISPR. The US Food and Drug Administration’s (FDA) emergency-use authority allows it to make tests and drugs available faster than usual in a public-health emergency. The new diagnostic kit is based on an approach co-developed by CRISPR pioneer Feng Zhang at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts. It will be used to test for the novel coronavirus behind the ongoing pandemic, SARS-CoV-2, in laboratories that are certified to provide clinical-test results.   Although the United States has ramped up testing in the past week — averaging nearly 250,000 tests per day, according to the non-profit organization The COVID Tracking Project — there are test shortages in some places. Widespread use of the new FDA-approved kit could help to alleviate backlogs and increase testing, says Mitchell O’Connell, a biochemist at the University of Rochester in New York, who was not involved in developing the test. But O’Connell cautions that it remains to be seen how well the test performs in real-world conditions, such as hospitals, compared with standard tests. The CRISPR-based diagnostic kit has been developed by Sherlock Biosciences, a biotechnology company also based in Cambridge. It works by programming the CRISPR machinery, which has the ability to home in on certain genetic sequences, to detect a snippet of SARS-CoV-2 genetic material in a nose, mouth or throat swab, or in fluid from the lungs. If the virus’s genetic material is found, a CRISPR enzyme generates a fluorescent glow. The test can return results in about an hour, according to the company. Researchers led by Zhang and bioengineer James Collins at MIT first described the basis of the test approach in 20171. A team composed of some of the same scientists showed that it could detect low levels of Zika and Dengue virus in 20182. Other labs are also developing SARS-CoV-2 diagnostic tests based on CRISPR. Last month, researchers in San Francisco, California, published details of an assay that could return results in about 40 minutes3. A similar approach has been reported in a preprint by scientists in Argentina and in California4.   The chief executive of Sherlock Bioscience, Rahul Dhanda, says that the company is now working to create a single cartridge that would not need to be processed in a laboratory and could be used at home. But such a test would need additional validations and another FDA authorization, Dhanda says. Since early April, the agency has issued emergency-use authorizations for more than 60 SARS-CoV-2 diagnostic tests. None of these tests have received clearance to be used and processed entirely at home. Clarification 11 May 2020: The story has been updated to include James Collins’s contribution. Gootenberg, J. S. et al. Science 356, 438–442 (2017). Myhrvold, C. et al. Science 360, 444–448 (2018). Broughton, J. P. et al. Nature Biotechnol. https://doi.org/10.1038/s41587-020-0513-4 (2020). Curti, L. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.02.29.971127 (2020). Download references Latest on: SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '135'>08 May 2020</date>
<url id = '136'>https://nature.com/articles/d41586-020-01403-8</url>
<title id = '136'>Research begins to pick apart the mechanisms behind a deadly COVID-19 complication.</title>
<body id = '136'>There are multiple reasons that blood clots could be forming in people with COVID-19.Credit: Steve Gschmeissner/Science Photo Library Purple rashes, swollen legs, clogged catheters and sudden death — blood clots, large and small, are a frequent complication of COVID-19, and researchers are just beginning to untangle why. For weeks, reports have poured in of the disease’s effects throughout the body, many of which are caused by clots. “This is like a storm of blood clots,” says Behnood Bikdeli, a fourth-year cardiology fellow at Columbia University in New York City. Anyone with a severe illness is at risk of developing clots, but hospitalized patients with COVID-19 seem to be more susceptible. Studies from the Netherlands and France suggest that clots arise in 20–30% of critically ill COVID-19 patients1,2. Scientists have a few plausible hypotheses to explain the phenomenon, and they are just beginning to launch studies aimed at gaining mechanistic insights. But with the death toll rising, they are also scrambling to test clot-curbing medications. Blood clots, jelly-like clumps of cells and proteins, are the body’s mechanism to stop bleeding. Some researchers view clotting as a key feature of COVID-19. But it’s not just their presence that has scientists puzzled: it’s how they show up. “There are so many things about the presentations that are a little bit unusual,” says James O’Donnell, director of the Irish Centre for Vascular Biology at the Royal College of Surgeons in Dublin.   Blood thinners don’t reliably prevent clotting in people with COVID-19, and young people are dying of strokes caused by the blockages in the brain. And many people in hospital have drastically elevated levels of a protein fragment called D-dimer, which is generated when a clot dissolves. High levels of D-dimer seem to be a powerful predictor of mortality in hospitalized patients infected with coronavirus3. Researchers have also observed miniature clots in the body’s smallest vessels. Jeffrey Laurence, a haematologist at Weill Cornell Medicine in New York City, and his colleagues examined lung and skin samples from three people infected with COVID-19 and found that the capillaries were clogged with clots4. Other groups, including a team led by O’Donnell, have reported similar findings5.   “This is not what you'd expect to see in someone who just has a severe infection,” he says. “This is really very new.” This might help to explain why some people have critically low blood-oxygen readings, and why mechanical ventilation often doesn’t help. It’s a “double hit”, says O’Donnell. Pneumonia clogs the tiny sacs in the lungs with fluid or pus, and microclots restrict oxygenated blood from moving through them. Why this clotting occurs is still a mystery. One possibility is that SARS-CoV-2 is directly attacking the endothelial cells that line the blood vessels. Endothelial cells harbour the same ACE2 receptor that the virus uses to enter lung cells. And there is evidence that endothelial cells can become infected: researchers from the University Hospital Zurich in Switzerland and Brigham and Women’s Hospital in Boston, Massachusetts, observed SARS-CoV-2 in endothelial cells inside kidney tissue6. In healthy individuals, the blood vessel is “a very smoothly lined pipe”, says Peter Liu, chief scientific officer at the University of Ottawa Heart Institute in Canada. The lining actively stops clots from forming. But viral infection can damage these cells, prompting them to churn out proteins that trigger the process. The virus’s effects on the immune system could also affect clotting. In some people, COVID-19 prompts immune cells to release a torrent of chemical signals that ramps up inflammation, which is linked to coagulation and clotting through a variety of pathways. And the virus seems to activate the complement system, a defence mechanism that sparks clotting. Laurence’s group found that small, clogged vessels in lung and skin tissue from people with COVID-19 were studded with complement proteins. All these systems — complement, inflammation, coagulation — are interrelated, says Agnes Lee, director of the Hematology Research Program at the University of British Columbia in Vancouver, Canada. “In some patients with COVID, all of those systems are kind of in hyperdrive.”   But Lee adds that there could be other factors at play that aren't specific to COVID-19. People with the disease who become hospitalized typically have a number of risk factors for clotting. They might be elderly or overweight, and could have high blood pressure or diabetes. They show up with high fevers and, because they’re seriously ill, have probably been immobilized. They might have a genetic predisposition to clotting, or be taking medications that increase the risk. “It's kind of like a perfect storm,” she says. Even as researchers begin to unravel how clotting occurs in people with COVID-19, they’re sprinting to test new therapies aimed at preventing and busting clots. Blood-thinning medications are standard of care for patients in the intensive-care unit, and those with COVID-19 are no exception. But dosing is a matter of hot debate. “The question is now, how aggressive should you be?” says Robert Flaumenhaft, chief of the division of haemostasis and thrombosis at Beth Israel Deaconess Medical Center in Boston. Researchers from Mount Sinai School of Medicine in New York City reported that hospitalized people with COVID-19 on mechanical ventilation who received blood thinners had a lower mortality than those who weren’t treated with them. But the team couldn’t rule out other explanations for the observation, and high doses of these drugs carry risks7. At Columbia University in New York City, researchers are launching a clinical trial to compare the standard clot-preventing doses of blood thinners with a higher dose in people who are critically ill with COVID-19. Similar trials are planned for Canada and Switzerland. And scientists at Beth Israel Deaconess Medical Center have begun enrolment for a clinical trial to evaluate an even more powerful clot-busting medication called tissue plasminogen activator, or tPA. This drug is more potent, but carries higher risks of serious bleeding than do blood thinners.   Scientists hope that these trials and others will provide the data necessary to help physicians to make difficult treatment decisions. Lee worries about the amount of ‘reactionary medicine’ happening. “People are changing their therapeutic approach in reaction to their local and personal experience,” she says. She understands the impetus, “but we have to remember the main thing is first do no harm”. ___________________   </body>
<date id = '136'>08 May 2020</date>
<url id = '137'>https://nature.com/articles/d41586-020-01394-6</url>
<title id = '137'>Repositories are rapidly disseminating crucial pandemic science — and they’re screening more closely to guard against poor-quality work.</title>
<body id = '137'> When Albert-László Barabási, a computational scientist at Northeastern University in Boston, Massachusetts, submitted a paper to the preprint server bioRxiv last month, he received an unexpected response. The biomedical repository would no longer accept manuscripts making predictions about treatments for COVID-19 solely on the basis of computational work. The bioRxiv team suggested that Barabási submit the study to a journal for rapid peer review, instead of posting it as a preprint.   Publication norms are changing rapidly for science related to the coronavirus pandemic, as scientists worldwide conduct research at breakneck speeds to tackle the crisis. Preprint servers — where scientists post manuscripts before peer review — have been flooded with studies. The two most popular for coronavirus research, bioRxiv and medRxiv, have posted nearly 3,000 studies on the topic (see ‘Preprint surge’). The servers’ merits are clear: results can be disseminated quickly, potentially informing policy and speeding up research that could lead to the development of vaccines and treatments. But their popularity is spotlighting the scrutiny that these studies receive. Without peer review, it’s hard to check the quality of the work, and sharing poor science could be harmful, especially when research can have immediate effects on medical practice. That has led platforms including bioRxiv and medRxiv, to enhance their usual screening procedures.  “We’ve seen some crazy claims and predictions about things that might treat COVID-19,” says Richard Sever, a co-founder of both servers. Much of that speculative work has been based on computational models, says Sever — so, after consulting with several experts in outbreak science, the team decided to bar those papers from bioRxiv. “We can’t check the side effects of all the drugs and we’re not going to peer review to work out whether the modelling they’re using has any basis,” Sever says. “There are some things that should go through peer review, rather than being immediately disseminated as preprints.” Barabási understands the need to ensure patient safety but disagrees with the decision. “It’s precisely the coronavirus that creates an environment where you need to share,” he says. The purpose of a preprint server, he says, “is that we decide what is interesting, not the referees”. He ended up posting the study on the physical-sciences preprint server arXiv. ArXiv, launched almost 30 years ago, was the first major preprint repository — but in recent years, discipline- and region-specific servers have mushroomed. Screening procedures vary, but an analysis of 44 servers posted last week on bioRxiv1 found that most have quality-control systems. Seventy-five per cent publicly provided information about their screening procedures, and 32% involved researchers in vetting articles for criteria such as relevance of content. “I think there was perhaps a misconception that there are no screening checks that go on with preprint servers,” says Jamie Kirkham, a biostatistician at the University of Manchester, UK, and a co-author of the study. “We have actually found that most of them do.”   BioRxiv and medRxiv have a two-tiered vetting process. In the first stage, papers are examined by in-house staff who check for issues such as plagiarism and incompleteness. Then manuscripts are examined by volunteer academics or subject specialists who scan for non-scientific content and health or biosecurity risks. BioRxiv mainly uses principal investigators; medRxiv uses health professionals. Occasionally, screeners flag papers for further examination by Sever and other members of the leadership team. On bioRxiv, this is usually completed within 48 hours. On medRxiv, papers are scrutinized more closely because they may be more directly relevant to human health, so the turnaround time is typically four to five days. Sever emphasizes that the vetting process is mainly used to identify articles that might cause harm — for example, those claiming that vaccines cause autism or that smoking does not cause cancer — rather than to evaluate quality. For medical research, this also includes flagging papers that might contradict widely accepted public-health advice or inappropriately use causal language in reporting on a medical treatment. But during the pandemic, screeners are watching for other types of content that need extra scrutiny — including papers that might fuel conspiracy theories. This additional screening was put in place at bioRxiv and medRxiv after a backlash against a now-withdrawn bioRxiv preprint that reported similarities between HIV and the new coronavirus, which scientists immediately criticized as poorly conducted science that would prop up a false narrative about the origin of SARS-CoV-2. “Normally, you don’t think of conspiracy theories as something that you should worry about,” Sever says. These heightened checks and the sheer volume of submissions has meant that the servers have had to draft in more people. But even with the extra help, most bioRxiv and medRxiv staff have been working seven-day weeks, according to Sever. “The reality is that everybody’s working all the time.” ArXiv and ChemRxiv, a preprint server for chemistry, have also seen their share of COVID-19 papers. ArXiv has posted more than 800 and ChemRxiv has around 200. Both platforms have enhanced their screening procedures for COVID-19-related papers, although neither has stopped posting all studies with treatment-related computational predictions. “If all the [preprint platforms] had the same standards, then we’d be systematically shutting out the same voices,” says Steinn Sigurdsson, arXiv’s scientific director. “We want to have somewhat overlapping domains.” Marshall Brennan, ChemRxiv’s publishing manager, says that when it comes to papers about treatments, they are “taking much more liberty than we normally would to send those back to the authors to say, ‘Look, this science here is suitable for a preprint server, but you can't make these claims in the context of a public-health crisis.’” He notes that in one such paper, the authors had recommended a home remedy for COVID-19 entirely on the basis of a computational analysis. That paper was swiftly rejected.  The abundance of coronavirus research is also reshaping peer review at journals. Several titles, including Science, journals published by Cell Press, The BMJ and Nature report a surge in coronavirus-related submissions, and many have accelerated the peer-review process to ensure rapid dissemination. A preprint posted in April on bioRxiv2 found that many medical-research journals had drastically speeded up publication pipelines for COVID-19 papers. The analysis, which included 14 journals, found that average turnaround times had fallen from 117 to 60 days (see ‘Rapid review’). (The study omitted several influential journals, such as JAMA, The Lancet and The New England Journal of Medicine because of a lack of appropriate data.) Some journals went from submission to publication in two weeks or less. “That really makes one wonder how thorough this process really is,” says the study’s author, Serge Horbach, a doctoral student at Radboud University in Nijmegen, the Netherlands.   Howard Bauchner, the editor-in-chief of JAMA, notes that low-quality submissions are rising. Journals in the JAMA Network have received 53% more submissions in the first quarter of this year than in the same period in 2019. “Many of these are related to COVID-19, but most are of low quality,” Bauchner says. To address the need for rapid review, a group of publishers and scholarly-communication organizations announced an initiative last month to accelerate the publication of COVID-19 papers using measures such as asking people with relevant expertise to join a list of rapid reviewers. The initiative’s members include Outbreak Science Rapid PREreview, a platform where researchers can request or provide swift reviews of outbreak-related preprints. Even in light of expedited publication, it is important to remember that “the role of the journal is to say: ‘This has been fairly peer-reviewed, statistically reviewed, and can be relied on,’ rather than, ‘This is coming out at you as fast as it possibly can,’” says Theodora Bloom, executive editor of The BMJ and a co-founder of medRxiv. Still, Bloom notes that the COVID-19 papers submitted to her journal “are being handled at the fastest rate possible”. Unlike preprint servers, being published in a journal gives papers the appearance of being reliable and valid knowledge, Horbach adds. “Nonsense or incorrect science in one of these papers is potentially much more harmful.” </body>
<date id = '137'>07 May 2020</date>
<url id = '138'>https://nature.com/articles/d41586-020-01355-z</url>
<title id = '138'>Strained health-care systems, lockdowns and safety requirements have hampered efforts to collect tissue from patients that is crucial to research.</title>
<body id = '138'>Researchers need tissue samples to determine what is killing patients affected by COVID-19.Credit: Giorgos Moutafis/Reuters When the coronavirus pandemic slammed into the city of Bergamo, Italy, starting in February, the Papa Giovanni XXIII Hospital there was quickly overwhelmed. Clinicians raced to dedicate the hospital, one of the largest in the region, to the care of people with COVID-19. Soon, however, pathologist Andrea Gianatti and his colleagues began to shift their attention to a less visible priority: autopsies. “The need arose to understand how the disease affects the various organs,” Gianatti says. “And the most effective way was performing autopsy.” Autopsies are painstaking work under normal conditions; during an infectious-disease outbreak, the added risk calls for safety precautions that make them even more arduous. Since 16 March, Gianatti’s team has performed 80 autopsies of people who tested positive for the coronavirus. The group typically handles only 150 autopsies in a year. Few hospitals in Italy have the safety equipment and resources to launch a similar undertaking, Gianatti says.   Researchers around the world have flocked to study COVID-19, a disease that mainly attacks the lungs, but also has bewildering effects on the heart, kidneys and brain. The raging pandemic and accompanying lockdowns have complicated efforts to collect the tissue samples that researchers need to understand how the new coronavirus wreaks such havoc. Now, pathologists are looking for ways to collect such samples systematically and share the results. “We need those tissues to determine what is killing patients affected by COVID-19,” says pathologist Roberto Salgado of the GZA-ZNA Hospitals in Antwerp, Belgium. “Is it pneumonia? Is it blood clots? Why do they develop kidney failure? We have no clue.” A pandemic is a difficult time to focus on tissue collection for research. Health-care systems are overwhelmed, essentials including personal protective equipment and lab reagents are in short supply, and health-care workers are already taking on tremendous personal risk to care for their patients. To go the extra step to collect blood and tissue samples can feel like a diversion in the face of so much acute need, says pathologist Andrew Connolly of the University of California, San Francisco. On top of that are complications from lockdowns and isolation procedures. People who are severely ill with COVID-19 are sometimes unconscious and on a ventilator; often, their families are not allowed to visit them in hospital. As a result, seeking consent from the patient to donate their body for research might be impossible, and to do so from their families can be fraught, without a bond of trust between families and hospital staff. “A death has occurred, and now someone they’ve never heard of before is asking for an autopsy,” says Connolly. The strict lockdown in San Francisco has also made it hard for Connolly to share samples with colleagues at other institutions, because of the difficulty getting the required forms and signatures needed to transfer potentially infectious material when so many people are confined to their homes.   As researchers struggle to understand the many effects of COVID-19 on the human body, they are clamouring for access to patient samples. The demand ramped up quickly in the early days of the outbreak in the United Kingdom, says Phil Quinlan, director of the UK Clinical Research Collaboration Tissue Directory and Coordination Centre at the University of Nottingham. Requests are now are a daily occurrence, but Quinlan still has few options. The UK National Biosample Centre in Milton Keynes, for example, has been converted into a COVID-19 test-processing centre. Even clinical samples like blood from COVID-19 patients are hard to come by. "If you don’t have a direct connection to a physician involved in a clinical-trial programme, you’re almost certainly not going to get samples right now,” says Quinlan. Even in the middle of the outbreak, some centres have found ways to collect data. In Brazil, pathologist Marisa Dolhnikoff at the University of São Paulo and her colleagues have been using minimally invasive autopsies to take tissue samples. Rather than using the standard procedure, which can require the removal of whole organs, Dolhnikoff’s team takes needle biopsies from various locations in the body, using ultrasound as a guide. The technique is considered safer than a normal autopsy, which exposes the pathologist to infectious agents and so must often be done in a dedicated room with airflow that minimizes risk — a set-up that few hospitals in Brazil have, says Dolhnikoff. Her team has analysed dozens of samples from the lungs, heart, kidney, liver, spleen, skin and brain, and is trying to understand why blood clots are common in people with severe COVID-191.  To determine what is happening in those organs, researchers need large numbers of samples, says Matthew Leavitt, chief medical officer at Lumea, a digital-pathology company in Lehi, Utah. “In a normal environment, autopsy answers questions about one patient,” he says. “In the instance of a newly emerging disease, autopsy is critical to all of humanity.” Pathologist Peter Boor of RWTH Aachen University in Germany has set up a database of COVID-19 autopsies so that researchers can share their data, stripped of identifying information. He would like to share internationally, but quickly found that even within Germany, it posed an enormous logistical challenge. Each county has different legal requirements governing autopsies and patient privacy: “Honestly, it was quite overwhelming,” Boor says. Salgado, Leavitt and a team of pathologists are taking up the challenge of creating an international COVID-19 pathology repository. They are working with the World Health Organization, whose International Agency for Research on Cancer maintains a tumour pathology database and has experience juggling legal requirements. And they are putting together guidelines for the safe collection of autopsy samples and a standardized way of recording the results. So far, researchers from 25 countries have said that they are interested in participating, although such a repository will probably take months to complete, says Amanda Lowe, a managing director at the digital-pathology company Visiopharm in Westminster, Colorado. “Everybody who steps forward and has access to tissue even from one patient is highly valuable.” Dolhnikoff, M. et al. J. Thromb. Haemost. https://doi.org/10.1111/jth.14844 (2020). Download references Latest on: Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 Medical research News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 Physiology Article 20 MAY 20 Article 20 MAY 20 News & Views 06 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '138'>07 May 2020</date>
<url id = '139'>https://nature.com/articles/d41586-020-01389-3</url>
<title id = '139'>Close living quarters and a lack of testing among homeless people across the United States threaten the nation’s ability to control the pandemic, researchers say.</title>
<body id = '139'>COVID-19 testing for homeless people is rare, but necessary.Credit: Al Seib/Los Angeles Times/Getty Researchers are beginning to test homeless individuals in the United States for the virus that causes COVID-19 — and are discovering that the situation is out of control: tests are rare and outbreaks are spreading below the radar. The lack of testing and assistance for people living in group settings — such as those in homeless shelters, nursing homes and prisons — threatens their lives as well as the nation’s ability to curb COVID-19, because these communities can rapidly become the epicentres of new outbreaks that will spread, say researchers. Scientists are now scrambling to collect data and to model the transmission of coronavirus under different group-living situations in hopes of guiding strategies to curb outbreaks. Evidence-based solutions might protect not only the roughly 1.4 million people who use a homeless shelter or transitional housing in the United States each year — a growing population as unemployment soars and prisons release people to ease crowding — but also other people who don’t have the luxury of separating themselves from others. “What we’re seeing in this first wave in the US is that the largest clusters are in populations where people don’t have a lot of agency,” says Gina Neff, a sociologist at the University of Oxford, UK. “These populations will become the sources of new outbreaks, even when we feel like we kind of have it under control.”   Current testing policies are missing a significant amount of infections in at-risk groups. In one recent study, researchers found that only one individual out of 147 who tested positive in a homeless shelter in Boston, Massachusetts, would have met the official criterion for testing — a fever1. Missed cases are a major problem because the disease has been shown to spread like wildfire in communal spaces. Singapore, for example, seemed to have successfully controlled the epidemic, until thousands of cases were discovered among migrant workers living in overcrowded dormitories. In the United States, surveying homeless populations is hampered by a lack of resources available for people living below the poverty line. Private rooms are hard to come by, as are funds for medical care and contact tracing. “The moment you get a positive test, there’s a spider web of decisions to make,” explains Shana McDevitt, a researcher working on COVID-19 testing at the University of California, Berkeley. “We are in a period of time where the policies need to catch up to the tests.” Before COVID-19 was reported in China, Helen Chu, an infectious-disease specialist at the University of Washington in Seattle, and her colleagues were studying how the influenza virus spreads through homeless communities. “We wanted to develop a strategy that could be implemented for treatment and prevention in case a pandemic hit,” she says. Coronavirus swooped in before they could finish. In March, Chu’s team began surveying its study participants for the new coronavirus, too. So far, she says, most of those who have tested positive don’t have obvious COVID-19 symptoms. Researchers found something similar in Boston. In the study of 147 people testing positive at one shelter, just 11 reported a cough1. That study is changing practices at the network of shelters affiliated with the Boston Health Care for the Homeless Program, says Travis Baggett, director of research at the programme and an author on the study. “Until that point, we were screening people by checking their temperatures, and using that as the basis for testing,” he explains. “But our data show that if we aren’t more proactive, we’ll be too late to prevent an outbreak.”   But most shelters still reserve tests for people with symptoms — or they only test broadly after an outbreak has occurred. The results of this policy are troubling. By the time a person from a shelter in San Francisco had been diagnosed with COVID-19 in April, for example, more than 90 other residents and 10 people who worked there were already infected. To influence policies, Baggett is running computer simulations to work out how many people will become infected, hospitalized or die from COVID-19 if the situation remains as it is — compared with the result if people are tested on a regular basis, regardless of symptoms. Costs are taken into account, too. “We’re trying to inform policymakers about different ways of doing things,” he says. At a convention centre in Seattle, Washington, beds are spaced two metres apart to reduce the spread of coronavirus.Credit: Karen Ducey/Getty Towards a similar goal, another team of researchers from three US universities released a report in late March that lays out some minimal needs that might slow the spread of COVID-19 among homeless people, such as providing rooms for individuals at high risk of severe disease because of underlying health conditions. In projecting the “costs of inaction”, they find that, without further interventions, more than 21,300 homeless people in the United States will need to be hospitalized for COVID-19, and 3,400 will die. The authors assume a higher rate of transmission than that in the general population, and more-severe cases because of the prevalence of underlying diseases. Although the average age of homeless individuals in several cities is around 50, studies have found that they experience strokes, falls and other health issues typical of people in their 70s and 80s. Health departments in the United States have started implementing interventions, such as relocating homeless people to stadiums, where beds are spaced two metres apart. And in San Francisco, Seattle and other cities, officials have reserved hotels as places in which to isolate people with COVID-19 who don’t have homes. Yet the vast majority of homeless individuals still remain in group facilities or in tents on the street, says Margot Kushel, a researcher–clinician who studies homelessness at the University of California, San Francisco. She points out that many of the people sleeping in shelters have low-paid ‘essential jobs’, such as those in grocery shops and warehouses. This means they could become infected at work or in the shelters, and spread the virus to others. “I’m not going to stop advocating for the use of hotels and dorms,” she says, “but I’m also pushing for a harm-reduction approach based on science.” Kushel says that, with data on how many people are infected in different settings, her team can estimate how often to screen, how far apart people need to be, whether distributing face masks helps, and whether encampments are safer than indoor options. This last aspect matters in California, because about 91,000 people there live outside.   But these comparisons require many more data on rates of infection. The shortcoming is not necessarily because ample tests don’t exist. For example, McDevitt says that her team at the University of California, Berkeley, has extra testing capability, but doctors and health officials are reluctant to recommend that everyone in a shelter is screened because officials lack plans for how to follow-up on the results when infected people have no health insurance, money or housing. Furthermore, she says, a positive result means that the health department must work out who else the person might have been in contact with — and screen them. It’s a laborious task, but one McDevitt wants to see done. She says surveillance of homeless populations can also inform policymakers about whether an outbreak is waxing or waning in their communities, because people there are so vulnerable to infections. “They’re kind of a canary in the coal mine,” she says. Many social workers await a stronger public-health response, too. Donald Frazier, the executive director of Building Opportunities for Self-Sufficiency, a non-profit organization based in Berkeley, says he cannot let new individuals into the shelters in his network without tests to reveal their coronavirus status. A related problem, he says, is that California is releasing thousands of inmates from prisons and jails to decrease the risk of outbreaks there, but they aren't being tested first — and many have nowhere to go. “It’s a mess,” he says. “I want to make sure my employees and our folks are safe, and there’s no way to know that.”   Researchers working to dampen the toll of COVID-19 in other crowded spaces, such as nursing homes and meat-packing plants, worry that policymakers aren’t concerned enough about outbreaks among marginalized populations. Kushel says, “As scientists, it's our role to raise up these issues and help the public understand how viruses do discriminate, since we live in an inequitable world.” </body>
<date id = '139'>07 May 2020</date>
<url id = '140'>https://nature.com/articles/d41586-020-01396-4</url>
<title id = '140'>Combination of biological membrane and artificial chemistry could power future synthetic organisms.</title>
<body id = '140'>There’s a new way to eat carbon dioxide. Researchers have built an artificial version of a chloroplast, the photosynthetic structures inside plant cells. It uses sunlight and a laboratory-designed chemical pathway to turn CO2 into sugar. Artificial photosynthesis could be used to drive tiny, non-living, solar-powered factories that churn out therapeutic drugs. And because the new chemical pathway is more efficient than anything nature has evolved, the team hopes that a similar process could some day even help to remove CO2 from the atmosphere — although it is not clear whether it could be turned into a large-scale, economically feasible operation. The work was published in Science on 7 May1. Nature has evolved six pathways for ‘fixing’ CO2 — that is, turning it into sugar using enzymes that harness solar or chemical energy. In 2016, Tobias Erb, a synthetic biologist at the Max Planck Institute for Terrestrial Microbiology in Marburg, Germany, and his colleagues designed a seventh2. “We simply used thermodynamic and kinetic considerations to ask if we could rethink CO2 fixation and make it more efficient,” says Erb. They named the pathway the CETCH cycle — a complicated network of enzymes that is 20% more energy efficient than the pathway used in natural forms of photosynthesis. But it wasn’t clear whether the CETCH cycle would be compatible with the rest of a living cell’s machinery. To explore that possibility, Erb’s colleague Tarryn Miller turned to spinach. She extracted light-harvesting membranes from chloroplasts, the photosynthetic organelles common to all plants, and placed them in a reaction vessel alongside the 16 enzymes of their CETCH cycle. After some tweaking, Erb, Miller and their collaborators found that they could get the spinach membranes and their CETCH cycle enzymes to function together. They had effectively created an artificial chloroplast, in which spinach chloroplast membranes harvest solar energy before the synthetic CETCH cycle enzymes use that energy to break down CO2. The enzymes convert the CO2 into a molecule called glycolate that can be used as a feedstock for making useful organic products. “It’s a profound discovery,” says Paul King, a physical biochemist at the National Renewable Energy Laboratory in Golden, Colorado, who wasn’t involved in the study. Although it’s just a proof of principle, it’s already possible to think of ways in which the artificial chloroplasts could be put to work, the authors say. Because of advances in synthetic biology, microbes can now be engineered to churn out useful molecules such as pharmaceutical drugs. But there are limits to what can be synthesized inside living cells. Erb says that the artificial chloroplasts could power non-living mini-reactors to produce molecules that living cells cannot. They might be able to do so more efficiently than microbes can, says Kate Adamala, a synthetic biologist at the University of Minnesota in Minneapolis. “Natural cells spend a lot of energy on staying alive, while synthetic [systems] do not need to grow, reproduce or maintain any life-like functions,” she says. This means that the entire ‘metabolism’ of a synthetic system could be focused on producing valuable chemicals. Adamala says it is even possible to imagine artificial chloroplasts having a role in sequestering atmospheric CO2. But there are problems to address before these applications can become reality. For example, the spinach membranes within the artificial chloroplasts function for just a few hours before they begin to degrade, limiting the working life of the system. And growing spinach and extracting membranes from its cells is relatively time consuming. “Using chloroplast extracts is not the smartest thing to upscale,” says Erb. Because of this, his team is also developing artificial systems to replace the spinach membranes. There is also the tantalizing possibility of using the artificial chloroplasts to build fully synthetic organisms — cells assembled in the lab from the basic biological building blocks of life — although, again, there are challenges to address. “We might be able to use the chloroplast mimics as an energy production system for artificial cells,” says Yutetsu Kuruma, a synthetic biologist at the Tokyo Institute of Technology. But in order to do so, he says that it would be helpful for the artificial chloroplasts to have some ability to self-repair and self-reproduce, like natural chloroplasts can. This is something they can’t do yet. But this hasn’t deterred Erb and his colleagues from beginning experiments with synthetic cells. The team has begun collaborating with researchers at the J. Craig Venter Institute in La Jolla, California, who in 2016 built tiny synthetic cells containing the minimal number of genes for life. The plan is to put the CETCH cycle inside the ‘minimal’ cells— which might be one small step towards making synthetic life that can feed itself by munching on CO2. “Nature can be very conservative, it never explored the full range of photosynthesis options,” says Erb. “That’s what excites us: we can realize solutions nature has never touched on.” Miller, T. E. et al. Science 368, 649–654 (2020). Schwander, T. et al. Science 354, 900–904 (2016). Download references Latest on: Renewable energy Article 19 FEB 20 Editorial 21 JAN 20 Article 08 JAN 20 Chemical synthesis Letter 20 NOV 13 News & Views Forum 12 DEC 12 News & Views 12 SEP 12 Synthetic biology Career Feature 04 MAR 20 Technology Feature 24 FEB 20 Obituary 07 FEB 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '140'>07 May 2020</date>
<url id = '141'>https://nature.com/articles/d41586-020-01354-0</url>
<title id = '141'>Schools are beginning to reopen — but scientists are still trying to understand what the deal is with kids and COVID-19.</title>
<body id = '141'>Children’s susceptibility and immune response to the new coronavirus are hotly debated.Credit: David Vaaknin/The Washington Post/Getty The role of children in spreading the coronavirus has been a key question since the early days of the pandemic. Now, as some countries allow schools to begin reopening after weeks in lockdown, scientists are racing to figure this out. Children represent a small fraction of confirmed COVID-19 cases — less than 2% of reported infections in China, Italy and the United States have been in people under 18 years old. But researchers are divided on whether children are less likely than adults to get infected and to spread the virus. Some say that a growing body of evidence suggests children are at lower risk. They are not responsible for the majority of transmission and the data support opening schools, says Alasdair Munro, a paediatric infectious-diseases researcher at University Hospital Southampton, UK.   Children in Germany and Denmark have already returned to school, and students in some areas of Australia and France are set to go back gradually over the coming weeks. Other scientists argue against a rushed return to classrooms. They say the incidence of infection in children is lower than in adults partly because they haven’t been exposed to the virus as much — especially with many schools closed. And children are not getting tested as often as adults, because they tend to have mild or no symptoms, the researchers say. “I do not see any strong biological or epidemiological reason to believe that children don’t get as infected,” says Gary Wong, a researcher in paediatric respiratory medicine at the Chinese University of Hong Kong. “As long as there is community transmission in the adult population, reopening of schools will likely facilitate transmission, as respiratory viruses are known to circulate in schools and day cares.” He says good surveillance and testing systems should be in place before schools reopen. If children are driving the spread of the virus, infections will probably spike in the next few weeks in countries where children have already returned to school, say scientists. But settling the debate will require large, high-quality population studies — some of which are already under way — that include tests for the presence of antibodies in the blood as a marker of previous infection. Other scientists are studying children’s immune responses to find out why they have milder symptoms than adults when infected, and whether that offers clues to potential therapies. A study published on 27 April in The Lancet Infectious Diseases1, which was first posted as a preprint in early March, analysed households with confirmed COVID-19 cases in Shenzhen, China. It found that children younger than ten were just as likely as adults to get infected, but less likely to have severe symptoms. “That preprint really scared everybody,” says Munro, because it suggested that children could be silently spreading the infection.   But other studies, including some from South Korea, Italy and Iceland, where testing was more widespread, have observed lower infection rates among children. Some studies from China also support the suggestion that children are less susceptible to infection. One, published in Science on 29 April2, analysed data from Hunan, where the contacts of people with known infections had been traced and tested for the virus. The authors found that for every infected child under the age of 15, there were close to 3 people infected between the ages of 20 and 64. But the data are less conclusive for teenagers aged 15 years or older, and suggest that their risk of infection is similar to that of adults, says Munro. Even less well understood is whether infected children spread the virus in a similar way to adults. A study3 of a cluster of cases in the French Alps describes one nine-year-old who attended three schools and a skiing class while showing symptoms of COVID-19, but did not infect a single person. “It would be almost unheard of for an adult to be exposed to that many people and not infect anyone else,” says Munro. Kirsty Short, a virologist at the University of Queensland in Brisbane, Australia, led an as-yet unpublished meta-analysis of several household studies, including some from countries that had not closed schools at the time, such as Singapore. She found that children are rarely the first person to bring the infection into a home; they had the first identified case in only roughly 8% of households. By comparison, children had the first identified case during outbreaks of H5N1 avian influenza in some 50% of households, the study reports. “The household studies are reassuring because even if there are a lot of infected children, they are not going home and infecting others,” says Munro. But Wong argues that such research is biased, because the households weren’t randomly selected but picked because there was already a known infected adult there. So it is also very difficult to establish who introduced the virus, he says. School and day-care closures could also explain why children aren’t often the main source of infection with SARS-CoV-2. Other respiratory viruses can transmit from adults to children and back, so “I don't believe this virus is an exception”, he says.   In fact, two preprints have reported that children with COVID-19 symptoms can have similar levels of viral RNA to adults. “Based on these results, we have to caution against an unlimited re-opening of schools and kindergartens in the present situation. Children may be as infectious as adults,” note the authors of one of the studies, led by Christian Drosten, a virologist at the Charité hospital in Berlin. However, it is not yet clear whether high levels of viral RNA are an indicator of how infectious a person is, notes Harish Nair, an epidemiologist at the University of Edinburgh, UK. Few studies exist of transmission from schools to the broader community, but an Australian report from an ongoing investigation suggests that it’s limited, and much lower than with other respiratory viruses, such as influenza. Among more than 850 people who had been in contact with 9 students and 9 staff members confirmed to have COVID-19 in primary and high schools in the state of New South Wales, only two cases of COVID-19 were recorded among those contacts, both in children. On the basis of the evidence, Munro says children should be allowed back to school. “Children have the least to gain from lockdowns, and they have a lot to lose,” such as missing out on education and not getting added social support such as free school meals, he says. Schools reopening does not mean a return to normal, says Short. There will be lots of restrictions and changes, such as moving desks apart in classrooms and closing playgrounds, to reduce transmission risk, she says. Studies of transmission in schools as they reopen will also be important, says Wong. Researchers in the Netherlands plan to closely monitor this as schools open gradually over the coming weeks. Researchers do agree, however, that children tend to deal with COVID-19 better than adults. The majority of infected children have mild or no symptoms, but some do get very ill or even die. There have been reports of a small number of children in London and New York developing an inflammatory response similar to the rare childhood illness Kawasaki disease. “I would not be surprised if COVID-19 is associated with Kawasaki disease, because many other viral infections have been associated with it,” says Wong. If the association proves to be genuine, it could have been missed in China, Japan and South Korea because Kawasaki disease is much more prevalent in Asia, he says.   One theory for why most children have milder symptoms, says Wong, is that children’s lungs might contain fewer or less-mature ACE2 receptors, proteins that the SARS-CoV-2 virus uses to enter cells. But to confirm this, researchers would need to study tissue samples from children, says Wong, and these are very difficult to get. Others have suggested that children are more routinely exposed to other coronaviruses, such as those that cause the common cold, which protects them from serious disease. “But that doesn’t seem to hold much water, because even newborn babies don’t seem to get very severe disease” from the COVID-19 coronavirus, says Munro. Wong suggests that children might mount a more appropriate immune response to the infection — strong enough to fight the virus, but not so strong that it causes major damage to their organs. His preliminary analysis of 300 individuals infected with COVID-19 has found that children produce much lower levels of cytokines, proteins released by the immune system. Patients of all ages with severe disease tend to have higher cytokine levels, he says. But he still needs to tease out the cause and effect. “Are they sicker because they have higher cytokine levels, or do they have higher cytokine levels because they are sicker?” </body>
<date id = '141'>07 May 2020</date>
<url id = '142'>https://nature.com/articles/d41586-020-01284-x</url>
<title id = '142'>Scientists say rigorous trial data are needed to show that remedies are safe and effective.</title>
<body id = '142'>Traditional Chinese medicine has been promoted as a treatment for COVID-19, despite a lack of evidence for its efficacy.Credit: CHINE NOUVELLE/SIPA/Shutterstock The Chinese government is heavily promoting traditional medicines as treatments for COVID-19. The remedies, a major part of China’s health-care system, are even being sent to countries including Iran and Italy as international aid. But scientists outside China say it is dangerous to support therapies that have yet to be proved safe and effective. There are currently no proven treatments for the deadly respiratory disease caused by the new coronavirus, although many countries are trialling existing and experimental drugs. So far, only one — the antiviral remdesivir — has been shown, in randomized control trials, to have some potential to speed up recovery. In China, senior government officials and the state media are pushing a range of traditional Chinese medicine (TCM) as being effective at alleviating COVID-19 symptoms and reducing deaths from the disease. However, there are no rigorous trial data to demonstrate that the remedies work.   Although the efficacy of some TCM remedies for COVID-19 is being tested, some researchers say the trials have not been rigorously designed and are unlikely to produce reliable results. Government officials and TCM practitioners deem the remedies safe because some have been used for thousands of years, but significant side effects have been reported. “We are dealing with a serious infection which requires effective treatments. For TCM, there is no good evidence, and therefore its use is not just unjustified, but dangerous,” says Edzard Ernst, a UK-based retired researcher into complementary medicines. Other world leaders have promoted unproven treatments for COVID-19. US President Donald Trump has pushed the use of hydroxychloroquine, an antimalarial drug with significant potential side effects, whose effectiveness against COVID-19 is still being studied. And the president of Madagascar, Andry Rajoelina, has also claimed that a herbal drink can cure people of COVID-19. But those leaders’ claims have been criticized by scientists in their countries. By contrast, in China, criticism of TCM is muted. The industry is worth billions of dollars per year, and receives aggressive government support. TCM is based on theories about qi, said to be a vital energy that helps the body to maintain health. Zhang Boli, president of the Tianjin University of Traditional Chinese Medicine and a member of the national team leading China's response to the coronavirus outbreak, said the severe cases could be attributed to a “noxious dampness,” which can cause qi to stagnate. By March, TCM remedies constituted some of China's health ministry’s recommended treatments for COVID-19, and included a couple of dozen pills, powders, injectable therapies and recipes to make herbal teas, known as decoctions. According to Chinese state media, the State Administration of Traditional Chinese Medicine says three formulas and three medicines “have proved” effective treatments for the disease. The newspaper China Daily has reported that “comparative experiments” showed that a group of people with COVID-19 who took Jinhua Qinggan, herbal granules developed to combat H1N1 influenza in 2009, got better faster than those who did not take the capsules, and tested negative for the new virus more than two days sooner. No further details were provided. Another comparative study described in China Daily reported that injections of Xuebijing, a concoction of five herbal extracts which is supposed to “detoxify and remove blood stasis”, reduced the mortality rate of patients with severe illness by 8.8%, when combined with standard medicines.   Huang Luqi, a TCM practitioner and head of the China Academy of Chinese Medical Sciences in Beijing, says that starting in January, he led trials of another three TCM remedies to treat COVID-19, and found that they were safe and effective. On China’s clinical-trials website, the treatments are described simply as traditional Chinese medicine. According to the website, one remedy aims to treat COVID-19 symptoms, another to keep mild cases from becoming severe or critical, and a third to reduce the time taken for a patient to test negative for the virus. Huang did not respond to requests for more details, but says the results will be published soon. Other scientists say there is no convincing evidence that these remedies are effective against COVID-19. Although the trials had control groups, practitioners and patients don't seem to have been blinded to who was receiving the experimental treatment. Double-blind trials are the gold standard for assessing a treatment’s efficacy. “Unless evidence can be demonstrated, it is unethical to market TCM methods with claims of effects,” says Dan Larhammar, a cell biologist at Uppsala University in Sweden. People’s faith in complementary medicines is understandable given that there is no agreement about what works against COVID-19, says Paul Offit, an infectious-disease researcher at the Children’s Hospital of Philadelphia in Pennsylvania. But suggesting that people try alternative medicines could do harm, he says. “People think doing something is better than doing nothing. History tells us that’s not true.” Several of the ‘decoctions’ promoted by the health ministry’s official COVID-19 treatment guidelines include a herb called ephedra, which contains the stimulant pseudoephedrine. Extracts of the herb containing this substance have been banned in the United States and several European countries after a string of deaths in the 1990s and 2000s among those who used it for dieting or energy enhancement. Ernst says that without clear evidence that these treatments work and are safe, China shouldn’t be sending them to other countries. “All parts of a package must be proven to work,” he says. Although TCM is a very important export item for China, promoting it during the pandemic “seems reckless and dangerous”, he says. China has also sent masks and other protective equipment and ventilators to many countries, including the United States, and contributed US$50 million to the World Health Organization (WHO) for its COVID-19 response.)   The WHO initially discouraged the use of traditional remedies to treat COVID-19. For the first months of the outbreak, they were listed on the agency’s website as “not effective against COVID-2019 and can be harmful”. The guidance has since been updated and the warning removed. A WHO spokesperson, Tarik Jašarević, says the original statement “was too broad and did not take into account the fact that many people turn to traditional medicines to alleviate some of the milder symptoms of COVID-19”. Jašarević says the guidance stresses that there is no evidence that any current medicine — traditional or otherwise — can prevent or cure the disease, and that the WHO does not recommend self-medication with any substance as a prevention or cure for COVID-19. Criticism of China’s own support for TCM treatments for COVID-19 is unlikely to gain a foothold inside the country. In late April, a doctor at a hospital in Hubei province was censured and demoted from his administrative positions after posting online that China’s recommendations on COVID-19 treatments, particularly TCM remedies, were not science-based. The doctor told Nature that he could not be interviewed on the topic. Latest on: Government News Q&A 22 MAY 20 Correspondence 19 MAY 20 World View 19 MAY 20 Health care News 25 MAY 20 News 22 MAY 20 News Q&A 22 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '142'>06 May 2020</date>
<url id = '143'>https://nature.com/articles/d41586-020-01348-y</url>
<title id = '143'>The month’s sharpest science shots, selected by Nature’s photo team.</title>
<body id = '143'>Permafrost crater. This aerial view shows the Batagaika crater, a massive land slump in Siberia that formed in the 1960s when deforestation caused the permafrost to melt. The tadpole-shaped crater is about one kilometre long and nearly 90 metres deep, and grows year by year as the warming climate thaws the frozen ground. The layers of sediment on its exposed walls offer a glimpse into 200,000 years of Earth’s geological history, and ice age fossils have been found buried in the sediment. This photo was taken by photographer Katie Orlinsky as part of a series on permafrost that was awarded third prize in the environment category of the 2020 World Press Photo Awards. Credit: Witvliet et al., 2020 Mind of a worm. This digital reconstruction shows every nerve and muscle fibre in the brain of the lab workhorse nematode worm Caenorhabditis elegans. Researchers led by Mei Zhen at the University of Toronto, Canada, created it using software to combine many individual images captured using an electron microcrope. The colours represent different cell types — for example, pink for sensory and blue for motor neurons. The team used these imaging techniques to investigate how different parts of these worms’ brains develop as they mature from larva to adult. Credit: Earth Science and Remote Sensing Unit, NASA Johnson Space Center Starlink express. Stargazers have photographed dozens of Starlink communication satellites — launched by spaceflight company SpaceX — have crossing the night sky. A ‘Starlink train’ of more than a dozen can be seen in this photo taken from the International Space Station — the satellites appear as horizontal dashes of light. The craft are part of a network designed to provide global Internet access, and tens of thousands more could be launched by SpaceX and other companies in the coming years. Many astronomers worry that such ‘megaconstellations’ might interfere with crucial observations of the Universe, disrupting radio frequencies used for scientific observation and raising the risk of collisions in orbit. Credit: Greg Rouse/Scripps Institution of Oceanography at UC San Diego/SOI Deep-sea delights. Scientists exploring the deep sea off the coast of Australia have discovered up to 30 new underwater species. The findings include sponges, molluscs, crustaceans and barnacles that might be completely new to science. The project also spied some species that have been seen for the first time in Western Australia, including the octopus squid Taningia danae. “We suspected these deep-sea areas would be diverse, but we have been blown away by the significance of what we have seen,” said Nerida Wilson, chief scientist at the Western Australian Museum who led a month-long expedition on the research ship Falkor. With the help of an underwater robot, Wilson and colleagues completed 20 dives at depths of up to 4,500 metres in an area known as the Gascoyne Coast Bioregion. Their aim was to collect samples and video footage from previously unexplored deep-sea canyons and coral reefs. Credit: ROV SuBastian/SOI Sea spiral. One of the most striking discoveries of the Australian deep-sea exploration expedition was this string-like creature known as a siphonophore, which might be the longest animal ever discovered. Measuring 46 metres — almost twice the average length of a blue whale — it is the largest specimen of the giant siphonophore Apolemia ever recorded. Although they look, behave and move around like individual organisms, siphonophores are actually floating colonies made up of tiny multicellular organisms called zooids that are attached to one another and cannot survive independently. They feed on small fish and crustaceans, using stinging tentacles to stun and capture their prey, in much the same way as jellyfish. Credit: NASA/JPL-Caltech VITAL machinery. A new high-pressure medical ventilator has been developed by engineers at NASA’s Jet Propulsion Laboratory in Pasadena, California. The device, which NASA has called VITAL (Ventilator Intervention Technology Accessible Locally), was developed in just 37 days. The agency was responding to reports of a national shortage of the machines resulting from the coronavirus pandemic. VITAL can be built faster and maintained more easily than conventional ventilators, NASA says, and the agency is seeking accelerated approval from the the US Food and Drug Administration so that medical centres can begin testing prototypes. “We specialize in spacecraft, not medical-device manufacturing,” said the laboratory’s director, Michael Watkins. “But excellent engineering, rigorous testing and rapid prototyping are some of our specialties.” Credit: Flavio Valenzi (CC BY 3.0 Unported) Jupiter in full colour. As our Earthly lives become increasingly stressful, those in search of a soothing glimpse of a world beyond our own can head to NASA’s Junocam imageprocessing gallery, where several new snaps of Jupiter have been shared. NASA uploads raw images taken by the camera on board its Juno spacecraft, and invites citizen scientists to download and process them, and then submit their own creations. In this shot, entitled ‘Jupiter in the dark’, the characteristic swirls and stripes that make up the planet’s atmosphere have been coloured to make it pop against the darkness of space. It was submitted by Flavio Valenzi. Credit: Sanjeev Verma/Hindustan Times/Getty Hosed down. A medic at New Delhi’s Hindu Rao Hospital walks through a disinfecting tunnel, 19 days after the government ordered the country’s roughly 1.3 billion people to stay at home in an attempt to stop coronavirus spread. As well as the lockdown, India has introduced a trace-and-quarantine strategy using a huge surveillance network: thousands of health-care workers are fanning out across the country to trace and isolate people who might have had contact with infected individuals. People are typically tested for coronavirus infection only if they develop symptoms. Clarification 13 May 2020: An earlier version of this story implied that the nematode work was led by researchers at Harvard University. In fact, the university was just one of several institutes in the collaboration. Latest on: Imaging News & Views 20 MAY 20 Outlook 13 MAY 20 Article 01 APR 20 Media Career Column 13 MAY 20 News 15 APR 20 Correspondence 14 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '143'>06 May 2020</date>
<url id = '144'>https://nature.com/articles/d41586-020-01247-2</url>
<title id = '144'>Developers and funders are laying the groundwork for efficacy trials, but only a handful of vaccines are likely to make the cut.</title>
<body id = '144'>An experimental vaccine for COVID-19 from Sinovac Biotech in Beijing.Credit: Nicolas Asfouri/AFP/Getty Less than five months after the world first learnt about the new coronavirus causing fatal pneumonia in Wuhan, China, there are more than 90 vaccines for the virus at various stages of development, with more announced each week. At least six are already being tested for safety in people. Now, developers, funders and other stakeholders are laying the groundwork for their biggest challenge yet: determining which vaccines actually work. This typically involves giving thousands or tens of thousands of people a vaccine or placebo and seeing, over months or even years, whether there is a difference between the two groups in how many people get infected in the course of their daily lives, as well as checking that no safety issues emerge.   But in this pandemic, scientists will have to accelerate and streamline that process. A vaccine may be the only way to generate immunity to the virus across a population: despite the millions of coronavirus cases worldwide, some preliminary studies suggest that only a small fraction of people in even hard-hit regions have been infected with SARS-CoV-2, and their immunity is unclear. This month, the World Health Organization (WHO) in Geneva, Switzerland, sketched out plans for a clinical trial that will test numerous vaccines in a single study. Some developers and funders have plans for their own efficacy trials. But key questions remain, such as which vaccines will be tested first — or at all — and how their effectiveness will be measured and compared. “It’s going to require a level of coordination that has never really happened before, and a time frame that’s never really been even imagined,” says Mark Feinberg, president and chief executive of the International AIDS Vaccine Initiative (IAVI) in New York City. “You can’t take 200 vaccines into efficacy trials,” says Seth Berkley, chief executive of Gavi, the Vaccine Alliance in Geneva, which funds immunizations in low and middle-income countries.   The WHO’s proposed Solidarity Vaccine Trial seeks to speed development with an adaptive design. This allows vaccines to be added to the trial on an ongoing basis. Participants will be enrolled continuously, and vaccines that don’t seem to be working can be dropped from testing. The WHO still needs to hammer out details, such as how a vaccine’s efficacy will be measured, says Marie-Paule Kieny, research director at the French National Institute of Health and Medical Research in Paris. But she thinks its overall approach makes sense. “One of the challenges is prioritization — which vaccine should you test first,” she says. The WHO has established an expert panel to prioritize vaccines for inclusion in its trial, but it is unlikely to be the only organization seeking to do this. “Some strategic alignment and coordination in this effort is going to be critically important or otherwise it'll become very chaotic,” says Feinberg. But the WHO plan “by itself may not be sufficient,” he adds.   The US National Institutes of Health (NIH) in Bethesda, Maryland, this month unveiled a partnership with more than a dozen companies that aims to coordinate the development of drugs and vaccines for coronavirus. And the Coalition of Epidemic Preparedness (CEPI), a global foundation that funds vaccine development, is supporting 9 different vaccines. The non-profit hopes to raise US$2 billion to pay for efficacy trials, manufacturing and other costs, says Melanie Saville, the organization’s director of vaccine research and development. Criteria for prioritizing vaccines for efficacy could include its production capacity and the immune response generated in early human trials and animal studies, says Kieny, as well as regulators’ experience with the specific type of vaccine. Some of the kinds of vaccine being developed, such as RNA vaccines, have not been widely tested in people or used in a vaccine that has won regulatory approval. A vaccine developed at the Jenner Institute at the University of Oxford, UK, is currently undergoing early-phase trials. “There’s a reasonable chance that we’ll be able to pick up the efficacy of the vaccine over the next couple of months,” Andrew Pollard, an infectious disease researcher at Oxford leading the trial, said at an online press briefing. A small number of developers with plans and funding to get their vaccine approved and scale up production will likely call the shots with regard to how efficacy trials are done, says Rip Ballou, a program leader at IAVI. “Doing a phase III trial to show efficacy is meaningless if it's not coupled to a plan to actually licence and deliver under some regulatory authority,” he says. “There's only a handful of players that will be able to meet that very high bar. Because otherwise, it's a publication. It's not a vaccine.” Another challenge will be determining how the different vaccines compare to one another. WHO’s proposal for an efficacy trial could allow the performance of different vaccines to be directly compared, but Kieny thinks that some developers may be unwilling to accept this because it could hurt a vaccine’s commercial prospects. Swati Gupta, IAVI’s Vice President and Head of Emerging Infectious Diseases and Scientific Strategy, says vaccine developers will want to understand how key decisions are made before committing to trials that involve comparisons with other vaccines, to make sure their vaccines have “a fair shot at being able to show its efficacy”. But it is essential to be able to compare different vaccines, even if it requires vaccines developers to set aside their short-term interests, says Charlie Weller, vaccine lead at the Wellcome Trust biomedical charity in London. “They work under commercial business models. That's not going to work for the situation we're in now," she says. Expected global demand for a coronavirus vaccine could make developers more willing to cooperate. “We need more than one vaccine,” says Kieney. “Monopoly is always very bad, and none of the vaccines may have enough production capacity.”   One factor that could encourage such cooperation is the shifting geography of the pandemic. “China would have been a great place in Wuhan to have done efficacy trials two months ago,” says Berkley. “Italy would have would have been a great place to do it a month ago.” As a result, developers have incentive to join initiatives such as the WHO’s or the NIH’s, because of their access to clinical trial infrastructure around the world that could bring vaccines to where there are coronavirus cases. “We need to be nimble,” adds Gupta. While most experts see large trials as a necessity to ensure that coronavirus vaccines are safe and effective, some developers are examining alternatives. One option is to look for signs that a vaccine works in early-stage trials involving hundreds of participants, and then seek permission from regulators to deploy the vaccine under ‘emergency use’ rules in high-risk groups, such as health-care workers, who are more likely to be infected with the coronavirus. Regulators such as the US Food and Drug Administration can grant emergency use, while additional data is collected to license a vaccine. Cansino Biologics in Tianjin, China, which is developing a vaccine comprised of a chemically inactivated form of SARS-CoV-2 virus, will consider this approach, according to a company spokesman. Johnson and Johnson said in a press release that its vaccine could be ready for emergency use in early 2021. No vaccine has ever been deployed under emergency-use provisions, says Katherine O'Brien, who heads WHO's immunizations, vaccines, and biologicals department. If coronavirus vaccines follow that path, regulators will seek extra reassurance that a vaccine is safe. “There is no compromise that can be made on the safety issues,” O’Brien adds.   Momentum is building for an even more radical proposal to determine which vaccines work: intentionally infecting young, healthy volunteers, negating the need to wait for trial participants to become infected naturally. These ‘human challenge’ studies are already used to study infectious diseases such as malaria and dengue, and some researchers say they should be considered to speed the development of coronavirus vaccines. Berkley says challenge trials could be used to rapidly determine which vaccines advance to large-scale trials. But he thinks they may be too risky without either an effective drug or a genetic test to identify the rare young individuals who are likely to develop severe disease. “Until you have a recognised treatment, I think that's a pretty tough story,” he says. Latest on: SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 Vaccines News 19 MAY 20 News 13 MAY 20 Editorial 13 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '144'>30 April 2020</date>
<url id = '145'>https://nature.com/articles/d41586-020-01295-8</url>
<title id = '145'>Despite conflicting studies, results from largest trial yet show the antiviral speeds up recovery, putting it on track to become a standard of care in the United States.</title>
<body id = '145'>Coronavirus causes severe respiratory illness in some people.Credit: Zhang Yazi/China News Service via Getty An experimental drug — and one of the world’s best hopes for treating COVID-19 — could shorten the time to recovery from coronavirus infection, according to the largest and most rigorous clinical trial of the compound yet. On 1 May, the US Food and Drug Administration (FDA) granted an ‘emergency use authorization’ for clinicians to use the drug, called remdesivir, which is administered intravenously, in hospitals for people with severe COVID-19. Remdesivir interferes with the replication of some viruses, including SARS-CoV-2, which is responsible for the current pandemic. On 29 April, Anthony Fauci, director of the US National Institute of Allergy and Infectious Diseases (NIAID), announced that a clinical trial in more than 1,000 people had showed that those taking remdesivir recovered in 11 days on average, compared with 15 days for those on a placebo. “Although a 31% improvement doesn’t seem like a knockout 100%, it is a very important proof of concept,” Fauci said. “What it has proven is that a drug can block this virus.” There were also fewer deaths among trial participants who received the drug, he said, but that trend was not statistically significant. The shortened recovery time, however, was significant, and was enough of a benefit that investigators decided to stop the trial early, he said, to ensure that those participants who were receiving placebo could now access the drug. Fauci added that remdesivir would become a standard treatment for COVID-19. The FDA’s authorization is not a final drug approval, and can be revoked when the conditions required for emergency use are no longer in effect. Distribution of the drug in the United States will be under government control. The news comes after weeks of data leaks and on a day of mixed results from clinical trials of the drug. The drug’s maker, Gilead Sciences of Foster City, California, announced on the same day that in its own trial, more than half of 400 participants with severe COVID-19 had recovered from their illness within two weeks of receiving treatment. But the study lacked a placebo-controlled arm, making the results difficult to interpret. Also on 29 April, a smaller trial run in China announced that it had found1 no benefits from remdesivir when compared with a placebo. But that trial was stopped early owing to difficulty in enrolling participants as the outbreak subsided in China. Nevertheless, onlookers are hopeful that the large NIAID trial provides the first glimmer of promise in a race to find a drug that works against the coronavirus, which has infected more than three million people worldwide. “There is a lot of focus on remdesivir because it’s potentially the best shot we have,” says virologist Stephen Griffin at the University of Leeds, UK. Fast-flowing, conflicting information on remdesivir in the past few weeks has left people reeling. In the rush to find therapies to combat COVID-19, small clinical trials without control groups have been common. “I’m just very annoyed by all of these non-controlled studies,” says Geoffrey Porges, a biotechnology analyst for the investment bank SVB Leerink in New York City. “It’s reassuring that 50–60% of patients are discharged from the hospital, but this is a disease that mostly gets better anyway.” With so much uncertainty, the remdesivir-watchers were waiting anxiously for final results from the NIAID trial, which were not expected until the end of May. In lieu of a vaccine, which could still be more than a year away, effective therapies are crucial in reducing deaths and limiting economic damage from the pandemic. Yet, despite the flood of small clinical trials, no therapy has been convincingly shown to boost survival in people with COVID-19. The NIAID results put a new sheen on remdesivir.The NIAID did not release detailed safety data. The study in China found no significant difference between remdesivir and placebo in the frequency of adverse events, but 12% of people who received remdesivir dropped out of the study due to side effects including nausea and cardiopulmonary failure, compared to only 5% on placebo. “It may not be the wonder drug that everyone’s looking for, but if you can stop some patients from becoming critically ill, that’s good enough,” says Griffin. Fauci said the finding reminded him of the discovery in the 1980s that the drug AZT helped to combat HIV infection. The first randomized, controlled clinical trial showed only a modest improvement, he said, but researchers continued to build on that success, eventually developing highly effective therapies. Remdesivir works by gumming up an enzyme that some viruses, including SARS-CoV-2, use to replicate. In February, researchers showed2 that the drug reduces viral infection in human cells grown in a laboratory. Gilead began to ramp up production of remdesivir well before the NIAID results came out. By the end of March, the company had produced enough to treat 30,000 patients. And by streamlining its manufacturing process and finding new sources of raw materials, Gilead announced, it hopes to produce enough remdesivir to treat more than one million people by the end of the year. That calculation was based on the assumption that people would take the drug for ten days, but the results announced from Gilead’s trial on 29 April suggest that a five-day course of treatment could work just as well. If so, that would effectively double the number of people who could be treated, says Porges. In the long term, clinicians will probably want a bevy of antiviral drugs — with different ways of disabling the virus — in their arsenal, says Timothy Sheahan, a virologist at the University of North Carolina in Chapel Hill, who has teamed up with Gilead researchers to study remdesivir. “There is always the potential for antiviral resistance,” he says. “And to hedge against that potential, it’s good to have not only a first-line, but also a second-, third-, fourth-, fifth-line antiviral.” Researchers are furiously testing a wide range of therapies, but early results, although not yet definitive, have not been encouraging. The malaria drugs chloroquine and hydroxychloroquine, both of which have anti-inflammatory effects, drew so much attention from physicians and the public that some countries have depleted their supplies of the drugs. Yet studies in humans have failed to show a consistent benefit, and some have highlighted the risks posed by side effects of the drugs that affect the heart. Early interest in a mix of two HIV drugs called lopinavir and ritonavir flagged when a clinical trial in nearly 200 people did not find any benefit of the mix for those with severe COVID-193. Another promising therapeutic hypothesis — that inhibiting the action of an immune-system regulator called IL-6 could reduce the serious inflammation seen in some people with severe COVID-19 — has met with mixed results thus far. Still, a host of other therapies are being tested in people, and many researchers are hunting for new drugs at the bench. Sheahan and his colleagues have found4 a compound that is active against SARS-CoV-2 and other coronaviruses, including a remdesivir-resistant variant of a coronavirus, when tested in laboratory-grown human cells. But much more testing would be needed before the compound could be tried in people. “What we’re doing now will hopefully have an impact on the current pandemic,” he says. “But maybe more importantly, it could position us to better respond more quickly in the future.” Update 04 May 2020: This story has been updated to note that on 1 May US regulatory authorities granted remdesivir ‘emergency use’ authorization for use in people with severe COVID-19. Wang, Y. et al. Lancet https://doi.org/10.1016/S0140-6736(20)31022-9 (2020). Wang, M. et al. Cell Res. 30, 269–271 (2020). Cao, B. et al. N. Engl. J. Med. https://doi.org/10.1056/NEJMoa2001282 (2020). Sheahan, T. P. et al. Sci. Transl. Med. 12, eabb5883 (2020). Download references Latest on: Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 Medical research News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '145'>29 April 2020</date>
<url id = '146'>https://nature.com/articles/d41586-020-01286-9</url>
<title id = '146'>Stagnant water in unused buildings can harbour infectious bacteria and heavy metals. Scientists point out that guidance on reopening is limited.</title>
<body id = '146'>Treated water could sit in the plumbing of some buildings for months as a result of COVID-19-associated lockdowns.Credit: Education Images/Universal Images Group via Getty As some regions prepare to lift the lockdowns in response to COVID-19, scientists who study water are worried about the potential for a secondary health crisis waiting in the dormant plumbing of offices, gyms, restaurants and schools. Because of the threats from heavy metals leaching out of pipes, and bacteria and parasites growing in stagnant water, researchers are studying locked-down water systems in an attempt to understand how people can safely reoccupy buildings. “There really isn’t a lot of scientific awareness of these larger buildings,” says Andrew Whelton, an environmental engineer at Purdue University in West Lafayette, Indiana. “Because of that, there’s no guidance.” Whelton and fellow Purdue environmental engineer Caitlin Proctor are doing their best to fill that information gap. The duo is currently leading several studies to understand the complex chemistry that occurs in dormant water systems. When Purdue’s campus shut down in March, Whelton’s laboratory began sampling water in several large buildings across and around the university. In one privately-owned residence hall, abandoned for about a month, Proctor found that no disinfectant remained in the water samples she collected. Although such buildings can go for two or more months with reduced student activity between semesters, the current shutdown is expected to last significantly longer. And labs, gyms and other campus buildings are less likely to have previously experienced such long disruptions. Proctor is interested in how the microbial communities in the buildings’ water systems will change over the coming months. The longer a building sits unused, the more potential there is for harm. That’s because the length of time between water treatment and use is one of the key factors determining bacterial regrowth, says Joan Rose, an environmental microbiologist at Michigan State University in East Lansing. One of the main concerns is Legionella, a genus of bacterium that causes Legionnaires’ disease. Legionnaires’ — the leading reported waterborne disease in the United States — attacks the body’s respiratory system. So, declining water quality could compound the strain on already stressed public-health systems, Proctor says. “The same people that are vulnerable to COVID are going to be vulnerable to these bacterial pathogens.” Water quality in a complex building can vary by season, by time of day or even from room to room. So one-size-fits-all guidelines for maintaining or flushing systems are unlikely to help, Proctor says. The most common advice after a period of dormancy is to run all taps at full flow for a set amount of time — usually 5 or 10 minutes — to flush the stagnant water from the pipes. But one building that the Purdue team studied required more than a full day to flush. And the complications don’t stop there. For large complexes such as universities, there might not be enough staff to routinely flush every building. Already-struggling small businesses could baulk at the increased water bills that would result from proper flushing. And without sufficient masks and respirators, staff carrying out flushing operations are at risk of inhaling Legionella and other pathogens that might be growing in the pipes. The most important things that building owners and managers can do right now, Whelton says, are to flush taps regularly, carry out low-cost testing and start to build up a water-management plan. His lab is currently consulting with a number of universities on how to bring their buildings back online safely after the summer. He and Proctor led a group of academic and industry engineers that published a non-peer-reviewed study laying out the considerations for building reuse on 7 April. The current situation offers an unprecedented chance to study our water systems and to make sure that society is better prepared for future disaster scenarios, Rose says. “We’re in an extraordinary time,” she says. “We should make every effort to learn as much as we can.” Latest on: Environmental sciences News & Views 13 MAY 20 Research Highlight 12 MAY 20 Editorial 12 MAY 20 Epidemiology News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '146'>28 April 2020</date>
<url id = '147'>https://nature.com/articles/d41586-020-01261-4</url>
<title id = '147'>While the world focuses on the pandemic, the United States is adopting controversial policies at the Environmental Protection Agency.</title>
<body id = '147'>The US Environmental Protection Agency is chipping away at the legal justification for power-plant emissions standards.Credit: Dane Rhys/Bloomberg via Getty The US Environmental Protection Agency (EPA) turns 50 this year, but scientists and environmentalists see little reason to celebrate. In the middle of a global pandemic that is making world leaders scramble to protect citizens and restart economies, the agency’s leadership is pressing forward with controversial efforts to roll back environmental regulations and fundamentally alter the way in which science is used to craft policy. In the past month alone, the agency has dialled down regulations on automobile emissions and fuel efficiency put in place under former president Barack Obama; it has weakened rules on mercury and other pollutants emitted by power plants; and it has shied away from strengthening standards to reduce fine-particle air pollution. “This is an extremely aggressive agenda,” says Betsy Southerland, who spent more than three decades as an EPA official before retiring in protest against the current administration’s policies in 2017. By Southerland’s latest tally, the EPA has targeted more than 80 rules for revision or elimination in just over three years, without providing any evidence that the underlying science has changed. But she and many others think that this is just the beginning of a regulatory overhaul that could hamstring future administrations’ attempts to craft health and environmental safeguards. Here, Nature looks at three recent decisions and two pending policy changes that could have a lasting impact. At the end of March, President Donald Trump's administration finalized a plan to scale back targets for automobile-emissions reductions from 5% per year to 1.5%, a change that the EPA acknowledges could result in an extra 867 million tonnes of carbon dioxide being released into the atmosphere by vehicles sold over the next decade.   In mid-April, the EPA released another rule, targeting Obama-era mercury-emissions standards for power plants. Although the agency left the original regulatory limits in place, it adjusted how the rules’ costs and benefits are calculated, weakening their economic justification. The original price tag reported for the 2011 regulation took into account health benefits from a reduction in particulate matter that would accompany cuts to mercury emissions. Taking these out of the equation makes the rule seem more expensive, says David Spence, a political scientist and law scholar at the University of Texas at Austin. It also sets a precedent that could undermine the mercury rule and others. Even more alarming, public-health experts say, was a decision on fine-particle pollution that EPA administrator Andrew Wheeler announced in mid-April. In that case, the EPA went against the advice of its own staff and many academic scientists by leaving the current standards in place — in spite of evidence that reducing such pollution could save thousands of lives each year1. In a report issued last September, EPA staff charged with reviewing the literature cited epidemiological and other evidence that would support cutting the maximum allowed average level of fine particulate matter from 12 micrograms per cubic metre of air to between 8 and 10. The regulatory process that prevented that change was tipped toward the interests of polluters from the outset, with little to no independent scientific oversight, says Christopher Frey, an environmental engineer at North Carolina State University in Raleigh. Frey formerly chaired the EPA’s scientific advisory committee on clean air, and was a member of a review panel for the issue that was disbanded in October 2018. “Rather than focusing on protecting public health, EPA is on a misguided mission to protect the profits of regulated industries,” Frey says. “But it’s all based on a lot of misconceptions and assumptions rather than facts or evidence.”   The ongoing coronavirus pandemic has been shown2 to have a higher death toll in communities affected by air pollution. Looking ahead, it could become even more difficult to bring health data and other evidence to the policymaking table if the EPA moves forward with a pair of proposals that would alter how science is used and evaluated at the agency. First is a “transparency” rule that could restrict the use of public-health studies — including much of the epidemiological research that the agency has used to set particulate-pollution standards in the past. A draft proposal states that if underlying data and models are not publicly available — which is often the case for private health-care data — the EPA could give them less weight or exclude them from consideration entirely when setting standards and conducting scientific assessments. The EPA released a supplemental proposal to the rule in March, providing additional options for how to implement the rule and expanding its application to cover all research used to support agency rules. After an outcry from scientists and environmentalists who have accused the agency of pushing through the rules while the public is focused on the coronavirus crisis, the EPA extended the public comment period by one month, to 18 May. Precisely how the rule would work remains unclear, but scientists and public-health advocates say that the latest changes do not solve the fundamental problem, which is that the rule could effectively sideline mainstream epidemiological research and undermine public-health regulations. “It’s headed in the wrong direction, and it would apply to pretty much all of EPA’s major work,” says Michael Halpern, deputy director of the Center for Science and Democracy at the Union of Concerned Scientists, an advocacy group in Cambridge, Massachusetts.   A second proposal, currently pending review at the White House, would change how the agency evaluates the costs and benefits of environmental and public-health regulations, much as it did in its re-evaluation of the mercury-emissions standards. Many experts think that the proposed guidance — which could be released in the next few months — will seek to reduce the consideration of incidental and indirect benefits from proposed rules. Taken together, the cost–benefit guidance and the transparency rules could help the Trump administration to justify removing regulations, and could hamper regulatory efforts by future administrations. These and other EPA decisions will inevitably be challenged in court, but scientists and environmentalists say that provides little solace. “They are rolling back progress, and we are losing time,” Halpern says. Southerland watched multiple administrations come and go during her time at the EPA, and says the scale and speed of the Trump administration's assault on science-based regulations is unprecedented. In many cases, the EPA’s leaders aren’t even presenting new evidence to justify their decisions, she says. “That’s why they can move so fast: they just say, ‘We no longer agree with the science and the facts.’” </body>
<date id = '147'>28 April 2020</date>
<url id = '148'>https://nature.com/articles/d41586-020-01248-1</url>
<title id = '148'>Researchers sift through data to compare nations’ vastly different containment measures.</title>
<body id = '148'>Scientists are scrambling to work out what effect specific measures, such as social distancing, have in slowing the spread of COVID-19.Credit: Ivan Romano/Getty Hong Kong seems to have given the world a lesson in how to effectively curb COVID-19. With a population of 7.5 million, it has reported just 4 deaths. Researchers studying Hong Kong’s approach have already found that swift surveillance, quarantine and social-distancing measures, such as the use of face masks and school closures, helped to cut coronavirus transmission — measured by the average number of people each infected person infects, or R — to close to the critical level of 1 by early February. But the paper, published1 this month, couldn’t tease apart the effects of the various measures and behavioural changes happening at the same time.   Working out the effectiveness of the unprecedented measures implemented worldwide to limit the spread of the coronavirus is now one of scientists’ most pressing questions. Researchers hope that, ultimately, they will be able to accurately predict how adding and removing control measures affects transmission rates and infection numbers. This information will be essential to governments as they design strategies to return life to normal, while keeping transmission low to prevent second waves of infection. “This is not about the next epidemic. It’s about ‘what do we do now’?” says Rosalind Eggo, a mathematical modeller at the London School of Hygiene and Tropical Medicine (LSHTM). Researchers are already working on models that use data from individual countries to understand the effect of control measures. Models based on real data should be more nuanced than those that, at the start of the outbreak, necessarily predicted the effect of interventions mainly using assumptions. Combining data from around the world will allow researchers to compare countries’ responses. And compared with studies of individual countries, it should also allow them to design models that can make more accurate predictions about new phases of the pandemic and across many nations. But untangling cause and effect is extremely challenging, in part because circumstances differ in each country and because there is uncertainty over how much people adhere to measures, cautions Eggo. “It’s really hard but it doesn’t mean we shouldn’t try,” she adds. Efforts to tackle these questions will get a boost in the coming weeks from a database that brings together information on the hundreds of different interventions that have been introduced worldwide. The platform, being prepared for the World Health Organization (WHO) by a team at the LSHTM, gathers data collected by ten groups already tracking interventions — including teams at the University of Oxford, UK, the Complexity Science Hub Vienna (CSH Vienna), and public-health organizations and non-profit organizations such as ACAPS, which analyses humanitarian crises. Source: Oxford Coronavirus Government Response Tracker (data); Nature (charts). The database will standardize the information collected by the different teams and should be more comprehensive than anything an individual group could generate, says Chris Grundy, a data scientist behind the LSHTM project. Agencies such as the WHO routinely track control measures used in a disease outbreak, but for COVID-19 the picture is complicated by pandemic’s speed and scale, says Grundy. The LSHTM has recruited an impressive corps of 1,100 volunteers to work on cleaning and combining the information. The data set will be open for anyone to use and will be improved in future releases, says Grundy. Speed is of the essence, he says. “Days make a difference right now.” The trackers lay bare the vast range of policies deployed in different nations. The Vienna team has captured details of around 170 interventions in 52 countries, ranging from small measures such as floor stickers that mark a two-metre separation to major, restrictive policies such as school closures. They are also following some countries’ recent efforts to restart daily life and measures that go with them, including making the wearing of face masks mandatory. Meanwhile, Oxford’s project, the COVID-19 Government Response Tracker, is monitoring 13 interventions in more than 100 countries. It compiles 7 of the 13 into a single ‘stringency’ index that captures the overall severity of each country’s response and allows for comparison between countries that take different approaches (see ‘Pandemic protections’). The team is revising how they calculate the index and adding a measure.   Already, scientists in both groups are analysing their data to explore differences in each country’s responses. The Vienna team is looking for patterns, and their methods include clustering countries by how early in their epidemics they began interventions and by the total number of restrictions introduced. In Europe, for example, algorithms group Sweden, the United Kingdom and the Netherlands together as countries that acted relatively slowly. In the early stages of their epidemics, all three implemented ‘herd immunity’ strategies, which involved few measures or ones that relied on voluntary compliance, although later, the United Kingdom and the Netherlands switched to more aggressive responses, including country-wide lockdown, says Amélie Desvars-Larrive, an epidemiologist at CSH Vienna and the University of Veterinary Medicine Vienna. Meanwhile, Germany and Austria stand out as nations that adopted aggressive and early control strategies compared with Italy, France and Spain, which implemented similar measures, including lockdown, but later in their epidemics, she says. So far, Germany and Austria have, per capita, seen a fraction of the deaths from COVID-19 of these other countries. Early findings from the Oxford team also suggest that poorer nations tended to bring in stricter measures than did richer countries, relative to the severity of their outbreaks. For example, the Caribbean nation of Haiti enforced lockdown on confirming its first case, whereas the United States waited until more than two weeks after its first death to issue stay-at-home orders. That might be because lower-income countries with less-developed health-care systems act more cautiously, says Anna Petherick, a public-policy researcher at Oxford. It could also reflect the fact that the outbreak reached these nations later, giving them longer to learn from others, she says.   Ultimately, researchers hope to use data from the LSHTM database to go beyond examining the differences in responses — and understand how effective these strategies were in limiting the outbreak. “We really need to evaluate those interventions in real time, so everybody can make real policies,” says Eggo, who was not involved in the database’s creation but plans to use it. “If we don’t know what works and we don’t know how much, it’s going to be really difficult to decide what to do next.” Eggo and colleagues will use the data to test the accuracy of mathematical models, which use equations to describe the rate of transmission and mechanisms behind it, under varying intervention types and timing. Ideally, researchers will be able to forecast how adding and removing interventions would change the number of infections over time. Policymakers could use such predictions, together with data on intensive-care capacity, to make decisions — on whether to reopen schools, for example — says Nils Haug, a mathematical physicist at CSH Vienna and the Medical University of Vienna. Haug is part of a 15-strong team of modellers exploring which statistical approaches to use. Rather than directly determining the precise effect of each intervention, these methods can be used to find ways to identify the measures that best predict infection rates. One approach involves using a machine-learning technique called a recurrent neural network to learn from patterns in the data and make predictions. Researchers can learn how important a given intervention is by looking at how predictions shift when they remove information about it from the network.   Another technique involves regression analysis, which estimates the strength of the relationship between a particular measure, such as school closure, and a metric, such as R, across all countries. Using a regression technique such as Lasso, for example, researchers can determine which measures reduce R most. But all methods have limitations, says Haug. The Lasso method assumes that a given measure always leads to the same reduction in R over time, regardless of the country to which it was applied. This is one of the biggest challenges in learning lessons across multiple countries. Researchers want to be able to account for national quirks such as some countries’ greater prevalence of intergenerational households, which could accelerate spread. The Vienna team will eventually try to factor these different features directly into their models. For now, they will capture them all as a single variable that alters R for each country. Without a vaccine or effective treatment, stopping transmission remains the only defence against COVID-19. Knowing the effects of each control measure is crucial to figuring out which ones can be safely altered or removed, says Petherick. “If we can learn what we should put in place and what works best so that we can stop the spread and also keep the rest of life going as best we can, I think that would be a huge contribution,” she says. </body>
<date id = '148'>27 April 2020</date>
<url id = '149'>https://nature.com/articles/d41586-020-01239-2</url>
<title id = '149'>The American Physical Society held its massive April Meeting online because of coronavirus — and registrations soared.</title>
<body id = '149'>The American Physical Society decided to hold its April Meeting online.Credit: American Physical Society Despite some last-minute scrambling, the first major physics conference to be held in cyberspace was a success, according to many attendees. The April Meeting of the American Physical Society (APS) was scheduled to take place on 18–21 April in Washington DC. But when the coronavirus pandemic made a physical gathering impossible, the organizers decided to hold the entire event online and made registration free and open to everyone. Whereas around 1,600–1,800 people typically attend the April Meeting, 7,267 registered this time, says Hunter Clemens, the APS director of meetings. And many participants say they were satisfied. “The virtual APS meeting has been by far the best online meeting I have attended,” says Niels Warburton, an astrophysicist at University College Dublin.   In early March, the APS was one of the first large organizations outside of China — where the first outbreak of the virus was reported — to bear the brunt of the pandemic. The society decided to cancel its much larger March Meeting in Denver, Colorado, just 36 hours before it was due to start. Some of that meeting took place anyway: would-be attendees quickly organized unofficial versions of the scheduled sessions online. Inspired in part by that surge of enthusiasm, the APS opted in late March to hold its next major meeting online, rather than cancelling or postponing it, says David Barnstone, a spokesperson for the society. “It was encouraging to see all the self-organization and everyone coming together online.” The society hired a company to provide the necessary online infrastructure and technological support. During the 4-day conference, it handled 175 live sessions, running up to 15 in parallel. The online platform they used for talks provided a chat window that appeared alongside the speaker’s video, which allowed attendees to exchange comments or links to relevant papers in real time. Astrophysicist Michael Johnson delivers an online talk during the APS meeting.Credit: Davide Castelvecchi/Nature Each parallel session was assigned a dedicated technician to make sure that everything ran smoothly and that attendees followed the APS code of conduct, says Mark Doyle, the society’s chief information officer. “There could be an instance when a speaker is misbehaving, or an attendee is typing something inappropriate into the window.” The APS made an effort to recreate the social experience of a large conference by organizing virtual meet-ups, and some delegates set up their own discussions independently using messaging tools such as Slack. Although a virtual meeting is not the real thing, it was still a good idea given the circumstances, says Xiaochao Zheng, a nuclear and particle physicist at the University of Virginia in Charlottesville. “Many other conferences are cancelled, which are big disappointments for people who had planned to attend,” she says. Lindley Winslow, an experimental physicist at the Massachusetts Institute of Technology in Cambridge, agrees. “In my field of neutrinos and dark matter, we have to do a lot of our meetings virtually. It works, but it is not as efficient as having everyone in the same room,” she says. Still, she adds, because she had a newborn baby at home, “It was a bit of relief to not have to figure out how to travel.” The virtual meeting had some other advantages compared with a physical one. Live talks could be paused or rewound, a useful feature for those who missed details or wanted to spend more time pondering a crucial slide.   And watching talks from home eased a bit of the pressure of attending a large conference that would require dashing from one session to another across a vast convention centre. “I’m kinda loving the minimal FOMO [fear of missing out] when you’re just feeling tired/introverted/overwhelmed that comes along with everyone being virtual,” tweeted Claire Lee, a particle physicist at Fermi National Accelerator Laboratory outside Chicago, Illinois. “I was able to attend a wider variety of sessions than I normally would have, since switching between parallel sessions was far more seamless,” says Julieta Gruszko, a neutrino physicist at the University of North Carolina in Chapel Hill. In the future, she says, it would help to have the speakers and attendees continue the discussion in the same chat window after a session. “The most useful conversations with people I don’t know well or haven’t met before generally happen immediately following the close of a session.” Susan Gardner, a particle physicist from the University of Kentucky in Lexington, says she was initially disappointed with the cancellation of the physical meeting, but that the conference was a positive experience for both her and her students. She praises “the hard work and dedication of the APS staff in bringing the meeting to pass in such short order”. The last-minute transition to cyberspace was not completely smooth, in part because the decision came long after the conference programme had been finalized. Although most speakers agreed to present their talks online, some did not. And some sessions, including many of the talks contributed by students, had to be pre-recorded to be watched ‘on demand’. This created confusion among participants, some of whom found out too late that they had to upload their talk ahead of time. “A lot of people in my session weren’t aware that the talks were being switched from live to recorded,” says Kelly Backes, a graduate student at Yale University in New Haven, Connecticut, who was a speaker in an on-demand session. The APS is still allowing presenters to upload their videos after the meeting, Clemens says. Most attendees contacted by Nature found the conference useful. “My quick take-away is that it was more successful than I thought it was going to be,” Backes says. “I got a lot more out of it than I expected.” She says nothing can replace the experience of face-to-face contact. But being able to watch an online talk — and then mention it when e-mailing the speaker — might help participants to establish professional connections. “Having anything to open an e-mail or a conversation with lowers the barrier to talking to people.” </body>
<date id = '149'>24 April 2020</date>
<url id = '150'>https://nature.com/articles/d41586-020-01253-4</url>
<title id = '150'>MOSAiC mission will return to its frozen-in platform, but disruption caused by team changeover will create a gap in its unique climate data set.</title>
<body id = '150'>Credit: Michael Gutsche When scientists were planning MOSAiC — an epic research expedition that would remain trapped in Arctic sea ice for one year — they considered the North Pole’s hazards. They worried about hypothermia, isolation, crushing ice and polar bears. They had dozens of contingency plans. But no one anticipated a pandemic. The travel restrictions and flight cancellations imposed because of the coronavirus outbreak have now forced mission planners to make a seemingly impossible decision. Polarstern, the German research vessel central to the expedition, will temporarily leave its position in the ice to exchange its crew, and so will be forced to abandon the research camp where it has been frozen since October. The disruption is a blow to the mission’s researchers, who have created a unique platform from which to study climate change in the Arctic, with a data due to be collected continuously over an entire year. Although they hope to refreeze the ship at the same camp after a three-week pause, the interruption will leave a hefty gap in the data set — and potentially miss a crucial time for data collection as Arctic ice begins its springtime melt. “Ideally, we would not leave the ice — but given all the circumstances, I think it’s amazing we were able to come up with a solution to continue the experiment,” says Donald Perovich, a geophysicist at Dartmouth College in Hanover, New Hampshire, and a member of MOSAiC’s project board. The news will come as a relief to the scientists and crew who have had to grapple with the uncertainty of having no return date. The travel stoppages have left the crew stranded since early April, when flights in and out of the Norwegian islands of Svalbard were meant to have swapped the ship’s researchers. “That put them in a much more difficult place psychologically,” says Allison Fong, a polar biologist at the Alfred Wegener Institute in Bremerhaven, Germany, and a co-leader of MOSAiC’s ecosystem team. So MOSAiC coordinators have been working tirelessly to find an alternative plan. But because the coronavirus pandemic has forced airports, military facilities and seaports worldwide to shut down, it has not been easy. The only solution requires Polarstern to fire its engines, break free from the ice and travel to a fjord in Svalbard. There, it will rendezvous with two other ships to swap scientists before returning to the Arctic research camp in roughly three weeks. (That timeline could change depending on the weather and ice conditions.) “The idea of leaving the camp and floe was certainly not something we would have considered in the original rotation plan,” Fong says. “But given what we’re encountering now, I think it’s an important compromise that recognizes that the human dimension to the work we do is very, very important.” The pandemic also means coordinators are using meticulous precautions to ensure that no one carries the virus to the ship. The next rotation of scientists will arrive in Hamburg, Germany, on 1 May and go to Bremerhaven by private bus. There, they will be tested for the virus before going into individual quarantine. Assuming everyone is negative, they will undergo safety training in group isolation. After two weeks, they will travel to Svalbard on two German research vessels and board Polarstern before it returns to the research camp. Although scientists plan to leave the research station mostly intact, certain measurements will have to stop. The remotely operated vehicle that dives into the ocean twice a week will be pulled out of the water. The tethered balloon that monitors the atmosphere will be packed away. And the scientists’ continuous collecting of ice and snow samples across the floe will stop. “We’re going to do the best we can with these constraints,” says Matthew Shupe, an atmospheric and oceanic scientist at the University of Colorado Boulder and co-leader of MOSAiC. “But in the end, it’s a bummer.” That is especially true, given that the gap will probably hit during a crucial time: when the ice begins to melt. Every spring, melting exposes dark ocean water, which absorbs more sunlight than does the ice. That warms the ocean further and spurs more melt in a vicious cycle that scientists are eager to study in detail. It’s also when life in the Arctic flourishes. As sunlight penetrates farther into the ice and upper ocean, sea-ice algae and phytoplankton form massive blooms that provide meals for the rest of the Arctic food web. It has never been studied before in the central Arctic. Now scientists might miss it. But there are a number of autonomous research stations on the ice that will continue to function, taking measurements of, for example, wind speed, temperature, pressure and humidity. Optical sensors measure how much sunlight is transmitted into the ocean and how much is reflected off the surface. And ocean sensors monitor chlorophyll — a proxy for biological activity. “It’s the greatest collection of autonomous instruments the Arctic Ocean has ever seen,” Perovich says. So scientists are hopeful that they will still have baseline information. And even the minimum will create a much larger data set than we’ve ever had during this time period in the central Arctic, Perovich says. That assumes the instruments will remain intact. In reality, they will be vulnerable to changing Arctic conditions — from shifting ice that might crush equipment to curious polar bears. Both scenarios have already occurred during the mission. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '150'>24 April 2020</date>
<url id = '151'>https://nature.com/articles/d41586-020-01219-6</url>
<title id = '151'>From Bangladesh to Somalia, researchers and aid workers are taking different steps to protect people among the most vulnerable to the pandemic.</title>
<body id = '151'>An aid worker provides protective face masks to migrants in Greece.Credit: Manolis Lagoutaris/AFP/Getty There are 70 million refugees, displaced people and asylum seekers around the world. Close living quarters, significant underlying health problems and limited access to sanitation and medical care mean that COVID-19 — which has no known treatment and has brought some of the best health systems in the world to their knees — poses an outsized threat to these communities. Although there are some reports of refugees testing positive for the virus, as of mid-April, there are no known COVID-19 outbreaks in major refugee camps, according to advocates and responders contacted by Nature. But many aid groups fear that it is only a matter of time before the disease strikes. According to advocacy groups, host nations have been slow to enforce preventative measures. And experts fear that aid organizations will struggle to rally and respond. Disease epidemics exact a heavy toll on displaced people, but the coronavirus pandemic poses a new kind of threat, says Annick Antierens, a strategic adviser to the medical department for Doctors without Borders, an international medical-aid organization, in Brussels. “Any kind of epidemic is never good, but particularly not this one, where physical distancing is impossible and home isolation is a joke.” Nature spoke to people in three major encampments across the world who are either involved in assessing the risk COVID-19 poses to refugees or who are working on the frontline to protect them, to ask how camps are preparing for the pandemic. Nearly 600,000 Rohingya people now live in the Kutupalong-Balukhali Expansion Site in Cox’s Bazar, Bangladesh, having fled Myanmar after persecution increased in 2017. It is one of the world’s biggest and most densely populated refugee camps. And it is the first to be used in a model of COVID-19, says Paul Spiegel at the Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland.   Spiegel and his team created a model that projects outcomes at the camp using data, primarily from China, on age, case severity and case fatality rates for the outbreak. The analysis1 was posted on 26 March, and has not been peer reviewed. The group modelled outcomes in low-, moderate- and high-transmission scenarios. There are five hospitals (run by non-governmental organizations and foreign governments) with a total of 340 beds in the encampment at Cox’s Bazar. The worst-case scenario exhausts that capacity just 58 days into the outbreak, and would result in more than 2,000 deaths. Theo Vos, an epidemiologist at the University of Washington in Seattle, who was not involved with the study, says that the modelling approach has limitations, including an assumption of a constant basic reproduction number (R0), a metric used to measure the transmission of a disease. In reality, Vos says, R0 for the coronavirus has varied over time. Spiegel says the results — even for the worst-case scenario — are probably underestimates. The toll the virus will take on groups with poor underlying health and malnutrition is still unknown. And when on-site hospitals are overwhelmed with COVID-19, the deaths from other diseases such as malaria might spike. “The death rate could be higher,” Spiegel notes. A spokesperson for the United Nations High Commissioner for Refugees (UNHCR), says that the organization has used the Johns Hopkins model to guide their response alongside partner humanitarian organizations. Although COVID-19 has not yet taken hold in Cox’s Bazar, it is having an impact. An aid worker who asked not to be named, because the humanitarian environment is sensitive and they cannot speak on behalf of the organization they work for, says that local NGOs have decreased the number of staff going into camps to reduce the risk of bringing in the virus. Exceptions are made for staff who provide crucial services, such as health care and food distribution. Some programmes that supported a large number of residents, including education and mental-health counselling sessions, have been stopped or reduced in size.   The aid workers still allowed in are training trusted community leaders, such as leaders of youth and women’s groups, teachers and religious leaders, to explain facts about the virus to residents rather than having aid workers do it directly. And, according to UNHCR, the coronavirus response might interfere with preparations for monsoon rains, expected in June. In the midst of these complications, aid workers are grappling with a unique communication challenge. In September 2019, Bangladesh restricted access to cell-phone service at the site. Residents are unable to communicate with each other quickly, or to access reliable information online. This has boosted the spread of misinformation among Rohingya residents about the virus. NGOs surveyed the residents and found that they believed inaccurate rumors, for example that health-care workers will kill those infected with the virus, and that the disease is not communicable. Aid groups fear that as a result, residents will be reluctant to get medical attention when they encounter COVID-19-like symptoms. Five Aegean islands off the coast of Greece host refugees and asylum seekers from the Middle East and south and central Asia before they can enter the country. They arrive at the island entry points, where Greece has facilities for about 6,000 people awaiting asylum decisions. But the camps have swelled to nearly 40,000 people over time. Many are living in rudimentary encampments among the olive groves with limited access to running water and toilets.   “They are living in tents, makeshift camps and carton boxes. It’s a highly overcrowded situation,” says Apostols Veizis, director of the medical operational support unit in Athens, Greece, for Doctors without Borders. Veizis co-wrote an editorial2 in the British Medical Journal in March, calling on European governments to include migrants in their response plans, and to vacate the island camps as a precautionary measure. Other groups, including Human Rights Watch and the International Rescue Committee, have issued a similar call. Although other parts of the world are prioritizing social distancing and lockdowns to slow the spread of coronavirus, the options are limited in a refugee camp. “There’s not really an ability to self-isolate or quarantine within these communities,” says Devon Cone, a senior advocate at Washington DC-based Refugees International, a group that advocates for displaced people. Volunteers distribute cleaning supplies to people at a refugee camp in Mogadishu, Somalia.Credit: Sadak Mohamed/Anadolu Agency/Getty Instead, in response to the pandemic, Doctors without Borders is prioritizing additional water and sanitation facilities at the camps in Greece. Veizis says there is a plan to protect about 2,400 high-risk residents — those over 60 and those with chronic conditions — by moving them to hotels on the mainland. And in April, the EU pledged €350 million (US$377 million) to support refugees and asylum seekers, including those in Greece. In Mogadishu, Somalia, gynaecologist Deqo Mohamed read updates from the US Centers for Disease Control and Prevention and the WHO through February, March and April. She’s concerned about the 300,000 people from within the country who have been displaced by conflict and natural disasters to camps along the so-called Afgooye corridor. As of early April, the UNHCR said that the nation’s 2.6 million internally displaced people were at high risk of catching the virus.   Unlike many others, these encampments outside Mogadishu are built on private land, with ‘gatekeepers’ who police entry, including by health-care workers. That means that the government does not have access to these spaces, nor do international humanitarian groups. Mohamed, who has worked alongside her Somali mother and sister to provide health services to displaced women in the region for years, has pivoted to training midwives and medical students to communicate social distancing, hand-washing and other hygiene information to communities at 12 sites, to prepare for the pandemic. She has closed her clinics in Mogadishu and the corridor except for deliveries and emergency cases. As part of the national response, Mohamed says she is part of an international team of Somali doctors and those from the diaspora who are helping the government create a national call center that can triage calls from people who have COVID-19 symptoms by phone, so people can avoid leaving their homes to visit a clinic. “We don’t have medical staff who are ready to handle COVID-19, we don’t have PPEs, we don’t have testing, we don’t have ventilation sites. Name it. Everything that the rest of the world is fighting for in COVID-19, it is actually zero in our site,” she says. </body>
<date id = '151'>24 April 2020</date>
<url id = '152'>https://nature.com/articles/d41586-020-01165-3</url>
<title id = '152'>With politicians touting the potential benefits of malaria drugs to fight COVID-19, some people are turning away from clinical trials of other therapies.</title>
<body id = '152'>Hydroxychloroquine is used to treat malaria and some auto-immune diseases.Credit: Manish Swarup/AP/Shutterstock People with COVID-19 who arrive at the Salvador Zubirán National Institute of Medical Sciences and Nutrition in Mexico City to search for treatment can choose from a menu of clinical trials, carefully presented by a worker trained to offer an unbiased portrait of the potential risks and benefits. But neurologist Sergio Iván Valdés-Ferrer already knows which trial most will choose — and it’s not his. Instead, many people opt for one involving hydroxychloroquine, a malaria drug that has been touted by US President Donald Trump and other influential figures as an effective coronavirus treatment. High demand for the drug in Mexico has quickly depleted the country’s supply. Its use is now limited to hospitals, and patients are eager to ensure that they receive it. “There’s a tremendous bias,” says Valdés-Ferrer, who is studying the effects of a dementia drug on COVID-19. “Studies of any other drug that are enrolling all ages and degrees of severity are in big trouble.”   Hydroxychloroquine and its close chemical cousin chloroquine have attracted disproportionate attention in the coronavirus pandemic, spurred by preliminary studies and endorsement from political leaders such as Trump and French President Emmanuel Macron. So far, there is very little data backing the idea that hydroxychloroquine works against coronavirus infection, yet the fervour surrounding it has created drug shortages and affected enrolment in clinical trials for other potential treatments. “When you have desperate people and, frankly, desperate doctors, you want to believe that you have something that works,” says Daniel Kaul, an infectious-disease specialist at the University of Michigan Medical School in Ann Arbor. “But at the end of the day that doesn’t help anybody if it isn’t effective, and if it precludes people from participating in other studies.” The US Food and Drug Administration (FDA) has approved chloroquine and hydroxychloroquine to treat malaria and, because of their anti-inflammatory properties, some autoimmune diseases such as rheumatoid arthritis and lupus. In February, researchers showed that chloroquine could reduce coronavirus infection of human cells grown in the laboratory1. A few days later came a report of clinical trials on patients with COVID-19 in ten hospitals in China. Taken together, these trials suggested that chloroquine treatment might shorten the duration of the disease2. Since then, a handful of small studies have been reported, but none have shown definitively whether or not chloroquine or hydroxychloroquine can benefit COVID-19 patients. Some research has even suggested that the treatment could cause harm in people with COVID-193. “Thus far, all of these studies published have been very small and not robust,” says Richard Whitlock, a surgeon and intensive care physician at the Population Health Research Institute in Hamilton, Canada. “Small trials can swing on both sides — from ‘yes it looks great’ to ‘no it’s harmful’ — and that’s what we’re seeing.”   But the findings were sufficient for some of the world’s most high-profile politicians, keen to offer worried voters and suffering economies a glimmer of hope. Trump claimed that he considered taking chloroquine as a precautionary measure. Hospitals in Iran, New York, Spain and China turned to hydroxychloroquine and chloroquine as a standard therapy for patients with COVID-19, despite guidance from the World Health Organization and several medical associations that the drugs should not be used against COVID-19 except in clinical trials. In treatment guidelines released on 21 April, the US National Institutes of Health noted that there is insufficient data to recommend either for or against the use of chloroquine and hydroxychloroquine in people with COVID-19. “Doctors are just trying it because they want to offer their patients something,” says Lauren Sauer, an emergency-medicine researcher at Johns Hopkins University in Baltimore, Maryland. “Anecdotes have turned into evidence, it seems.” Demand also soared outside hospitals. On 28 March, the FDA issued an emergency permit to use of national stockpiles of chloroquine and hydroxychloroquine to treat patients with COVID-19 — a move that many people took as a sign of the agency’s faith in the drugs’ efficacy. The resulting race to take, or even to hoard, chloroquine led to global shortages, and people who take it to treat autoimmune diseases struggled to access supplies. Meanwhile, there were reports of illness and deaths linked to chloroquine overdoses in the United States and Nigeria. Speed is crucial in the hunt for COVID-19 treatments, and a slow-down in clinical trials could cost lives. Some people don’t want to participate in clinical trials that would require them to give up chloroquine treatments. This has made it difficult to enrol people into his trial of HIV drugs as potential COVID-19 treatments, says infectious-disease specialist Sung-Han Kim at the University of Ulsan College of Medicine in Seoul . Kim’s story isn’t unique. Psychiatrist Eric Lenze of Washington University in St. Louis, Missouri, recently launched a trial of an antidepressant that he hopes could lessen the immune response linked to some severe COVID-19 cases. The trial has so far enrolled ten participants; three others declined to take part because they were already planning to take hydroxychloroquine. And Sauer says that she and her colleagues have encountered similar resistance while enrolling participants into a trial of the antiviral drug remdesivir. As a result, some researchers are making compromises in study design. In Iran, pathologist Alireza Ghaffarieh gave up his plans to exclude chloroquine treatment from his trial of an iron-chelating medicine in people with COVID-19 at the Kermanshah University of Medical Sciences. “The patients have already started with two or three drugs,” he says. “They are never happy to drop this medication.” Instead, he accepts participants who might be taking other medications, and hopes that this will not complicate interpretation of his results.   Delays in clinical-trial enrolment can endanger a trial, particularly during a pandemic, says Prashant Malhotra, an infectious-disease specialist at North Shore University Hospital in Manhasset, New York. In some outbreaks, it’s best if trials can be completed early, he says, because as health-care systems become overwhelmed, the quality of care could decline. “You’re running out of basic supplies and may not be able to provide the same level of care as what was given earlier,” he says. “The results of clinical trials that were done early on may not be comparable to those done later in pandemic settings.” Data from clinical trials can also be skewed by excluding participants who refuse to give up chloroquine treatment. Chloroquine can cause heart arrhythmias, for example, and so may not be given to some people who have pre-existing heart problems. That means that a trial that excludes chloroquine-takers could end up enrolling a disproportionately large number of participants with heart conditions, says Malhotra. But allowing participants to take chloroquine could muddle efforts to interpret data. Chloroquine’s effects, especially its effect on the immune system, could take days to fade, says Malhotra. This could make it difficult for trial investigators to distinguish the effect of the treatment they are testing from the effect of the chloroquine. Researchers might have settled some of these questions weeks ago if there had been a rapid, international effort to develop a rigorous chloroquine clinical trial, says Ole Søgaard, an infectious-disease physician at Aarhus University Hospital in Denmark. Now, there are more than 100 clinical trials that aim to test chloroquine or hydroxychloroquine against COVID-19. It’s a worthwhile effort, he says, despite the lack of evidence supporting the drugs in that setting. “Being able to cross something like hydroxychloroquine off the list and move on to other things would be a major achievement,” he says. “Then you could shut down a lot of trials and replace them with something you believe in.” </body>
<date id = '152'>24 April 2020</date>
<url id = '153'>https://nature.com/articles/d41586-020-00154-w</url>
<title id = '153'>Nature recorded major events as the pandemic spread across the globe.</title>
<body id = '153'>The first US deaths related to coronavirus might have occurred weeks earlier than previously thought.Credit: Andrew Caballero-Reynolds/AFP via Getty We are no longer updating this page. Find Nature’s latest coronavirus coverage here. Read highlights from the coronavirus research literature here. 22 April 17:05 bst — Deaths suggest the coronavirus was in the United States weeks earlier than thought The first US COVID-19 death might have occurred in California on 6 February — three weeks before the first reported death, in Washington state. After autopsies, three people who died in Santa Clara County between 6 February and 6 March have been confirmed as having died of COVID-19, according to a statement released by the county’s department of public health on 21 April. The updated statistics include two people who died at home and a third whose location of death was not specified. Previously, the first COVID-19 death in the county was thought to have occurred on 9 March. The revised cause of death shows that the deadly disease had footholds in the United States earlier than previously thought. Similar reports have surfaced elsewhere in recent weeks. In late March, a non-peer-reviewed epidemiology study of the Lombardy region in northern Italy found that the virus might have been circulating there for more than a month before it was detected. 22 April 16:45 bst — Climate scientist and IPCC veteran dies of the coronavirus John Houghton, a climate scientist and a senior member of the Intergovernmental Panel on Climate Change (IPCC), died of COVID-19 on 15 April, aged 88. Houghton was the lead editor of the first three IPCC assessment reports — massive, influential studies that summarize the state of scientific knowledge on climate change — and accepted the 2007 Nobel Peace Prize on behalf of the organization, alongside former US vice-president Al Gore. Researchers who knew Houghton commended him for his scientific rigour, his leadership and his ability to connect with policymakers and the public. After studying at the University of Oxford, UK, Houghton became a professor there in 1958. He was a pioneer in the use of emerging satellite technologies for understanding the Earth system. As the director-general of the UK Met Office, the nation’s weather agency, from 1983 to 1991, he helped to establish the Hadley Centre for Climate Science and Services in Exeter and advised the UK government on the scientific consensus on climate change. “He knew they could only really make progress if they had very clear and well-communicated scientific advice,” says Richard Betts, a climate scientist at the University of Exeter, who worked on several IPCC reports with Houghton. Houghton’s lifetime of leadership in the climate-change community is a significant legacy, Betts says. “He wasn’t just a scientist. He was someone who cared passionately.” A devout Christian, Houghton co-founded and led the John Ray Initiative, a charity in Gloucester, UK, focused on promoting environmental stewardship through both science and Christianity. He convinced a prominent US evangelical lobbyist of the compatibility of climate change and Christian thought in the early 2000s. His former students remember him fondly. “John was an excellent leader,” says James Drummond, a retired atmospheric scientist who most recently worked at Dalhousie University in Halifax, Canada. Houghton supervised Drummond through his undergraduate and graduate studies at Oxford in the 1970s. “If I do a quarter of what he did, I will be doing very well,” says Drummond. 21 April 20:40 bst — 1,500 people volunteer for controversial vaccine-study approach Momentum is building to speed the development of coronavirus vaccines by intentionally infecting healthy, young volunteers with the virus. A grass-roots effort has attracted nearly 1,500 potential volunteers for the controversial approach, known as a human-challenge trial. The effort, called 1Day Sooner, is not affiliated with groups or companies developing or funding coronavirus vaccines. But co-founder Josh Morrison hopes to show that there is broad support for human-challenge trials, which have the potential to deliver an effective coronavirus vaccine more quickly than standard trials. Typical vaccine trials take a long time because thousands of people receive either a vaccine or a placebo, and researchers track who becomes infected in the course of their daily lives. A challenge study could in theory be much faster: a much smaller group of volunteers would receive a candidate vaccine and then be intentionally infected with the virus, to judge the efficacy of the immunization. “We want to recruit as many people as possible who want to do this, and pre-qualify them as likely to be able to participate in challenge trials should they occur,” says Morrison, who is also the executive director of organ-donation advocacy group Waitlist Zero. “At the same time, we feel that the public policy decisions around challenge trials will be better informed if they highlight the voice of people interested in participating in such trials.” Morrison says that the people who have signed up to be part of a challenge trial tend to be young and live in urban areas, and are highly motivated to do something constructive to address the coronavirus pandemic. “Many note that they recognize the risk but believe the benefits of vaccine acceleration are so tremendous that it is worth it to them,” he says. A team led by bioethicist Nir Eyal at Rutgers University in New Brunswick, New Jersey, argued that a human challenge trial could be conducted safely and ethically, in a paper in The Journal of Infectious Diseases last month. The approach is also gaining some political support. This week, 35 members of the US Congress, led by Bill Foster (Democrat, Illinois) and Donna Shalala (Democrat, Florida), called on Department of Health and Human Services director Alex Azar to consider human-challenge trials of coronavirus vaccines. Charlie Weller, head of the vaccines programme at Wellcome, a biomedical-research funder in London, says the charity has begun discussing the ethics and logistics of a human-challenge trial for a coronavirus vaccine. But she says it is unclear whether such a trial could actually speed vaccine development. Researchers first need to determine how to expose humans to the virus as safely as possible, and to consider how and even whether such studies can be done ethically. “I think there’s potential,” Weller adds, “but we’ve got so many questions to work through to understand whether it can help in the timelines we have.” 21 April 15:30 bst — Coronavirus brings forward Mars plans The Arab world’s first Mars mission — a spacecraft called Hope — will ship from the United Arab Emirates (UAE) to Japan weeks earlier than planned, as a result of travel restrictions imposed during the COVID-19 pandemic. Accelerating the shipment will mean that the probe, which will orbit the red planet and study the Martian atmosphere, will miss out on some planned tests. “We had to expedite activities in Dubai and basically focus only on the critical testing,” project leader Omran Sharaf told a virtual meeting of the Mars Exploration Program Analysis Group, a NASA-led international forum for planning Mars activities, on 17 April. “It got too risky for us if we waited,” he said. Built by UAE and US engineers, the orbiter is scheduled to launch from Tanegashima Space Center in Japan in a window that starts on 15 July, when Earth and Mars are suitably aligned. The craft should reach Martian orbit in 2021. The United Arab Emirates’ Mars probe at the Mohammed Bin Rashid Space Centre in Dubai.Credit: Natalie Naccache for Nature Hope was scheduled to ship to Japan by cargo plane in early May. But mission leaders brought the date forward to 20 April, to account for a 14-day quarantine imposed on all travellers entering the country. Team members who travel with the probe will be quarantined for two weeks when they reach Japan, but moving the spacecraft early gives them enough time to arrive at the site and prepare for launch. A team that will receive the craft in Japan has already travelled to the nation and been in quarantine. The mission team has also had to navigate new restrictions on travel worldwide, including getting approval to fly at all, said Sharaf. The UAE’s airspace and airports are closed, and Japan has enhanced its visa requirements. Last month, the European Space Agency and the Russian space agency Roscosmos delayed launch of a major rover in their ExoMars mission for two years, citing travel restrictions imposed as a result of the coronavirus pandemic among their reasons. Relatives of a person who died of coronavirus at a burial in New Delhi.Credit: Danish Siddiqui/Reuters 17 April 16:00 bst — Wuhan death toll jumps by 50% China’s reported death toll from the coronavirus has jumped sharply, after a review of records for the city of Wuhan, where the virus emerged last year, added another 1,300 fatalities to its official count. The revision raises the death toll in the 11-million-person city by 50%, to 3,869, and brings China’s overall death toll to more than 4,600. Deaths from the coronavirus are difficult to count in the absence of widespread testing. Chinese officials said the revision included the addition of deaths of people at home and at medical institutions that weren’t reporting data to its epidemic network. 15 April 15:00 bst — Infections pass two million The number of reported COVID-19 cases worldwide has passed two million, according to data compiled by researchers at Johns Hopkins University in Baltimore, Maryland. The milestone comes just two weeks after one million infections were recorded. The United States has the most cases — more than 600,000 — followed by Spain and Italy. The death toll from the disease has also surged past 128,000 worldwide. Some 24,000 people have died in the United States, with more than 20,000 deaths in Italy. The pandemic has spread to almost every region of the globe, with only about a dozen of the World Health Organization’s member states not yet reporting cases. The majority of these are small island nations in the Pacific Ocean, including Vanuatu, Tuvalu and Palau. Coronaviruses take their name from their crown-like shape.Credit: Getty 15 April 02:00 bst — Trump suspends World Health Organization funding US President Donald Trump said on 14 April that he would halt the country’s funding for the World Health Organization (WHO), pending a review of how the organization has handled the coronavirus pandemic. The president accused the WHO of mismanaging the outbreak and “covering up the spread” of COVID-19 in a White House press briefing on Tuesday evening. The announcement comes amid criticism of the Trump administration’s own handling of the outbreak, which has now claimed more than 26,000 lives in the United States. But the WHO warned on 23 January that the coronavirus would be exported from China. And on 30 January, the organization called the coronavirus outbreak a public health emergency of international concern — its highest alert level. The United States has contributed more than US$800 million to the WHO over the past two years, making it the largest contributor to the organization’s roughly $2.8-billion annual budget. 14 April 17:00 bst — Mathematician John Conway dies of COVID-19 John Conway.Credit: Princeton University/Office of Communications/Denise Applewhite Renowned British-born mathematician John Conway died aged 82 on 11 April, reportedly of complications arising from COVID-19. Conway attained a legendary status among maths and science enthusiasts, mostly by designing mathematical games and puzzles. But his work transcended the boundary between recreational and ‘serious’ maths, turning play into research and vice versa. “He’s been known to carry on his person a few decks of cards, dice, ropes, pennies, coat hangers, sometimes a Slinky, maybe a miniature bicycle, all props he deploys to extend his winning imagination,” wrote biographer Siobhan Roberts in 2015. His celebrity status was cemented by Martin Gardner’s popular maths column in the magazine Scientific American, which featured many of Conway’s contributions, beginning in 1970 with his Game of Life, a minimalistic 2D model universe in which ‘organisms’ grow and multiply on the basis of simple rules. Conway had taught at Princeton University in New Jersey since 1987, and his research spanned several fields of mathematics. In algebra, he studied the ‘monster group’, an object that has an enormous — but not infinite — number of symmetries. With the late British mathematician Simon Norton, he formulated the ‘monstrous moonshine’ conjectures, linking the monster group to an entirely different subject in mathematical analysis. Conway is also remembered for his work on the foundations of quantum mechanics. He investigated the idea of an experimenter’s free will and suggested that “if experimenters have free will, then so do elementary particles”. Medical staff at a drive-in COVID-19 testing facility in the UK.Credit: Paul Ellis/AFP/Getty 9 April 22:30 bst — United Kingdom launches massive coronavirus diagnostic network The UK government opened its first mass coronavirus-testing facility on 9 April. The lab, in Milton Keynes, is the first of three such facilities to open. The others — in Glasgow and Alderley Park in Nether Alderley — are scheduled to open within the next two weeks. In March, the UK government requisitioned polymerase chain reaction machines from university labs across the country in order to outfit these central testing facilities. The labs — which the government says make up the largest diagnostic network in UK history — will prioritize processing samples from health-care providers who are currently self-isolating, in order to allow them to return to work. The Milton Keynes facility can currently process thousands of tests per day, but is continuing to ramp up its capacity through the use of robotics. The government hopes to be able to analyse 100,000 coronavirus tests daily by the end of the month, health secretary Matt Hancock reiterated on 9 April. Fewer than 300,000 tests have been carried out in the country so far, according to official reports. Universities across the United Kingdom and around the world are also running COVID-19 diagnostic tests, as Nature previously reported. But these efforts in the United States have been hampered by bureaucratic and logistical barriers, and a lack of a cohesive national strategy. 8 April 22:00 bst — CERN scientists join the COVID-19 fight Beniamino Di Girolamo coordinates the CERN Against COVID-19 taskforce, an effort to bring the particle physics lab's resources to bear on the pandemic.Credit: CERN More than 100 researchers and staff members at CERN are finding innovative ways to combat the coronavirus pandemic. Scientists, engineers and technicians at the world’s largest particle-physics laboratory, near Geneva, Switzerland, are teaming up to fill crucial gaps in the local and international responses to the outbreak — from manufacturing and distributing large quantities of hand sanitizer to designing an open-source ventilator. One of CERN’s strengths is its ability to connect people across a wide range of expertise and locations, says Beniamino Di Girolamo, a CERN particle physicist and the chair of the CERN Against COVID-19 task force. The group is working closely with local agencies, biomedical experts and the World Health Organization to ensure that CERN’s resources are being put to best use and that its designs are safe for patients.   One of its main projects so far is the design of the High Energy Physics Community Ventilator, HEV. Because the researchers and technicians who work on CERN’s Large Hadron Collider have extensive experience in managing gas flows and control systems, Di Girolamo says, they were well-positioned to take on this project. The team posted its ventilator designs on the arXiv preprint server on 1 April and is currently soliciting feedback on a prototype from several medical professionals. The design should be ready for production “in a month, maximum”, says Di Girolamo. CERN staff are also manufacturing 3D-printed masks and face shields, and making hand sanitizer for local emergency-response departments. The centre is offering high-performance computing resources to epidemiologists and virologists searching for a COVID-19 vaccine. And some staff are distributing necessities to elderly and otherwise at-risk community members. Going forward, Di Girolamo says, the task force plans to continue to source ideas from CERN staff, as well as taking requests from community partners in need of its expertise. “We cannot just lock down,” Di Girolamo says. “There is a lot of energy at CERN. There are a lot of people who can help.” 8 April 16:00 bst — Tracking-app data suggest loss of smell is a key COVID-19 symptom The loss of sense of smell and taste should be considered a symptom of COVID-19, suggest the first results from a UK-based symptom-tracking app. There have been many anecdotal reports of the phenomenon in relation to COVID-19, but loss of smell — known scientifically as anosmia — is not currently listed as a symptom of the coronavirus by the World Health Organization (WHO). The COVID Symptom Tracker smartphone app, which has recruited more than 1.5 million people in the United Kingdom, asks users to record health information on a daily basis, including their temperature, any tiredness and other potential symptoms of coronavirus infection. An analysis of data collected between 24 and 29 March found that users who tested positive for COVID-19 were three times more likely to report losing their sense of smell and taste than were those who had symptoms of the virus but tested negative. Other common symptoms experienced by people who tested positive for COVID-19 were fever, persistent cough, fatigue, diarrhoea, abdominal pain and loss of appetite. Source: COVID Symptom Tracker team “This is an important study because it is the first to demonstrate scientifically and in a large population sample that loss of smell is a characteristic feature of COVID,” said Trish Greenhalgh, a primary-care health scientist at the University of Oxford, UK, in a statement to the Science Media Centre in London. The analysis, which was published on the preprint server medRxiv on 7 April, looked at responses from app users who were tested for the virus, which in the United Kingdom is usually done only in hospitals. Of the 579 who tested positive for COVID-19, 59% reported losing their sense of smell, compared with 18% of the 1,123 who tested negative. Updated but as-yet unpublished figures from the same group show a similar trend (see ‘Tracking symptoms’). The study’s authors say that people experiencing loss of smell should self-isolate. The team behind the app, at King’s College London and health-care start-up ZOE Global in London, also used the data to estimate the prevalence of COVID-19 among people who have not yet been tested. In an unpublished analysis described on their website, the researchers estimated that on 1 April, 4.9% of all users — around 80,000 individuals in the United Kingdom — are likely to have had the virus. By taking into account the location, age and gender of their users, and extrapolating their sample to the whole population, the researchers estimated that 1.9 million people in the United Kingdom aged between 20 and 69 had symptomatic COVID-19 on this date. The latest data suggest that by 5 April, this figure might have fallen to 1.4 million, which they say indicates that social-distancing measures implemented in the nation are slowing the spread of the virus. Efforts are under way to roll out a similar app in the United States. 8 April 07:00 bst — Modelling reveals the worst-case scenario that Australia has avoided Australia has implemented strict physical distance measures. Credit: Cameron Spencer/Getty Australia’s health-care system has averted disaster and is in a good position to cope with the projected rise in COVID-19 cases, thanks to measures introduced there, say researchers whose models have informed the government’s response. Some of these models, presented to the government in early February, were released on 7 April by the Peter Doherty Institute for Infection and Immunity and the University of Melbourne. The theoretical models, which have not been peer reviewed and are based on information on outbreaks outside Australia, suggest that without measures to mitigate the virus’s spread, hospitals would have been overwhelmed. This scenario would have seen a peak daily demand for 35,000 intensive care unit (ICU) beds by around May, according to government figures. The figures suggested that social distancing, isolation and quarantine would reduce that peak daily demand to less than 5,000 beds. Australia currently has some 2,200 ICU beds, but plans to increase that to 7000. Australia restricted entry to the country for travellers from mainland China on 1 February. Health officials have conducted more than 304,000 coronavirus tests, and have introduced contact tracing, quarantining and isolation, as well as broader physical-distancing measures, such as closing businesses and restricting gatherings. “Australia has acted sufficiently early based in part on the work that we provided,” said James McCaw, an infectious-diseases epidemiologist at the University of Melbourne who contributed to the models, at an online press conference. Australia currently has more than 6,000 confirmed cases, with 50 deaths. The daily count of new confirmed cases has declined to around 100, from a high of more than 500. “It looks like the curve is flattening, and flattening strongly,” said McCaw. “But our population is still largely susceptible. And so if we relaxed and went back to normal, we would see a rapid and explosive resurgence in epidemic activity.” The authors conclude that social distancing needs to be part of ongoing efforts to isolate infected people and quarantine contacts to ensure that hospitals can cope, although they did not look at the effects of specific measures. “We’re really in a very lucky position where we can think about the next steps and the very challenging questions ahead, but from a position of relative calm as opposed to crisis,” said McCaw. Medical workers perform drive-through swabbing tests in Alessandria, Italy.Credit: Miguel Medina/AFP/Getty Scientists welcomed the government’s decision to share the modelling that has informed its COVID-19 response. The move will allow experts to analyse the model and make recommendations, says Mikhail Prokopenko, a complex-systems researcher at the University of Sydney. At the moment, the model does not include detailed predictions of what might actually occur in Australia, which would be useful. Australia’s proactive response and strong health system has put it in a position to control the virus — a privilege that not many countries can afford, said Jodie McVernon, an epidemiologist at the Doherty Institute, who led the modelling study. The researchers will now refine their models with data on the emerging outbreak in Australia to assess the effectiveness of individual measures. 7 April 21:00 bst — No new reported COVID-19 deaths in China For the first time since January, Chinese health officials reported no new deaths from COVID-19 on 6 April. According to the Chinese National Health Commission, there were 32 new confirmed cases and 12 new suspected cases in the country yesterday. All were imported into the country from elsewhere. There are also 1,033 people with the infection but no symptoms who are under medical observation.   Scientists and public-health officials are keeping a close eye on the situation in China as lockdowns begin to lift in response to the slowing of the outbreak there. Since the outbreak began in December, there have been more than 80,000 reported cases in China, including over 3,300 deaths — the majority of which occurred in Wuhan, where the pandemic originated. The restrictions on movement into and out of Wuhan are lifting on 8 April. 6 April 20:50 bst — HIV researcher among scientists who have died of COVID-19 Gita Ramjee, a world-renowned HIV researcher, died of COVID-19 last week. She was 63. Ramjee, who was based in South Africa, was known for her tireless work developing HIV-prevention tools such as vaginal microbicides, drugs designed to help stop the transmission of the virus to women. She ran many trials in South Africa — which has the world’s largest number of people with HIV — and often focused on at-risk populations such as sex workers. HIV researcher Gita Ramjee.Credit: The Aurum Institute Ramjee was the chief scientific officer at the Johannesburg-based Aurum Institute, a health-care non-profit organization aimed at eradicating HIV and tuberculosis. Scientists who worked with her praised her tenacity, passion and ability to connect with the communities in which she worked. After completing her PhD in paediatric kidney diseases at what is now the University of KwaZulu-Natal in Durban, South Africa, Ramjee joined the country’s Medical Research Council. It was there that she found her calling as an HIV researcher with a focus on microbicides. Ramjee had a “natural talent” for clinical-trial work, says Salim Abdool Karim, an epidemiologist at the Centre for the AIDS Programme of Research in South Africa (CAPRISA) in Durban. Such work is by nature often difficult and thankless, says Abdool Karim. But despite a series of underwhelming trial outcomes in the field and in her own work, he says, Ramjee never hesitated to dive into the next study. “It takes a particular kind of person to persist despite a string of negative results. And she was one of those.” Ramjee was also an active advocate for women, says Quarraisha Abdool Karim, another CAPRISA epidemiologist. “Gita was a unique investigator who understood that clinical trials are not just about clinics and laboratories and products, but fundamentally about people,” says Mitchell Warren, the executive director of AVAC, previously known as the AIDS Vaccine Advocacy Coalition, in New York City. “These were not trials about products. These were trials about women’s lives.” Several other renowned scientists have also died from COVID-19 in recent weeks. Among them are John Murray, a pioneering tuberculosis clinical researcher at the University of California, San Francisco; James Goodrich, a paediatric neurosurgeon at Einstein College of Medicine in New York City, known for separating conjoined twins; and molecular biologist Michael Wakelam, the director of the Babraham Institute in Cambridge, UK. Ramjee’s colleagues note the cruelty of losing her to the COVID-19 pandemic when she devoted so much of her life to eradicating another global outbreak. But, Warren notes, researchers tackling COVID-19 would do well to follow the example that Ramjee set. “We mourn. And then we do what Gita would do.” 2 April 21:00 bst — COVID-19 cases cross the one-million mark More than one million cases of COVID-19 have been confirmed as of 2 April, according to data compiled by researchers at Johns Hopkins University in Baltimore, Maryland. But in much of the world, people exhibiting mild or no symptoms are unable to get tested, meaning that the true number of cases could be much higher.  The outbreak, which started in Wuhan in Hubei province, China, in early December, has now spread to 181 countries and regions. China was relatively successful in containing its outbreak to Hubei. But the virus has spread rapidly between and within other countries. China accounted for almost 80% of the first 100,000 patients identified, but the United States now contains more cases than any other country, with more than 20% of the world’s total. The pandemic has resulted in a reported 51,485 deaths around the globe, and more than 209,000 people have been identified as having recovered from the disease. 2 April 00:00 bst — COVID-19 delays COP26 International climate talks scheduled for November in Glasgow, UK, have been postponed until 2021 as a result of the ongoing coronavirus crisis, United Nations officials announced on 1 April. The climate meeting, known as COP26 — the 26th annual conference of the parties to the UN Framework Convention on Climate Change — was to be the most important set of talks since the signing of the Paris climate agreement in 2015. So far, international commitments to reduce carbon emissions have fallen well short of what would be needed to prevent temperature increases of more than 1.5–2 °C above preindustrial levels, the stated goal of the Paris agreement. Countries were expected to update and strengthen their commitments at the Glasgow meeting.   UN climate-change executive secretary Patricia Espinosa called COVID-19 the most urgent threat currently facing humanity, but stressed that climate change remains the biggest long-term danger. As the coronavirus threat recedes, she said, nations must look for ways to bolster their climate efforts. “This is a chance” Espinosa said “to shape the twenty-first-century economy in ways that are clean, green, healthy, just, safe and more resilient.” The announcement followed a decision to delay a preparatory meeting, originally scheduled for June in Bonn, Germany, until October. 1 April 03:00 bst — Nearly 80% of US intensive-care patients have underlying conditions More than three-quarters of people with COVID-19 in intensive-care units in the United States have at least one ‘underlying condition’ — a chronic health problem, such as diabetes or heart disease, that has been shown to contribute to hospitalization and severe illness. The finding comes from the 31 March Morbidity and Mortality Weekly Report, published by the US Centers for Disease Control and Prevention. A study found that of about 7,000 people with COVID-19 for whom information about chronic conditions had been reported, just over one-third had an underlying condition. People with such conditions made up 71% of those hospitalized for COVID-19 and 78% of those who required intensive care. Data from China and Italy have also shown that underlying conditions correlate with more severe COVID-19 outcomes, but this is the first such study in the United States. As of last week, the United States has more confirmed cases than any other country. A woman looks out of her balcony in locked-down Rome.Credit: Antonio Masiello/Getty 31 March 16:30 bst — Thousands of Italians take part in citizen-science project Images and videos of locked-down Italians singing on their balconies to applaud health-care workers have circulated the world. But last week, thousands of citizens were out for another cause: science. For three evenings, on 23–25 March, 6,000 Italians appeared on their balconies to take part in a citizen-science experiment that has never attempted before — measuring light pollution with their smartphones. The project, called Science on the Balcony and run by the Italian National Research Council, asked participants to turn off all the lights in their homes and launch an app designed for the study. Then they were asked to turn their phone screens towards the main light source that could be seen from their windows — for example, a street light or a sign. Using the phone’s sensors, the app measured the light source’s illuminance, or brightness, in units of lux. The study was conceived by Luca Perri, an astrophysicist and science communicator at the Astronomical Observatory of Milan, and Alessandro Farini, a vision scientist at Italy’s National Institute of Optics in Florence. Like many nations, Italy has seen a steady increase in night-time light in recent decades. Such light pollution compromises astronomers’ view of space and presents environmental, economic, safety and public-health problems; it can, for instance, affect the immune system.   But widespread data collection on light pollution requires a significant investment of time and money, and might need researchers to visit homes or place sensors there. And satellites detect only light reflected skywards, so they don’t give a full picture. The latest project allowed researchers to measure light inside homes, harnessing the collaboration and enthusiasm of locked-down citizens. Initial data confirm that people in every Italian province participated. And Farini sees another advantage for science. “This pandemic risks creating doubts about science, because a lot of fake news is circulating,” he says. “With this experiment, we wanted to bring citizens closer to measurement techniques, to let them see the often complex process and allow them to participate in the scientific method.” 31 March 03:00 bst — Lockdowns might already have averted tens of thousands of deaths in Europe Infection-control measures such as national lockdowns in many European countries are reducing the spread of the coronavirus. Across 11 countries, between 21,000 and 120,000 deaths were probably avoided by the end of March, according to a model by a group at Imperial College London. The study, published by the Imperial College COVID-19 Response Team on 30 March, estimates the effects of non-pharmaceutical interventions, which include closing schools and banning mass gatherings, on the spread of the virus across parts of Europe, including Italy, Spain and the United Kingdom. The authors measured the effects through a change in the virus’s effective reproduction number. If the effective reproduction number is greater than 1, infections will proliferate and the outbreak will continue to spread. If it is less than 1, the rate of new infections will decrease until the outbreak is under control. The report shows that some of the countries studied — including Italy, which implemented stringent lockdown measures three weeks ago — might have fallen below this threshold as a result of the interventions. Previous work by other groups has found that the strict lockdowns in China reduced this value to below 1, stemming the tide of the epidemic there.   Because many countries still lack the capacity for widespread testing, the latest model uses reported COVID-19 deaths, rather than infection rates, to track the spread of the virus. Given the lag time between infection and mortality, it could take several weeks for the full effects of the interventions to be felt, especially in countries in the relatively early stages of their epidemics, such as the United Kingdom, according to the report. The researchers are careful to note that they cannot attribute the transmission reduction to any particular intervention. And it’s too early to know whether the interventions as a whole are having the intended effect in some European countries. But, “if current trends continue, there is reason for optimism”, they write. 27 March 15:00 gmt — Virus could have killed 40 million without global response The COVID-19 pandemic could have infected 90% of the world’s population and killed 40.6 million people if no mitigation measures had been put in place to combat it, according to estimates from an influential modelling group at Imperial College London. The report from the Imperial College COVID-19 Response Team, published on 26 March, highlights the importance of acting early to suppress the outbreak. The analysis says that introducing social distancing, testing and isolation of infected people would cut worldwide deaths to 1.9 million, if carried out when each country’s fatality rate was 0.2 deaths per 100,000 people per week. Implementing these measures only when the death rate reaches 1.6 per 100,000 people per week would lead to 10.5 million lives lost globally, it finds. According to Nature’s analysis of death rates from Our World in Data — counting each day at the centre of a rolling weekly window of deaths — Italy hit the 0.2 threshold on 2–3 March, the United Kingdom on 17 March and the United States on 22 March. The report did not quantify the social and economic impact of mitigation policies. The analysis was published as the UK Prime Minister Boris Johnson and the nation’s health minister, Matt Hancock, announced that they had tested positive for the coronavirus. In a video address to the nation, Johnson said he had only mild symptoms and would continue to work remotely while isolating for seven days. 27 March 03:00 gmt — Global infections number half a million The number of confirmed COVID-19 cases around the world crossed 500,000 on 26 March, according to statistics compiled by the Center for Systems Science and Engineering at Johns Hopkins University in Baltimore, Maryland. The pandemic currently spans 175 countries and every inhabited continent. By the end of the day on 26 March, the United States had overtaken China for the highest number of confirmed cases. Italy is also poised to surpass China in the coming days. Italy and Spain now have the two highest death tolls, with Italy accounting for more than one-third of the global total. COVID-19 has claimed the lives of nearly 23,000 people. More than 120,000 have recovered from the disease. Source: Johns Hopkins University 26 March 13:15 gmt — United Kingdom pledges to roll out extensive antibody testing The United Kingdom could begin large-scale testing for coronavirus antibodies within days, government officials have said. If the roll-out goes ahead as planned, the country could become the first to implement at-home testing on this scale — but researchers caution that properly validating the accuracy of such tests and manufacturing them in large quantities presents a significant challenge.   On 25 March, a UK government official said that the country had ordered 3.5 million ‘finger-prick’ tests and planned to order millions more. The test will analyse drops of blood for antibodies that show whether a person has previously been infected with the coronavirus that causes COVID-19. This will show who might now be immune and aid researchers in understanding the virus’s spread. These ‘serological tests’ should become available to the public in days rather than weeks or months, said Sharon Peacock, director of the national infection service at Public Health England (PHE), a UK health agency. Peacock suggested that the bulk of the UK tests, which will be available to buy from Amazon and pharmacies to perform at home, had not yet arrived. Most tests to diagnose coronavirus infection have involved laboratory-based testing using the technique known as PCR, which checks for active infection. But an urgent goal has been to develop serological tests, which can detect past infection. These are now being deployed worldwide. Singapore, for instance, has been using the tests for more than a month to trace known infections and monitor at-risk populations. But thoroughly evaluating the efficacy of the tests and the rate of false positives is essential, says Robert Garry, a virologist at Tulane University School of Medicine in New Orleans, Louisiana, whose team is developing its own serological test. The blood test will need to distinguish between antibodies against the COVID-19 virus and those against other seasonal coronaviruses to which people are commonly exposed, he says. “I would expect the false-positive rate to be very high because of this prior exposure — unless they figured out how to make the serological test very specific,” he says. Garry says that for a validated test, he would expect a false-positive threshold of less than 5% — meaning fewer than 5 out of 100 people without the antibodies test positive — although he could see that being relaxed to 10%. Even with ready access to clinical samples, understanding false-positive levels on this timescale would require a massive effort, says Garry.   PHE is evaluating the tests to ensure they work as claimed, said Peacock, who anticipates this will be done by the end of the week. But supply is likely to remain limited, says David Wraith, an immunologist at the University of Birmingham, UK. It will be challenging for companies to manufacture millions of tests and for any one government to secure so many during a global pandemic, meaning that health-care workers must be given priority access, he says. It is not clear who is developing the UK test. A PHE spokesperson said the agency was talking to a range of companies. Peacock added that highly vulnerable members of the public who test positive will also require further tests before they can resume normal life. Travel in India is restricted for 21 days.Credit: Arun Sankar/AFP/Getty 26 March 3:00 gmt — 100 scientific organizations call to end Trump’s ban on fetal-tissue research A group of 100 research societies, professional organizations and universities is calling on US President Donald Trump to lift restrictions on the use of fetal tissue in research, arguing that the limits delay necessary work on potential treatments for COVID-19. “We believe that researchers should have all of the biomedical research tools out there to develop treatments for COVID-19,” says Eric Anthony, director of policy at the International Society for Stem Cell Research in Skokie, Illinois, a key signatory of the letter sent on 25 March. Fetal tissue, often received from abortion clinics, has long been used in vaccine research. But last June, the Trump administration, following a campaign by opponents of abortion, moved to ban government scientists from using fetal tissue in research once their existing supplies ran out. The rules also included restrictions for non-government scientists funded by the US National Institutes of Health (NIH), requiring that new grant applications for research that uses fetal tissue pass ethics review by an agency-appointed board. The US Department of Health and Human Services (HHS), which oversees the NIH, invited nominations to that ethics board in February, and the comment period closed last week. Signatories to the letter asking for relief from the federal restrictions include the American Academy of Pediatrics, Johns Hopkins University, Massachusetts General Hospital and Harvard University. At least one government scientist has appealed for an exemption to the rules in order to do coronavirus research, according to a story in The Washington Post last week. An HHS spokesperson told Nature that the agency has made no decision about the request, and that the “Trump Administration has activated a whole-of-government, whole-of-America approach to prepare for and respond to COVID-19”. 25 March 11:00 gmt — India starts three-week lockdown India has commenced a 21-day lockdown, after Prime Minister Narendra Modi ordered the country’s 1.3 billion residents to remain in their homes from midnight on 24 March. Many countries have introduced travel restrictions to slow the spread of the coronavirus. But the lockdown in India, which has the world’s second-largest population, will dwarf similar measures in other nations. Currently, the country has roughly 500 reported cases, and 9 reported deaths, but researchers say that infections are probably going undetected there given the size of its population. The country had tested about 15,000 people for the coronavirus by 20 March, according to Our World in Data. Under the new restrictions, flights and train services are suspended, and road access is restricted in every state. Medical centres, petrol stations and grocery stores are exempt from the lockdown. “The nation will have to certainly pay an economic cost because of this lockdown. However, to save the life of each and every Indian is our topmost priority," said Modi during a televised address. 25 March 01:00 gmt — The outbreak in Italy went undetected for weeks The novel coronavirus SARS-CoV-2 was present in northern Italy as early as 1 January. An epidemiological analysis of Lombardy, the epicentre of the outbreak in Italy, reveals that the first onset of symptoms in the country occurred weeks before COVID-19, the disease caused by the virus, was reported there on 20 February. The study looks at nearly 6,000 laboratory-confirmed cases to track how the outbreak unfolded in the region. It was posted to the arXiv preprint server on 20 March.   The undetected spread in January is “very striking”, says Michele Tizzoni, who models infectious diseases at the ISI Foundation in Turin, Italy, but was not involved in the work. “At that time, we were probably still talking about Wuhan.” Instead, by the time the first case was detected in Italy, the virus had already spread to most towns and cities in southern Lombardy. Over the next several weeks, nearly half of the patients who tested positive for COVID-19 were hospitalized; about one-fifth of those required intensive care. The new picture of the outbreak in Lombardy makes it clear that “aggressive containment strategies are required” to stop the spread of the virus, the authors write. Although public activities and gatherings in the region were banned just three days after the first positive test, the virus’s undetected spread in the previous weeks meant that it had already taken hold, with the number of cases doubling roughly every three days. These data will be vital to other countries and public health organizations getting ready to face their own outbreaks of the pandemic, Tizzoni says. His advice to them? “Be prepared. Even if you don’t see much.” 24 March 11:00 gmt — Lockdowns eased in Hubei province Travel restrictions in Wuhan will be lifted from 8 April.Credit: AFP/Getty Chinese authorities are reopening travel to and from Hubei province, the epicentre of the COVID-19 outbreak, ending two months of lockdowns that were introduced to help slow the disease’s spread. Only one new infection has been reported in Hubei since 18 March.   From 25 March, flights and trains in and out of all cities in Hubei province — except for the capital, Wuhan — will resume, and roads will reopen, according to an announcement posted on Hubei province’s website. Access to Wuhan, where the first cases were reported and spread, will remain restricted until 8 April. Until then, all those entering or leaving the city will have to take a test to prove that they are not infected with SARS-Cov-2, the virus that causes COVID-19. The Hubei announcement says that factories and businesses can gradually reopen, as long as they follow transmission prevention measures. Universities, schools and child-care centres will remain closed pending “a scientific assessment of the epidemic control situation”. (Schools in most other regions of China also remain closed.) Other cities in China are slowly returning to some sense of normality. But scientists are warning that there is a risk of renewed COVID-19 transmission once people start mixing again. 23 March 21:15 gmt — United Kingdom implements stringent lockdown The United Kingdom is implementing drastic measures to prevent the spread of the coronavirus, joining other nations that have put in place unprecedented rules to fight the pandemic. Prime Minister Boris Johnson announced in a televised broadcast on 23 March that people living in the United Kingdom must stay at home and travel outside only for essential purposes. The police will have powers to enforce the rules and fine people who stray from them. People will be allowed to leave their houses only to shop for food and medical supplies, exercise once a day, and travel to work if strictly necessary. Shops that sell non-essential items will close and gatherings of more than two people in public will be banned. Public gatherings of more than two people will now be banned in the United Kingdom.Credit: Tolga Akmen/AFP/Getty The United Kingdom had already issued guidelines for social distancing, but Johnson said that although huge numbers are complying, “the time has now come for us all to do more”. The government had also been broadly criticized for not putting in place stricter measures to curb the spread of the virus sooner. The latest rules will be kept under review but will last for at least three weeks. The United Kingdom has recorded 6,650 confirmed cases of the virus, with 335 deaths, but the true figures are feared to be much higher. The country joins other European nations that are already on lockdown; in some of these, measures are stricter and people cannot leave their houses without a permit. Lockdowns are in effect in Spain and Italy — which is experiencing the world’s worst outbreak of COVID-19, the disease caused by the coronavirus. Germany has also banned public gatherings of more than two people. More than a dozen US states are implementing stay-at-home orders, affecting more one-third of the US population. Details of these orders, and the lengths of time for which they will apply, vary from state to state, but they generally include the closures of all non-essential businesses, and continued or more stringent bans on public gatherings. On 19 March, California, the nation’s most populous state, was the first state to order a lockdown, following the lead of San Francisco three days earlier. In New York state, home to about 5% of worldwide COVID-19 cases, authorities can fine businesses that do not comply with orders, but people will not face penalties. In Oregon, however, people who violate the governor’s order can be tried for a misdemeanour offence. President Donald Trump has also approved the mobilization of US National Guard troops in New York, California and Washington to set up medical stations and deliver aid to residents. A sign advising social distancing at Manly Beach in Sydney, Australia.Credit: Cameron Spencer/Getty 23 March 17:35 gmt — Global infections pass 300,000 The number of COVID-19 coronavirus infections worldwide passed 300,000 over the weekend. The World Health Organization (WHO) reported 332,935 cases — across nearly every country — as of 23 March, with 14,510 deaths. WHO director-general Tedros Adhanom Ghebreyesus said in a press briefing that it took 67 days from the first reported case to reach the first 100,000 cases, 11 days after that to reach 200,000 and just another 4 days to top 300,000 infections. 20 March 12:15 gmt — Most COVID-19 deaths now in Italy Deaths from COVID-19 in Italy have exceeded those reported in China, after 473 people died in Italy in 24 hours. The coronavirus has so far killed 3,405 people there, according to official statistics from 19 March. In China, 3,242 people have died of the disease since the outbreak began in December, according to the World Health Organization (WHO). Globally, the number of confirmed coronavirus cases has doubled in less than two weeks, to more than 200,000. On 19 March, seven countries confirmed their first coronavirus cases, including Zambia, Gambia, Mauritius and Kyrgyzstan, according to the WHO. Medical teams from across China began leaving Wuhan this week after infections there have dropped.Credit: STR/AFP/Getty 19 March 20:00 gmt — Coronavirus hospitalizations occur at any age, says US CDC COVID-19 can be severe enough to require hospitalization in adults of any age, according to a report from the US Centers for Disease Control and Prevention (CDC).   The report, released on 18 March, analyses the severity of COVID-19 cases in the United States from 12 February to 16 March by age group. Although mortality is highest in adults over 65 — consistent with data reported from China, Italy and elsewhere — 20% of US hospitalizations occurred in people aged 20–44 years. The CDC’s report excludes cases imported from Wuhan, China, or from Japan, so it did not count people who were infected on cruise ships in those areas. In total, age data were collected for 2,449 infected people. Of them, 29% were between 20 and 45 years old. That statistic is roughly consistent with numbers released by the Italian Higher Institute of Health, which reported on 15 March that 24% of cases in the country occurred in adults aged 19–50. These data do not include age-group breakdowns for severe cases, only fatalities. Deaths from COVID-19 in Italy surpassed those in China on 19 March.   Among people with COVID-19 aged 20–44 in the United States, the CDC’s report estimates that up to 21% required hospitalization. But only 2–4% of infected people in that age bracket required intensive care, significantly fewer than in any other adult age group. And fatalities remain low among young adults, the report shows. No data were collected regarding underlying conditions that have been shown to make people more susceptible to the disease, so it is unclear whether the young adults being hospitalized are those who are most vulnerable. Still, the report stresses, severe illness “can occur in adults of any age”. 19 March 11:00 gmt — No new confirmed cases in Hubei province On 18 March, Hubei, the Chinese province at the centre of the coronavirus outbreak, recorded no new cases of COVID-19 for the first time since the beginning of the epidemic, according to the country’s National Health Commission. Eight deaths were reported in the province that day. A month ago, Hubei was confronting several thousand new confirmed cases each day. Since December, it has recorded more than 67,000 people with COVID-19, and more than 3,000 deaths. Across China, there were 39 new cases recorded on 18 March, and 13 deaths. Italy now faces the largest number of new cases per day, with 3,526 confirmed yesterday. New cases have also surged in the United States, Iran, Spain, France and Germany. A train in Moscow is disinfected.Credit: Alexander Nemenov/AFP/Getty 18 March 10:00 gmt — Deaths outside China surpass those inside the country The total number of people who have died from COVID-19 outside China has overtaken deaths inside the country for the first time since the disease emerged, according to reports by the World Health Organization (WHO) on 16 March. The number of confirmed infections outside China surpassed those inside the country on the same day. As of 17 March, there were 179,112 confirmed cases of COVID-19 globally, including 81,116 in China. Of the 7,426 deaths from the disease, 3,231 have been in China. Europe had the largest 24-hour spike in new infections, with 8,507 reported since 16 March, and 428 deaths. Several regions recorded their first cases, including Somalia, Benin, Liberia and the Bahamas. 17 March 00:30 gmt — First vaccine clinical trials begin in the United States Pharmacist Michael Witte delivers a shot in an early-stage clinical trial for a potential coronavirus vaccine on 16 March.Credit: Ted S Warren/AP/Shutterstock The first phase I clinical trial for a potential COVID-19 vaccine has begun in Seattle, Washington. Four adults, the first of 45 eventual participants, received their first doses of an experimental vaccine developed through a partnership between the US National Institute of Allergy and Infectious Diseases (NIAID) and Moderna, a biotechnology company based in Cambridge, Massachusetts. But although it is an important milestone, the phase I trial is just the beginning of a long process to test the drug’s safety and efficacy. The trial is being conducted at Kaiser Permanente Washington Health Research Institute, and will test a range of doses of the vaccine. Over the next 6 weeks, participants will receive their first doses, followed by a second 28 days later. Follow-up visits both in person and over the phone will assess participants’ health over a 14-month period, and blood samples will help researchers evaluate the body’s immune response to the experimental vaccine. The potential vaccine is based on messenger RNA, which directs the body to make a protein found on the coronavirus’s outer shell. The hope is that this will elicit an immune response that protects against infection.   The team at Moderna had already been working on a vaccine for Middle East respiratory syndrome, which is caused by another coronavirus. The viruses’ similarities helped the researchers pivot to the search for a COVID-19 vaccine. As a result, the phase I trial was “launched in record speed”, according to a statement from NIAID Director Anthony Fauci on 16 March. It took just 66 days from genetic sequencing of the virus to the first human injection of the vaccine candidate. Researchers hope to have initial clinical-trial data within three months. But even in the best-case scenario, the vaccine would not be widely available to the public for at least another year, according to NIAID. 13 March 23:00 gmt — US president declares ‘national emergency’ US President Donald Trump called the coronavirus outbreak a national emergency on Friday afternoon. This gives his administration broad authority in its response to the disease, including access to up to US$50 billion in federal funds to combat the epidemic. Trump said that as many as half a million tests would be ready by early next week.   Earlier in the day, the president announced measures to speed up testing in the United States, including providing funding to develop rapid tests and appointing a new federal coordinator to oversee the efforts. More than 1,800 people have tested positive for the virus in the United States and at least 41 have died, according to The New York Times. The virus has now been detected in 47 states and the District of Columbia. 13 March 22:10 gmt — Harvard University orders research labs to shut down Research laboratories at Harvard University in Cambridge, Massachusetts, have been ordered to prepare to shut down research operations amid the growing coronavirus outbreak. Harvard is one of the first major research universities to announce that it will wind down laboratory research. Dozens of universities worldwide have already moved teaching activities online or been closed in a bid to control the spread of the virus. Despite Harvard’s move, labs there doing direct research on coronavirus will be able to continue their operations, a representative of the university’s medical school told Nature. All labs must begin implementing a plan to stop all laboratory research activities by 18 March, said e-mails sent from deans to students and staff members in the faculty of arts and sciences and the medical school on 13 March. The suspension is expected to last for at least six to eight weeks, the e-mails say. Labs that work with live animals will be able to designate staff members for essential animal care, but microbial labs have been ordered to “freeze everything down”, says Tanush Jagdish, an evolutionary biologist at the university.   Exemptions will be made for essential experiments that “if discontinued would generate significant financial and data loss”, according to the e-mails. Jagdish says that the announcement caught everyone in his lab off-guard. “For labs to be shut down in general was something we did not expect.” The labs that he works in had already implemented measures to prevent the spread of the coronavirus disease COVID-19. These included alternating shifts and making cleaning protocols stricter, in addition to extra cleaning that was instituted at the department and university levels. Until lab work can resume, researchers are devoting their time to writing grant proposals and theses, among other remote work, he says. On 10 March, Harvard had mandated that gatherings of more than 25 people be held remotely, but the latest guidelines state that all meetings and courses should do this, regardless of size. In addition to conducting lab meetings by video chat, people have been discussing starting daily or weekly remote social hours, Jagdish says. “It helps to know that we’re all in this together.” 13 March 22:00 gmt — Europe now centre of pandemic, says WHO Italy on lockdown: Rome’s Via del Corso.Credit: Giuseppe Fama/Pacific Press/LightRocket/Getty Europe has now become the epicentre of the COVID-19 pandemic, says the World Health Organization (WHO). More cases are now being reported there every day than were reported at the height of China’s epidemic, director-general Tedros Adhanom Ghebreyesus said in a 13 March press briefing. There are more reported cases and deaths in Europe than in the rest of the world combined, apart from China, Tedros said. Italy, which has the largest outbreak in Europe, reported 2,651 new cases in the past day. More than 132,000 cases of COVID-19 have now been reported from 123 countries and territories, according to the WHO. 11 March 16:35 gmt — Coronavirus outbreak is a pandemic, says WHO After weeks of resisting mounting pressure from scientists, politicians and others, the World Health Organization (WHO) in Geneva, Switzerland, has decided to describe the coronavirus outbreak as a pandemic.   The declaration comes after a 13-fold rise in the number of cases outside China in the past two weeks, and a trebling of countries affected — but does not change WHO strategy for tackling spread of the virus, director-general Tedros Adhanom Ghebreyesus said in an 11 March press briefing. “WHO has been assessing this outbreak around the clock and we are deeply concerned both by the alarming levels of spread and severity, and by the alarming levels of inaction,” said Tedros. “Describing the situation as a pandemic does not change WHO’s assessment of the threat posed by this coronavirus. It doesn’t change what WHO is doing, and it doesn’t change what countries should do,” he said. “Pandemic is not a word to use lightly or carelessly. It is a word that, if misused, can cause unreasonable fear, or unjustified acceptance that the fight is over, leading to unnecessary suffering and death.” Many scientists had been calling for the change in language for weeks — after large outbreaks were detected in South Korea, Iran and Italy. At the time, some researchers suggested that countries would soon move from efforts that involve containing as many new cases as possible to social-distancing measures, such as school closures, that do not rely on knowing who is infected with the virus and who is not. The virus has now been found in more than 100 countries. It has infected some 120,000 people, killing more than 4,000 of them. Several nations have closed schools in a bid to stop the virus, and Italy has entered an unprecedented countrywide lockdown. “This is not just a public health crisis, it is a crisis that will touch every sector — so every sector and every individual must be involved in the fight,” said Tedros. Researchers have been working rapidly since the outbreak came to light in January to characterize the virus, work out why it is so infectious, find out where it came from and help with diagnosing infections. 11 March 12:30 gmt — Transgenic animals for coronavirus research in high demand Labs are scrambling to get their hands on transgenic animals that can be used to study the coronavirus and test drugs and vaccines. Ordinary mice seem to be resistant to infection by this coronavirus, so researchers conduct studies in rodents that produce a human version of the protein ACE2, which the virus uses to enter cells.   But these mice — originally developed for research into severe acute respiratory syndrome, or SARS — are in short supply. One US breeding facility, the Jackson Laboratory in Bar Harbor, Maine, has received requests for 3,000 mice, and is now establishing a colony of these animals. Researchers have also already reported initial results from studies in monkeys infected with coronavirus. The animals seemed to experience only a mild illness, similar to that in many humans. Scientists are now seeking animal models to mimic the more severe version of the illness. 11 March 07:30 gmt — Major chemistry meeting among cancelled scientific conferences The American Chemical Society (ACS), the world’s largest scientific society, cancelled its meeting in Philadelphia, Pennsylvania, on Monday, 13 days before it was due to begin on 22 March. The conference is one of a growing number of scientific meetings and conferences being cancelled because of coronavirus outbreaks. Governments and health officials are increasingly calling for the restriction of large gatherings, in an attempt to reduce the virus’s spread. The ACS said the decision to cancel was based on several factors, including a rise in COVID-19 cases in the greater Philadelphia area and input from members who were increasingly concerned about travelling to and attending a large meeting. About 800 participants had already cancelled their registrations before the announcement, an ACS press officer told Nature. The American Physical Society’s huge March Meeting was also cancelled last week, about a day before it was due to begin. Scores of other meetings are being postponed or being held virtually. 10 March 03:30 gmt — Call for more funding At least US$8 billion is needed to address the most pressing threats posed by the new coronavirus, says the Global Preparedness Monitoring Board (GPMB), an independent group co-convened by the World Health Organization and the World Bank Group to combat public-health emergencies. The money is needed in addition to the tens of billions of dollars already pledged by the International Monetary Fund, the World Bank Group and individual governments. The GPMB released a report on 9 March calling on advanced economies, such as the members of the Group of Seven and Group of 20 industrialized nations, and financial institutions, to provide money to address five priority areas. These include strengthening weak health-care systems; supporting the World Health Organization’s efforts to help vulnerable countries; developing diagnostics, therapeutics and vaccines; strengthening regional surveillance; and ensuring that sufficient protective equipment is available for health workers. 9 March 04:00 gmt — Global cases pass 100,000 The number of known global cases of COVID-19 passed 100,000 over the weekend. On 8 March, the World Health Organization reported 105,586 confirmed cases across more than 100 countries and territories. Although the outbreak has been slowing in China, where it originated, the country still accounts for almost 80% of confirmed cases. 6 March 11:30 gmt — US Congress approves US$8.3 billion for coronavirus response The United States Congress has passed an emergency spending bill that will allocate US$8.3 billion for the country’s coronavirus response. The House of Representatives passed the bill in a near-unanimous vote on Wednesday afternoon; the Senate followed suit on Thursday. The bill will provide more than $3 billion to the Centers for Disease Control and Prevention, the National Institutes of Health, and the Food and Drug Administration for research on diagnostics, therapeutics and vaccines. Each US state will reportedly receive at least $4 million for state and local government responses. The bill also includes a provision to ensure that an eventual vaccine is affordable. Other funding will contribute to the global coronavirus response and provide support for small businesses that are struggling in the wake of the outbreak. The bill will now go to President Donald Trump to be signed. Elsewhere, the US National Science Foundation in Alexandria, Virginia, opened up a Rapid Response Research funding mechanism for non-medical coronavirus-related research. Such calls are often used to award funding for work exploring the impacts of natural disasters such as hurricanes and wildfires, and allow for an expedited review of research proposals. 5 March 15:30 gmt — China study suggests children are as likely to be infected as adults Children are just as likely to become infected with the new coronavirus as adults, finds one of the most detailed studies yet published on the spread of the virus, known as SARS-CoV-2. The analysis — based on data from Shenzhen in China — provides a partial answer to one of the most pressing questions surrounding the outbreak: the role of children. Previous studies have suggested that kids are much less likely than other age groups to develop severe symptoms when infected by the coronavirus. But it was not clear whether this was because they weren’t getting infected or because they were fighting off the infection more effectively. “Kids are just as likely to get infected and they’re not getting sick,” says Justin Lessler, an infectious-disease epidemiologist at Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland. He co-led the study with three other epidemiologists — Qifang Bi, also at Johns Hopkins, Ting Ma at the Harbin Institute of Technology in Shenzhen and Tiejian Feng at the Shenzhen Center for Disease Control and Prevention. They posted the analysis to the medRxiv preprint server on 4 March. The study is unique in that it looked at not only people who were infected with the virus, but also large numbers of their close contacts, some of whom were infected and many of whom were not. The researchers followed 391 people who were diagnosed on the basis of their symptoms, and 1,286 of their close contacts to see whether these contacts tested positive for the virus even if they didn’t show symptoms. Overall, the team found that children under 10 who had potentially been exposed to the virus were just as likely to become infected as other age groups, with between 7% and 8% of contacts of known cases later testing positive. The authors also found that people who lived in the same household as someone infected with the virus were about six times more likely get infected than those who made contact with an infected person in other settings. “This may be the first clear evidence that children are as susceptible as adults to SARS-CoV-2 infection,” says Ben Cowling, an infectious-disease epidemiologist at the University of Hong Kong. He wonders whether the fact that outbreaks haven’t been observed in schools could be down to the fact that children’s symptoms are mild. Lessler says it’s still not clear whether children are important in transmitting the virus, as they are for influenza; children routinely develop flu symptoms and are common hubs in chains of transmission. “That’s one of the current critical remaining questions and we’re trying to figure out how to answer it,” he says. “I have a 7-month-old and a 6-year-old and I can’t imagine that, if they have any virus at all, they’re not getting it on somebody.” The study could have important implications for slowing the spread of the virus through measures such as school closures. “Once we say containment is not an option, we can’t ignore the kids,” says Lessler. “This is a key piece of data that may support school closures as an effective intervention,” Caitlin Rivers, an epidemiologist at Johns Hopkins Bloomberg School of Public Health, said in a tweet on 5 March. 5 March 12:55 gmt — World Bank pledges US$12 billion for coronavirus response On 3 March, the World Bank Group pledged up to US$12 billion, including $8 billion in new funding, to support countries dealing with the coronavirus outbreak. The funding will be fast-tracked and consists of grants and low-interest loans, as well as technical support. Most of the package is earmarked for strengthening health systems and improving access to treatment. The World Bank said it would prioritize funding for countries at high risk and with low capacity to deal with the outbreak. Half of the pledged money is from the International Finance Corporation, and is intended to strengthen global supply chains and support key industries such as pharmaceuticals. 4 March 12:00 gmt — Repurposed drugs in coronavirus spotlight   Drugs used to treat HIV and an experimental antiviral drug developed to fight Ebola virus are among those that are rapidly being tested against the new coronavirus. There are no approved treatments for diseases caused by coronaviruses. But hundreds of clinical trials of drug candidates are planned or underway, with much of the focus on remdesivir, a candidate drug originally developed to treat the Ebola virus. The drug, developed by the pharmaceutical firm Gilead Sciences of Foster City, California, is being tested in partnership with Chinese health authorities in randomized, controlled trials; two of these are set to finish in April. The compound works by trying to prevent the replication of the virus. “Remdesivir has quite high efficacy across all different coronaviruses and therefore it is one of the prime candidates to start being tested,” says Vincent Munster, chief of the viral ecology unit at the US National Institutes of Health. 2 March 21:00 gmt — Infections worldwide top 90,000 The number of people worldwide who have been infected with the coronavirus has passed 90,000. More than 3,000 have died since the outbreak began in December. The vast majority of cases — more than 80,000 — have occurred in China, but around 60 other countries are now also dealing with outbreaks. Many nations are preparing for a global pandemic, as reports of cases caused by spread within communities — rather than being imported from China — rise. South Korea, Italy and Iran are fighting the largest outbreaks outside China. Source: Johns Hopkins University 2 March 20:45 gmt — WHO raises alert to ‘very high’ At a press briefing on 29 February, the World Health Organization (WHO) announced that it had raised the global alert for COVID-19 to the highest possible level, short of calling it a pandemic. The virus has now spread to some 60 locations outside China, with new cases detected in Ireland, Monaco, Azerbaijan, Qatar and Ecuador. The global alert for the spread and impact of the coronavirus outbreak increased from ‘high’ to ‘very high’. The alert remains ‘very high’ in China. The global change was based on an assessment by WHO epidemiologists, which took into account the continued increase in the number of cases and affected locations, and the difficulties that some regions, including Iran and Italy, are facing in containing the spread of the coronavirus. Tedros Adhanom Ghebreyesus, director-general of the WHO, said at the briefing that most cases were linked and could still be traced to known contacts or clusters, with no evidence of the virus spreading freely in communities. “As long as that is the case, we still have a chance of containing this virus, if robust action is taken to detect cases early, isolate and care for patients and trace contacts,” said Tedros. The organization therefore once again resisted declaring the outbreak a pandemic. Mike Ryan, director of the WHO’s emergencies programme, said that such a decision would mean that efforts to contain and slow down the spread of the virus have failed, which has proved to be untrue in China, Singapore and other regions. The WHO is still holding out hope that the virus can be contained, but we have probably crossed that threshold, says Adam Kamradt-Scott, a global health-security researcher at the University of Sydney, Australia. Some countries have already begun to prepare their pandemic plans, which is an important precautionary measure, says Nigel McMillan, an infectious-disease researcher at Griffith University in Brisbane. Australia, for example, initiated its coronavirus emergency response on 27 February. The WHO is being overly cautious in not declaring a pandemic, says McMillan. 2 March 20:30 gmt — Transmission details emerge from WHO China analysis China has mounted “perhaps the most ambitious, agile and aggressive disease containment effort in history” against a new infectious disease, the World Health Organization (WHO) said in a report released on 28 February, after nine days of meetings and site visits in China from 16 to 24 February. The report analyses data from the outbreak in China, and recommends steps that the country and others should take to curb COVID-19. Daily reports of new cases are declining in the country, the WHO confirmed — so much so that authorities are now having problems recruiting participants for the more than 80 clinical trials there that are testing potential treatments for the coronavirus. Some experimental treatments should be prioritized over others, the health agency recommended. The report’s analysis of data from China finds 99.9% similarity between the 104 strains of the coronavirus, named SARS-CoV-2, collected from people between December 2019 and mid-February 2020. This means that the virus is not mutating significantly. The median age of people infected is 51 years. And most cases of spread from person to person have been in hospitals, prisons or households, which implies that close contact is often required for the virus to spread between people. Airborne spread is not believed to be a major driver of transmission, the report says. In one preliminary study from the province of Guangdong, people in the same household as someone with COVID-19 had a 3–10% chance of being infected. The WHO credits China’s ability to rein in the epidemic to a variety of measures. One is that 1,800 teams of epidemiologists have rapidly tracked tens of thousands of contacts of people infected with the virus in Hubei province, where the outbreak emerged. Up to 5% of these contacts ended up having the disease and were diagnosed quickly. And the report says the lockdown on travel out of Hubei — an unprecedented measure in a province of this size — curbed wider spread of the disease to China’s 1.4 billion citizens. 2 March 17:45 gmt — Coronavirus fears cancel huge physics meeting   Senior Nature reporter Davide Castelvecchi reports from Denver, Colorado, where the world’s biggest physics meeting has been cancelled because of coronavirus fears. The meeting was scheduled to host 11,000 attendees. Some researchers are finding other ways to share their work, including informal meet-ups and virtual talks. Several other scientific meetings have also been cancelled, as virus outbreaks emerge and escalate in countries around the world. 28 February 12:45 gmt — Coronavirus spreads to sub-Saharan Africa The coronavirus outbreak has spread to 46 countries other than China — and now seems to be spreading faster outside China than inside.   Several nations reported their first infections this week; cases included the first to be confirmed in sub-Saharan Africa, in Nigeria. The Nigeria Centre for Disease Control reported the case on 27 February and said it was working to trace the infected person’s contacts. Health authorities and researchers have feared the virus’s spread to African countries including Nigeria, where weak health systems could quickly be overwhelmed by a local outbreak. The World Health Organization reports that more than 82,000 people worldwide have now been infected — more than 3,600 of those outside China. Cases in South Korea, which is handling the world’s second-largest outbreak, have exploded to more than 2,300. China’s outbreak seems to be slowing, with the daily number of new cases dropping. Authorities reported 327 new infections nationwide on 27 February. A week earlier, on 20 February, that figure had been around 900. Outside China, about 750 new cases were reported on 27 February. 26 February 18:30 gmt — Brazil reports first case in South America A case of COVID-19 has been confirmed in Brazil — the first in South America. On 26 February, Brazil’s minister of health, Luiz Henrique Mandetta, confirmed that a man who travelled to northern Italy between 9 and 21 February has the disease. Italy’s outbreak has escalated to 324 cases and 12 deaths, according to a virus tracker maintained by Johns Hopkins University in Baltimore, Maryland. The Brazilian case is in a 61-year-old man who sought care for a fever, cough and sore throat yesterday at a hospital in São Paulo, where he tested positive for COVID-19. Officials say he is in stable condition and will be quarantined at home for 14 days. “We should only take those with severe respiratory conditions to the hospital,” said Mandetta in a tweet. Algeria, Greece, Afghanistan, Bahrain, Iraq and Oman have all reported their first cases within the past two days. The virus’s appearance in South America now means that it has spread to every continent except Antarctica. 25 February 22:30 gmt — Trump requests emergency funding for coronavirus response The administration of President Donald Trump has requested up to US$2.5 billion to fund the US response to COVID-19. In a 24 February letter to Congress, the Office of Management and Budget requested $1.25 billion in new funding and proposed to make up the rest by repurposing funds allocated to other programmes, including $535 million assigned to the Ebola response.   Democratic legislators immediately criticized the requested sum as overdue and insufficient. The US government has allocated several times that figure for past outbreak responses: Congress approved about $5.4 billion for the 2014 Ebola outbreak, and in 2009 about $7.7 billion was directed to the H1N1 influenza pandemic response. Public-health policy analysts expect the government to make some funds available, but it’s unclear how much will be appropriated, and how soon. “My expectation is that something will eventually be passed,” says Josh Michaud, a public-health policy analyst at the Kaiser Family Foundation, a non-profit organization in Washington DC. “What eventual form that is and whether it conforms to the risk that coronavirus poses is another question.” 25 February 21:00 gmt — Researchers anticipate unstoppable spread Despite the World Health Organization’s decision not to describe the escalating coronavirus outbreak as a pandemic, some scientists say that the spread is already moving into a new, dangerous phase. They say measures to limit spread will have to shift from containment to mitigation, as countries including Iran, Italy and South Korea report growing outbreaks with hundreds of infections. 25 February 18:30 gmt — ‘There is lots of anxiety’: a scientist’s view from South Korea   Bartosz Grybowski is a crystal chemist in South Korea, where cases of the coronavirus have surged. Crystal chemist Bartosz Grzybowski talks to the Nature Podcast’s Nick Howe about how the surging outbreak in South Korea is affecting daily life as a researcher. Grzybowski is at the Ulsan National Institute of Science and Technology, about 60 kilometres from the city of Daegu, where most of the country’s coronavirus cases have occurred. He says panic and fear reign — but the nation seems well prepared. 24 February 16:30 gmt — WHO says outbreak isn’t a pandemic At a press briefing on 24 February, the World Health Organization (WHO) said that, despite the spread of the disease, the coronavirus outbreak does not yet amount to a pandemic. “Using the word pandemic now does not fit the facts, but it may cause fear,” said WHO director-general Tedros Adhanom Ghebreyesus. “For the moment, we are not witnessing the uncontained global spread of this coronavirus, and we are not witnessing large-scale severe disease or death,” said Tedros. “Does this virus have pandemic potential? Absolutely. Are we there yet? From our assessment, not yet.” Mike Ryan, director of the WHO’s emergencies programme, justified the organization’s position by explaining that the virus’s transmission remains poorly understood — and it seems that the rate of new infections is declining in China. He advised countries to focus on treating patients and reducing the chance of people spreading the virus to others. 24 February 14:00 gmt — Cases outside China are rising The number of cases of COVID-19 outside China jumped over the weekend, with Italy, South Korea and Iran reporting new infections. Kuwait, Bahrain, Afghanistan and Iraq also confirmed their first cases on 24 February.   Officials in Iran have reported up to 61 cases and 12 deaths. But the figures have been in flux, and case numbers will probably rise in the coming days, given that the number of deaths compared to overall cases is much higher than reported in other countries. South Korea had confirmed 833 infections and 8 deaths as of 24 February, according to a virus tracker maintained by researchers at Johns Hopkins University in Baltimore, Maryland. The virus is also spreading in Italy, where 230 people have been infected and 5 have died, according to the tracker. At a meeting organized by the African Union on 22 February, Tedros Adhanom Ghebreyesus, the director-general of the World Health Organization, said he was especially worried about the rise in cases in Iran, South Korea and Italy. “The increasing signs of transmission outside China show that the window of opportunity we have for containing this virus is narrowing,” he said. Meanwhile, more than 77,000 people have been infected in China, and the death toll there has passed 2,500. 20 February 13:30 gmt — Coronavirus death toll passes 2,000 More than 2,100 people worldwide have now died from COVID-19, the disease caused by the new coronavirus. On 20 February, Chinese authorities reported that 2,118 people there had died from the illness. Infections worldwide have topped 75,000; more than 74,000 are in China. 20 February 13:00 gmt — China’s case-counting methods raise concerns Scientists have questioned the way in which China is counting cases of the coronavirus. China’s official reports on the number of infections have not been including people who have tested positive for the virus but have no symptoms. Researchers fear the practice is masking the epidemic’s true scale — but some public-health experts say China is right to prioritize tracking ill people who are spreading the disease. Medical staff working in an exhibition centre converted into a hospital in Wuhan, ChinaCredit: AFP/Getty 18 February 11:00 gmt — Has the outbreak in China peaked? A study of nearly 45,000 confirmed COVID-19 cases in China suggests that the outbreak might already have reached its climax. The report, from the country’s Center for Disease Control and Prevention, says that the peak — the day with the highest number of new infections — occurred around the end of January. The number of new laboratory-confirmed cases per day declined from then to 11 February, the end of the study period. However, the number of new suspected cases and cases diagnosed by physicians using chest scans, known as clinically diagnosed cases, stayed at roughly the same levels.   The latest data on coronavirus infections in China appear to show a decline in new cases, said Tedros Adhanom Ghebreyesus, director-general of the World Health Organization (WHO), at a press briefing on 17 February, the same day the Chinese report was released. But he said the trend must be interpreted cautiously. “It’s too early to tell if this new reported decline will continue,” he said. “Every scenario is still on the table.” Raina MacIntyre, a physician and epidemiologist at the University of New South Wales in Sydney, Australia, agrees that the data need to be considered with caution, but says the general trends are informative. The WHO’s reports also show a decline in new cases reported per day in China and worldwide, she says. But the extended Chinese holiday period that ended on 9 February means there might be another increase in new cases around 21 February, as people return to work. “Often with epidemics we see more than one peak,” says MacIntyre. Epidemiologists have been trying to estimate roughly when the outbreak will peak, so public-health officials can prepare hospitals and work out when it will be safe to lift travel restrictions in Wuhan and several nearby cities. Some models suggest that the climax will happen any time now. Others say that it is months away and that the virus will infect millions — or, in one estimate, hundreds of millions — of people before then. This model assumes that many more people have been infected than is reflected by official counts, but that these people have no symptoms or are not ill enough to seek medical treatment. The coronavirus SARS-CoV-2, shown in a scanning-electron-microscope image.Credit: NIAID-RML/de Wit/Fischer 17 February 00:30 gmt — First case in Africa detected The first case of the new coronavirus has been reported in Africa — in a person in Egypt. The country’s health ministry and the World Health Organization said on 15 February that the case had been detected as a result of Egypt’s programme to trace visitors who had come from affected countries. The person tested positive for the virus but had no symptoms. They are currently being isolated and are in a stable condition.   Researchers have been apprehensive about the virus arriving in Africa, and about its potential to spread there. Vittoria Colizza, who models infectious diseases at the Pierre Louis Institute of Epidemiology and Public Health in Paris, told Nature that she’s most concerned about seven African nations that have a moderate risk of importing the virus, but whose weak health-care systems, low economic status or unstable political situation make them highly vulnerable. These countries are Nigeria, Ethiopia, Sudan, Angola, Tanzania, Ghana and Kenya. Electron-microscope image of the new coronavirus, now designated SARS-CoV-2.Credit: NIAID-RML/de Wit/Fischer 14 February 14:00 gmt — Chinese cases spike after diagnosis change A sudden spike in new coronavirus cases reported in China this week shocked some researchers. But the huge bump in numbers isn’t a sign that the epidemic is worsening, say others; it is instead the result of authorities changing how cases are confirmed. On 12 February, Hubei province reported nearly 15,000 new cases of COVID-19, the disease caused by the coronavirus, representing a 33% jump in total infections worldwide in a single day. Total infections in China now number around 64,000, with more than 1,300 deaths. But most of the Hubei cases — about 13,000 — have been reported as a result of a new policy that means physicians in the province can diagnose suspected cases of COVID-19 on the basis of chest scans, rather than having to wait for genetic tests to confirm the presence of the virus, which can take days. The policy was established in response to pleas from doctors who are overwhelmed by patients with respiratory diseases, and don’t have time to wait for lab results, says Wu Zunyou, chief epidemiologist at the Chinese Center for Disease Control and Prevention, who helped design and implement the policy. “The clinicians in Hubei made a very strong request to modify the criteria because of their heavy workload,” he says. Now they can care for people more quickly and ensure they are properly isolated to protect others, says Wu. “We need to save lives.” The policy makes sense from a medical point of view, says Michael Mina, an infectious-disease immunologist and epidemiologist at the Harvard T.H. Chan School of Public Health in Boston, Massachusetts. “Triaging based on symptomatic evaluation and physical exam is the bedrock of hospital-based and clinical triage,” he says. The new diagnosis method was listed in updated disease-reporting guidelines issued last week. It applies only to Hubei, where the virus originated in the city of Wuhan. Wu says that other provinces aren’t as overwhelmed with cases so will still need to confirm suspected cases with genetic tests or lab cultures of the virus taken from patients. The Chinese state media outlet Xinhua urged calm after the spike was reported. “Although the figures rose, it does not mean that the epidemic in Wuhan has deepened,” it said. On 14 February, Chinese authorities revealed for the first time the number of infections in medical staff. They said 1,716 health workers had contracted the virus, 6 of whom had died. 13 February 12:15 gmt — Chinese virologists raise concerns about virus name Some researchers in China are unhappy with the designated name for the new coronavirus, SARS-CoV-2. They worry that the use of ‘SARS-CoV’ will confuse the public and impede efforts to control the pathogen’s spread.   On 11 February, the International Committee on Taxonomy of Viruses announced the name, which was chosen on the basis of an analysis of the new coronavirus’s evolutionary history and the pathogen that causes severe acute respiratory syndrome (SARS), called SARS-CoV. Classification rules and established naming practice also informed the decision. Although the two viruses belong to the same species, SARS-CoV-2 spreads much faster than SARS-CoV but is less deadly, says Shibo Jiang, a virologist at Fudan University in Shanghai. SARS-CoV retreated in the summer, but nobody knows what the new virus will do in the coming months, he says. People might think it will behave similarly and stop taking precautions come summer. Scientists also worry about the potential social and economic impact for China of revisiting the bad memories associated with SARS. “That name can cause panic to people, and may cause gross economic loss to the affected countries when the virus is circulating,” says Guo Deyin, a virologist at Sun Yat-sen University in Guangzhou.   On 9 February, virologists in China suggested in a paper in Virologica Sinica that the latest virus be called transmissible acute respiratory syndrome, or TARS-CoV. Sun Caijun, an infectious-disease researcher also at Sun Yat-sen University, says that he would have preferred the virus be named clustered acute respiratory syndrome coronavirus (CARS-CoV) or rapid spread respiratory syndrome coronavirus (RARS-CoV), given the virus’s quick spread. Not everyone is bothered by the designated name. Lijun Rong, a virologist at the University of Illinois in Chicago, doesn’t think it will affect the public’s response to the outbreak. People just want the virus to go away as quickly as possible, he says. “A name is just a name.” 13 February 12:00 gmt — Listen: coronavirus gets formal name Nature’s European bureau chief Nisha Gaind and Nature Podcast’s Nick Howe discuss the naming of the new coronavirus disease and the suggestion that the pathogen might have originated in pangolins.   13 February 03:00 gmt — Researchers worry cases are going undetected in some regions Cases of the new coronavirus, which is now known as SARS-CoV-2, might be going undetected in some nations considered at high risk of an outbreak because they are reporting fewer cases than expected or none at all, say scientists. Infections have been detected in 24 countries outside China so far. Researchers are using flight data to create models of the virus’s possible spread around the world. One model identified 30 countries at risk of importing SARS-CoV-2, on the basis of the large number of flights from Wuhan, the outbreak’s epicentre, and from other cities in China with lots of travellers from Wuhan. But several of those countries, including Thailand, Vietnam, Cambodia and Malaysia, have reported fewer cases than the model predicts. Indonesia, another country at risk, has yet to report a single case. The possibility of unreported cases is particularly concerning in countries with weaker health-care systems, such as those in southeast Asia and Africa, which could quickly be overwhelmed by a local outbreak. No cases have been reported in Africa so far, but some countries there, such as Nigeria, are at particular risk because of strong business ties to China. 12 February 17:45 gmt —  WHO calls for speedy vaccine, drug and diagnostic development World Health Organization officials outlined their top research priorities for controlling the outbreak of the coronavirus-associated disease now known as COVID-19. At the close of a two-day international forum in Geneva, Switzerland, assessing the outbreak, WHO director-general Tedros Adhanom Ghebreyesus spoke about the importance of developing candidate therapeutics and easy-to-apply diagnostics for identifying active, asymptomatic and resolved infections.   Research should also look to understand the best approach for infection prevention, including assessing whether lockdowns in major Chinese cities have had a positive or negative effect on slowing the spread of virus. Virologist Marie-Paule Kieny, who co-chaired the forum, said that there were four vaccine candidates in development. In around three months, she suggested, one or two of those might be in human trials. Still, it will be more than a year until they might be available for wider use. Officials called for research on how to control and counter misinformation that has spread since the start of the outbreak. Finally, Tedros encouraged research into preventing the transmission of zoonotic diseases — which originate in animals — to stem future outbreaks of this type. Although researchers are already investigating some of these questions, and many others, WHO officials hope that its research blueprint will help to direct donor funding to the most productive avenues. 11 February 15:25 gmt — Coronavirus disease officially named COVID-19 The World Health Organization has officially named the disease caused by the coronavirus COVID-19. This will replace various monikers and hashtags given to the emerging illness over the past few weeks. Most recently, on 8 February, China’s National Health Commission decided to temporarily call the disease novel coronavirus pneumonia, or NCP. But because viruses continue to spread from animals to people, this coronavirus won’t be novel for long. “COVID-19 stands for coronavirus disease in 2019,” said Soumya Swaminathan, chief scientist at the World Health Organization in Geneva, Switzerland, at a press briefing. She explained that there are many coronaviruses, and this style of naming will provide a format for referring to new coronavirus diseases in future years. “The virus itself is named by an international group of virologists who will look into the taxonomy,” she said. “But it is important to have a name for this disease that everybody uses.” Two other diseases caused by coronaviruses were given names describing the clinical manifestations: SARS (severe acute respiratory syndrome) and MERS (Middle East respiratory syndrome). Shortly after the WHO announced the disease’s official name, the virus causing it was named SARS-CoV-2 by the International Committee on Taxonomy of Viruses. In a paper posted to the bioRxiv preprint server, the committee’s study group on coronaviruses explains that this term highlights the new virus’ similarity to the SARS virus identified in 2003. 11 February 11:45 gmt — Coronavirus deaths in China pass 1,000 More than 1,000 people in China have been killed by the coronavirus, the nation’s health authorities report. Worldwide, more than 43,000 people have been infected. Millions of people in China returned to work on Monday after the Lunar New Year holiday was extended by more than a week in an effort to halt the spread of the virus — although many businesses remain closed. Researchers will be watching closely to see whether the number of cases increases as a result of people going back to work. A microscope image of a cell infected with the novel coronavirus, grown in culture at the University of Hong Kong. Multiple virus particles are being released from the cell surface.Credit: John Nicholls, Leo Poon and Malik Peiris/The University of Hong Kong 10 February 14:00 gmt — Watch: how scientists are fighting the coronavirus outbreak Nature senior biology reporter Heidi Ledford describes how researchers including epidemiologists, geneticists and virologists are mobilizing in response to the escalating outbreak.   10 February 11:30 gmt — Pangolins claimed as outbreak source Two researchers at the South China Agricultural University in Guangzhou have suggested that pangolins — long-snouted mammals often used in traditional Chinese medicine — are the probable animal source of the coronavirus outbreak. Shen Yongyi and Xiao Lihua reported at press conference on 7 February that they had identified the pangolin as the potential source of the virus, named nCoV-2019, based on a genetic comparison of coronaviruses taken from the animals and from humans infected in the outbreak. The identity of the animal source of nCoV-2019 has been one of the key questions that researchers have been trying to answer. Scientists have already suggested that nCoV-2019 originally came from bats, on the basis of the similarity of its genetic sequence to those of other known coronaviruses, but the virus was probably transmitted to humans by another animal. Researchers say that the suggestion that pangolins spread the coronavirus to people seems plausible — but caution that the researchers’ work is yet to be published. Pangolins are scaly mammals often used in traditional Chinese medicine.Credit: Getty 10 February 04:30 gmt — Deaths in China surpass toll from SARS More than 900 people in China have died from the new virus, a greater number than those who died from severe acute respiratory syndrome (SARS) in the 2002–03 epidemic. That outbreak, which also originated in China, killed 774 people worldwide. The number of people in the country infected with the new coronavirus has risen to 40,171, according to the latest update from China’s National Health Commission. 7 February 10:15 gmt — Global infections pass 30,000 The number of people worldwide infected with the coronavirus has passed 30,000 — with the vast majority of cases occurring in China. Chinese health authorities reported on 7 February that 31,161 people had contracted the infection in China, and more than 630 people had died. Among other countries, those with the most cases include Singapore, Thailand and Japan, which have each reported between 25 and 30 infections so far. Researchers at Johns Hopkins University in Baltimore, Maryland, are visualizing global infection data in real time. Chinese paramilitary policemen prepare to patrol at Nanning Wuxu International Airport in the city of Nanning in southern China.Credit: Costfoto/Barcroft Media/Getty 4 February 18:30 gmt — Listen: how scientists are responding to the coronavirus outbreak Nature senior reporter Ewen Callaway and the Nature Podcast’s Benjamin Thompson discuss the fast-paced research happening in response to the outbreak — from diagnostics to finding the pathogen’s animal host.   4 February 11:00 gmt — Cases in China pass 20,000 The number of people in China infected with the coronavirus reached 20,438 on 3 February, after more than 3,000 new cases were reported in a day. China’s National Health Commission also reported another 64 deaths, bringing the total to more than 420 in mainland China. And media outlets are reporting the death of a man in Hong Kong, the second fatality to be reported outside the mainland. 3 February 16:30 gmt — Scientists eager to study samples of live virus Researchers worldwide are keen to get their hands on samples of the coronavirus, to probe the pathogen’s biology in detail and to test drugs and develop vaccines. Virologists in China who first isolated the virus found that it can kill cultured human cells and that it enters cells through the same molecular receptor as the coronavirus that causes severe acute respiratory syndrome (SARS).   Scientists say that physical samples have advantages over genetic sequences, dozens of which are now available. They can be used to engineer animal models of the infection and to answer key questions about how the pathogen spreads. Labs in the United States, France, Germany and Hong Kong are isolating and preparing to share virus samples taken from local patients. “It is essential that viruses are shared,” said Maria Van Kerkhove, an infectious-disease epidemiologist at the World Health Organization in Geneva, Switzerland. Workers assemble detection kits for the coronavirus.Credit: EPA-EFE/Shutterstock 31 January 05:00 gmt — Experts consider what’s next As cases of the coronavirus continue to climb, experts are considering best- and worst-case scenarios based on the behaviour of previous epidemics and what scientists already know. Cases in China surged to 9,692 on 30 January. One model estimates that the virus could infect about 39,000 of the 30 million people living in the region of Wuhan. “It seems like the virus has got out of hand in China, spread too far, too quickly to really be contained,” says Ian Mackay, a virologist at the University of Queensland in Brisbane, Australia. One big question is whether the coronavirus will continue to circulate in the community. If efforts to contain the virus fail, there’s a reasonable chance that it will become endemic. As with influenza, this could mean that deaths occur every year as the pathogen circulates. During the current outbreak, there have been several cases of infected people displaying no symptoms. If such cases are common, it will be more difficult to control the disease’s spread, making it more likely that the virus will become endemic. A worker disinfects a neighbourhood in Qingdao, in east China's Shandong province. Credit: Yu Fangping/Utuku/Ropi/Zuma/eyevine 30 January 19:45 gmt — World Health Organization declares global emergency The World Health Organization (WHO) has declared the coronavirus outbreak a global health emergency. The organization’s director-general, Tedros Adhanom Ghebreyesus, said his main concern was that the outbreak could spread to countries with fragile health systems. Declaring a “public health emergency of international concern” (PHEIC) is the WHO’s highest level of alarm — a step it reserves for events that pose a risk to multiple countries and that require a coordinated international response. The WHO considered declaring the coronavirus a global emergency last week, but ultimately decided against the move. At that time, only one country outside China — Vietnam — had confirmed person-to-person transmission of the virus within its borders. Now, four other places outside mainland China — Japan, Taiwan, Germany and the United States — have reported person-to-person transmission, as the size and reach of the outbreak have grown. The WHO says that a total of 7,818 cases have been confirmed in 18 countries. Almost 99% of those — 7,736 cases — are in China. Tedros praised China for its handling of the outbreak, and recommended against restricting international trade with, and travel to, the country. “This declaration is not a vote of no confidence in China,” he said. “This is the time for solidarity, not stigma.” Alexandra Phelan, a global-health lawyer at Georgetown University in Washington DC, praised Tedros’s decision and message. “Historically, a PHEIC is seen as an alert to all countries,” she says. “But this focus on countries with weak health systems highlights the obligation of wealthier countries with stronger health systems to help them prepare for potential cases.” 30 January 17:45 gmt — Human-to-human transmission confirmed in the United States The first instance of the coronavirus spreading between people in the United States has been confirmed. The US Centers for Disease Control and Prevention says that a woman living in Illinois who had visited Wuhan passed the virus to her spouse. Coronaviruses take their name from their crown-like shape.Credit: Getty 29 January 04:00 gmt — Australian researchers grow virus in cell culture Researchers in Melbourne, Australia, are the first outside China to announce that they’ve grown the new coronavirus in cell culture. The group at the Peter Doherty Institute for Infection and Immunity says it isolated the virus from the first person diagnosed with the infection in Australia, on 25 January. The team will now share the virus with research labs around the world recommended by the World Health Organization (WHO) to help the development of more accurate diagnostic tests and vaccines, says Mike Catton, a deputy director of the institute. “There are some things that are much easier to do when you have the virus,” says Catton.   Although scientists in China say they’ve been able to grow the virus in the lab, they have not yet shared samples with international researchers — they have shared only the virus’s genetic sequence, says Julian Druce, head of the Virus Identification Laboratory at the Doherty Institute. He says he and his team had heard that labs outside China had struggled to grow the virus, but they found it quite easy. He thinks the success was due to the lab’s combined expertise in diagnosing infections as well as isolating and growing viruses in culture. “We’ve got two parts of the puzzle together in one laboratory,” he says. Catton says having samples of the virus will enable scientists to create tests that can detect specific immune cells — antibodies — that indicate whether a person has been infected with the new virus. Such tests are especially useful for people with mild or no symptoms. Making a test for antibodies is difficult without samples of the virus, he says. A study of a family in Shenzhen, China, identified a child who was infected with the virus but showed no symptoms. The WHO has also reported that three people with the infection outside China have been asymptomatic. Ian Mackay, a virologist at the University of Queensland in Brisbane, Australia, says the Melbourne group’s announcement is fantastic news. He says lab-grown samples are essential for research into the behaviour of the virus in culture or in animal hosts. Although virus samples can also be used to validate molecular diagnostic tests, most labs have moved away from using whole viruses in favour of synthetically producing parts of the virus from partial genomes, says Mackay. A worker produces protective suits at a factory in Nantong in China’s eastern Jiangsu province.Credit: AFP/Getty 28 January 16:00 gmt — First human-to-human transmission outside China The new coronavirus has spread between humans outside China for the first time. A German man acquired the infection from a colleague who had returned from Wuhan, according to news reports and a statement from Bavaria’s health ministry. In a 27 January update, the World Health Organization confirmed that a person in Vietnam had acquired the virus from an infected family member. And media reports indicated today that a tour-bus driver in Japan who had transported tourists from Wuhan had also tested positive for the coronavirus.   Bavaria’s health ministry said it considered the risk of the virus spreading further to be low. However, more cases of human-to-human spread outside China are likely, said Michael Head, an epidemiologist at the University of Southampton, UK, in a statement distributed by the UK Science Media Centre. “But the indications are at this stage that onwards transmission will be limited.” David Heymann, an epidemiologist at the London School of Hygiene and Tropical Medicine, agrees that more cases of human-to-human transmission beyond China are likely. Health authorities can limit transmission by isolating infected people and closely following individuals they have had contact with. But these measures will be effective only if the virus is not able to spread widely through the air. “You can generally keep an outbreak at very low levels unless it’s aerosolized,” says Heymann. If the virus can spread through the air, contact-tracing and isolation are unlikely to stem transmission. Evidence so far suggests that the coronavirus spreads only through close contact and through saliva droplets, but Heymann says there is an urgent need to find out whether it can also spread through air. “The frustrating point is that until now we don’t have all the evidence that can tell exactly how this disease is transmitted.” 28 January 05:00 gmt — Cases increase by more than 60% The number of confirmed cases in China has jumped to 4,515, up from 2,744 on 26 January, according to the Chinese Center for Disease Control and Prevention. Authorities also report that more than 100 people have died as a result of the infection. Confirmed cases outside China have reached at least 37, but no deaths have been reported outside the country.   Raina MacIntyre, an epidemiologist at the University of New South Wales in Sydney, Australia, says that although the rise in cases probably reflects an increase in the authorities testing and detecting the virus, the dramatic jump is concerning. “It’s very much a dynamic picture, and until we have an indication that cases are declining, it’s going to continue to be of concern,” she says. But MacIntyre also notes that researchers are struggling to accurately model the outbreak and predict how it might unfold, because the case-report data that’s being released by the Chinese authorities is incomplete. “What we need to identify is when people got sick, not when the cases were reported, and all we’ve seen so far is when the cases were reported.” 27 January 13:30 gmt — Scientists estimate how quickly virus spreads As the number of confirmed cases of the novel coronavirus climbs into the thousands, scientists around the world are estimating how easily the virus is passed between people — and trying to determine whether those without symptoms can spread it. One number that epidemiologists want to know is how many people one person with the virus tends to infect — known as R0, or R-naught. An R0 higher than 1 means that countermeasures, such as quarantine, will be needed to contain the pathogen’s spread. On Thursday evening, after a meeting of an emergency committee responding to the outbreak, the World Health Organization (WHO) published an estimated R0 of 1.4 to 2.5. Other teams have since come up with slightly higher values1,2. These estimates are similar to the R0 of SARS during the early stages of the 2002–03 outbreak, and of the novel strain of H1N1 influenza that caused a pandemic in 2009. But they are higher than R0 figures estimated during outbreaks of the Middle East respiratory syndrome (MERS) virus, a coronavirus similar to SARS.   “Now it’s in the range of these other important epidemics, and that indicates the potential that it will cause a similar scale of public-health concern if nothing else happens,” says Mark Woolhouse, an epidemiologist at the University of Edinburgh, UK. But researchers caution that R0 estimates come with large uncertainties, because of gaps in the data, and the assumptions used to calculate the figure. They also point out that R0 is a moving target that changes over the course of an outbreak — as control measures are implemented — and varies from place to place. In the coming days, health authorities and researchers will be looking for signs that the steps the authorities have taken to stem transmission, such as the travel restrictions in Wuhan and other Chinese cities, have reduced the R0 there. Another important unanswered question surrounding the virus’s spread is whether — and how extensively — people without symptoms can infect others. A study of a cluster of five infections in a family in Shenzhen identified a child who was infected with the virus but did not show any symptoms3. If such asymptomatic cases are common and these individuals can spread the virus, then containing its spread will be much more difficult, researchers say. Few SARS cases were asymptotic, and this was key to controlling the virus.   “Defining the scale of asymptomatic transmission remains key: if this is a rare event then its impact should be minimal in terms of the overall outbreak,” Jonathan Ball, a virologist at the University of Nottingham, UK, said in a statement distributed by the UK Science Media Centre. “But, if this transmission mode is contributing significantly then control becomes increasingly difficult.” One way to determine whether symptom-free people can spread the virus would be to study its spread in individual households in China, says Sheila Bird, a biostatistician at the University of Cambridge, UK. By closely monitoring all the members of a household in which one person is infected, it should be possible to determine who else contracts the virus and how. Such studies would also be helpful for identifying ways of stopping spread in households, Bird adds. Travel in and out of the Chinese city of Wuhan has been suspended.Credit: China Daily via Reuters 27 January 12:45 gmt — Scientists speak out from locked-down Wuhan Nature has spoken to several scientists who are in Wuhan, which has been on lockdown since last week in a bid to halt the spread of the coronavirus. “The street is near empty,” says Fei Chen, a materials scientist at the Wuhan University of Technology. Researchers say that they are spending most of their time at home, and some have had to cancel travel to conferences. A scientist who is trying to get into Wuhan — to work with collaborators to test drug compounds that could work against coronaviruses — also described his experiences and motivation to Nature. Rolf Hilgenfeld, who is based at the University of Lübeck in Germany, has been trying to develop coronavirus drugs since the 2002–03 SARS outbreak, and wants to test two compounds on animal models in Wuhan. 27 January 03:00 gmt — Death toll rises At least 80 deaths have now been associated with the virus, all in China, and confirmed cases of the infection, mostly in mainland China, have passed 2,700. Cases have also been confirmed in Taiwan — and in Thailand, Australia, Malaysia, Singapore, France, Japan, South Korea, the United States, Vietnam, Canada and Nepal. 24 January 20:00 gmt — What you need to know about the virus   24 January 16:30 gmt — Second US infection The US Centers for Disease Control and Prevention (CDC) confirmed on 24 January that a second person in the United States had been infected with the new coronavirus. A woman in her sixties returned to her home in Chicago, Illinois, on 13 January after visiting Wuhan, the Chinese city where the outbreak began. She experienced symptoms a few days later. Doctors immediately suspected an infection with the coronavirus on the basis of her travel history. They admitted her to an isolation room and sent blood samples to the CDC’s laboratory. She remains in hospital but, in a press release, the CDC says that she is doing well. The agency warns that there will probably be more US cases of the coronavirus in the coming weeks. But it adds: “The immediate risk of this new virus to the American public is believed to be low at this time.” 23 January 20:00 gmt — World Health Organization decides against emergency declaration The WHO has decided not to declare the coronavirus outbreak a global health emergency, it said on 23 January. “At this time there is no evidence of human-to-human transmission outside China,” said WHO director-general Tedros Adhanom Ghebreyesus. “That doesn’t mean it won't happen.” The WHO committee that considered whether to declare a global emergency — the agency’s highest level of alarm — met for two days before issuing its verdict. The panel decided against the declaration in part because the virus’s rate of spread between humans remains unknown. “For now, it appears limited to family groups and health workers caring for infected patients,” Tedros said. A team of researchers pointed to the many-banded krait snake as one possible source of the coronavirus that originated in Asia.Credit: Alamy 23 January 15:45 gmt — Scientists dismiss claim that snakes spread virus Scientists are trying to identify the animals in which the epidemic probably began. In a controversial study published last night, a team of researchers in China claimed snakes were the culprit. But many scientists are sceptical of this claim and say there is no proof that viruses such as those behind the outbreak can infect species other than mammals and birds. “Nothing supports snakes being involved,” says David Robertson, a virologist at the University of Glasgow, UK. 23 January 15:00 gmt — Chinese authorities lock down Huanggang A second city in China — Huanggang — is going into lockdown similar to that in Wuhan. Huanggang has a population of about 7 million people and is around 70 kilometres from Wuhan. Public bus and railway operations will be suspended from midnight, Reuters reports. A third city, nearby Ezhou, has shut its train stations. 23 January 04:00 gmt — Chinese government closes off Wuhan Chinese authorities have suspended all travel in and out of Wuhan — the city at the centre of the outbreak, home to more than 11 million people — in an effort to control the worsening outbreak. Since 10 a.m. Chinese local time, planes and trains leaving the city have been suspended, and buses and the city’s subway have also stopped running. The announcement is a considerable escalation in China’s response to the outbreak, but whether it will be effective is unclear, says Ian Mackay, a virologist at the University of Queensland, Brisbane. Although quarantining the city might help to curb the international spread of the virus, it won’t stop it from being transmitted between people in the city. Mackay worries that the authorities might have “just created a large cell-culture dish in which all these people will share the infection and create a lot more cases all stuck in Wuhan”. Authorities are monitoring travellers at several airports to contain the spread of the coronavirus.Credit: Mast Irham/EPA-EFE/Shutterstock Mackay also questions whether the city will be able to feed its citizens and manage the increasing number of people who have become sick with the virus, as well as with seasonal influenza, without the free flow of supplies and aid from outside the city. He says the lockdown could have a psychological effect on people. 22 January 20:00 gmt — World Health Organization delays decision on emergency declaration The WHO has postponed a decision on whether to declare the outbreak a “public health emergency of international concern”— a step it reserves for events that pose a risk to multiple countries and that requires a coordinated international response. The move follows a meeting of a committee organized to respond to the outbreak. The same committee will meet again on 23 January. “This is an evolving and complex situation,” WHO director-general Tedros Adhanom Ghebreyesus said in a press conference after the meeting. 22 January 16:45 gmt — Five questions researchers have about China virus Scientists around the world are racing to find out more about the coronavirus — including how it spreads and information about its genetic sequences. Researchers have already sequenced several strains of the virus taken from infected people. This information can help to reveal how easily the virus can pass between humans and whether the outbreak has the potential to persist. Researchers in China are also hoping to study whether drugs could be developed to fight the virus. 21 January 19:45 gmt — First US case confirmed The United States has confirmed its first case of the new coronavirus, the US Centers for Disease Control and Prevention (CDC) said on 21 January. A 30-year-old man in Washington state has been diagnosed with the illness after a trip to China, making the United States the fifth country to report the disease — and the first outside Asia. The man had been admitted to a hospital in Washington last week with pneumonia, but “is right now, very healthy”, Nancy Messonnier, director of the CDC’s National Center for Immunization and Respiratory Diseases in Atlanta, Georgia, told reporters. He is under observation at the hospital.   The CDC says that the man did not have symptoms on his arrival in Seattle, Washington, but developed a fever on 16 January and sought treatment. A hospital in Washington state collected blood from the man and shipped it to the CDC, which identified the virus in the samples on 20 January. The CDC is now tracking down individuals who had contact with the man. International airports in New York City, Los Angeles and San Francisco, California, have been screening arriving passengers for signs of coronavirus infection since 17 January. All three receive direct flights from Wuhan. The CDC says it will now expand the screening to airports in Atlanta and Chicago, Illinois. All travellers leaving Wuhan for the United States will be routed to one of the five airports that have screening programmes. 21 January — Researchers must share sequences In an editorial, Nature says that researchers have a crucial role in publishing and sharing genome sequences. It also calls on China’s health authorities to continue reporting what they know and what more they are uncovering, and on the WHO to lead and coordinate the global response. 21 January — Chinese health workers infected Infections have been confirmed in 15 health-care workers in Wuhan; scientists say this suggests that the virus is more adept at human-to-human transmission than was first thought. Previously, Chinese authorities and the WHO had said that there had been some limited cases of human-to-human transmission between family members, but that animals seemed to be the most likely source of the virus. In response to the worsening outbreak, the World Health Organization has called a meeting on 22 January to decide whether to declare a public-health emergency. Officials want to know but predictions vary wildly, from now to after hundreds of millions of people are infected. As HIV drugs, stem cells and traditional Chinese medicines vie for a chance to prove their worth, the World Health Organization attempts to bring order to the search. Concerns are rising about the virus’s potential to circulate undetected in Africa and Asia. Genetic sequences of viruses isolated from the scaly animals are 99% similar to that of the circulating virus — but the work is yet to be formally published. Scientists need the pathogen to probe the biology of the emerging infection and to develop tests, drugs and vaccines. Experts weigh up the best- and worst-case scenarios as the World Health Organization declares a global health emergency. Research papers and preprints are appearing every day as researchers worldwide respond to the outbreak. Video: How science can help control the outbreak. Measures to contain a new virus’s spread have cut off the city's researchers. Structural biologist Rolf Hilgenfeld has been working on coronavirus treatments since the SARS outbreak. One genetic analysis suggests reptilian reservoir — but researchers doubt that the coronavirus could have originated in animals other than birds or mammals. Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond. Chinese officials have confirmed that the virus is spreading between people, but it’s still unclear how easily this happens. Vigilance, preparedness, speed, transparency and global coordination are now crucial to stopping a new infectious disease from becoming a global emergency. Chinese officials reported more than 100 new infections and South Korea confirmed its first case. The legacy of SARS has haunted the race to understand a respiratory infection that has affected 60 people. Correction 31 January 2020: An earlier version of this story misstated the number of countries in which human-to-human transmission has been confirmed. Read, J. M. et al. Preprint at MedRxiv https://www.medrxiv.org/content/10.1101/2020.01.23.20018549v1 (2020). Liu, T. et al. Preprint at BioRxiv https://www.biorxiv.org/content/10.1101/2020.01.25.919787v1 (2020). Fuk-Woo Chan, J. et al. Lancet https://doi.org/10.1016/S0140-6736(20)30154-9 (2020). Download references Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Infection News 22 MAY 20 News 19 MAY 20 News 18 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '153'>22 April 2020</date>
<url id = '154'>https://nature.com/articles/d41586-020-01179-x</url>
<title id = '154'>Support grows for a controversial ‘human challenge’ vaccine study — but no trial is yet planned.</title>
<body id = '154'>An effective vaccine is seen by many scientists as the only way out of the coronavirus pandemic.Credit: Martin Mejia/AP/Shutterstock Momentum is building to speed the development of coronavirus vaccines by intentionally infecting healthy, young volunteers with the virus. A grass-roots effort has attracted nearly 1,500 potential volunteers for the controversial approach, known as a human-challenge trial. The idea is also gaining traction with US politicians. The effort, called 1Day Sooner, is not affiliated with groups or companies developing or funding coronavirus vaccines. But co-founder Josh Morrison hopes to show that there is broad support for human-challenge trials, which have the potential to deliver an effective coronavirus vaccine more quickly than standard trials.   Typical vaccine trials take a long time because thousands of people receive either a vaccine or a placebo, and researchers track who becomes infected in the course of their daily lives. A challenge study could in theory be much faster: a much smaller group of volunteers would receive a candidate vaccine and then be intentionally infected with the virus, to judge the efficacy of the immunization. “We want to recruit as many people as possible who want to do this, and pre-qualify them as likely to be able to participate in challenge trials should they occur,” says Morrison, who is also the executive director of organ-donation advocacy group Waitlist Zero. “At the same time, we feel that the public policy decisions around challenge trials will be better informed if they highlight the voice of people interested in participating in such trials.” Morrison says that the people who have signed up to be part of a challenge trial tend to be young and live in urban areas, and are highly motivated to do something constructive to address the coronavirus pandemic. “Many note that they recognize the risk but believe the benefits of vaccine acceleration are so tremendous that it is worth it to them,” he says. Challenge studies have been conducted before for diseases including influenza and malaria. A team led by bioethicist Nir Eyal at Rutgers University in New Brunswick, New Jersey, argued that a human challenge trial could be conducted safely and ethically, in a paper in The Journal of Infectious Diseases last month.   The approach is also gaining some political support. This week, 35 members of the US Congress, led by Bill Foster (Democrat, Illinois) and Donna Shalala (Democrat, Florida), called on Department of Health and Human Services director Alex Azar to consider human-challenge trials of coronavirus vaccines. Charlie Weller, head of the vaccines programme at Wellcome, a biomedical-research funder in London, says the charity has begun discussing the ethics and logistics of a human-challenge trial for a coronavirus vaccine. But she says it is unclear whether such a trial could actually speed vaccine development. Researchers first need to determine how to expose humans to the virus as safely as possible, and to consider how and even whether such studies can be done ethically. “I think there’s potential,” Weller adds, “but we’ve got so many questions to work through to understand whether it can help in the timelines we have.” Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Ethics Editorial 13 MAY 20 News Feature 13 MAY 20 Correspondence 14 APR 20 Vaccines News 19 MAY 20 News 13 MAY 20 Editorial 13 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '154'>22 April 2020</date>
<url id = '155'>https://nature.com/articles/d41586-020-01163-5</url>
<title id = '155'>Despite uncertainties, some scientists are betting that blood tests will help end lockdowns and get people back to work.</title>
<body id = '155'>Several laboratories in the US are offering antibody tests for COVID-19, but the results require nuanced interpretation.Credit: Beaumont Health Last week, laboratories across the United States launched initiatives to test hundreds of thousands of people for antibodies against the coronavirus that causes COVID-19. And they’re telling those people the results — despite uncertainty about what they mean. Researchers have been using antibody tests to try to reveal the true spread of the coronavirus in communities and cities. But these latest efforts are more controversial because they focus on an individual’s immune status. Unlike PCR-based tests, which identify genetic sequences belonging to the virus, these tests of the body’s immune response require more nuanced interpretation. One class of antibodies may serve as a proxy for whether someone is currently infected. Some researchers argue that this is useful information to have when PCR tests are unavailable or delayed. Even greater interest surrounds a different set of antibodies that could reveal whether a person is protected from contracting COVID-19 again. Betting that antibody tests will loom large in the fight against the pandemic, clinics and labs affiliated with hospital networks in Virginia, Washington, Kentucky, Michigan and Ohio have already invested hundreds of thousands of dollars in equipment and infrastructure to run the tests at scale.   They’re couching the results they give to people in careful terms, and expect to adapt their procedures as new information comes to light. But because of the urgent need to get ahead of the outbreak, to prevent more people from dying and allow businesses to reopen, they argue that it’s important to just get going. The United States is the world’s epicentre of COVID-19 — more than 42,000 people have died with the disease in seven weeks. If the bets on antibody testing pay off, their initiatives will provide a model that other states can follow. “One thing we have learned is that we can’t let perfect be the enemy of good — we need to act,” says Keith Jerome, director of the clinical-virology department at the University of Washington School of Medicine in Seattle, which has built capacity to process 15,000 antibody tests per day. “If we wait for every question to be answered, we won’t be ready for months.” In mid-March, Rashid Chotani, an infectious-disease epidemiologist at CareLife Medical rented a floor of a medical building in Merrifield, Virginia to start offering rapid antibody tests. He has been using a 10-minute pin-prick test to screen people at risk of COVID-19, such as health-care workers, essential workers such as those with construction jobs, and the family members of those who have the disease. Many who visit his clinic can’t get a PCR-based test because a lot of hospitals offer them only to people with symptoms. Antibody testing misses early infections, but Chotani suggests it could still be useful for people who are asymptomatic but afraid of passing the virus to someone else at work or at home. In general, the immune system produces different waves of antibodies to fight off invaders. One of the first waves, called immunoglobulin M (IgM), is generated around 4–10 days after a coronavirus infection.   Chotani recalls how a doctor who came to his clinic three weeks ago tested positive for IgM. Chotani backed up that result with a second test, then took a nasal swab and shipped it to a commercial lab to confirm the result with an official PCR-based diagnosis. But because those tests can take at least two days to process, he advised the doctor not to treat people until then. This is against the rules at many US hospitals, which ask nurses and doctors to work unless they have symptoms or a positive PCR test. But many health workers are nervous about this rule because the rate of asymptomatic coronavirus infections might be around 40%. “This is the dilemma we’re facing,” Chotani says. In a white paper sent to policymakers in six states, he and his colleagues suggest that rapid antibody tests could help curb the spread of the virus in workplaces that can’t screen their staff with PCR. But another dilemma is that the results of the rapid antibody tests can be wrong. The test Chotani is using has a 2% chance of giving a false positive result and a 15% chance of a false negative, according to its manufacturer, Boston Biopharma. Around 70 antibody tests have been developed in the past month, and they have not been thoroughly vetted for accuracy by the US Food and Drug Administration (FDA). That’s why the FDA and the World Health Organization say that only PCR-based tests for the virus should be used to confirm a case.   That point is debatable. China has updated its clinical guidelines to include a positive IgM result as an indication of an acute coronavirus infection. And many outbreak specialists are tolerant of imperfections because of the urgency of the situation, the lack of PCR tests and potential inaccuracies in those as well. Ranu Dhillon, an epidemic-response specialist at Harvard Medical School in Boston, Massachusetts, says that he, too, would stay at home if he got a positive result for IgM with a test that was relatively reliable. “Unless it’s a war-like situation when it’s all hands-on-deck,” he says, “I’d assume I was infected until proven otherwise.” Testing for immunity to COVID-19 is also fraught. In the past week, politicians around the world have begun to focus on a later wave of antibodies generated in an immune response — called immunoglobulin G, or IgG — because of the hope that these antibodies might protect a person from getting the disease again. The immune system slowly produces this class of antibody, which ‘remembers’ a particular virus after an infection has passed. For some diseases, IgG antibodies encountering that virus neutralize it so that it can’t harm cells. Beaumont Health in Detroit, Michigan is screening thousands for antibodies that could indicate immunity to COVID-19.Credit: Beaumont Health In Kentucky, Aruni Bhatnagar, an environmental health scientist at the University of Louisville helped to convince officials to test 6,000 health workers in the city with rapid antibody tests made by Cellex and Becton Dickinson. Hospital staff are at high risk of infection, so the team can observe whether those who test positive for IgG have a reduced risk of getting COVID-19. Bhatnagar’s team also runs additional experiments on IgG-positive blood samples. They measure antibody levels with a more sophisticated assay called an ELISA. And they assess whether the IgG antibodies can stop the coronavirus from infecting cells in a dish. If they do, researchers will ask the individuals who gave the samples to donate blood for a plasma bank that they hope can help treat people with COVID-19. “We aren’t hit so hard with the disease now,” Bhatnagar explains, “so we can do this research in Kentucky that will help the United States.” At the same time, researchers at Beaumont Health in Detroit, Michigan, are screening 38,000 doctors, technicians and other hospital staff for IgG antibodies, using an ELISA diagnostic test from EuroImmun. This test will allow them to learn if a certain level of the antibodies is required for protection.   In all of these cities, researchers are telling people their antibody status, but are doing so in careful terms. They say that people take comfort in being given more information at this bewildering time — even if that exposes the uncertainty inherent in the scientific process. But many scientists argue that it’s too early to use these tests and reveal the results. Amy Lockwood, a public-health specialist at the San Francisco Department of Health in California, worries about inaccuracies, or causing confusion. At the moment, she says, only PCR tests for the virus reliably reveal if it’s there: “Everything else is magical thinking.” Antibody tests are a litmus test for scientists’ personalities, observes Scott Boyd, an immunologist at Stanford University in California. “Some people are incredibly uncomfortable because we lack a lot of information,” he says, “and some say it’s the best we can do now, so let’s just do something.” Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Medical research News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '155'>21 April 2020</date>
<url id = '156'>https://nature.com/articles/d41586-020-01154-6</url>
<title id = '156'>An online survey reveals bottlenecks and challenges and barriers faced by more than 1,700 biology labs.</title>
<body id = '156'>A lab scientist runs a clinical test at University of Washington Medicine in Seattle.Credit: Karen Ducey/Getty A survey of more than 4,000 researchers in the United States suggests that better coordination at an institutional and national level could make hundreds of thousands more tests for coronavirus available. The survey was prompted by a Nature investigation published on 9 April, revealing that several top university laboratories that have received regulatory approval to process tests for SARS-CoV-2 are operating at half their potential capacity.   Testing is urgently needed. Hospitals continue to face delays in providing test results, and nursing homes, homeless shelters and other shared-living facilities report a lack of tests for screening at-risk residents. US officials mainly agree that more testing is needed if they are to loosen social-distancing and lockdown measures. To find out what is preventing molecular-biology laboratories from helping with this effort, Giovanni Paternostro, a biomedical researcher at Sanford Burnham Prebys Medical Discovery Institute in La Jolla, California, and Joshua Graff Zivin, an economist at the University of California, San Diego, sent a survey to 35,000 principal investigators who had received grants from the US National Institutes of Health (NIH) in 2018.  Of the more than 4,000 researchers who responded within the first week, about 130 were already running tests to detect the new coronavirus. Nearly 1,600 said that they had the main tool needed to run tests, a real-time PCR machine, and operated under the biosafety conditions required for working with pathogenic organisms such as coronavirus. But they were not testing. Both groups — those who are testing and those who could — were asked what they would need to process more tests or to begin testing. Resources such as reagents and funding were a popular response for both groups, as was coordination by the NIH or their own institution. About 95% of labs not currently testing said they needed more information on protocols and regulations, such as the key Clinical Laboratory Improvement Amendments (CLIA) certification for providing clinical test results. But 43% of labs currently doing testing said that they could do with more information in these areas as well.  Almost 1,000 investigators left comments, with many describing the bottlenecks they face or their interest in assisting with the coronavirus response in other ways. One respondent, George Murphy, a stem-cell biologist who is running a coronavirus-testing operation at the Boston University School of Medicine in Massachusetts, says the survey and comments made him realize how lucky he was that his administrative leaders were flexible and pushed hard through regulatory hurdles. “Coordination at the institutional level was key,” he says. Latest on: Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Medical research News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 SARS-CoV-2 News 22 MAY 20 Career Column 22 MAY 20 News Q&A 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '156'>21 April 2020</date>
<url id = '157'>https://nature.com/articles/d41586-020-01153-7</url>
<title id = '157'>An unprecedented signal from unevenly sized objects gives astronomers rare insight into how black holes spin.</title>
<body id = '157'>A visualization of a collision between two differently sized black holes.Credit: N. Fischer, H. Pfeiffer, A. Buonanno (Max Planck Institute for Gravitational Physics), Simulating eXtreme Spacetimes (SXS) Collaboration Gravitational-wave astronomers have for the first time detected a collision between two black holes of substantially different masses — opening up a new vista on astrophysics and on the physics of gravity. The event offers the first unmistakable evidence from these faint space-time ripples that at least one black hole was spinning before merging, giving astronomers rare insight into a key property of these these dark objects. “It’s an exceptional event,” said Maya Fishbach, an astrophysicist at the University of Chicago in Illinois. Similar mergers on which data have been published all took place between black holes with roughly equal masses, so this new one dramatically upsets that pattern, she says. The collision was detected last year, and was unveiled on 18 April by Fishbach and her collaborators at a virtual meeting of the American Physical Society, held entirely online because of the coronavirus pandemic.   The Laser Interferometer Gravitational-Wave Observatory (LIGO) — a pair of twin detectors based in Hanford, Washington, and Livingston, Louisiana — and the Virgo observatory near Pisa, Italy, both detected the event, identified as GW190412, with high confidence on 12 April 2019. The LIGO–Virgo collaboration, which includes Fishbach, posted its findings on the arXiv preprint server. LIGO made the first discovery of gravitational waves in September 2015, detecting the space-time ripples from two merging black holes. LIGO, later joined by Virgo, subsequently made ten more detections in two observing runs that ended in 2017: nine more black-hole mergers and one collision of two neutron stars, which helped to explain the origin of the Universe’s heavy chemical elements. The third and most recent run started on 1 April 2019 and ended on 27 March 2020, with a month-long break in October. Greatly improved sensitivity enabled the network to accumulate around 50 more ‘candidate events’ at a rate of roughly one per week. Until now, the international collaboration had unveiled only one other event from this observation period — a second merger between two neutron stars, dubbed GW190425, that was revealed in January. The latest event is unique. One of the two black holes that merged had an estimated mass of around 8 solar masses, and the other was more than 3 times larger, at 31 solar masses. This imbalance made the larger black hole distort the space around it, so the other’s trajectory deviated from a perfect spiral. This could be seen in the resulting gravitational waves, which were created as the objects spiralled into each other. All the other merger events that have been unveiled produced a wave that forms a similar ‘chirp’ shape — which increases in both intensity and frequency up to the moment of collision. But GW190412 was different: its intensity didn’t simply rise as in a chirp. “This makes this system very interesting, just looking at the morphology of the signal,” Fishbach said. Physicists had eagerly awaited such ‘non-vanilla’ events because they provide new, more precise ways of testing Albert Einstein’s theory of gravity, the general theory of relativity. “We are in a new regime of testing general relativity,” said Maximiliano Isi at the Massachusetts Institution of Technology in Cambridge, another LIGO member who was presenting at the meeting. In particular, researchers were able to use this data to discern the ‘spin’ of black holes. “We know with confidence that this heavier object had to be spinning,” said Isi. Previous events had left researchers baffled: observations of black holes in the Milky Way suggested that black holes should have high spins, but this did not show up in gravitational-wave data from the first two runs. Astrophysicists hope that detecting spins can shed light on how the black holes formed and came to orbit each other. The richer information in asymmetrical mergers helps to measure an event’s distance from the Milky Way with better precision. Accumulating many such measurements could provide a new way to map the history of expansion of the Universe. The LIGO–Virgo collaboration will continue to publish more results from its vast trove of unpublished data, including individual events that are particularly interesting or exciting, says Virgo’s Jo van den Brand, a physicist at the National Institute for Subatomic Physics in Amsterdam. “I think the harvest is quite good, let me put it like that.” Latest on: Astronomy and astrophysics News & Views 20 MAY 20 Article 20 MAY 20 News Q&A 19 MAY 20 Physics Article 21 MAY 20 News & Views 20 MAY 20 Article 20 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '157'>20 April 2020</date>
<url id = '158'>https://nature.com/articles/d41586-020-01115-z</url>
<title id = '158'>Touted as society’s way out of widespread lockdowns, scientists say the true potential of these rapidly developed tests is still unknown.</title>
<body id = '158'>Antibody tests might be used to help stem the COVID-19 pandemic — but first must overcome several hurdles.Credit: Greg Baker/AFP/Getty British Prime Minister Boris Johnson called them a ‘game changer’. Antibody tests have captured the world’s attention for their potential to help life return to normal by revealing who has been exposed, and might now be immune, to the new coronavirus. Dozens of biotech companies and research laboratories have rushed to produce the blood tests. And governments around the world have bought millions of kits, in the hope that they could guide decisions on when to relax social-distancing measures and get people back to work. Some have even suggested that the tests could be used as an ‘immunity passport’, giving the owner clearance to interact with others again. Many scientists share this enthusiasm. The immediate goal is a test that can tell healthcare and other essential workers whether they are still at risk of infection, says David Smith, a clinical virologist at the University of Western Australia in Perth. In the future, they could also assess whether vaccine candidates give people immunity.   But as with most new technologies, there are signs that the promises of COVID-19 antibody tests have been oversold, and their challenges underestimated. Kits have flooded the market, but most aren’t accurate enough to confirm whether an individual has been exposed to the virus. And even if tests are reliable, they can't indicate whether someone is immune to reinfection, say scientists. It will be a while before kits are as useful as hoped, says Smith. “Countries are still gathering the evidence.” The UK government learnt about this the hard way after it ordered 3.5 million tests from several companies in late March, only to later discover that none of these tests performed well enough. “No test is better than a bad test,” says Michael Busch, director of the Vitalant Research Institute in San Francisco. Antibody tests are also being used by researchers globally to estimate the extent of coronavirus infections at a population level, which is extremely valuable given that many places aren’t doing enough standard testing, and people with mild or no symptoms will probably be missed in official case counts. These surveys test a portion of the population and use that to estimate infections among the broader community. More than a dozen groups worldwide are doing such studies. When a virus invades the body, the immune system produces antibodies to fight it. Kits detect the presence of antibodies using components from the virus, known as antigens. Tests generally fall into one of two categories: lab tests that need to be processed by trained technicians and take about a day, and point-of-care tests that give rapid, on-the-spot results within 15 minutes to half an hour. Several companies, including Premier Biotech in the United States and China-based Autobio Diagnostics, offer point-of-care kits, which are designed to be used by health professionals to check if an individual has had the virus — but some companies market them for people to use at home. The tests don’t detect the virus itself, so have limited use in diagnosing active infections, say health agencies. But in some countries, such as the United States and Australia, tests are being used in some cases to diagnose people who have suspected COVID-19, but who test negative on a standard PCR test, says Smith. (A study1 by researchers at Shenzhen Third People’s Hospital in China found that PCR tests did not always diagnose patients infected with the virus.) Early studies in people who have recovered from COVID-19 have detected three kinds of SARS-CoV-2-specific antibody, and manufacturers and research institutes have developed tests that target these antibodies. For instance, the German biopharmaceutical company EUROIMMUN has developed a lab test that detects SARS-CoV-2-specific immunoglobulin G and immunoglobulin A. Because of the ongoing emergency, the US Food and Drug Administration (FDA) has relaxed the rules that govern the use of such tests. It has authorized their use in laboratories and by health-care workers to diagnose active COVID-19 infection, with the disclaimer that they have not been reviewed by the FDA and that results should not be used as the sole basis for confirming that someone has the disease. Australia has also introduced similar emergency authorizations. These measures are appropriate given the pandemic situation, says Smith. Antibody tests in people who might be actively infected can be an important part of managing patients at hospitals, and contact tracing, although the results need to be interpreted cautiously, he says. One problem, however, is that most kits have not undergone rigorous testing to ensure they’re reliable, says Busch. During a meeting at the UK Parliament’s House of Commons Science and Technology Select Committee on 8 April, Kathy Hall, the director of the testing strategy for COVID-19, said that no country appeared to have a validated antibody test that can accurately determine whether an individual has had COVID-19.   Kits need to be trialled on large groups of people to verify their accuracy: hundreds of people who have had COVID-19, and hundreds of people who haven’t, says Peter Collignon, a physician and laboratory microbiologist at Australian National University in Canberra. But so far, most test assessments have involved only some tens of individuals because they have been developed quickly. It seems that many tests available now are not accurate enough at identifying people who have had the disease, a property called test sensitivity, and those who haven’t been infected, known as test specificity. A high-quality test should achieve 99% or more sensitivity and specificity, adds Collignon. That means that testing should turn up only about 1 false positive and 1 false negative for every 100 true positive and true negative results. But some commercial antibody tests have recorded specificities as low as 40% early in the infection. In an analysis2 of 9 commercial tests available in Denmark, 3 lab-based tests had sensitivities ranging 67–93% and specificities of 93–100%. In the same study, five out of six point-of-care tests had sensitivities ranging 80–93%, and 80-100% specificity, but some kits were tested on fewer than 30 people. Testing was suspended for one kit. Overall, the sensitivity of all the tests improved over time, with the highest sensitivity recorded two weeks after symptoms first appeared. Some of these tests are also being used to test individuals in other countries, including Germany and Australia. Point-of-care tests are even less reliable than tests being used in labs, adds Smith. This is because they use a smaller sample of blood — typically from a finger prick — and are conducted in a less controlled environment than a lab, which can affect their performance. They should be used with caution, he says. The WHO recommends that point-of-care tests only be used for research. Without reliable tests, “we may end up doing more harm than good,” says Collignon. One unknown that affects both kinds of test is the interplay between timing and accuracy. If a test is done too soon after a person is infected and the body hasn’t had time to develop the antibodies the test is designed to detect, it could miss an infection. But scientists don’t yet know enough about the timing of the body’s immune responses to SARS-CoV-2 to say exactly when specific antibodies develop.   By contrast, false positives crop up if a test uses an antigen that doesn't only target antibodies produced to fight SARS-CoV-2, and instead picks up antibodies for another pathogen as well, says Smith. An analysis3 of EUROIMMUN’s antibody test found that although it detected SARS-CoV-2 antibodies in three people with COVID-19, it returned a positive result for two people with another coronavirus. Ironing out all these issues takes time and involves trial and error, says Collignon. It took several years to develop antibody tests for HIV with more than 99% specificity, he says. Another big question surrounding antibody tests is the extent to which being infected with a pathogen confers immunity to reinfection. To have protective immunity, the body needs to produce a certain type of antibody, called a neutralizing antibody, which prevents the virus from entering cells. But it’s not clear whether all people who have had COVID-19 develop these antibodies. An unpublished analysis4 of 175 people in China who had recovered from COVID-19 and had mild symptoms reported that 10 individuals produced no detectable neutralizing antibodies — even though some had high levels of binding antibodies. These people had been infected, but it’s unclear whether they have protective immunity, says Wu Fan, a microbiologist at Fudan University in Shanghai, China, who led the study. “The situation for patients is very complicated,” says Fan. So far, researchers say they have not seen any evidence that people can get reinfected with the virus. Rhesus macaques infected with SARS-CoV-2 could not be reinfected at just under one month following their initial infection, according to an unreviewed study5 by researchers at Peking Union Medical College in Beijing. “We should presume that once you have been infected, your chance of getting a second infection two to three months later is low,” says Collignon. But how long that protective immunity will last is not known. Even if it becomes clear that most people do develop neutralizing antibodies, most tests currently don’t detect them. And tests that do are more complex to develop and not widely available.   The fact that most antibody tests can't detect neutralizing antibodies is also relevant because some politicians are pushing the idea that these tests be used to clear those with past COVID-19 infections to interact with others again, a so-called immunity passport. Researchers are trying to determine whether the antibodies detected by current kits can act as a proxy for protective immunity, says Smith. Another complicating factor for immunity passports is that antibody tests can’t rule out that a person is no longer infectious, says Smith. A study6 published in Nature this month found that viral RNA declines slowly after antibodies are detected in the blood. The presence of viral RNA could mean that the person is still shedding infectious virus. Despite the challenges, once reliable antibody tests are available, they could be important to understanding which groups of people have been infected how to stop further spread, says Collignon. They could even be used to diagnose active infections when PCR tests fail, adds Smith. </body>
<date id = '158'>18 April 2020</date>
<url id = '159'>https://nature.com/articles/d41586-020-01120-2</url>
<title id = '159'>As lockdowns force scientists worldwide to put their research on hold, funders are introducing measures to minimize stress.</title>
<body id = '159'>Credit: Matthew Horwood/Getty While science is crucial to the fight against COVID-19, researchers confined to their homes and unable to carry on with grant-funded work are becoming increasingly concerned about how the coronavirus pandemic will affect their funding. The disruption — such as the closure of labs and universities — means researchers face challenges in completing projects by their deadlines and might struggle to pay lab members when grants run out. “If this situation lasts for more than two to three months it will be impossible to finish the projects on time,” says Juan Astorga-Wells, a biochemist at the Karolinska Institute in Stockholm. He is involved in two projects supported by grants from the European Union’s Horizon 2020 research programme, with collaborators in four EU countries. “This is problematic, since most salaries and materials are being paid for using these grants.” The increasingly likely prospect of a long-term economic downturn also means science funding could face longer-term impacts, says Mark Harrison, research director at the Borneo Nature Foundation, which conducts ecological research in Indonesia. That could reduce the amount of funding from direct donations as well as government and charitable grants, says Harrison. Nature spoke to the world’s major research funders to find out how they are adapting their funding policies in response to the pandemic. Researchers funded by the European Union’s flagship research programme will be allowed “maximum flexibility”, according to guidance released in mid-March. Researchers can ask to extend the duration of Horizon 2020-funded projects by up to six months, and are allowed to reallocate funds budgeted for research, training and networking to meet the costs of working remotely, or to help pay the salaries of researchers who are unable to continue with experiments because of lockdowns or lab closures. Projects can also be reoriented towards research on COVID-19 or coronaviruses — requests will be dealt with on a case-by-case basis. Christian Ehler, a member of the European Parliament who is involved in coordinating research policy, told Nature that the deadlines for all open funding calls in Horizon 2020 have been extended. In addition, the EU is funding several new research projects related to the pandemic. The NSF says it will process daily grant payments to recipients without interruption during the pandemic. On 19 March, the funder introduced a series of measures to provide “administrative relief” to researchers affected by COVID-19. These include extensions to due dates for project reports, and the ability to charge costs to grants that would not normally be allowed — such as when events and travel are cancelled, or research activities have to be paused and restarted. Recipients can apply for no-cost extensions to their grants following the normal procedure, and some deadlines for submitting proposals have been extended. The NSF is also accepting proposals for new non-clinical research aimed at understanding the SARS-CoV-2 virus and developing strategies to respond to the pandemic. The NIH is implementing similar measures to the NSF. It says that institutions affected by COVID-19 can continue to provide stipend payments to fellows and trainees who may be unable to work as a result of COVID-19. It will also be “highly accommodating” of late grant applications submitted until 1 May, and recognizes that the situation will lead to unavoidable delays in submitting progress reports. Where plans for active research projects are disrupted, the period covered by the grant can be extended for up to 12 months beyond the original completion date. Non-refundable costs associated with grant-related travel that has been cancelled because of COVID-19 can be paid for by the NIH award if they would have otherwise been allowable. In a 13 March statement, the DOE said that it will consider requests for extensions to grant applications, letters of intent and progress reports if the lead principal investigator or the applicant’s institution are subject to a quarantine or closure. Researchers do not require approval if they need to reallocate funds because of meeting cancellations or changes to travel plans. Like other agencies, NASA is open to the possibility of extending grants if research has been disrupted by the pandemic. “We know that progress on funded research may slow and in some cases even stop due to necessary telework implemented by universities, for example and other research institutions,” said Thomas Zurbuchen, associate administrator of NASA’s Science Mission Directorate, during a virtual town hall meeting on 20 March. “We’re looking at shifting some due dates and implementing a lenient ‘late’ policy on a case-to-case basis. We want to serve the community the best way we can, encouraging all of you to continue to pay graduate students, post-docs and lab staff.”Due dates for various funding proposals have also been extended. “While we know this situation presents a number of difficulties for our missions, we are confident there is no team better prepared for doing hard things,” NASA administrator Jim Bridenstine said in a statement on 18 March. “Teams across the agency are well-practiced in responding to mission contingencies and reacting to unforeseen challenges.” “During the coronavirus pandemic, UKRI has two priorities: the safety and wellbeing of our workforce and, as far as possible, the continuation of our business as a national funder of research and innovation,” UKRI chief executive Mark Walport said in a statement on 23 March. UKRI is working mitigate the impact of the pandemic and plans to continue its funding programmes. It will also identify specific calls for funding or research disciplines that might be affected by the pandemic. In online guidance for grant recipients, it says deadlines for funding calls will be reviewed and extended if required. It also says it plans to allow no-cost extensions to existing grants where work has been disrupted by institution closures or social-distancing measures. On 9 April, UKRI announced that final-year PhD students it funds whose work has been affected by the pandemic will be given costed extensions — an extra six months and additional funding to complete their research. UKRI has put out a rapid-response call for coronavirus-related research proposals, with £20 million (US$25 million) in funding available. It has also issued guidance for researchers who want to repurpose their grant funds to focus on studying COVID-19. A statement on the society’s website says it will aim to minimize the effect of the coronavirus on funded activities and that it will provide “pragmatic support” to researchers who are funded by one of its fellowships or grants. It is working with institutions and the UK government to ensure that appropriate support is in place. Funding programmes and calls remain open for applications, without changes to deadlines, interview dates or decision dates. The UK biomedical-research charity says it will support scientists it funds who are unwell, need to self-isolate or have caring responsibilities for someone affected by COVID-19. It will supplement grants for the cost incurred by a researcher’s employer paying their salary while they’re away, minus any recoverable statutory pay. For grants due to end between 1 March and 31 December this year, Wellcome says it will provide an extra six months’ funding for staff salaries and PhD stipends. For grants due to end in 2021, it will provide an extra three months’ funding. It will also pay the running costs and salaries of anyone employed on a grant, as well as extending the grant, if a grant holder is called away to work on the coronavirus response for a month or more — for example, if clinicians are drafted in to assist the National Health Service. Wellcome-funded researchers who have incurred costs for an event or travel which has been cancelled can claim these costs against their grant if they can’t be reimbursed or claimed for under insurance. The charity also says that all of its funding schemes and calls remain open for applications. It has extended the deadlines by one week for any schemes with an application deadline up to the end of April 2020, but won’t consider requests to extend deadlines further. All other application, shortlisting, interview and decision dates will not change. The UK-based cancer-research charity will offer increased flexibility to researchers it funds, to minimize the impacts of institution closures or clinicians being redeployed. Recipients can delay the start of their grant by up to six months, or apply for a no-cost extension. And researchers are free to reallocate funds where there is underspending — for example money that would have been spent on travel could be used to cover other costs incurred as a result of the pandemic. Grants can be used to fund paid sick leave when staff or students are unwell, self-isolating or caring for someone affected by COVID-19. The agency says its grant guidelines already allow applications to be submitted late in exceptional circumstances, which would cover coronavirus disruption. It also says that grant agreements allow for variations to research projects if circumstances change after the grant has been made. “Should you need to vary your research, to either seek an extension or change its scope, we can do that later in the year when there is a more complete understanding of the impact,” said the ARC’s chief executive officer Sue Thomas in an open letter to researchers on 23 March. “Researchers will be affected in different ways by the ongoing events and we wish to assure you that we will approach any extensions with understanding.” The letter also stated that the ARC has extended the closing dates for applications to some schemes, and that it is looking at ways to minimise the administration of projects. Three-month extensions to the submission of final reports will be automatically granted on request to researchers who have been directly affected by the pandemic. The DFG says it wants its funding activities to continue as smoothly as possible. If projects are interrupted, DFG-funded scientists can transfer 2020 grant money to 2021 without paperwork. They can also ask for extra support to cover costs that arise from projects being delayed or derailed, for example staff salaries or cancellation fees. Deadlines for a number of funding proposals have been extended. Following a ‘flash call’ for proposals and a streamlined application process, the ANR has awarded funding for 44 urgent new research projects related to COVID-19. And on 19 March, the French government announced that it will increase its research budget by €5 billion (US$5.4 billion) over the next ten years. One-fifth of this will go to healthcare research to prepare for future outbreaks. It will also contribute €50 million to an emergency fund for coronavirus vaccine research. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '159'>17 April 2020</date>
<url id = '160'>https://nature.com/articles/d41586-020-01095-0</url>
<title id = '160'>Study estimates a more than 50-fold increase in coronavirus infections compared to official cases, but experts have raised concerns about the reliability of antibody kits.</title>
<body id = '160'>Limited global testing for coronavirus (yellow) has led researchers to try to estimate the prevalence of infection within specific communities.Credit: CDC/Science Photo Library Widespread antibody testing in a Californian county has revealed a much higher prevalence of coronavirus infection than official figures suggested. The findings also indicate that the virus is less deadly than current estimates of global case and death counts suggest. But some scientists have raised concerns about the accuracy of kits used in such studies because most have not been rigorously assessed to confirm they are reliable. An analysis of the blood of some 3,300 people living in Santa Clara county in early April found that one in every 66 people had been infected with SARS-CoV-2. On the basis of that finding, the researchers estimate that between 48,000 and 82,000 of the county’s roughly 2 million inhabitants were infected with the virus at that time — numbers that contrast sharply with the official case count of some 1,000 people reported in early April, according to the analysis posted today on medRxiv. The work has not yet been peer reviewed.   The results are some of the first of more than a dozen ‘sero-prevalence surveys’ being carried out in cities worldwide to try to estimate populations’ true infection rates, in the absence of widespread diagnostic testing. The World Health Organization is also running a global sero-prevalence study, known as Solidarity II. Many surveys are using commercial antibody kits to detect antibodies against the virus in blood samples. The presence of SARS-CoV-2-specific antibodies reveals that a person had been infected for at least a week earlier, even if they have had no symptoms. “A sero-survey gives you a snapshot in time of who is infected in your given population,” says Kanta Subbarao, a virologist at the Peter Doherty Institute for Infection and Immunity in Melbourne. This is especially important for an infection such as SARS-CoV-2, for which some people show no symptoms, or only mild ones, she says.   When combined with information about age, gender, symptoms, co-morbidities and socioeconomic status, these surveys can also help to answer questions about factors such as the role of children in spreading the infection, and the portion of cases that are asymptomatic. “This is a really inexpensive way to get an incredible amount of information,” says Jayanta Bhattacharya, a health economist at Stanford University in California and a co-author of the study. News of the Santa Clara analysis follows preliminary results from a similar study in Germany, released on 9 April, that tested some 500 people in a village of more than 12,000 and found that one in seven had been infected with SARS-CoV-2. The German team also looked for active infections, using diagnostic tests based on the polymerase chain reaction, and when those figures were combined with those who had antibodies, estimate that the town’s overall infection rate was 15%. But this result might not be indicative of what’s happening across Germany, says virologist Christian Drosten, who heads the Institute of Virology at the Charité university hospital in Berlin, because many people in the town celebrated at a carnival in February. “There was a big point source outbreak in that village,” he says.   The fact that both studies detected much higher rates of infection than official figures suggest is not surprising, says Peter Collignon, a physician and microbiologist at the Australian National University in Canberra. The virus had been spreading in the United States and parts of Europe for at least a month before it was detected as spreading in the community. But Collignon notes that the commercial antibody tests used in both studies were evaluated onusing only a small number of people, which could also affect the accuracy of the survey results. Antibody kits aren’t just being used for population studies. Kits are also being marketed for testing whether individuals have had the disease. But experts warn that most tests haven’t been rigorously evaluated to ensure they are reliable. Sero-surveys can also provide a better estimate of how deadly a virus is, using a measure known as the infection fatality rate (IFR) — the proportion of all infections, not just those confirmed through clinical testing, that result in death. An accurate IFR can improve models being used to decide public-health responses. If a disease turns out to be less deadly than previously estimated, this could reframe discussions around the measures being introduced to contain it, and their economic and social impact, says Neeraj Sood, a health economist at the University of Southern California in Los Angeles, who is leading a separate antibody study in Los Angeles and is also a co-author in the Santa Clara study. “We are trying to prevent the spread of disease, but at the same time we have rising unemployment in the US because of the preventive measures, so there is a trade-off here,” he says.   The Santa Clara team estimated an IFR for the county of 0.1–0.2%, which would equate to about 100 deaths in 48,000-82,000 infections. As of 10 April, the county's official death count was 50 people. The study's IFR is lower than the IFR used in models by researchers at Imperial College London, which estimated an IFR for Great Britain on the basis of data from China to be 0.9%. In another study, the same group estimated an IFR for China of 0.66%, and a study of deaths on the Diamond Princess cruise ship estimated an IFR of 0.5%. Figures vary in different places for several reasons, including the age distribution of the population and the extent of testing. Fatality rate estimates have been revised down over time as more people have been tested and researchers have gained more insight into less-severe cases, as happened with swine flu in 2009, says Eran Bendavid, a population-health researcher at Stanford University who led the Santa Clara study. But scientists have concerns about the reliability of antibody tests, particularly in regards to the number of false positives they produce, which could inflate infection rate estimates. The Santa Clara study reports using a kit purchased from Premier Biotech, based in Minneapolis, Minnesota. According to the preprint, the manufacturer's kit performance data noted 2 false positives out of 371 true negative samples. But with that false positive rate, a large number of the positive cases reported in the study — 50 out of 3300 tests — could be false positives, says Marm Kilpatrick, an infectious disease researcher at the University of California Santa Cruz.   To ensure a test is sensitive enough to pick up only true SARS-CoV-2 infections, it needs to be evaluated on hundreds of positive cases of COVID-19 and thousands of negative ones, says Michael Busch, an infectious-diseases researcher and director of the Vitalant Research Institute in San Francisco, California, who is also leading a sero-prevalence survey. But most kits have not been thoroughly tested, and health agencies are particularly concerned about the accuracy of some rapid tests, says Busch. The researchers involved in the Santa Clara study say that they assessed the sensitivity and specificity of the antibody tests in an initial 37 positive samples and 30 negative controls. The tests identified 68% of the positive samples and 100% of the negatives. An unpublished follow-up assessment in 30 positive and 88 negative controls found that the test correctly identified 28 positives and all 88 negatives, says Bendavid. Bendavid says they adjusted for the test kit’s performance and differences in the survey population relative to the county to estimate the prevalence of SARS-CoV-2 in Santa Clara. Survey participants included a higher proportion of white, female and affluent individuals than is found in the county’s population. Kilpatrick says another potential source of bias in the study is that participants were recruited using social media. As a result, the sample could include a disproportionally higher number of people who thought they were exposed to the virus and volunteered to get tested, he says. “The real prevalence might be half as high, a tenth as high, or it might even be the number presented in the paper - we don’t know because recruiting participants over Facebook presents an unknown bias,” he says Bhattacharya says the results probably undercount the prevalence in the wider population, because they miss anyone who has been infected too recently to have mounted an immune response, and exclude people in prisons, nursing homes and other institutions. Results are expected soon from sero-prevalence surveys run by other groups around the world, including teams in China, Australia, Iceland, Italy, Germany and several others in the United States. Update 19 April 2020: This story has been updated to include comments from Marm Kilpatrick. Correction 22 April 2020: This story originally misstated the nature of the false positive rate. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '160'>17 April 2020</date>
<url id = '161'>https://nature.com/articles/d41586-020-01114-0</url>
<title id = '161'>NASA and the European Space Agency firm up plans to return the first samples from the Red Planet.</title>
<body id = '161'>An illustration of NASA's Perseverance rover, which is due to land on Mars in February 2021.Credit: NASA/JPL-Caltech The plan to steal rocks from the surface of Mars is taking shape. The first step in this interplanetary heist will come in July, when NASA launches its Perseverance rover to roll around on the Martian surface and collect tubes of dust and rock. Now, officials have laid out exactly how those tubes might find their way back to Earth. It’s a soaringly ambitious, sure-to-be-expensive, international endeavour that involves sending multiple spacecraft to Mars to fetch the precious samples. If it works, scientists will finally get their hands on rocks retrieved from the red planet in just over 10 years. “What we can learn about Mars in our own laboratories is going to be fantastic,” said Michael Meyer, lead scientist for the Mars exploration programme at NASA headquarters in Washington DC, who outlined the plans at a virtual meeting on 15 April. The Perseverance rover is the first stage in the process of returning a sample from Mars. If it launches as planned in the coming months, it will land next February in Jezero crater, which is home to an old river delta that could hold signs of ancient Martian life. As it drives around for many kilometres, Perseverance will drill or scoop up material to fill around 30 small geological sampling tubes. Until now, it hasn’t been clear how those tubes might get back to Earth. But over the past few months, and after four years of designing and plotting, NASA and the European Space Agency (ESA) have finalized a plan that involves sending a pair of spacecraft to Mars in 2026. The first spacecraft would land in Jezero crater. There, a small rover would make its way to Perseverance, pick up the filled sampling tubes and transfer them to a “Mars ascent vehicle” — essentially a small rocket with a container to carry the samples. The Mars ascent vehicle would then blast off and place the container into Martian orbit. The second spacecraft would then manoeuvre itself next to the sample container, pick it up and fly it back to Earth. It would plummet to the ground at high speed, probably landing in a military training ground in Utah. The whole process will require many interplanetary firsts. No nation has ever launched a craft from the surface of Mars, or had two spacecraft rendezvous in Mars orbit. “This is by no means a simple task,” says Jim Watzin, head of NASA’s Mars exploration programme in Washington DC. “But we have kept it as simple as possible.” Landing and working in Jezero crater should be easier the second time around, notes Bethany Ehlmann, a planetary geologist at the California Institute of Technology in Pasadena. Perseverance will have had years to scout out the landscape and prepare the way for the small rover to join it. NASA and ESA have divided the work based on their experience working on Mars. NASA plans to build the sample-retrieval lander and the Mars ascent vehicle; ESA will work on the small rover and on flying the samples home. Unlike NASA, ESA has not yet sent a rover to Mars. It was due to launch one this year in a joint effort with the Russian space agency, but was forced to delay the project partly because of the coronavirus pandemic. Watzin declined to comment on how much the whole Mars sample-return project might cost, but it is likely to require at least several billion dollars from each agency. ESA officials committed to the plan at their triennial ministerial meeting last November. If all goes to schedule, the sample-return spacecraft will reach Mars in 2028. The small rover would do its work collecting and loading tubes during a season that is free of Martian dust storms and cold winter temperatures. After leaving Mars, the samples would land in Utah in September 2031. NASA and ESA put out a call this month for scientists interested in helping to plan how those precious samples will be studied. Latest on: Planetary science Article 20 MAY 20 News Feature 18 MAY 20 Comment 11 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '161'>16 April 2020</date>
<url id = '162'>https://nature.com/articles/d41586-020-01108-y</url>
<title id = '162'>Some scientists welcome government vetting because it could stop poor-quality COVID-19 papers being published – others fear it is an attempt to control information.</title>
<body id = '162'>Research from China is crucial to understanding the COVID-19 pandemic.Credit: Xinhua News Agency/Shutterstock China’s government has started asserting tight control over COVID-19 research findings. Over the past two months, it appears to have quietly introduced policies that require scientists to get approval to publish — or publicize — their results, according to documents seen by Nature and some researchers. This fits with media reports that at least two Chinese universities have posted notices online stating that research on the virus’s origins needs to be approved by the university’s academic committee and the Ministry of Science and Technology (MOST) or Ministry of Education (MOE) before being submitted for publication. Scientists in China say the changes are probably a response to poor-quality studies on the virus, which have been posted online and reported widely — and several welcome them. But some academics have suggested that the policies are part of China’s attempt to control information about the start of the outbreak.   Awareness of the new rules is mixed among researchers working on the virus in China. Some scientists who spoke to Nature say they heard about the vetting process from their institutions, but others were in the dark. The ministries seem not to have posted notices about the policies on their websites, and they have not yet responded to Nature’s attempts to confirm that they have released the documents. Researchers outside the country are concerned that the vetting process could delay publication of important insights that could help to control the pandemic. Some also fear that the Chinese government is interfering in the scientific review process. “Researchers and research institutions should be free to share knowledge without oversight in general, provided it has been conducted according to our current ethical conventions and standards,” says Ashley St. John, a virologist at the Duke–NUS Medical School in Singapore. “Where there is a review or vetting process in place, it should be only scientific in nature.” Last month, China’s foreign-ministry spokesperson, Zhao Lijian, made sensational claims that the virus might have come to the country from the United States, prompting concerns that the Chinese government’s statements were not always guided by science. Although the exact origin of the virus is unknown, researchers think it probably came from bats and then spread to a carrier animal before infecting the first people somewhere in central China late last year. Government oversight of COVID-19 research seems to have started with a directive to universities. A document obtained by Nature that seems to be from the MOE, and is dated 10 March, orders institutions to get approval from the ministry and the Joint Prevention and Control Mechanism, run by the powerful State Council, before publicly announcing results on the origin of the SARS-COV-2 virus, its transmission routes or treatments or vaccines. The document states that universities need to consider “the questions society is concerned about” when publicizing research on the virus. (Nature was sent the document, which is stamped by the MOE and includes the name of an agency official, by a researcher who did not want to comment.)   The education ministry seems to have issued another order after a meeting of the Joint Prevention and Control Mechanism on 25 March, according to a second notice that also appears to come from the MOE and has been posted on Pincong, a Chinese-language forum. This notice, dated 7 April, states that studies on the virus’s source must be approved by a university academic committee and the education ministry’s science and technology department before being published in a journal or posted on a preprint server or blog. Academic committees must evaluate all other COVID-19 papers for “academic value and timing”, the notice states. It also warns that studies must not exaggerate the efficacy of vaccines or treatments. According to archived web pages, the 7 April notice was reproduced on the website of the School of Information Science and Technology at Fudan University in Shanghai, but was subsequently removed. UK newspaper The Observer has reported that a similar notice was posted on, and then removed from, the website of the China University of Geosciences in Wuhan. Several researchers in China think the vetting process for COVID-19 studies is a good idea. Alice Hughes, a conservation biologist at the Chinese Academy of Sciences (CAS) Xishuangbanna Tropical Botanical Garden, says the measure will stop the dissemination of potentially inaccurate and sensationalist research, such as a controversial study published in the Journal of Medical Virology on 22 January, which suggested that snakes were the virus’s host. Scientists criticized the study for its lack of evidence, but it still received widespread media coverage. Hughes says her institute’s director told her in late February that research on COVID-19 required MOST approval. She has not seen official policy documents herself. In early March, she says, she had a paper approved by the CAS, and then by MOST within 72 hours.   Hughes hasn’t noticed any major effects on research publications. “We are continuing to see China publishing papers on the origins through this system,” she says. Zhang Zhigang, an evolutionary microbiologist at Yunnan University in Kunming who published on the outbreak’s origins before the vetting process came in, also thinks it’s a good way to control research quality and reliability. Poor-quality research could hurt global efforts to fight the virus, he says. But news of the policies hasn’t reached all scientists. Chen Jin-Ping, an animal-disease researcher at the Guangdong Institute of Applied Biological Resources in Guangzhou who is also studying the virus’s origins, says he hasn’t been told that he needs ministry approval for his research to be published. Even some institutions seem to be in the dark. Fei Ma, dean of research and graduate studies at Xi’An Jiaotong-Liverpool University in Suzhou, China, says he hasn’t heard of the need for coronavirus-related research to be approved by MOST or other government agencies. Denis Simon, executive vice-chancellor at Duke Kunshan University in Kunshan, says his institute hasn’t received any official notices that coronavirus research needs ministry approval, but researchers are discussing it. “People have heard about this but nothing has arrived on our doorstep,” he says. Some researchers outside China fear the vetting process could hold up the release of important research. “Right now we desperately need all kinds of research relating to SARS-CoV-2, from basic studies to understand mechanisms of disease to vaccines and therapeutics,” says St. John. “We can’t afford any delays right now.”   Understanding the origin of SARS-CoV-2 could also lead to early-warning systems for future virus spillovers from animals to people, she says. Sarah Cobey, an infectious-disease researcher at the University of Chicago in Illinois, adds that it would be very problematic if results from China were being filtered or suppressed for reasons other than quality. Observations of viral spread across countries inform the use of interventions such as social distancing, she says. “If the research presents a biased picture, much of the record can eventually be corrected through studies of SARS-CoV-2 elsewhere,” she says, “but the distortion and delay would probably come at the cost of human health.” </body>
<date id = '162'>15 April 2020</date>
<url id = '163'>https://nature.com/articles/d41586-020-01096-z</url>
<title id = '163'>Statistical analyses suggest that surveillance efforts for the next pandemic look beyond the flying mammals.</title>
<body id = '163'>Bats host several viruses that cause severe disease in people.Credit: Stephen Belcher/Minden Pictures/FLPA Bats and rodents are considered high-risk viral reservoirs — a source for diseases that can hop over to humans, and sometimes lead to epidemics. Some scientists have even argued that the animals have certain traits that increase the likelihood of spillover events from animal to people, and that they should be monitored more closely as a result. But a new analysis suggests that bats and rodents are “unexceptional” in their propensity to host viruses that infect humans. Looking at the largest dataset of viruses and hosts across several orders of mammal and bird, researchers from Scotland found that the number of bats and rodents viruses that have infected people is proportionate to the number of species contained in those groups. There's “a fairly rational numerical explanation for what can seemingly be striking patterns,” says disease ecologist Daniel Streicker at the University of Glasgow, UK, who co-led the analysis published in PNAS1 published on 13 April. Future surveillance efforts to identify disease threats from animal sources should look beyond specific animal groups and focus on regions of high biodiversity, he says.   But not everyone agrees that this is practical given limited resources. And as bats harbour several viruses that cause severe disease in people, including rabies, Ebola and severe acute respiratory syndrome-related coronavirus (SARS-CoV) it makes sense that they — along with rodents — are the focus of most viral-detection and surveillance efforts. Bats are also a prime suspect as the source of SARS-CoV-2, the virus responsible for the current pandemic. Research examining individual species has found that bats have proportionally more viruses than other mammals2, but Streiker and Nardus Mollentze, also at the University of Glasgow, decided to look at whether this pattern exists across different groups of mammals and birds. Streiker says looking at animal orders removes some of the uncertainty around exactly which species is the host of a new virus. But researchers can be reasonably confident about which animal group is involved, on the basis of genetic comparisons between the human-infecting virus and those circulating in animal hosts. The pair compared human-infecting viruses across 11 orders, including chiroptera (bats), rodentia (rodents) and passeriformes (songbirds). Building upon their own and other databases3, they compiled data on 415 DNA and RNA viruses from animals that have spread to people2,4.   Their statistical analysis estimated that animal groups with more species tend to have more viruses, and consequently, a larger number of viruses that can jump to people. For instance, rodents were the most species-rich order of mammals in the study; they also had the largest number of viruses that had moved to people, Streicker says. In another statistical analysis, the pair considered the importance of host biology compared with viral factors. The model found that virus biology, such as how a virus replicates or whether it is transmitted by insects, was more of a factor in spillovers than the reservoir's physiological or ecological traits.   For instance, although bats are thought to accommodate many different viruses because of their immune systems, Streicker says these unique features don’t increase the risk of those viruses spilling over. “There weren't single groups of animal hosts which were consistently elevating the risk that viruses posed to people,” he says. "If we want to be able to predict which viruses are most likely to infect humans, the traits of the viruses might be more informative than the traits of the hosts," he says. Streicker suggests that future work should focus on the virus traits that might enhance their propensity to jump to people, and should consider how other factors, such as wildlife trade and environmental change, push animals into contact with more people and influence the emergence of viruses. The finding that species diversity corresponds with viral richness is a compelling reason to broaden surveillance beyond certain mammal groups, says ecologist Kevin Olival, vice president at EcoHealth Alliance, an environmental non-profit organization in New York City.   But Olival doesn’t think the study will put to rest the debate about whether special reservoirs exist. In the study, viruses were examined across orders of animal hosts, rather than species — which means that species-specific information about the hosts was lost, such as population size, density, species abundance and amount of contact with people. All these factors can influence viral diversity and transmission, he says. He adds that it seems logical to continue some targeted surveillance efforts on bats and rodents given their track record. Virologist Jemma Geoghegan at the University of Otago, New Zealand, says that before researchers can use viral traits to predict the next spillover event, many more viruses need to be sampled and characterized to reveal the true diversity of viruses in nature. Until then, she thinks surveillance efforts are better directed towards genomic surveillance at the ‘fault lines’ where people and animals interact, such as live animal markets. "That way, we can quickly recognize any viruses that spill over,” she says. Mollentze, N. & Streicker, D. G. Proc. Natl Acad. Sci. USA https://www.pnas.org/cgi/doi/10.1073/pnas.1919176117 (2020). Olival, K. J. et al. Nature 546, 646–650 (2017). Babayan, S. A., Orton, R. J. & Streicker, D. G. Science 362, 577-580 (2018). Woolhouse, M. E. J. & Brierley, L. Sci. Data. 5, 180017 (2018). Download references An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '163'>14 April 2020</date>
<url id = '164'>https://nature.com/articles/d41586-020-00924-6</url>
<title id = '164'>As climate- and ecological-monitoring projects go dark, data that stretch back for decades will soon contain coronavirus-associated gaps.</title>
<body id = '164'>Scientists have had to skip trips to clean and maintain sensors used for the Ocean Observatories Initiative.Credit: Rebecca Travis/Woods Hole Oceanographic Institution Twice per year, Ed Dever’s group at Oregon State University in Corvallis heads out to sea off the Oregon and Washington coasts to refurbish and clean more than 100 delicate sensors that make up one segment of a US$44-million-per-year scientific network called the Ocean Observatories Initiative. “If this had been a normal year, I would have been at sea right now,” he says. Instead, Dever is one of many scientists sidelined by the coronavirus pandemic, watching from afar as precious field data disappear and instruments degrade. The scientific pause could imperil weather forecasts in the near term, and threaten long-standing climate studies. In some cases, researchers are expecting gaps in data that have been collected regularly for decades. “The break in the scientific record is probably unprecedented,” says Frank Davis, an ecologist at the University of California, Santa Barbara. Davis is the executive director of the Long Term Ecological Research (LTER) programme, a network of 30 ecological sites stretching from the far north of Alaska all the way down to Antarctica. Consisting of both urban and rural locations, the LTER network allows scientists to study ecological processes over decades — from the impact of dwindling snowfalls on the mountains of Colorado to the effects of pollution in a Baltimore stream. At some sites, this might be the first interruption in more than 40 years, he says. “That’s painful for the scientists involved.” Other monitoring programmes are facing similar gaps. Scientists often ride along on the commercial container ships that criss-cross the world’s oceans, collecting data and deploying a variety of instruments that measure weather, as well as currents and other properties of the ocean. Most of those ships are still running, but travel restrictions mean that scientists are no longer allowed on board, says Justine Parks, a marine technician who manages one such programme at the Scripps Institution of Oceanography in La Jolla, California. Port strikes and political instability have halted specific cruises in the past, Parks says. But to her knowledge, this is the first time that the entire programme has shut down for an extended period of time. Measurements made at sea are important for forecasting weather over the oceans, as well as for keeping longer-term records of ocean health and climate change, says Emma Heslop, a programme specialist in ocean observations at the Intergovernmental Oceanographic Commission in Paris. Her group is still trying to assess the extent of the damage that the pandemic is doing to the ocean-observing community as a whole, but researchers are already feeling some effects. Over the past 2 months, they’ve seen steadily declining numbers of shipboard observations — amounting to, since the beginning of February, a 15% loss of stations that are reporting data. And although the community is working hard to figure out other ways to collect important data, the situation is likely to worsen as the pandemic stretches on. “The longer the restrictions are in place,” she says, “the longer it will take for our operations to recover.” Field collection in remote areas such as McMurdo Dry Valleys in Antarctica were halted for the Long Term Ecological Research programme.Credit: Barb Woods Commercial flights provide invaluable weather data, too — measuring temperature, pressure and wind speeds as they cruise. The meteorological data provided by the US aircraft fleet had decreased to half its normal levels as of 31 March, according to the US National Oceanic and Atmospheric Administration (NOAA). Satellites and weather balloons can fill in some gaps, but certain aircraft data are irreplaceable. “It’s certainly the case that with the virtual loss of worldwide aviation, there is a gap in some of the records,” says Grahame Madge, a spokesperson for the UK Met Office in Exeter. The Met Office estimates that the loss of aircraft observations will increase their forecast error by 1–2%, but notes that, in areas where flights are typically more abundant, scientists' forecast accuracy might suffer even more. The Met Office maintains more than 250 UK weather stations that provide continuous or daily feeds of autonomously collected atmospheric and weather data. For now, those systems are functioning just fine, but if an instrument goes down, Madge says, it will be difficult to get staff out to fix the problem. Much of the world’s atmospheric-monitoring data are collected with little to no human intervention, and such projects should be able to keep running. The Advanced Global Atmospheric Gases Experiment, for example, measures ozone-depleting compounds, greenhouse gases and other trace components in the atmosphere at 13 remote sites around the globe. Many of their systems are autonomous: the stations are each staffed by one or two people who perform routine maintenance to keep the instruments running. Ray Weiss, an atmospheric chemist at Scripps who leads the project, says that two instruments have broken down so far, but the loss of a single instrument or even a whole site for a few weeks is unlikely to jeopardize the network’s monitoring capabilities. Arlyn Andrews, who runs NOAA’s greenhouse-gas-monitoring programme, says that impacts on that network have been “relatively minor”, and less than 5% of the NOAA sites have lost data so far. Unless the situation gets a whole lot worse, Weiss anticipates that the programme will escape relatively unscathed. “We’re limping through, is the bottom line.” </body>
<date id = '164'>13 April 2020</date>
<url id = '165'>https://nature.com/articles/d41586-020-01093-2</url>
<title id = '165'>Just days before announcing the official end of a devastating outbreak, a new case emerges.</title>
<body id = '165'>Doctors administer an experimental vaccine at treatment centre in Beni, Democratic Republic of the CongoCredit: Andia/Universal Images Group via Getty A new case of Ebola has been recorded in the Democratic Republic of the Congo (DRC). It was the first such report in 52 days, and came just 2 days before the WHO was set to declare the official end of the outbreak. Already, clinicians and scientists on the frontline of that crisis have shifted their attention to COVID-19. But now, their efforts at the site of the Ebola outbreak must continue, too. “I am so sad,” says Marie-Roseline Darnycka Bélizaire, an epidemiologist who has coordinated the WHO’s Ebola response in northeastern DRC for two years. Experts have called the Ebola outbreak, which started in August 2018, one of the most complex health emergencies the world has ever seen because it occurred in a region of the DRC ravaged by decades of war and political instability. More than 2,270 people have died of Ebola — around two-thirds of the outbreak’s reported infections. In addition, more than 70 responders and people with the disease were injured in targeted attacks by armed groups. Cases began dwindling in January this year, and until today, the most recent person with the disease was diagnosed on 17 February. She survived, and was discharged from the hospital on 3 March. In the 40 days that followed, more than 2,000 people had been tested for Ebola, and no new cases had come to light — until now. This case means that the 42-day count-down to an official declaration of victory must begin again. Yesterday, when it seemed the outbreak was nearly over, Bélizaire said she felt an “indescribable joy”. Today, she says, “I expected a sporadic case earlier, but not two days before the end.” Latest on: Developing world World View 14 MAY 20 Article 29 APR 20 World View 28 APR 20 Diseases News 25 MAY 20 News 21 MAY 20 Article 20 MAY 20 Ebola virus News 20 MAR 20 News 21 FEB 20 News 26 NOV 19 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '165'>10 April 2020</date>
<url id = '166'>https://nature.com/articles/d41586-020-01511-5</url>
<title id = '166'>An antiretroviral injection given every two months prevents men and trans women from becoming infected with HIV, according to results that have not yet been peer-reviewed. Plus: the golden age of physical volcanology and the globe-straddling scientific endeavor to solve the structure of the coronavirus.</title>
<body id = '166'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Judy Gallagher/Flickr (CC BY 2.0) Researchers have demonstrated that a photonic device can produce twisted pulses of light, meaning that their electromagnetic waves swirl in a vortex. The ring-shaped, microscopic ‘tunable vortex microlaser’ is a step towards faster optical communications. Contrary to the properties of light that are commonly used to encode information — wave phase and amplitude — vortices come in discrete types. This makes it harder to mistake one phase for another, which could help to increase the capacity of optical fibres. Ars Technica | 6 min read Source: Science paper University supercomputers in the United Kingdom, Germany and Switzerland were disabled last week after hackers tried to take control of them to mine cryptocurrency. "We now believe this to be a major issue across the academic community as several computers have been compromised in the UK and elsewhere in Europe," said staff at the University of Edinburgh. BBC | 3 min read Animals, such as minks, that can be infected with the coronavirus could reveal clues about the virus’s origins.Jorma Luhta/Nature Picture Library Animal source still eludes scientists Researchers are using computer models, cell studies and animal experiments to discover the intermediate host of SARS-CoV-2. Researchers overwhelmingly think that the virus originated in the wild in bats, and moved to people through another species — but no one has found the virus in the wild yet. Pangolins were among the first suspected, but the link is far from confirmed. “It is quite possible we won’t find it. In fact, it would be exceptionally lucky if we land on something,” says geneticist Lucy van Dorp. Nature | 7 min read The outsized toll on people of colourEvidence from the United States and United Kingdom shows that COVID-19 is taking a disproportionate toll on people of colour. For example, in Michigan, black people make up 14% of the population, but account for 32% of COVID-19 cases and 41% of deaths. Some of the causes are well-known — “we are exposed more and less protected,” notes epidemiologist Camara Phyllis Jones. Some dangerous pre-existing conditions have a higher incidence in many minority ethnic and racial groups. And the way tests have been allocated — to people who have recently travelled internationally, for example — excludes those from disadvantaged socio-economic backgrounds.The UK government announced in mid-April that it would launch an inquiry into the reasons why the disparities exist. In the United States, researchers and lawmakers are calling for a national commission devoted to identifying racial disparities in health that would act as a unified voice in trying to overcome them. Nature | 6 min read The COVID-19 cartoonists The animations made by cartoonist Toby Morris and microbiologist Siouxsie Wiles manage to be both helpful and charming — and even got a cameo in New Zealand Prime Minister Jacinda Ardern’s press conference. Come away feeling smarter about bubbles and curves after reading this collection of all their work for The Spinoff. The Spinoff | 5 min read Cognitive psychologist Stephan Lewandowsky says that COVID-19 conspiracy theorists enjoy a bottomless, never-ending pit of skepticism — so it’s better to focus your debunking efforts elsewhere. (ProPublica | 4 min read) Betting against the standard model of particle physics is like most gambling: the house always wins. And the standard model usually smothers hints of contradictory experimental results. When anomalies occur — except for marvellous exceptions like the discovery of the Higgs boson in 2012 — most turn out to be flukes. But that doesn’t stop physicists from chasing clues that could point to what is beyond the standard model. “We think there should be more to life than just what the standard model can predict,” says physicist Chris Polly. APS Physics | 9 min read Six physicists who are science-communication maestros — Katie Mack, ‘Dr Karl’ Kruszelnicki, Lisa Randall, Jess Wade, Jim Al-Khalili and Vlatko Vedral — reflect on how to reach out. “People want answers — at a rate faster than we can find them,” says Randall. “But they also want to hear trustworthy information from people who are not afraid to tell the truth.” Nature Reviews Physics | 11 min read When clinical researcher and psychiatrist Anita Thapar and her team went public with research about the genetic factors behind attention-deficit hyperactivity disorder, they were shocked by an unexpected backlash. “Usually, my team is just a bunch of people quietly getting on with research,” writes Thapar. “In the future, I would clear time in my diary to handle the resulting feedback and fallout.” Nature | 5 min read Wastie, a robot built by the Mahidol University Engineering Department near Bangkok, will be used to collect infected rubbish inside the COVID-19 ward of Golden Jubilee Medical Center at the university.Athit Perawongmetha/Reuters Space exploration can inspire us during this difficult time, says planetary geologist and newly-minted NASA astronaut Jessica Watkins. (Nature | 4 min read) Where I live, it’s baby songbird season — so here’s a handy guide (in cartoon form) to what to do if you find one by naturalist Rosemary Mosco. So useful to know if it’s a hatchling, a nestling, a fledgling, or a deadly dinosaur that’s about to chase you down the street. Let me know what’s taking flight near you — and any other feedback on this newsletter — at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '166'>19 May 2020</date>
<url id = '167'>https://nature.com/articles/d41586-020-01490-7</url>
<title id = '167'>An antiretroviral injection given every two months prevents men and trans women from becoming infected with HIV, according to results that have not yet been peer-reviewed. Plus: the golden age of physical volcanology and the globe-straddling scientific endeavor to solve the structure of the coronavirus.</title>
<body id = '167'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here When Anak Krakatau in Indonesia erupted on 22 December 2018, part of the island collapsed into the ocean, causing a deadly tsunami.Credit: Nurul Hidayat/Antara Foto/Bisnis Indonesia via Reuters On 22 December 2018, the southern flank of the Anak Krakatau volcano in Indonesia plunged into the sea, causing tsunami waves that killed at least 430 people. Multiple instruments had been recording signs of unusual activity for months, but the collapse still caught scientists by surprise. Researchers hope to use new machine-learning techniques to make sense of an increasing wealth of available information — including data from volcanic-gas signals, precision topographic mapping and infrasound sensing. “I think that when people look back on this period, they will imagine this is the golden era of physical volcanology,” says volcanologist Christopher Kilburn. Nature | 13 min read On 18 May 1980, a giant landslide — the largest in recorded history — carried away the north flank of Mount St Helens, triggering an eruption.Credit: USGS An antiretroviral injection given every two months prevents men and trans women from becoming infected with HIV, according to results that have not yet been peer-reviewed. The experimental drug, called cabotegravir, protects on a level on par with Truvada, a once-daily pill approved for pre-exposure prophylaxis (PrEP) in the United States. “It’s really exciting,” says epidemiologist Jared Baeten. “It gives another option for people who can’t or don’t want to take daily pills.” Science | 6 min read Reference: HIV Prevention Trials press release Credit: Cognition Studio Inc. The sprint to solve coronavirus protein structures — and disarm them with drugs On 10 January, the complete genome of the SARS-CoV-2 virus was released — and structural biologists around the world sprung into action. Their goal: to sift through the 29,811 RNA bases in the virus’s genome in search of instructions for each of its estimated 25–29 proteins. With those blueprints, the scientists could recreate the proteins in the laboratory, visualize them and then, hopefully, identify drug compounds to block them or to develop vaccines to incite the immune system against them. Nature | 13 min read FDA shuts down Gates-backed home testing The US Food and Drug Administration (FDA) has shut down a coronavirus at-home testing programme sponsored by Bill Gates. The programme, called the Seattle Coronavirus Assessment Network, had already collected thousands of samples and identified dozens of new cases. The FDA had assumed the tests were being used as a ‘surveillance’ research tool, and objected to the fact that test-takers were being informed of the results. The agency now claims the tests must be regarded as ‘diagnostics’, which require tighter regulations. Others think it is a bureaucratic obstacle to a valuable programme. “To withhold that information from people is downright absurd,” says Eric Topol, director of the Scripps Research Translational Institute in California. New York Times | 5 min read Recruit your own best criticsTo guard against rushed and sloppy science, pandemic researchers should ask ‘devil’s advocates’ to find errors and challenge assumptions, argues human–technology interaction researcher Daniël Lakens. “The best time for scrutiny is before you have fallen in love with your results,” writes Lakens. Nature | 5 min read Coronapod: The misinformation pandemicGet the expert view from Nature’s news team in our weekly audio overview of the state of coronavirus science. This week, we dig into the epidemiology of misinformation and explore researchers’ funding fears. Nature Coronapod | 32 min listen Structural biologist Rolf Hilgenfeld was supposed to retire on 1 April owing to a mandatory retirement policy. He’s still at work — as one of the leading investigators of the structure of the SARS-CoV-2 virus. (Nature | 13 min read) Read more: This scientist hopes to test coronavirus drugs on animals in locked-down Wuhan (Nature, from January) Vaccine shields monkeys from lung damageAn experimental vaccine from a US–UK team protected monkeys from pneumonia and prompted a strong immune response in the animals. Researchers injected six rhesus macaques (Macaca mulatta) with the vaccine before giving the animals high doses of virus. All vaccinated monkeys developed neutralizing antibodies — which can prevent a virus from entering cells — against SARS-CoV-2. Two of the three control monkeys developed pneumonia; none of the vaccinated monkeys did.Reference: bioRxiv preprint (not yet peer reviewed) ‘Superspread’ at a choir practice infects dozensA single ill person who attended a choir practice in Washington State led to the probable infection of more than 50 choir members, including 2 who died. This superspreading event emphasizes the importance of avoiding crowds and close interactions to keep the virus at bay, the authors say.Reference: Morbidity and Mortality Weekly Report paper Youngest children are most likely to enter hospitalChildren with COVID-19 are at a lower risk of death than are adults with the disease, according to the largest study of infected children in Europe. Researchers analysed data from children under the age of 18 who turned up at hospitals and clinics with COVID-19 symptoms. All 168 who tested positive for the coronavirus recovered fully. Nearly 80% of infants under the age of one were hospitalized, compared with 53% of those between the ages of 11 and 17 (the overall hospitalization rate for infected children in Italy is much lower — around 4%). Two-thirds of the children had at least one infected parent, whose symptoms often appeared before the child’s did.Reference: Eurosurveillance preprint (not yet peer reviewed) Get more of Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Mathematician, counsellor and social scientist Ann Mitchell, who had a pivotal role in cracking war-time codes at Bletchley Park, has died aged 97 after testing positive for COVID-19. (The Scotsman | 7 min read) On Friday, Leif Penguinson went forest bathing on the beautiful Plombergstein, near Salzburg. Did you spot the penguin? When you’re ready — here the answer. In Friday’s Briefing, we wrote that the estimated number of organisms deep underground is around 1030 cells. That was a formatting error — it should have said 1030 cells. Too bad I spent the weekend coming up with 1,030 different names for the little fellas. This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi and David Cyranoski An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '167'>18 May 2020</date>
<url id = '168'>https://nature.com/articles/d41586-020-01478-3</url>
<title id = '168'>Deep life (but not as we know it), the first large ancient-genome studies of East Asia and tackling one of the biggest drug-making challenges the world has ever faced.</title>
<body id = '168'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here An 8,400-year-old skeleton from the Qihe cave archaeological site in Fujian, China.Credit: Xiujie Wu/Institute of Vertebrate Paleontology and Paleoanthropology The first large ancient-genome studies of East Asia suggest that many of its inhabitants descend from two once-distinct populations. Two research groups analysed dozens of genomes from individuals who lived in the region, including what is now China, between 9,500 and 300 years ago. The genomes show that the two populations remained separate for thousands of years, but over time they began interbreeding. This could solve the mystery of how farming spread through the region — it was adopted as farmers and hunter-gatherers mixed. That’s different from what happened in western Eurasia; ancient-genome studies have found that farmers with Middle Eastern ancestry largely replaced hunter-gatherers in Europe. Nature | 5 min read Reference: Science paper & bioRxiv preprint Scientists: as coronavirus lockdowns start to ease, Nature wants to hear from you. Have you returned to your lab or place of work? Please tell us about your situation in our poll. Production of remdesivir, an antiviral drug approved to treat COVID-19, is ramping up.Credit: Ulrich Perrey/POOL/AFP/Getty How to make enough coronavirus drugs Dozens of coronavirus drugs are in development (for example, there were encouraging drips of data from a large clinical trial of the antiviral remdesivir last month). Next, we face the challenge of ramping up complex manufacturing processes to produce sufficient quantities of successful therapies to treat everyone in need. Drug manufacturers face supply-chain weaknesses and sourcing issues. And each treatment will face different challenges when scaling up production. (Nature | 7 min read) “The stakes have never been higher” The government of Togo wants to send cash payments to thousands of vulnerable households. In Uganda, the local phone company is helping to direct government assistance to people in vulnerable communities. In Bangladesh, the government hopes to use phone records to determine who is eligible for emergency assistance. All these efforts could dramatically increase the timeliness and effectiveness of humanitarian responses — while minimizing the need for face-to-face contact, argues information-scientist Joshua Blumenstock. He outlines how the immense privacy risks and practical challenges of these methods can be overcome with the cautious use of machine learning. (Nature | 5 min read) Past pandemics reveal lessons for the present Lessons from the Black Death in fourteenth-century London and smallpox in Native American communities 400 years later can teach us about the unequal toll of epidemics. Researchers who study those outbreaks, and others, are uncovering how racism, oppression and poverty made a lie of the ‘equal-opportunity killer’. “The ways that social inequalities are manifested … put people at higher risk,” says historian Monica Green. “We should all be learning in our bones, in a way that will never be forgotten, why [the coronavirus pandemic] has happened the way it has.” (Science | 13 min read) My mouse army The Jackson Laboratory (JAX) in the United States has become a nucleus in researchers’ efforts to understand COVID-19 in mouse models. After a warning from a colleague in China, the lab rushed to produce stocks of transgenic mice using frozen sperm from SARS-susceptible animals developed by prescient microbiologist Stanley Perlman. JAX is also acting as an ark for carefully bred mouse colonies that scientists elsewhere were forced to cull when their labs closed. Lab leader and neuroscientist Cat Lutz describes the lab’s round-the-clock work. (Knowable | 8 min read) Kari Stefánsson is the founder and CEO of deCODE, the genetics company that helped Iceland to test (for free) much of its population to help quash its outbreak. (Science|Business | 7 min read) Basically Virtually everywhere they look, scientists are finding microorganisms living in porous rock deep underground, both on land and under the seafloor. Some estimates now put the number of subsurface organisms at around 1030 cells — an order of magnitude more than those living in soil, for example. Many of the microbes might not depend on photosynthesis, even indirectly. Instead, they could thrive on sources of energy such as hydrogen produced by naturally radioactive isotopes. Because energy is scarce, life down there is extreeeemely sssloooow. “It might take them 100 years or 1,000 years to divide just once,” says ocean ecologist Martin Fisk. Quanta | 12 min read Andrew Robinson’s pick of the top five science books to read this week includes smart-technology spies, a final warning on the environment, and the staggering costs of cancer. Nature | 3 min read Microbiologist Elisabeth Bik’s observations are among the most revered (and feared) in science. Last year, she left her job to work full-time on spotting duplicated images in the scientific literature. Eschewing software, Bik relies on her natural knack for spotting repeated areas, and doesn’t shy away from sharing her suspicions on PubPeer and Twitter. Bik tells the Nature Podcast “Once you see those patterns, it’s really hard to not,” says Bik. “To see the same thing, over and over again in the same photo — sometimes it’s just hilarious.” Nature Podcast | 20 min listen Bik estimates that her discoveries have led to at least 172 retractions and more than 300 errata and corrections. How would you fare? Check if you’re a super-spotter with the quiz in our accompanying feature. (Nature | 14 min read) Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Jessika Trancik is an energy-systems researcher at the Massachusetts Institute of Technology in Cambridge.Credit: Kayana Szymczak for Nature Jessika Trancik using an app she and her students developed, which provides information on vehicles’ carbon footprint. Trancik is an energy-systems researcher who studies how electric batteries might affect fossil-fuel use. Her findings help policymakers and have helped to guide the US delegation to the Paris climate negotiations. “It also helps those considering investing in carmakers, as well as engineers who are developing road networks,” she writes. (Nature | 3 min read) Snakes have friends too — and they’re choosy about who they spend time with, says behavioral ecologist Morgan Skinner. (National Geographic | 6 min read) Reference: Behavioral Ecology and Sociobiology paper This weekend Leif Penguinson is forest bathing on the beautiful Plombergstein, near Salzburg. Can you spot the penguin? The answer will be in Monday’s Briefing, all thanks to photo czar Tom Houghton. I hope you find the weekend equally renewing. When sufficiently reinvigorated, why not let me know what you think of this newsletter? Your feedback is always welcome at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '168'>15 May 2020</date>
<url id = '169'>https://nature.com/articles/d41586-020-01460-z</url>
<title id = '169'>Despite more than 27,000 confirmed deaths from COVID-19 in France, only 4.4% of the population has been infected, way too low to slow the outbreak — and it’s a similar story in Spain. Plus, the first two dogs reported to be infected probably caught it from their owners and the inner pulsations of stars.</title>
<body id = '169'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Microscope images that have been altered or duplicated can appear in research papers.Credit: Universal Images Group via Getty The world’s largest science publishers are teaming up to establish standards for catching suspicious images in manuscripts submitted for peer review. Altered or duplicated images can result from honest mistakes, efforts to improve readability — or fraud. Elsevier, Wiley, Taylor & Francis, Springer Nature and others aim to set standards for software that can compare a large number of images across many papers at once. Nature | 5 min read (Nature is editorially independent of its publisher, Springer Nature — as is this Briefing.) A Pomeranian in Hong Kong was one of the first dogs to test positive for the coronavirus.Credit: Xinhua News Agency/Shutterstock Dogs caught coronavirus from their owners The first two dogs reported to have coronavirus probably caught it from their owners. Researchers found that the genetic sequences of the virus were almost identical between the animals and members of the infected households in Hong Kong. The direct genomic link strongly supports the idea that the infection had been passed from the owners to the dogs. Neither dog got sick, and there’s no evidence that dogs can pass the virus to each other or back to humans. (Nature | 4 min read) Reference: Nature paper Herd immunity is not happening Despite more than 27,000 confirmed deaths from COVID-19 in France, only 4.4% of people have actually been infected. The percentage is far below the required level — something more than 50% — to achieve herd immunity. Herd immunity would slow — but not stop — the outbreak. Results announced by Spain's health minister show a similar situation: more than 27,000 deaths and just 5% of the population tested had antibodies to the virus. “Population immunity appears insufficient to avoid a second wave” if lockdown measures are removed, say the authors of the French study. (Reuters | 4 min read) Reference: Science paper How to suppress further COVID-19 outbreaks The only plausible way to achieve herd immunity is through mass vaccination, argues a Nature Biomedical Engineering editorial. The alternative — letting the virus spread naturally at an infection fatality rate of something around 0.5–1% — implies that millions would die before transmission slows down. The journal outlines why widespread testing, technology-aided contact tracing, case isolation and the quarantining of contacts will continue to be essential to sustainedly suppress further outbreaks. (Nature Biomedical Engineering | 7 min read) In rare cases, coronavirus might ail children Pediatric multisystem inflammatory syndrome— a serious condition in children that has been compared with an illness called Kawasaki disease — seems to be linked to the coronavirus. Physicians in Bergamo, at the heart of the COVID-19 outbreak in Italy, report a 30-fold increased incidence of Kawasaki-like disease. New York governor Andrew Cuomo said this week that the syndrome has affected around 100 children in the state, 2 of whom have died. Pediatricians stress that the problem is extremely rare, and most children who have it get better. (BBC | 4 min read) Reference: The Lancet paper Scientists: as coronavirus lockdowns start to ease, Nature wants to hear from you. Have you returned to your lab or place of work? Please tell us about your situation in our poll. Robert May, a pioneering theoretical ecologist and outspoken government adviser, died last month at age 84. In a career that spanned continents and disciplines, May helped to revolutionize the study of biodiversity, population dynamics and infectious-disease epidemiology. As chief scientific adviser to the UK government, he emphasized principles particularly relevant in the current pandemic: transparency, seeking a wide range of views and fully acknowledging uncertainties. May was known for his plain way of speaking and his competitiveness: “It was once said that when he went home to play with his much-loved poodle Perri, he played to win,” write ecologists John Krebs and Michael Hassell. Nature | 5 minute read Astronomers have decoded the inner pulsations of a class of stars called δ Scuti by precisely tracking how their brightness evolves in time. Stellar pulsations can help researchers to understand the inner structure of a star, “like the standing sound waves responsible for the sounds of musical instruments such as violins and oboes”, write astronomers József Benkő and Margit Paparó. δ Scuti stars have so far been difficult to understand because their pulsations appear to be less regular than those of other types of star. Space scientist and musician Chris Boshuizen used the real data on a δ Scuti called HD 31901 to create this delightful audio and visualization of its pulsations. Nature News & Views | 6 min read Source: Nature paper  Groups opposing vaccines are small in size, but their online-communications strategy is worryingly effective and far-reaching. In contrast, pages that explain the benefits of and the scientific case for vaccination are largely disconnected from the “main battlefield” for public sentiment. Researchers analysed Facebook pages followed by 85 million individuals to map the connections. The messages could undermine efforts to establish herd immunity to the new coronavirus, say researchers. (Nature | 5 min read) A summer of planning for better online learning this autumn will be wasted if lecturers do not begin from the premise that students are anxious, says educational researcher Cathy Davidson. (HASTAC blog | 6 min read) Have a go at docking with the International Space Station with this new web-based simulator from SpaceX that uses the actual interface astronauts use on the Dragon 2 vehicle. Let me know which near-useless skill you’re working on (I’m learning Scottish Gaelic!) — or any other feedback on this Briefing — at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by David Cyranoski and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '169'>14 May 2020</date>
<url id = '170'>https://nature.com/articles/d41586-020-01435-0</url>
<title id = '170'>Meet scientific-integrity super-spotter Elisabeth Bik, explore how antibodies work (and why antibody tests sometimes don’t) and discover a pioneering stem-cell treatment for heart-disease.</title>
<body id = '170'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Heart muscle derived from induced pluripotent stem cells (pictured) is being trialed in people.Credit: Thomas Deerinck, NCMIR/SPL Nature reveals that two men in China are the first people in the world to receive an experimental heart-disease treatment based on ‘reprogrammed’ stem cells. In January, a similar approach achieved a world first when sheets of induced pluripotent stem (iPS) cells were grafted onto a patient’s heart. This time, the iPS cells were injected right into the organ. No results have been published yet, so researchers not involved in the work have cautioned that there is no way to confirm whether the treatment works. But the cardiac surgeon who performed the procedures says both men are doing well one year on. Nature | 7 min read Researchers on a customized Gulfstream jet have seen just how ‘gravity waves’, which churn the air above the Southern Ocean, arise from a surprisingly distant source. Atmospheric physicists observed how the nascent waves are born on the high mountains of the southern Andes and the Antarctic Peninsula before being drawn to the powerful Antarctic polar vortex. It’s the first experimental proof showing how the waves weaken the vortex and warm the Antarctic. Gravity waves (not gravitational waves) arise when layers of a fluid with different densities — for example, ocean water of different salinities or air at different temperatures — bob up and down with respect to each other. Science | 6 min read   An infodemic of misinformation What makes coronavirus misinformation and conspiracy theories so potent — and how might they be stopped? A Nature video explores how researchers are studying the viral power of fake news and its impact on the spread of COVID-19. (Nature (on YouTube) | 6 min video) Cal State goes all-in online Almost all classes at the huge 23-campus California State University system are going virtual when they resume this autumn. “Our university, when open without restrictions and fully in person … is a place where over 500,000 people come together in close and vibrant proximity,” said chancellor Timothy White. “That approach sadly just isn’t in the cards now.” (Los Angeles Times | 5 min read) “He considers everyone else to be worthless” Microbiologist Didier Raoult is a larger-than-life figure in French science, who has never shied away from bold claims. But his research touting a regimen of hydroxychloroquine and azithromycin to treat COVID-19 has ignited a scientific wildfire — not least because it was called a ‘cure’ by US President Donald Trump. “I really do think we’re in a theater,” says Raoult. “In my play, the people who judge me as a doctor are my patients. As a scientist, it’s my colleagues. And time.” (The New York Times | 29 min read) All about antibody tests Antibody tests — which indicate whether a person has already been infected with the virus — offer the tantalizing promise of revealing who might be immune. But inaccurate tests and a dearth of knowledge about how the coronavirus leaves its mark on the immune system mean that the promise is still far from being fulfilled. Vox attempts to answer pretty much every question you might have about the tests, along with an animation that explains how antibodies work. (Vox | 17 min read) Read more: Will antibody tests for the coronavirus really change everything? (Nature, from April) Health-care workers are not immune from the virus of fear, says physician Susan Murray. She recalls how the spectre of Ebola once hindered her treatment of a terminally ill man. (New England Journal of Medicine | 6 min read) Commuters and New York infection hotspots Researchers compiled coronavirus test results from about 1,700 women who gave birth in New York City hospitals. The neighbourhoods that were home to more infected women correlate with those with the highest number of commuters over the past three months. Many of the commuters are probably ‘essential workers’, who should be protected to prevent the virus’s spread, the researchers say. Reference: Harvard preprint (not yet peer reviewed) Risk for minority ethnic groups is a troubling mystery People who are not white face a substantially higher risk of dying from COVID-19 than do white people — and pre-existing health conditions and socioeconomic factors explain only a small part of the higher risk. Researchers examined the medical records of more than 17 million people in England and found that there must be something more to the grim inequality than health conditions or social disadvantages. Reference: medRxiv preprint (not yet peer reviewed) Speedy technique churns out synthetic viruses Researchers have used yeast cells to create a synthetic version of the SARS-CoV-2 genome in just a week — much more quickly than other methods. The technique could be used to assemble viruses rapidly to study the biological effects of new mutations, the researchers say. Reference: Nature paper Strong and sweeping antibody response to coronavirus Nearly everyone who recovers from COVID-19 makes antibodies against the new coronavirus. A study of more than 1,300 people who had symptoms of the disease found that more than 99% of them eventually developed antibodies — though some didn’t produce detectable antibodies until one month after they first started feeling ill. Reference: medRxiv preprint (not yet peer reviewed) A separate study found that people infected by the new coronavirus make antibodies against several of the virus’s proteins. Much of the effort to develop vaccines and diagnostic tests has focused on a viral protein called Spike. But these results, which have not yet been peer-reviewed, suggest that other proteins might also be important determinants of immunity against SARS-CoV-2. Reference: medRxiv preprint (not yet peer reviewed) Get more of Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Credit: Gabriela Hasbun for Nature Microbiologist Elisabeth Bik’s observations are among the most revered (and feared) in science. Last year, she left her job to work full-time on spotting duplicated images in the scientific literature. Eschewing software, Bik relies on her natural knack for spotting repeated areas, and doesn’t shy away from sharing her suspicions on PubPeer and Twitter. “I’ve been called a bitch a couple of times,” she says. “It comes with the work I do.” Bik estimates that her discoveries have led to at least 172 retractions and more than 300 errata and corrections. How would you fare? Check if you’re a super-spotter with our quiz. Nature | 14 min read The history of our planet is far from decided. When did the earliest life arise, and when did it move onto land? What was the deal with ‘snowball Earth’, when the planet was (almost) completely iced-over? We need data wrestled from ancient rocks to find the answers, argue five geoscientists. They lay out a plan to ensure that key geological samples are available for research and don’t languish in private collections. Nature | 10 min read Astrophysicist Elizabeth Tasker took virtual meetings to the next level when she invited a group of students to learn about the Hayabusa2 mission while standing on the touchdown site on the asteroid Ryugu. Let me know how you’re spicing up your video calls — plus any other feedback on this newsletter — at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '170'>13 May 2020</date>
<url id = '171'>https://nature.com/articles/d41586-020-01428-z</url>
<title id = '171'>The long, hard road to recovery for some coronavirus survivors, why not enough autopsies are being done to determine how COVID-19 kills and the longest animal ever discovered.</title>
<body id = '171'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here People wait outside an immigration office in Berlin.Credit: Adam Berry/Getty One of the largest and most detailed studies of psychological health in young refugees found that the violent and life-threatening events they often encounter add to their risk of developing psychiatric problems. The uncertainty and stress of navigating asylum systems in their host nations make matters even worse. Researchers worked with 133 apparently healthy young migrants, nearly one-third of whom travelled alone as children. Participants’ stories of trauma and abuse were so horrific that they left the researchers needing counselling themselves. Nature | 4 min read Move over, plain helium. Pionic helium is here: a helium atom in which one of the two electrons has been replaced by a negative pion, a composite particle made of one quark and one antiquark. Exotic atoms can help physicists to make exquisitely precise measurements of the fundamental constants of nature, such as the size of the proton. Pionic helium could provide a direct measurement of the mass of a related fundamental particle, the neutrino. That has been estimated indirectly, says physicist Masaki Hori, but “it is always nice to have a direct laboratory determination”. Pionic helium is the latest addition to a zoo of exotic atoms, including positronium, muonium, muonic hydrogen, muonic deuterium and antihydrogen. No dilithium crystals yet, though. Physics World | 6 min read Source: Nature paper. Go deeper with the related News & Views and the Nature Podcast. Researchers need tissue samples to determine what is killing patients affected by COVID-19.Credit: Giorgos Moutafis/Reuters Autopsy slowdown is hindering research Strained health-care systems, lockdowns and safety requirements have hampered efforts to collect tissue from people who have died after coronavirus infection. “We need those tissues to determine what is killing patients affected by COVID-19,” says pathologist Roberto Salgado. “Is it pneumonia? Is it blood clots? Why do they develop kidney failure? We have no clue.” (Nature | 5 min read) Travel limits hit Australian research jobs Some 7,000 researchers — more than 4% of Australia’s research force — could lose their jobs within the next 6 months because of a drop in international students. Australian universities, which employ nearly half of the country’s researchers, depend on foreign students for about one-quarter of their revenue. Because of a dramatic drop in the number of these students due to travel bans and visa restrictions, universities will lose billions of dollars — costing scientists their jobs and hobbling PhD projects, according to a report from the country’s chief scientist. (Nature | 3 min read) How the coronavirus is changing publishing Fast-tracked peer review, extended scoop-protection policies and video calls between authors and editors are among the many new measures journals are taking in an effort to share coronavirus discoveries more quickly and openly. “COVID-19 may help make these ideas standard,” says journal editor Bernd Pulverer. (Nature Index | 5 min read) ‘Four tests, and I still don’t know if I’ve had it’ Journalist Stephanie Baker thought she might have had COVID-19, so she decided to get an antibody test, which indicates whether a person has been infected. In fact, she took four — and was left with “conflicting results that left me even more anxious — and with more questions than at the start”. Her experience demonstrates why such tests can do more harm than good, and why countries are still scrambling to find tests that are accurate enough to power large-scale disease-control efforts. (Bloomberg Businessweek | 6 min read) Read more: Will antibody tests for the coronavirus really change everything? (Nature, from April) For some, surviving is just the start From hard-hit Italy, physicians report that some people who survived the coronavirus face long convalescences — which are even longer for people with lighter symptoms. “It leaves something inside you,” says 77-year-old Albertina Bonetti. “And you never go back the way you were before.” (The New York Times | 7 min read) The number of people that authorities in Wuhan, China, aim to test for COVID-19 within a ten-day period. (BBC | 5 min read) HIV taught public-health experts that an abstinence-only message doesn’t work for sex. It won’t work for social contact either, argues infectious-disease epidemiologist Julia Marcus. (The Atlantic | 7 min read) The dots that dapple wildly diverse monkeyflower petals are the result of a tug-of-war between two genes — and evidence for a decades-old theory by mathematician Alan Turing. Turing’s ‘reaction–diffusion’ model explains how chemicals with opposite effects can interact to create patterns in a variety of organisms, from seashells to zebra stripes. Researchers genetically altered monkeyflowers in the laboratory to observe how the two genes generate an activator molecule and a repressor molecule to produce the stunning variety of the blossoms. National Geographic | 7 min read Reference: Current Biology paper The web-based tool BioRender offers a pared-down set of features specifically for life-science and medical illustration. The results typically serve as illustrated explanations of proposed models, experimental methods or biochemical pathways. A library of around 30,000 icons, which includes depictions of everything from the SARS-CoV-2 virus particles to fruit flies, can cut figure-drawing time down from days to minutes. Nature | 5 min read Credit: ROV SuBastian/SOI Scientists exploring the deep sea off the coast of Australia have discovered up to 30 new underwater species — including this string-like creature known as a siphonophore, which might be the longest animal ever discovered. Measuring 46 metres — almost twice the average length of a blue whale — it is the largest specimen of the giant siphonophore Apolemia ever recorded. Although they look, behave and move around like individual organisms, siphonophores are actually floating colonies made up of tiny multicellular organisms called zooids that are attached to one another and cannot survive independently. See more of the month’s sharpest science shots, selected by Nature’s photo team. With key international environmental meetings on hold and economies on life-support, a Nature editorial looks forward to economist Partha Dasgupta’s upcoming report on ways to revolutionize the way in which we calculate economic progress. What’s the best way to make it clear that you’re talking about a preprint? Webcomic xkcd has a suggestion. This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '171'>12 May 2020</date>
<url id = '172'>https://nature.com/articles/d41586-020-01412-7</url>
<title id = '172'>Real spinach and artificial chemistry combine to turn sunlight into sugar. Plus, the first CRISPR test for the coronavirus is approved in the United States and how swamped preprint servers are blocking bad coronavirus research.</title>
<body id = '172'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Membranes from spinach chloroplasts — the light-harvesting organelles in plant cells — were put to work in a microfluidic chip. Credit: Getty A biological membrane from spinach leaves can work in tandem with a highly efficient engineered chemical pathway to turn CO2 into sugars. The artificial chloroplast could someday help produce therapeutic drugs or suck up carbon from the atmosphere. “That’s what excites us: we can realize solutions nature has never touched on,” says synthetic biologist Tobias Erb. Nature | 4 min read Source: Science paper Wriggling aquatic worms can alter the physical properties of the water they swim in, making it flow 10–100 times more smoothly. Experiments have demonstrated that worms can provide macroscopic models for the behaviour of ‘active’ polymers — molecules that move on their own and spontaneously form patterns. Researchers controlled the activity of the worms by raising or lowering the temperature of the water, or by adding alcohol to it. The alcohol seemed harmless to the worms, which took just half an hour to sober up after being returned to pure water, says physicist Antoine Deblais. “Sometimes it’s better to be a worm.” APS Physics Focus | 5 min read Source: Physical Review Letters paper The US Food and Drug Administration (FDA) has granted emergency-use approval for a new coronavirus test that takes advantage of the gene-editing technology CRISPR. It works by programming CRISPR machinery, which can home in on certain genetic sequences, to detect a snippet of SARS-CoV-2 genetic material from a sample. The test can return results in about an hour, according to the company. Researchers say widespread use of the kit could help to alleviate backlogs and increase testing. It remains to be seen how well the test performs in real-world conditions, such as hospitals, compared with standard tests. Nature | 3 min read Purple rashes, swollen legs, clogged catheters and sudden death — all are signs of blood clots, large and small, that are a frequent complication of COVID-19, and researchers are just beginning to untangle why. It’s not just the presence of blood clots that has scientists puzzled: it’s how they show up. “There are so many things about the presentations that are a little bit unusual,” says vascular biologist James O’Donnell. Nature | 6 min read Researchers are scrambling to understand how COVID-19 is spreading under the radar in group-living situations, such as nursing homes, prisons and homeless shelters. The answers are essential to protect people who don’t have the luxury of separating themselves from others — and to eventually end the outbreak. Especially difficult is shielding the roughly 1.4 million people who use a homeless shelter or transitional housing in the United States each year. Nature | 6 min read Reporter Amy Maxmen tells the Nature news team about the story on the Nature Coronapod (28 min listen) Preprint servers, where scientists post manuscripts before peer review, have been flooded with coronavirus research: bioRxiv and medRxiv have posted nearly 3,000 such studies between them. The servers are walking a tightrope between quickly disseminating coronavirus science and avoiding poorly conducted research that could fuel misinformation and conspiracy theories. They have enhanced their usual screening procedures to look even more carefully for claims that might cause harm, and are flagging papers that might contradict widely accepted public-health advice or inappropriately use causal language in reporting on a medical treatment. Nature | 6 min read Virologist Peter Piot, who co-discovered Ebola and spent years leading the fight against HIV, shares his very personal insights from having COVID-19. (Science | 8 min read) • Phase I/II trials have started in the United States for COVID-19 vaccine candidates from pharmaceutical giant Pfizer and BioNTech, a German biotechnology company. The drugs are a group of four related RNA-based formulations. Reference: Pfizer press release • Roche’s COVID-19 antibody test has been granted emergency-use authorization by the FDA. The company says that the test has 100% sensitivity (correctly identifying those who have had the disease) and over 99.8% specificity (correctly identifying those who have not). Reference: Roche press release • A study of more than 7,000 confirmed COVID-19 cases in Hubei, China, revealed that type 2 diabetes significantly increases the risk of all-cause mortality. Reference: Cell Metabolism paper • An antibody cloned from a llama inhibited the SARS-CoV-2 virus in cell cultures. The llama has been immunized with a protein from a different coronavirus, SARS-CoV-1. Reference: Cell paper Read more in Nature Medicine’s weekly round-up of the latest coronavirus research. Is there room in the crowded canon for a new biography of Galileo Galilei? You bet, writes Alison Abbott in her review of just the thing, penned by astrophysicist Mario Livio. With science denialism stronger than ever, Galileo’s grapple with heresy charges is chillingly relevant. Nature | 3 min read Fictional dinosaur attraction Jurassic Park celebrates an end to lockdown. (McSweeney’s Internet Tendency) On Friday our roaming Rockhopper took in the stunning Valle de la Luna in Bolivia. Did you spot Leif Penguinson? When you’re ready — here’s the answer. This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '172'>11 May 2020</date>
<url id = '173'>https://nature.com/articles/d41586-020-01401-w</url>
<title id = '173'>Dark-matter device will use a Bose–Einstein condensate of rubidium-87 atoms to search for axions. Plus, the science still isn’t clear on how children spread the coronavirus and the month’s best science images.</title>
<body id = '173'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Credit: Katie Orlinsky for National Geographic This aerial view shows the Batagaika crater, a massive land slump in Siberia that formed in the 1960s when deforestation caused the permafrost to melt. The tadpole-shaped crater is about one kilometre long and nearly 90 metres deep, and grows year by year as the warming climate thaws the frozen ground. The layers of sediment on its exposed walls offer a glimpse into 200,000 years of Earth’s geological history, and ice-age fossils have been found buried in the sediment. This photo was taken by photographer Katie Orlinsky as part of a series on permafrost that was awarded third prize in the environment category of the 2020 World Press Photo Awards.See more of the month’s sharpest science shots, selected by Nature’s photo team. Nature | Leisurely scroll A device will use the ‘fifth state of matter’ to search for axions, a relatively neglected candidate for dark matter. The ‘comagnetometer’ consists of two extremely sensitive magnetic-field detectors made from rubidium-87 atoms. The atoms are cooled to near absolute zero to create a Bose–Einstein condensate, so each essentially acts as a single atom. Because they have different spin states that respond differently to magnetic fields, any variation measured by the two detectors could signal the presence of axions. Physics World | 3 min read Reference: Physical Review Letters paper Children represent a small fraction of confirmed COVID-19 cases, but researchers are divided on how susceptible they are to infection and how much they contribute to the disease’s spread. Some scientists point to a growing body of evidence suggesting that children are at lower risk of infection and are not responsible for the majority of transmission — and thus we can reopen schools. Others argue that the incidence of infection in children is lower than in adults partly because they haven’t been exposed to the virus as much — especially with many schools closed. Children are also not getting tested as often because they have mild or no symptoms. If this is the case, we’ll see infections spike in places where kids return to class. Settling the debate will require large, high-quality population studies — some of which are already under way — that include antibody tests for previous infection. Scientists are also seeking answers to why children seem to suffer less from the infection, and how it might help the rest of us. Nature | 4 min read Despite some news reports, it’s too soon to say whether there is more than one strain of the SARS-CoV-2 coronavirus — let alone one that’s more dangerous or more contagious. The virus does mutate, and tracking how it changes helps to trace its spread — which is beautifully visualized on Nextstrain. But scientists generally consider these variations to be the same strain because there is no strong evidence of significant differences, such as in how easily they spread. The Atlantic | 9 min read Disabled people have long been “masters of invention” in academia. The pandemic’s disruption shows how much the field could learn from them. (Nature | 4 min read) Seeking the roots of mental illness, researchers have found that many of the same genes underlie seemingly distinct disorders. Now, psychiatrists are turning away from the old system of categorizing mental disorders into neat boxes. Instead, they’re building a framework grounded in biology that will hopefully improve diagnosis, treatment and understanding of the underlying causes. Nature | 11 min read Source: Ref. 8 “Eppur si muove” — “And yet it moves”. That phrase, supposedly muttered by Galileo Galilei after his conviction for heresy, has become a symbol of intellectual defiance against science denialism. Astronomer Mario Livio talks to Nature Podcast about his new biography of the Pisan scientist, and the role he played in establishing the scientific method. “He really started the modern way of doing physics — and science in general,” Livio says. Nature Podcast | 22 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Andrew Robinson’s pick of the top five science books to read this week includes smart-technology spies, a final warning on the environment and the staggering costs of cancer. Nature | 3 min read Israel Hershkovitz is director of the Dan David Center for Human Evolution and Biohistory Research at Tel Aviv University.Credit: Corinna Kern for Nature Anthropologist Israel Hershkovitz uses a pneumatic drill on a half-tonne breccia to gently expose the bones of a human who lived between 100,000 and 120,000 years ago. He and a colleague dug the sedimentary block from a cave in central Israel, and realized that it could hide evidence of the earliest known deliberate burial. “I call myself a biohistorian,” Hershkovitz says. “I’m trying to understand human history, not from human artefacts, temples or big walls surrounding old cities, but from bones.” (Nature | 3 min read) This weekend our roaming Rockhopper is taking in the stunning Valle de la Luna in Bolivia. Can you spot Leif Penguinson? The answer will be in the Briefing on Monday, all thanks to photo czar Tom Houghton. It’s a holiday tomorrow round these parts, so there will be no Briefing tomorrow. See you on Monday, and take care. Your feedback is always welcome at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '173'>07 May 2020</date>
<url id = '174'>https://nature.com/articles/d41586-020-01384-8</url>
<title id = '174'>A black hole merely 1,011 light years from our solar system is the closest ever discovered. Plus, how to boost good COVID-19 science on social media and the full-genome lion family tree.</title>
<body id = '174'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here This artist’s impression shows the orbits of the objects in the HR 6819 triple system. The system is made up of an inner star (orbit in blue) and a newly discovered black hole (orbit in red), as well as a third star in a wider orbit (also in blue).ESO/L. Calçada A black hole merely 1,011 light years from our solar system is the closest ever discovered. Astronomers uncovered its presence while studying the binary star system HR 6819 in the constellation Telescopium. The central star orbits a mysterious object every 40 days or so, and the outer star encircles the central star and the object, which has a mass of 4.2 suns. A regular star of that size would shine brightly — so a black hole is the only option. “It seems like it’s been hiding in plain sight,” says astronomer Kareem El-Badry. National Geographic | 8 min read Reference: Astronomy & Astrophysics paper Geneticists have reconstructed the relationships of 20 lions, including several extinct ones, by sequencing their genomes. The results support the idea that lions migrated out of Africa, like humans did. Researchers took samples from both living and dead animals, including two 30,000-year-old cave lions (Panthera leo spelaea) preserved in permafrost in Siberia and Canada’s Yukon territory. The findings reveal details about how lions took over the world — they were once the most globally widespread mammal species — and where genetic diversity has been quashed by shrinking populations. National Geographic | 4 min read Source: PNAS paper The Chinese government is heavily promoting traditional medicines as treatments for COVID-19. The remedies, a major part of China’s health-care system, are even being sent to countries including Iran and Italy as international aid. But scientists outside China say it is dangerous to support therapies that have yet to be proven safe and effective. Nature | 5 min read The conversation about COVID-19 science is happening on social media whether we like it or not — so it’s worthwhile for scientists to counter the trolls and conspiracy theorists, argues neuroscientist and science communicator Samantha Yammine. She recommends avoiding hot takes, amplifying the good stuff and being compassionate rather than dismissive. Nature | 6 min read Source: Research Investments in Global Health study (RESIN), University of Southampton The number of confirmed coronavirus deaths in the United Kingdom — now surpassing Italy as the most coronavirus deaths in Europe. There are many caveats: the UK population is about 10% larger than Italy’s, and Italy has done more testing. (BBC | 6 min read) Physician Anthony Fauci, the director of the US National Institute of Allergy and Infectious Diseases and the face of the country’s COVID-19 response, is a very busy person. (National Geographic | 15 min read) In some areas, conservationists have contributed to dislodging people who have lived sustainably for generations. Anthropologist Jerome Lewis saw this happen to the BaYaka Pygmies, who had lived in the forests of Central Africa’s Congo Basin for some 55,000 years. “The formerly active, well-fed and lively BaYaka are now often malnourished, depressed and alcoholic casual laborers dwelling on the edges of their former territories, terrorized by so-called eco-guards and subjected to commercial and sexual exploitation by outsiders.” In various parts of the world, new programmes are now attempting a better approach. Scientific American | 20 min read Nature is rooting for the staff who have chosen to remain at the US Environmental Protection Agency (EPA) — and stands with researchers and businesses fighting to maintain the agency’s legacy. (Nature editorial | 5 min read) So, Tom Cruise is going to film a movie on the Space Station. Which star would you most like to see doing flips in microgravity — and in what type of film? (For me it’s Leif Penguinson in a rom-com, naturally.) Let me know — along with your feedback on this newsletter — at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '174'>06 May 2020</date>
<url id = '175'>https://nature.com/articles/d41586-020-01358-w</url>
<title id = '175'>Climate change is pushing much of humanity out of the comfortable ‘climate niche’ we have enjoyed for the past 6,000 years. Plus: a symbiotic microbe that naturally blocks malaria in mosquitoes and the complex biology powering the coronavirus pandemic.</title>
<body id = '175'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here An Indian farmer walks across the bed of a pond that has dried during a water crisisSanjay Kanojia/AFP via Getty Images Models of population growth and warming indicate that climate change is pushing much of humanity out of the comfortable ‘climate niche’ we have enjoyed for the past 6,000 years. If no substantial action is taken to cut greenhouse-gas emissions, places home to one-third of the global population will experience temperatures similar to the Sahara’s within 50 years. Even under optimistic scenarios, the homelands of more than 1 billion people will become unbearably hot by 2070 — with catastrophic consequences for human migration and food production. “I’ve previously studied climate tipping points, which are usually considered apocalyptic. But this hit home harder. This puts the threat in very human terms,” says climate scientist Tim Lenton. The Guardian | 6 min read Source: PNAS paper A microbe discovered in mosquitoes on the shores of Lake Victoria in Kenya seems to prevent the insect from harbouring the malaria parasite. The microsporidian symbiont, Microsporidia MB, occurs naturally in the insect and doesn’t seem to harm them. It’s not exactly clear how the microbe prevents malaria in the mozzies. Researchers hope that infecting more mosquitoes with Microsporidia MB could someday limit malaria transmission to humans. BBC | 4 min read Reference: Nature paper The new coronavirus, SARS-CoV-2, has an array of adaptations that make it much more lethal than the coronaviruses humanity has met so far. Unlike close relatives, SARS-CoV-2 can readily attack human cells at multiple points, with the lungs and the throat being the main targets. Once inside the body, the virus makes use of a diverse arsenal of dangerous molecules. And genetic evidence suggests that it has been hiding in nature possibly for decades. But there are many crucial unknowns about this virus, including how exactly it kills, whether it will evolve into something more — or less — lethal and what it can reveal about the next outbreak from the coronavirus family. Nature | 14 min read Source: M. F. Boni et al. Preprint at bioRxiv https://doi.org/10.1101/2020.03.30.015008 (2020). Physicist-turned-economist Arthur Turrell tells Nature how Bank of England researchers are fusing economics and epidemiology to study the complex effects of the pandemic. Two of the most pressing topics: the interaction between the macroeconomy and the progression of the disease, and how people’s economic choices might change the risk that they come into contact with the virus. Nature | 5 min read An artificial-intelligence (AI) tool called Scite.ai aims to reveal whether COVID-19 studies stand up to subsequent research — especially preprints that haven’t undergone peer review. Unlike conventional citation-metrics tools, Scite.ai tells users how often a paper has been supported or contradicted by the studies that cite it, as well as how many times it has simply been mentioned. The tool is limited to open-access papers, but it has managed to analyse more than 16 million full-text scientific articles so far. Nature | 6 min read Meditation apps are not enough to stem the coming tide of despair, argues community-health psychologist Rochelle Burgess. (Nature | 5 min read) How China stopped the outbreak Drastic social distancing in Shanghai and Wuhan was enough to bring the epidemic under control in the two cities. Modelling work suggests that, in Shanghai, school closures alone would not have stopped the epidemic — but they did lower the number of new infections per day at the epidemic’s peak, which relieved stress on hospitals. Another study shows that quick detection and isolation of infected people were the most effective steps for containing COVID-19 in China. But even with those efforts in place, the number of cases would have soared if officials hadn’t restricted travel and social interactions. Reference: Science paper & Nature paper Portraits of a viral enzyme could aid the hunt for drugs Molecular snapshots of a key SARS-CoV-2 enzyme in action provide clues to how drugs, including the experimental therapy remdesivir, attack the virus. Two teams looked at the action and structure of a viral enzyme called an RNA-dependent RNA polymerase. Reference: bioRxiv preprint & Science paper Get more of Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Computer scientists have built a system that can make simple guesses — for example, what happens after “Gary stacks kindling and logs and drops some matches”. Machine-learning techniques, such as neural networks, can make statistical guesses after absorbing large amounts of data, but they are notoriously lacking common sense. Other attempts relied on knowledge bases containing millions of rules and sentences handpicked by humans. COMET combines the two approaches, using a knowledge base to train a neural network. Lead researcher Yejin Choi says she was surprised that no one had tried this approach before. “It’s almost as if nobody bothered because they were so sure this would never work.” COMET’s guesses are still statistics-based, but at least they’re often right. Quanta | 14 min read This brain scan of a microscopic C. elegans nematode was created as part of a study into how its nervous system changes as its body grows and its behaviours mature. Reference: bioRxiv preprintWitvliet et al., 2020 Particle physicist Daniel Whiteson recalls how one research group at CERN tackled meeting overload. (Twitter) Hats off to the father of Brandon Truett, who took out a billboard to congratulate his son on achieving a PhD. Don’t make me go that big to find out what you think of this newsletter. Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '175'>05 May 2020</date>
<url id = '176'>https://nature.com/articles/d41586-020-01340-6</url>
<title id = '176'>Major efforts have begun to spur plasma donations from survivors and develop antibody products to treat coronavirus. Plus: scientists stared at rocks for two and a half years to prove bacteria make dirt.</title>
<body id = '176'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Alastair Fyfe Ephemeral labels are springing up on the plucky plants that inhabit urban streets. ‘Rebel botanists’ are chalking names near weeds and trees to strengthen people’s connection to nature and raise awareness of overlooked flora. The trend first exploded in France and has now taken root in other countries — including the United Kingdom, despite it being illegal to chalk anything on public land. “Botanical chalking gives a quick blast of nature connection, as the words encourage you to look up and notice the tree above you, the leaves, the bark, the insects, the sky,” says one anonymous chalker in London. “And that’s all good for mental health.” The Guardian | 7 min read A two-and-a-half-year experiment has found evidence that bacteria make dirt. Scientists have long believed that microorganisms are involved in turning rock into soil — the mix of solid, gaseous and liquid matter, part mineral and part organic, that supports most land ecosystems. But the process tends to happen too slowly to be observed in a lab. Researchers started with an exceptionally fast-weathering rock called quartz diorite and ground it to speed things up even more. After 30 months, samples that had been kept sterile retained sharp, smooth edges, whereas those that had been exposed to bacteria looked ragged and pitted. They also contained abundant ATP, a chemical generated by feasting microbes. Scientific American blog | 5 min read Reference: PNAS paper Treating people with the antibody-laden blood of those who have already survived an infection is a century-old approach to battling viruses. Evidence for its efficacy against COVID-19 is thin on the ground, but it has two big advantages over new treatments: it’s available now and it’s relatively safe, as long as the blood is screened. In the United States and the United Kingdom, major efforts have begun to spur plasma donations from survivors. And large pharma companies are joining forces to develop antibody products purified from the pooled plasma of donors, or produced in genetically modified cattle. Nature Biotechnology | 10 min read Read more: How blood from coronavirus survivors might save lives (Nature, from March) Get the expert view from Nature’s news team in our weekly audio overview of the state of coronavirus science. This week, we dig into the promise of contact-tracing apps, early results from a US trial of the antiviral drug remdesivir and things that have made us smile in the past week. Nature Coronapod | 32 min listen The coronavirus can probably infect great apes — putting endangered animals in Africa and Asia at risk. Tourism in African parks has been stopped, and researchers who observe chimpanzees in Uganda and Côte d’Ivoire are taking extra precautions, such as wearing masks and changing their clothes, to prevent animal infections. If these measures fail and apes get too sick to defend themselves from leopards and poachers, researchers plan to sleep nearby to protect them. On the Indonesian island of Sumatra, an orangutan-conservation programme has moved some of the island’s orangutans to another site to reduce the risk to the whole population. Science | 6 mins Researchers forced to prematurely halt their experiments are facing grief and uncertainty over what comes next. Early-career researchers, senior scientists and a researcher who bid a final farewell to a mentor share their experiences. “In the end, I found myself euthanizing mice by the masses in the university basement,” says neuroscientist Kathleen Beeson, who grieves for her hand-reared colony of experimental animals and her interrupted research. “It was the punctuation on a sad and disorienting week.” STAT | 8 min read Stress is the hidden pandemic for health-care workers, argues psychiatrist John Krystal. (Nature Medicine | 4 min read) Young children are not immune to COVID-19 Children are as likely as adults to contract SARS-CoV-2 after close contact with an infected person, according to a study in Shenzhen, China. Researchers analysed nearly 400 cases of COVID-19 and 1,300 people who were ‘close contacts’ of the infected people. Seven per cent of close contacts younger than age 10 became infected — roughly the same as in the population overall. The researchers also found that just 9% of original cases — the ‘superspreaders’ — were responsible for 80% of infections detected in close contacts. Reference: The Lancet Infectious Diseases paper SARS-CoV-2 might hijack its host’s immune defences The new coronavirus invades human cells after one of its proteins binds with ACE2, a protein found in cells in many human organs. Researchers studied airway cells from people with influenza (influenza virus also invades the respiratory tract), and found that signalling molecules called interferons — which normally fend off viruses — switch on the host genes encoding the ACE2 protein. The result suggests that the body’s defences against viral attack drive the activation of the gene for ACE2. Reference: Cell paper Immune system shows abnormal response to COVID-19 The immune response to SARS-CoV-2 differs from the response prompted by other respiratory viruses, according to an analysis of infected cells, ferrets and humans. The results suggest an immune imbalance: low levels of interferons reduce a cell’s ability to limit viral replication, and the activation of less-specific immune responses promotes inflammation. Reference: Cell paper Get more of Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. You might never have had to access subscriber-only Springer Nature content from home before, but if you have access at work, you probably already can. Here’s a quick guide to getting logged on to your institution’s Nature and SpringerLink subscriptions from off campus. Theoretical physicist Piers Coleman remembers Philip Anderson, one of the most influential theoretical physicists of the twentieth century, who died on 29 March, aged 96. (Nature | 5 min read) Last week, Leif Penguinson visited the majestic Kolugljúfur Gorge in Iceland. Did you spot the penguin? When you’re ready — here’s the answer! This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '176'>04 May 2020</date>
<url id = '177'>https://nature.com/articles/d41586-020-01333-5</url>
<title id = '177'>Explore how long a coronavirus vaccine will take, and how scientists will choose the best from the dozens in development. Plus, an AI that generates new songs by dead artists (and living ones), and how alchemists tackled the plague and economic wobbles.</title>
<body id = '177'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here OpenAI Do you want to hear a brand new Frank Sinatra song? Beware: it’s about how Christmas time is hot-tub time, and it was written and sung by a computer. Software by machine-intelligence powerhouse OpenAI can produce songs in the style of any existing act, although it requires human help to compose the lyrics. Contrary to earlier attempts, OpenAI’s Jukebox is trained with sound files rather than musical scores, and its output is a sound stream. It’s technically impressive, but for now the results sound vaguely like the overlapping stations heard on a poorly tuned radio. Jukebox’s creators acknowledge that “there is a significant gap between these generations and human-created music”. The Verge | 3 min read Reference: OpenAI blog and preprint More than 90 vaccines for the coronavirus are at various stages of development, and at least six are being tested for safety in people. Now, developers, funders and other stakeholders are laying the groundwork for their biggest challenge yet: determining which vaccines actually work. The World Health Organization has proposed an adaptive trial design that allows vaccines to be added and dropped on an ongoing basis. The agency still has to work out which vaccines to test first, and how to convince drug developers to have their products pitted against each other. Large trials are usually necessary to determine safety and efficacy. An alternative is to administer vaccines that look safe in early-stage trials to high-risk groups — such as health-care workers — under ‘emergency use’ rules. Nature | 7 min read The US National Institutes of Health (NIH) has taken the unusual step of suddenly cutting the funding for a US-China project that investigates how coronaviruses, such as SARS-CoV-2, move from bats to humans. The NIH declined to comment on why it canceled the grant, which was in its sixth year. The project seems to have fallen victim to a conspiracy theory about the origin of the virus. “This is cutting off your face to spite your nose,” says infectious-disease researcher Gerald Keusch, a former director of the NIH’s Fogarty International Center. “This is the worst kind of thing that political interference can cause in a democracy.” Science | 9 min read Scientists are breaking new ground with the speed and ambition of vaccine development for the coronavirus, but no one knows for sure how long it will take to succeed. The New York Times offers interactive graphics, informed by expert advice, to demonstrate how desperate measures might shorten the timeline — and at what risk. The New York Times | 15 min read Author Karen Russell explores the phrases — ‘self-isolation’, ‘social distancing’, ‘abundance of caution’ — that have infected our discourse. (The New Yorker | 9 min read) Before his life was cut short at the age of 26, Frank Ramsey made fundamental contributions to economics, mathematics and philosophy. As 17-year-old student at Cambridge University, he would converse as equals with John Maynard Keynes over long walks. Merging ethical considerations with economics, Ramsey wrote that making choices that ignore the interests of future generations is “ethically indefensible and arises merely from the weakness of the imagination”. Keynes called that paper one of the most remarkable ever written. For decades after his death, people spoke of a “Ramsey effect”, writes author Anthony Gottlieb in his review of the full biography of Ramsay, by philosopher Cheryl Misak. “You’d make a thrilling breakthrough only to find that Ramsey had got there first.” The New Yorker | 12 min read Researchers have found that the sniff reflex can indicate whether a person is in a vegetative state, and even the likelihood that they will regain consciousness. Plus: the stupefying effect of carbon dioxide, and a chameleon gemstone that tricks your eyes. The Nature Podcast has been shortlisted for a Webby award. If the podcast is one of your favourites, please consider casting your vote in our favour. Nature Podcast | 23 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. The ‘Serpent of Arabia’, philosophical mercury, dissolves gold and silver to create an elixir — its own life-giving blood.Credit: Leonard Smethley MS. Roll/Princeton University Library Fifteenth-century Europe ran low on bullion and faced a problem with plague. Curator and author Jennifer Rampling explores how alchemists sought to resolve the economic shortfall by transmuting base metals into gold and silver and pursued medicinal elixirs to heal the sick. All is revealed in the secretive allegorical language and obscure, fantastical imagery of the fabulous Ripley Scrolls. Nature | 6 min read The COVID-19 pandemic is exposing and exacerbating inequalities around the world. Read against this backdrop, economist Thomas Piketty’s latest book is timely, but inadequate, says reviewer Ingrid Harvold Kvangraven. Nature | 5 min read Jukka Pätynen is an acoustician at Akukon in Helsinki, Finland.Credit: Jarkko Mikkonen for Nature Acoustician Jukka Pätynen stands in an anechoic space, designed so that no surface reflects sound. Hard foam wedges cover the walls to absorb any incident sound; the room itself is a box within a box, with the inner space floating on elastic materials for vibration isolation. “On the decibel scale, 0 dB is the lower limit of human hearing,” says Pätynen. “Our calculations suggest that the actual background noise [in the room] could be as quiet as −10 dB.” (Nature | 2 min read) Researchers at the Galaxy Project, a web platform for open data analysis, pay tribute to influential computational biologist and co-founder James Taylor. Taylor died on 2 April, aged 40. (Galaxy Project blog) This week, Leif Penguinson is visiting the majestic Kolufoss waterfall in Kolugljúfur Gorge, Iceland. But can you spot the penguin? The answer will be in Monday’s e-mail, all thanks to Briefing photo editor Tom Houghton. Leif is in the stunning land where I had planned to spend my Easter holiday, the lucky little scamp. Let me know where you would like to visit in brighter days and maybe Leif will go there next! While you’re at it, please tell me what you think of this newsletter. Your feedback is always welcome at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '177'>01 May 2020</date>
<url id = '178'>https://nature.com/articles/d41586-020-01306-8</url>
<title id = '178'>New fossil suggests Spinosaurus was a ‘river monster’ powered by a fin-like tail. Plus: hopeful signs from the largest and most rigorous remdesivir trial yet and trace the spread of the SARS-CoV-2 virus through its mutations.</title>
<body id = '178'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Reconstruction of Spinosaurus in life: long, narrow jaws with conical teeth, and a tail adapted for aquatic locomotion.Davide Bonadonna A new fossil of one of the most unusual dinosaurs, Spinosaurus aegyptiacus, suggests it was a “river monster” powered by a fin-like tail. A new fossil found in Morocco has revealed shockingly long spines on the dinosaur’s tail that could have made it work like a giant fin. Hydrodynamic tests with plastic reconstructions of various tail shapes showed that the spinosaurus’s newt-like appendage would produce much more thrust than the tails of other dinosaurs. Spinosaurus aegyptiacus was probably even bigger than Tyrannosaurus rex, and the only other skeleton ever found was destroyed in the Second World War, says palaeontologist Nizar Ibrahim. Nature | 5 min video Reference: Nature paper   The US Patent and Trademark Office has ruled that artificial-intelligence (AI) systems cannot be credited as inventors. The case was raised in response to patent applications for a food container and a flashing light, both created by software called DABUS. The organization behind it was not arguing that an AI should own intellectual property, only that it should be listed as an inventor. But patent law clearly refers to humans, using pronouns such as ‘he’ or ‘she’, the agency concluded. The UK and European patent offices reached similar conclusions last year. The Verge | 3 min read The largest and most rigorous clinical trial yet of the antiviral medicine remdesivir against the coronavirus raises hopes that the drug could shorten the time to recovery. Yesterday, US National Institute of Allergy and Infectious Diseases director Anthony Fauci announced that the trial of more than 1,000 people showed that those taking remdesivir recovered in 11 days on average, compared with 15 days for those on a placebo. Results from other trials of the drug had been inconclusive. The full results of this study have yet to be published. “Although a 31% improvement doesn’t seem like a knockout 100%, it is a very important proof of concept,” Fauci said. “What it has proven is that a drug can block this virus.” A flood of small trials for various drugs has yet to convincingly show any that have boosted survival in people with COVID-19. Nature | 5 min read The coronavirus pandemic is “a problem that is now too big for any one person to fully comprehend”, argues science journalist Ed Yong in a long read about the factors in play. We struggle to master knowledge that is new to many of us: the definition of a ‘coronavirus’. The characteristics of this specific one, SARS-CoV-2, and the disease it causes, COVID-19. What the research on treatment and vaccines really means, and which experts and self-appointed experts we can trust. And how we can parse misinformation, shifting public-health advice and the uncertainty inherent in a fast-moving scientific landscape. The Atlantic | 26 min read Some of the history of the SARS-CoV-2 virus is recorded in its mutations. In this infographic-packed feature, follow the journey of the virus’s progress from Wuhan, China, to everywhere by tracing tiny variations in its RNA. The New York Times | 7 min read With people eager to clean and disinfect during the outbreak, accidental poisonings have spiked in the United States. Almost two-thirds of the incidents involved bleach, which can create a toxic gas when mixed with vinegar or other cleaning agents. Chemistry World breaks down exactly what happens when mixtures go wrong — and what to do if you detect the hissing, heat or bubbling that indicates a potential problem. Chemistry World | 6 min read An ‘every country for itself’ run on COVID-19 tests has shoved Africa out of the diagnostics market, warns Africa CDC head John Nkengasong. (Nature | 4 min read)  There are at least eight types of vaccine being tried against the coronavirus, and they rely on different viruses or viral parts. Around 25 research groups say they are working on the type shown above — viral-vector vaccines. Read Nature’s full graphical guide to how all the types work. (Nature | 8 min read) Reconnecting with dormant contacts can be helpful, inspiring, and fun — and not as awkward as we might fear, says organizational psychologist Adam Grant. He even suggests an opening line. (The New York Times | 5 min read) This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by David Cyranoski and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '178'>30 April 2020</date>
<url id = '179'>https://nature.com/articles/d41586-020-01300-0</url>
<title id = '179'>There are at least eight types of vaccine being tried against COVID-19 — this is how each of them works. Plus, a call for COVID-19 stimulus measures to focus on a healthier tomorrow, and the hazards lurking in unused buildings’ stagnant water</title>
<body id = '179'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Bio-bots are propelled by a ring of muscle on a hydrogel skeleton. Illinois researchers have been the first to innervate them with rat spinal-cord segments, giving the 'spinobots' a natural walking rhythm.Image courtesy Collin Kaufman Researchers have built a biological robot that is controlled with part of a rat’s spinal cord. The 6-millimetre robot is made of 3D-printed muscles made of laboratory-grown mouse cells, connected to the section of a rat’s spine that controls the hind legs. Nerves grew from the rat tissue into the muscles and made them contract, and the team could control their ‘Spinobot’ with neurotransmitters. Eventually, the technology could be used in prosthetics, says roboticist Collin Kaufman, but that would probably involve human tissue. “Nobody will have scary rat-spine hands.” New Scientist | 3 min read Reference: APL Bioengineering paper While the world focuses on the pandemic, the United States is adopting controversial policies at the Environmental Protection Agency (EPA), which could hamstring future administrations’ attempts to craft health and environmental safeguards. Nature looks at three recent decisions and two pending policy changes that could have a lasting impact. “Rather than focusing on protecting public health, EPA is on a misguided mission to protect the profits of regulated industries,” says environmental engineer Christopher Frey. Nature | 6 min read More than 90 vaccines are being developed against SARS-CoV-2 by research teams across the world. They are trialling different technologies, some of which haven’t been used in a licensed vaccine before. At least six groups have already begun injecting formulations into volunteers in safety trials; others have started testing in animals. Nature’s graphical guide explains each vaccine design. Nature | 8 min read Graphics: Nik Spencer/Nature Dormant plumbing in offices, gyms, restaurants and schools could harbour infectious bacteria and heavy metals, say researchers studying locked-down water systems to understand how people can safely reoccupy buildings. The longer a building sits unused, the more potential there is for harm. The most important things that building owners and managers can do are flushing taps regularly, carrying out low-cost testing and building up a water-management plan, says environmental engineer Andrew Whelton. Nature | 4 min read “Future pandemics are likely to happen more frequently, spread more rapidly, have greater economic impact and kill more people if we are not extremely careful about the possible impacts of the choices we make today,” write environmental researchers Josef Settele, Sandra Díaz and Eduardo Brondizio, who authored a chilling United Nations review of the wellbeing of our planet last year. They outline how COVID-19 stimulus measures must create a better — but very different — world that focuses on the shared good health of human and nature. Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services guest article | 4 min read The flood of COVID-19 studies is letting bad or incomplete science slip through the peer-review cracks, says scientific-integrity journalist Ivan Oransky. (Mother Jones | 14 min read) A massive assessment of education shows that only 61% of children worldwide will complete secondary education. The overall trend has been in the right direction, but we might not meet the UN Sustainable Development Goal of quality schooling for all by 2030. (To make matters worse, the modelling was done before the COVID-19 pandemic, which is preventing many children from attending school.) Better data can help, and a more concerted push is required, argues a Nature editorial. Nature | 5 min read The Hanford Site in Washington state has been called the most polluted place in the Western Hemisphere: during the Cold War, it produced plutonium for more than 60,000 nuclear weapons. Now, engineers are struggling to clean up the radioactive mess left behind — in particular, 177 ageing tanks, inside which “watery liquids rest atop goop as thick as peanut butter and salt cakes resembling wet beach sand”. A plan to secure the waste by turning it into glass has been in the works for decades, and could end up costing as much as US$550 billion and last 60 years. IEEE Spectrum | 18 min read Setting a routine, identifying a workspace and getting dressed can all help to motivate you while working from home as a PhD student, says neuroscientist Melina Papalampropoulou-Tsiridou. Physicist Alex Klotz spotted a delightful recurring error in which Google Scholar parses cafeteria lunch menus as papers, giving author credit to such luminaries as B Noodles and C Fajitas. Send me the best name you’ve ever spotted on a paper (could even be yours!) or any other feedback on this newsletter to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '179'>29 April 2020</date>
<url id = '180'>https://nature.com/articles/d41586-020-01288-7</url>
<title id = '180'>The Financial Times compared all known deaths during the outbreak in 14 countries with the average deaths in those places over the past four years. Plus: how scientists are discovering which COVID-fighting strategies work best and we celebrate 30 years of the Hubble telescope (and look at why it’s still going strong).</title>
<body id = '180'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A colorful image resembling a cosmic version of an undersea world teeming with stars commemorates the Hubble Space Telescope's 30 years of viewing the wonders of space. In the Hubble portrait, the giant red nebula (NGC 2014) and its smaller blue neighbor (NGC 2020) are part of a vast star-forming region in the Large Magellanic Cloud, a satellite galaxy of the Milky Way, located 163,000 light-years away.NASA, ESA and STScI The Hubble Space Telescope just turned 30, but some of its biggest discoveries could still lie ahead, says Ken Sembach, the director of the institute that manages the venerable observatory. He shares recollections of some of Hubble’s breakthroughs, including measuring the expansion of the Universe three times more precisely than it was originally designed to do. Scientists are now working on tripling that precision yet again, as well as conducting studies that none of its planned successors could do, because they will lack the capability to sense ultraviolet rays. The institute hopes the telescope might last another five years, possibly more. “I would never bet against Hubble,” Sembach says. Scientific American | 8 min read Despite having suspended most communications last year as tensions escalated following a terror attack, India and Pakistan are working to find common ground against the huge locust swarms threatening their people. Officials have been meeting at a place on the border known as Zero Point, to discuss pest breeding patterns and control strategies. “Whether we like it or not, we have to cooperate,” says political scientist Ashok Swain. Undark | 8 min read Closing schools? Mass testing? Mandatory face masks? Shutting borders? Working out the effectiveness of various measures is one of scientists’ most pressing questions as we seek an end to lockdowns without a wave of fresh infections. Researchers are gathering data from groups across the globe and exploring which statistical approaches to use to analyse it. So far, Germany and Austria stand out as nations that adopted aggressive strategies early, compared with Italy, France and Spain — and have seen a fraction of the deaths from COVID-19 of these other countries. Nature | 7 min read Source: Oxford Coronavirus Government Response Tracker (data); Nature (charts). A Financial Times analysis indicates that the death toll from coronavirus might be far higher than reported in official counts. The newspaper compared all known deaths during the outbreak in 14 countries with the average deaths in those places over the past four years. Extrapolating to the whole world, there might have been as many as 318,000 deaths in excess of normal levels at the time of writing, compared with the official global COVID-19 death count of 201,000. Financial Times | 5 min read The scientific community must take up cudgels in the battle against bunk, argues Timothy Caulfield. First, we must stop legitimizing pseudoscience at reputable institutions — no more reiki at health clinics, or homeopathy offered by public-health providers in Canada and the United Kingdom. Disinformation expert Claire Wardle says that the best way to fight misinformation is to swamp the landscape with accurate information. “So, let’s get swamping,” says Caulfield. Nature | 5 min read UK Prime Minister Boris Johnson, who returned to work yesterday after a spell in intensive care with COVID-19, has urged people not to lose patience with the lockdown. (BBC | 8 min read) Wide-scale genetic testing for SARS-CoV-2 has been hampered, in part, by shortages of the solutions used to store sampling swabs and extract viral RNA from them. To overcome this difficulty, researchers have developed a procedure for detecting viral RNA in swabs without the highly sought solutions. Reference: bioRxiv preprint Researchers tested the concentration of SARS-CoV-2 RNA in aerosols — fine airborne particles — at two hospitals treating people with COVID-19. They detected elevated levels in locations such as a small toilet used by patients, and staff changing rooms. No viral RNA was detected in staff rooms after they had been disinfected. Low to undetectable levels were found in the hospitals’ well-ventilated patient wards. The presence of airborne viral RNA suggests that SARS-CoV-2 has the potential to spread by way of aerosols, the researchers say. They suggest that measures such as routine disinfection and better ventilation could help to control the virus’s spread. Reference: Nature paper Get more of Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Whales are singing lower, King penguins are being muffled by the wind and snapping shrimp — one of the loudest creatures in the ocean — are losing the will to snap. Around the world, ecologists are documenting how climate change is altering the soundscape of the natural world. For example, as seawater soaks in carbon and gets more acidic, snapping shrimp — which produce their ear-popping noises to stun prey — seem to be snapping less. “It’s not that ocean acidification completely takes away their ability to make loud snaps,” says marine biologist Ivan Nagelkerken. “They can still do that but essentially don’t want to do that any more.” New York Times | 6 min read Planta Researchers inserted four genes from a bioluminescent mushroom called Neonothopanus nambi into the DNA of tobacco plants to make them glow. The greenish glow — which is visible to the naked eye — reveals when the plants are stressed and how they develop. The scientists are also hoping to bring an ornamental version to the market. (The Guardian | 5 min read) Reference: Nature Biotechnology paper After nearly seven months at the International Space Station, astronaut Jessica Meir is back on Earth — but coronavirus means there’s even more to adjust to than usual. (Vanity Fair | 15 min read) This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '180'>28 April 2020</date>
<url id = '181'>https://nature.com/articles/d41586-020-01258-z</url>
<title id = '181'>The long road to full recovery after being hit hard with COVID-19. Plus: why physicians are becoming more hesitant to use ventilators to treat the illness.</title>
<body id = '181'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Pierre Brye/Alamy The US Supreme Court has ruled that federal law does protect underground water that flows into lakes, rivers and the sea. The case concerned a municipal sewage-treatment plant in Hawaii that pumps around 15 million litres of treated wastewater into underground wells each day. In its decision, the court cited a complex study by the University of Hawaii in Honolulu that demonstrated a link between these discharges and algal blooms and coral-reef damage. Science | 6 min read An aid worker provides protective face masks to migrants in Greece.Credit: Manolis Lagoutaris/AFP/Getty Close living quarters, widespread underlying health problems and limited access to sanitation and medical care mean that COVID-19 poses an outsized threat to the 70 million refugees, displaced people and asylum seekers around the world. “Any kind of epidemic is never good, but particularly not this one, where physical distancing is impossible and home isolation is a joke,” says Annick Antierens. (Nature | 3 min read) Doctors rethink rush to ventilate COVID-19 patients Mechanical ventilators are crucial for saving the lives of some people with COVID-19 — but physicians are increasingly becoming more hesitant to use them too often. Early data indicate that the survival rates of critically ill people with COVID-19 who are mechanically ventilated appear to be lower than expected. The treatment is invasive — it involves a tube in the airway and sedation — and requires a highly trained operator. “It’s not just about running out of ventilators, it’s running out of expertise,” says pulmonology and critical-care physician David Hill. (Reuters | 11 min read or in graphic form) Makers turn to their 3D printers to stop COVID-19 Engineers, designers and hobbyists are turning to open-source methods to produce tools for testing and treating coronavirus. Explore some of the most successful designs, the challenges these designs face and how the open-source approach might bring lasting changes to how things are done. (Nature | 8 min read) Draft remidesivir results released by accident The premature release of results from a long-awaited clinical trial conducted in China dampens hopes that the antiviral medicine remdesivir might speed recovery of people with COVID-19. According to a summary that was temporarily visible on the World Health Organization website, remdesivir was “not associated with a difference in time to clinical improvement” compared with a standard-of-care control. The drug’s maker, Gilead Sciences, says the post was a draft, included “inappropriate characterization of the study” and didn’t reflect “trends in the data [that] suggest a potential benefit for remdesivir, particularly among patients treated early in disease”. Gilead says the study manuscript is now in peer review. (STAT | 6 min read) What happens after you survive For those people who recover from a severe bout of COVID-19, both the illness and the treatment can have lingering effects. Clinicians are learning lessons from other diseases about the physical, cognitive and mental-health problems that might be in store. (Science | 5 min read) Don’t drink or inject bleach, you could die Doctors and manufacturers are warning people against injecting or consuming bleach or any household cleaning product after statements by US President Donald Trump in today’s White House coronavirus task force briefing. Responding to research confirming that disinfectants kill the virus on surfaces, Trump suggested researchers investigate whether “we can do something like that, by injection inside”. Trump responded to evidence that sunlight impedes virus transmission via surfaces by suggesting “we hit the body with a tremendous — whether it's ultraviolet or just very powerful light”. Physician Deborah Birx, the US coronavirus response coordinator, stated in the same briefing that light is not a treatment for coronavirus. (BBC | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Particle physicist Claire Lee is just one of the record numbers of attendees enjoying the virtual version of the huge April Meeting of the American Physical Society conference. (Nature | 5 min read) (FOMO is ‘fear of missing out’, BTW.) An ingenious experiment published in an 1856 paper by polymath Eunice Foote was the first to suggest that changes in the atmosphere’s composition could directly affect the climate. Foote’s findings predated John Tyndall’s similar but better-known work by three years, but we don’t know for sure whether Tyndall was aware of them, and her results slipped into obscurity. “I would like to see her known as the mother of global warming and climate change [science],” says historian John Perlin. Chemistry World | 8 min read Reference: Foote’s 1856 American Journal of Science and Arts paper By combing through the DNA of over 27,000 modern-day Icelanders, researchers have uncovered new insights about the ancient hominin species who interbred with Homo sapiens. Plus, the scent of lemur love, a hidden Viking trade route and ‘gargantuan’ hail. Nature Podcast | 23 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Many former military bunkers, such as this one near Edgemont, South Dakota, are being repurposed into doomsday communities.Credit: Jim Lo Scalzo/EPA-EFE/Shutterstock Mark O’Connell’s Notes From An Apocalypse is an eerily prescient mix of confession, political critique, meditation and comic monologue on living in the face of societal collapse. After a journey to various bunkers, including billionaire Peter Thiel’s luxury New Zealand bolthole, O’Connell finds a deeper solace in the way in which children connect with, rather than retreat from, the world. Nature | 5 min read After emancipation, Georgia’s infamous Milledgeville hospital for people with mental illness began to accept black patients — but segregated them and treated them without concern for the extreme racial violence that they had experienced. Physician and historian Mical Raz examines a new book about the institution by feminist and anti-racist scholar Mab Segrest, which combines archival research with fictionalized scenes. Nature | 4 min read Andrew Digby is Science Adviser Kakapo/Takahe for the New Zealand Department of Conservation.Credit: Deidre Vercoe/New Zealand Department of Conservation Andrew Digby works to protect the kakapo, a critically endangered New Zealand bird. To know them is to love them, Digby says of the large, flightless, nocturnal parrots. During the breeding season, which happens every few years when the rimu tree fruits, Digby spends months on the four predator-free sanctuary islands that are the kakapos’ last refuge. The perils of working from home were illustrated when spacecraft-operations engineer Daniel Lakey’s cat invaded a European Space Agency teleconference. (The Atlantic | 6 min read) This week, our intrepid friend Leif Penguinson is hiding among the beautiful swamp cypress (Taxodium distichum) trees of Sukko Lake, Russia. Can you spot the penguin? The answer will be in Monday’s e-mail, all thanks to Briefing photo editor Tom Houghton. This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '181'>24 April 2020</date>
<url id = '182'>https://nature.com/articles/d41586-020-01243-6</url>
<title id = '182'>From Galileo to Rachel Carson’s Silent Spring, catch up on some of the science classics you’ve always intended to read (or pretended to have read). Plus, the first COVID-19 death in the US might have occurred 3 weeks earlier than thought and why one climate scientist says there is no silver lining to coronavirus.</title>
<body id = '182'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Alyson Kelvin in her containment lab.Credit: Danny Abriel Autopsy results change US coronavirus timeline The first COVID-19 death in the United States might have occurred in California on 6 February — more than three weeks before the first reported death in Washington state. Three deaths in Santa Clara County between 6 February and 6 March have now been attributed to COVID-19 after autopsies. Similar reports have surfaced elsewhere in recent weeks, including Italy. (Nature | continuously updated) US vaccine chief in bitter row over hydroxychloroquine Senior US-government official Rick Bright says he was abruptly ousted as the chief of the agency in charge of coronavirus-vaccine development because he did not support unproven treatments promoted by US President Donald Trump. “I believe this transfer was in response to my insistence that the government invest the billions of dollars allocated by Congress to address the Covid-19 pandemic into safe and scientifically vetted solutions, and not in drugs, vaccines and other technologies that lack scientific merit,” said Bright in a statement. Following his resignation, unnamed colleagues in the agency have criticized Bright for his leadership style, and for the strategy and pace of decision-making in his department. (Politico | 7 min read) “It’s hard. It’s complicated. But we’ve got to do this right.” Jim Yong Kim is building on his reputation for tackling problems that we thought were intractable, earned as director of the World Health Organization’s HIV/AIDS programme and president of the World Bank. With luminaries such as top US doctor Tony Fauci on speed dial, Kim is now dedicating himself to the COVID-19 response — in particular, by spearheading a robust public-health response in Massachusetts. Kim speaks passionately to Nature about his fears for low- and middle-income countries, and why the best thing for the economy is to get the virus under control. (Nature | 6 min read) Life as a coronavirus-vaccine researcher Virologist Alyson Kelvin is at the forefront of efforts to develop a coronavirus vaccine in Canada. She describes her whirlwind journey across the country with her laboratory members to join forces with other vaccine researchers and quickly gain funding and permits to work with the virus. (Nature | 4 min read) Climate scientist: there is no silver lining to coronavirus Climate-scientist Kate Marvel brings her straight-talking analysis to the question of whether we should be happy that the pandemic is reducing greenhouse-gas emissions. “All this suffering will not make the planet any cooler,” says Marvel. “If the air quality is better now, if fewer people die from breathing in pollution, this is not a welcome development so much as an indictment of the way things were before.” (Drilled News | 3 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Former World Bank and WHO chief Jim Yong Kim says that to defeat COVID-19, you’ve got to trace where the virus is going and chase it down. (Nature | 6 min read) Palaeontologist Jennifer Clack, who made groundbreaking discoveries on the emergence of vertebrates out of water and onto land, died on 26 March at age 72. Clack transformed our knowledge of the four-legged, salamander-like animals that evolved from fish and slowly adapted to surviving outside water, starting 419 million years ago. Some of her most celebrated finds came from a 1987 Greenland trip, inspired by her chance discovery of a specimen in a museum drawer in Cambridge, UK. These finds included animals that had seven or eight toes on each foot. “A happy convergence of brilliance, tenacity, opportunity, generosity and modesty enabled Clack (née Agnew) to rejuvenate an entire research field,” writes Per Ahlberg, one of Clack’s students who was part of her Greenland expedition. Nature | 5 min read Physicists have been developing a field called quantum thermodynamics, which aims to reconcile the science that propelled the steam machines of the Industrial Revolution with modern quantum mechanics. Theoretician Nicole Yunger Halpern likens this to steampunk, a literary and lifestyle genre that blends science-fiction technology with Victorian style. “‘Quantum steampunk’ unites 21st-century technology with 19th-century scientific principles,” she writes. “The spotlight has swept from trains to nanoscale engines, living cells' molecular motors and the smallest possible refrigerators.” These researchers confront fundamental questions — such as why the arrow of time points forward — and practical ones on how to engineer future quantum computers. Scientific American | 12 min read If you’ve got time on your hands, why not catch up on some of the science classics you’ve always intended to read (or pretended to have read). • Galileo Galilei’s Sidereus nuncius (The Celestial Messenger) was the first telescopic survey of the sky, published 400 years ago. Historian John Heilbron reflects on the little book that changed everything. (Nature | 5 min read) • Self-taught mathematician Mary Fairfax Somerville first achieved an overview of scientific achievement — and arguably launched popular science as a genre. Science writer Richard Holmes enjoys her brilliant and original 1834 book, On the Connexion of the Physical Sciences. (Nature | 6 min read) • More than 50 years after it was published, Rachel Carson’s Silent Spring lit a beacon of reason that continues to burn, writes evolutionary biologist Rob Dunn. Carson dared to criticize the then-wanton use of pesticides, changed US and international policy and helped to give rise to the environmental movement. (Nature | 5 min read) • Alfred Russel Wallace's masterpiece of biogeography, The Malay Archipelago, takes readers on a joyride through the vast chain of islands stretching eastward from Sumatra. Science writer David Quammen enjoys “a wondrous book of travel and adventure that wears its deeper significance lightly”. (Nature | 6 min read) • Science writer Philip Ball reviews the surprising insight and imagination of John Dalton’s New System of Chemical Philosophy: “one of those foundational books that doesn't say what you might think it should”. (Nature | 6 min read) Governments must resist the urge to bail out carbon-intensive industries and keep building momentum towards climate justice, argues a Nature editorial. For some hard-science beauty today, take a look at #ThinSectionThursday: nature’s wonders, from fossils to meteorites, very thinly sliced. I’d appreciate even a wafer-thin portion of your feedback on this newsletter. Please send your opinions to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '182'>23 April 2020</date>
<url id = '183'>https://nature.com/articles/d41586-020-01199-7</url>
<title id = '183'>People are volunteering to be infected with coronavirus in an effort to speed up vaccine development. Plus: astronomers might have detected an elusive exoplanet, and cancer biologists are harnessing the power of AI.</title>
<body id = '183'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here  It’s far from certain, but astronomers might have captured an image of the exoplanet Proxima c, thought to orbit the red dwarf star Proxima Centauri in our nearest neighbouring planetary system. Researchers analysing data from the European Southern Observatory’s Very Large Telescope identified infrared images that seem to show the planet appearing across several years of routine observations. If genuine, the detection could tell us Proxima c’s size and the angle at which it orbits its star. However, some say it could be the result of random noise unwanted light from artefacts or background stars — in the data . Scientific American | 7 min read arXiv preprint A series of summer heatwaves made 2019 Europe’s hottest year on record, according to new data from the European Union’s Copernicus Climate Change Service. These show a ‘clear warming trend’ in the region over the last four decades. High temperatures caused drought in several countries over the summer months. Others experienced flooding and landslides in November and December, which brought unusually heavy rainfall. Bloomberg | 4 min read Several laboratories in the US are offering antibody tests for COVID-19, but the results require nuanced interpretation.Credit: Beaumont Health Laboratories across the United States have launched initiatives to test hundreds of thousands of people for antibodies against the coronavirus that causes COVID-19. And researchers are telling those people the outcomes — despite uncertainty about what they mean. They’re couching the results in careful terms, but because of the urgent need to stem the outbreak, to prevent more deaths and to reopen businesses, they argue that it’s important to just get going. “One thing we have learned is that we can’t let perfect be the enemy of good — we need to act,” says virologist Keith Jerome. (Nature | 6 min read) 1,500 people volunteer for controversial vaccine study A grassroots movement has attracted nearly 1,500 volunteers prepared to take part in controversial ‘human challenge’ trials — which involve intentionally infecting healthy, young volunteers with the coronavirus to test potential vaccines. The effort is not affiliated with vaccine-developing groups or companies, but co-founder Josh Morrison hopes to show them that there is broad support for this kind of trial. “We want to recruit as many people as possible who want to do this and pre-qualify them as likely to be able to participate in challenge trials should they occur,” he says. (Nature | 3 min read) The untapped potential of US testing labs A survey of more than 4,000 researchers suggests that better coordination could make hundreds of thousands more tests for coronavirus available across the United States. Nearly 1,600 of those polled said they had the necessary tools and biosafety conditions, but were not testing. About 95% of labs not currently testing said they needed more information on protocols and regulations. The survey was prompted by a Nature investigation revealing that several top US university labs approved to process coronavirus tests are operating at half their potential capacity. (Nature | 3 min read) Transplants on hold owing to COVID-19 concerns People are having to wait longer for life-saving organ transplants because hospitals are concerned about putting both donors and recipients at risk of coronavirus infection, and need to prioritize the use of intensive-care beds and ventilators for critical patients with COVID-19. (STAT | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. For research-policy manager Elizabeth Gadd, the pandemic has highlighted the importance of open science. (Wonkhe | 6 min read) This week, Nature joins media outlets around the world in a week of intensive reporting called Covering Climate Now. For the second year running, we aim to focus attention on the need for urgent climate action. So in the Briefing this week, you’ll see more than the usual number of climate-change stories, although not exclusively. This year, the focus is on climate solutions. To read more about why we are uniting with colleagues and competitors around the world to highlight the issue of climate change, read the Nature editorial from last year. Climate scientists are studying a fresh set of socio-economic scenarios to model the future of the planet. The simulations range from optimistic worlds — in which governments join forces to advance low-carbon technologies — to bleak ones, with countries ramping up their use of cheap fossil fuels to pursue economic growth at any cost. The research could have a key role in the negotiations around a new set of commitments to reduce emissions. “We need these model results to give us insights into the impacts of our choices,” says environmental-health researcher Kristie Ebi. “This is not science fiction.” Nature | 9 min read Habitat loss, climate change and fungal disease are conspiring to threaten many amphibians: 41% of those species are estimated to be at risk of imminent extinction. But some hope that in vitro fertilization could help. Last year, researchers created the first amphibian born from sperm that had been frozen, a tiny Puerto Rican crested toad (Peltophryne lemur) named Olaf. The team fertilized an egg from a captive mother with sperm from a wild father. Both parents had died. “We were able to recover a genetic lineage that had disappeared,” says ecologist Andy Kouba. The Guardian | 9 min read Cancer biologists are harnessing the power of artificial intelligence to deal with an explosive growth of data. The latest machine-learning techniques can find patterns that researchers might have missed. And increasingly, researchers are able to use off-the-shelf tools, rather than having to develop them in their labs. Applications range from predicting drug responses on the basis of a person’s cancer-genome sequence to assessing protein localization. Nature | 10 min read Ahead of this year’s Earth Day, primatologist Jane Goodall shares her hopes that lockdowns will help people to appreciate the natural world. (Scientific American | 2 min read) Has lockdown left you wondering why video chats are so exhausting? Researchers Gianpiero Petriglieri and Marissa Shuffler offer some insights on ‘zoom fatigue’ and how to reduce it. Let’s stay connected! Send your feedback on this newsletter to briefing@nature.com. Emma Stoye, news editor, Nature With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '183'>22 April 2020</date>
<url id = '184'>https://nature.com/articles/d41586-020-01171-5</url>
<title id = '184'>Anders Tegnell explains why Sweden has chosen a different path — and what he regrets. Plus, a merger between black holes of very different sizes and people underestimate the total cost of owning a car by about 50%.</title>
<body id = '184'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A visualization of a collision between two differently sized black holes.Credit: N. Fischer, H. Pfeiffer, A. Buonanno (Max Planck Institute for Gravitational Physics), Simulating eXtreme Spacetimes (SXS) Collaboration Gravitational-wave astronomers witnessed a merger between black holes of two greatly different sizes, one nearly four times more massive than the other. The unprecedented observation gave them insight into how one of the black holes spins, which had eluded them in previous gravitational-wave experiments examining mergers of equally sized black holes. The new data promise new ways to test Einstein’s general theory of relativity. “It’s an exceptional event,” said astrophysicist Maya Fishbach. Nature | 5 min read Reference: arXiv preprint John Martinis, the physicist who spearheaded Google’s quantum-supremacy breakthrough last October, has left the company after being reassigned to an advisory role. Martinis joined Google in 2014 and brought some of his laboratory members from the University of California, Santa Barbara, where he retained an academic position. He will return to full-time academia. “Since my professional goal is for someone to build a quantum computer, I think my resignation is the best course of action for everyone,” says Martinis. Wired | 4 min read Epidemiologist Anders Tegnell (centre).Credit: Jonathan Nackstrand/AFP/Getty Sweden has stood almost alone in Europe in avoiding a lockdown, and in relying on voluntary, trust-based measures to stem the spread of COVID-19. “As a society, we are more into nudging,” says Anders Tegnell, the epidemiologist behind the controversial strategy. He argues that closing borders is pointless when the disease is already everywhere, and shutting schools has little effect unless it’s done very early in an outbreak. In general, Tegnell is happy with the approach, although he regrets how older people in care homes were not sufficiently protected. (Nature | 6 min read) The coronavirus's deadly march through the body Although the lungs are ground zero for damage caused by COVID-19, in some people the disease can devastate the heart and blood vessels, kidneys, gut,and brain — and much of its effects remain poorly understood. Science steps through the disease’s destructive path from infection onwards, with a handy infographic. (Science | 14 min read) Pandemic brings forward UAE Mars mission The Arab world’s first Mars mission — a spacecraft called Hope — will ship from the United Arab Emirates (UAE) to Japan weeks earlier than planned, as a result of travel restrictions imposed by the COVID-19 pandemic. “We had to expedite activities in Dubai and basically focus only on the critical testing,” project leader Omran Sharaf. Built by UAE and US engineers, the orbiter is scheduled to launch from Japan during a period that starts on 15 July, in which Earth and Mars are suitably aligned. (Nature | Continuously updated) Experts look back on when everything seemed fine Even some experienced infectious-disease experts did not fully appreciate how serious the COVID-19 outbreak would become. Some thought that the virus would be successfully contained in China, others were cautious about overreacting and some just couldn’t believe it could happen to them. “Everybody was in denial of this coming, including the U.S. And everybody got hit — just as simple as that,” says leading Canadian infectious-disease researcher Gary Kobinger. Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Exposure to air pollution, specifically NO2, might be an important contributor to COVID-19 deaths, says environmental-scientist Yaron Ogen. (The Guardian | 6 min read) This week, Nature joins media outlets around the world in a week of intensive reporting called Covering Climate Now. For the second year running, we aim to focus attention on the need for urgent climate action. So in the Briefing this week, you’ll see more than the usual number of climate-change stories, although not exclusively. This year, the focus is on climate solutions. To read more about why we are uniting with colleagues and competitors around the world to highlight the issue of climate change, read the Nature editorial from last year. Environmental economists surveyed more than 6,000 people in Germany and found that people underestimated the total cost of owning a car by about 50% — and that’s not counting extras, such as financing costs. The researchers suggest that helping people to clock the true cost of their transport choices could reduce car ownership by up to 37% and cut associated transport emissions by 23%. Nature | 11 min read Ten years after an explosion on a seafloor well killed 11 workers and spilled oil across the Gulf of Mexico, experts say that a similar disaster could happen at any time. The US government has slashed safety regulations that had been implemented in the wake of disaster at the Deepwater Horizon rig — the largest single accidental release of oil and gas into the ocean. “Of course it could happen again, and I think one of the things of most concern is that our ability to control a spill is pretty much the same as it was ten years prior,” says Frances Beinecke, who has served on an independent commission that investigated the disaster. The Guardian | 8 min read Read the review article in Nature Reviews Earth & Environment.  Fomalhaut b was thought to be among the few extrasolar planets to have been imaged directly. Then it vanished. Astronomers say it might have been a cloud of dust all along, produced by the collision of two objects about 200 kilometres across. (The Register | 5 min read) Reference: PNAS paper Political scientist Aisha Ahmad, who has worked in war zones and disaster areas around the world, advises us how to get through a hard day in lockdown. (The Chronicle | 6 min read) Museums are battling it out on Twitter to showcase their creepiest exhibit — warning, there is a LOT of weird taxidermy. If you fancy something more soothing, try this Cell Press colour-in brain-cell comic book, which is only slightly creepy. This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi and David Cyranoski. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '184'>21 April 2020</date>
<url id = '185'>https://nature.com/articles/d41586-020-01158-2</url>
<title id = '185'>A map of 850 distant galaxy clusters suggests “astonishing and depressing” news that the Universe might not be uniform. Plus: what we do and don’t know about antibody tests for coronavirus, and a space tow truck has rescued a commercial satellite for the first time.</title>
<body id = '185'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A view of the IS-901 satellite from MEV-1 during approach from approximately 20 metres, with Earth in the background.Northrop Grumman For the first time ever, a space ‘tow truck’ has rescued a commercial satellite. US aerospace-technology company Northrup Grumman’s Mission Extension Vehicle-1, or MEV-1, docked with Intelsat 901, an ageing communications satellite, in late February. Last Friday, MEV-1 adjusted the satellite’s inclination and orbit to give it a new lease on life. MEV-1 itself has a lifespan of 15 years, and can dock to and undock from multiple satellites. Ars Technica | 3 min read A map of 850 distant galaxy clusters hints that the Universe might not be uniform. Combining data from US, European and Japanese X-ray space telescopes, researchers have revealed galaxy clusters that were around 30% brighter or fainter than expected, suggesting that their distances had been poorly estimated. Taking these clusters as beacons of the rate of cosmic expansion, the findings would mean that one region is expanding slower than the rest of the Universe, and another is expanding faster. Astrophysicist Megan Donahue comments that a lopsided expansion “would be astonishing and depressing” because it suggests that our understanding of the Universe could be permanently incomplete. Scientific American | 6 min read Reference: Astronomy & Astrophysics paper Limited global testing for coronavirus (yellow) has led researchers to try to estimate the prevalence of infection within specific communities.Credit: CDC/Science Photo Library Dozens of biotech companies have scrambled to distribute antibody tests that tell whether someone has been infected with the virus that causes COVID-19. These tests are being hailed as a game changer that could help to decide when to end controversial lockdown measures. But many tests developed so far have not been reliable. “No test is better than a bad test,” says infectious-disease researcher Michael Busch. (Nature | 8 min read) Infections might vastly exceed official counts Widespread antibody testing in a Californian county has revealed that the prevalence of coronavirus infections might be more than 50 times higher than official figures suggest. The results — which have not yet been peer reviewed — are some of the first of more than a dozen ‘seroprevalence surveys’ being carried out in cities worldwide to estimate true infection rates. But questions about the reliability of testing kits mean that the survey results might not be accurate. (Nature | 7 min read) How research funders are trying to help Researchers confined to their homes and unable to carry on with grant-funded work are becoming increasingly concerned about how the coronavirus pandemic will affect their funding. Nature spoke to the world’s major research funders to find out how they are adapting their funding policies. (Nature | 8 min read) Coronavirus chemicals by ear Do you know your lopinavir from your remdesivir and your chloroquine from your hydroxychloroquine? Catch up with a series of short podcasts about the chemical compounds being tested in the World Health Organization’s global SOLIDARITY drug trials (ritonavir is coming next week). (Chemistry World | Several sub-10 min listens) . Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. US President Donald Trump’s decision to withhold funding from the WHO is wrong and must be reversed, argues a Nature editorial. This week, Nature joins more than 400 of the world’s media organizations in a week of intensive reporting called Covering Climate Now. For the second year running, we aim to focus attention on the need for urgent climate action. So in the Briefing this week, you’ll see more than the usual number of climate-change stories, although not exclusively. This year, the focus is on climate solutions. Listen to Nature’s chief magazine editor Helen Pearson tell the Nature Backchat podcast last year about why we are uniting with colleagues and competitors around the world to highlight the issue of climate change. Cutting personal carbon emissions might slash 10 tonnes per person each year — and contributing to a successful campaign to shutter a coal-powered plant could eliminate one million times that, notes science writer Emma Marris. She argues that scientists are perfectly placed to help to hold governments and companies accountable on the climate — and that such activities need not conflict with our scientific objectivity and rigour. (Nature | 5 min read) Nature | 5 min read In 2019, the Urgenda Foundation, a Dutch non-profit group, successfully sued the government of the Netherlands for doing too little to reduce emissions. Dennis van Berkel, who was Urgenda’s legal counsel, argues that scientific evidence has a crucial role in arguing the case in climate lawsuits. Nature | 5 min read Computational biologist Atma Ivancevic recommends her favourite light-hearted science podcasts, videos and other treats to help remind you why you like the job in the first place. (Nature | 4 min read) Last week, we remembered that time when Leif Penguinson went to the Moon with Apollo 15. Did you spot the penguin? When you’re ready — here’s the answer! Please let me know how you think Leif fared in lunar gravity (or any other feedback on this newsletter) at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi, Emma Stoye and David Cyranoski. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '185'>20 April 2020</date>
<url id = '186'>https://nature.com/articles/d41586-020-01142-w</url>
<title id = '186'>The addition of deaths of people at home and at medical institutions that weren’t reporting data has added another 1,300 fatalities to the official count. Plus: NASA’s soaringly ambitious plan to bring back rocks from Mars and apple hunters rediscover ten lost varieties.</title>
<body id = '186'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here An illustration of NASA's Perseverance rover, which is due to land on Mars in February 2021.Credit: NASA/JPL-Caltech The US and European space agencies have unveiled a daring plan to bring the first rocks back from Mars. The samples will be collected by NASA’s Perseverance rover, which is due to launch in the coming months. The ambitious interplanetary pilgrimage to return the rocks will involve two spacecraft that travel to Mars in 2026: a small rocket that will blast off the Martian surface, carrying the rocks into orbit, where a second craft will take them and fly back to Earth by 2031. “This is by no means a simple task,” says Jim Watzin, head of NASA’s Mars exploration programme in Washington DC. “But we have kept it as simple as possible.” Nature | 4 min read A volunteer-run organization has rediscovered 10 lost apple varieties in the northwestern United States. The Lost Apple Project seeks to rescue some of the 17,000 apple cultivars thought to have once been grown in North America, of which only about 4,500 are known to exist today. Members use old maps, county-fair records, newspaper clippings and nursery sales ledgers to find abandoned orchards and collect fruits. They then ship them to an Oregon conservancy, where they are studied. The organization also tries to reconstruct the history of the orchards. “When I find an apple that’s lost, I want to know who homesteaded it, when they were there, who their children were, when they took their last drink of water,” says amateur botanist E. J. Brandt. Associated Press | 7 min read Sam McNeil/AP/Shutterstock The city of Wuhan in China has added another 1,300 fatalities to its official count. The revision puts the number of deaths in the 11-million-person city at 3,869. China’s overall death toll is more than 4,600. Chinese officials said the reasons for the revision included the addition of deaths of people at home and at medical institutions that weren’t reporting data to its epidemic network. (Nature | Continuously updated) Most people infected on aircraft carrier have no symptoms The US aircraft carrier Theodore Roosevelt has become another natural experiment for the transmission of COVID-19. The Navy has tested almost the entire crew after one member died from the disease. Many who tested positive had no symptoms, just like on the cruise ship Diamond Princess. On the cruise, which was packed with older people, the asymptomatic proportion was 18%. On the aircraft carrier — unsurprisingly home to mostly young, healthy people — it was 60%. “We’re learning that stealth in the form of asymptomatic transmission is this adversary’s secret power,” said Rear Admiral Bruce Gillingham, surgeon general of the Navy. (Reuters | 3 min read) Wisps of hope for remdesivir The researcher overseeing a closely watched clinical trial in Chicago has indicated that the antiviral medicine remdesivir might be showing positive signs against COVID-19. More than 100 people, most with severe disease, were treated with daily infusions of remdesivir (there was no control group). “Overall our patients have done very well,” infectious-disease specialist Kathleen Mullane told colleagues in an internal video call. Study leaders urge extreme caution about these preliminary and as-yet-unpublished results. (STAT | 5 min read) Watching the wave engulf New York Emergency-room physician Helen Ouyang describes how she went from hearing how the coronavirus outbreak overwhelmed hospitals in Lombardy, Italy, to seeing it happen first-hand in her hospital in New York. “It has been less than six weeks, but I’ve never felt less useful as a doctor,” writes Ouyang. “The one thing I can do — what I think will matter most, in the end — is just to be a person.” (The New York Times | 43 min read) Not a great time to be a parent-scientist Alessandra Minello, a social demographer who studies how families manage household and paid work, explores how the pandemic will affect researchers who are working from home and caring for children. She highlights the role of women, who tend to spend significantly more time on household work than do men, even in the most gender-egalitarian countries. (Nature | 5 min read) The last coronavirus-free continent Researchers in Antarctica, who are used to isolation, have taken things up a notch to maintain the continent’s status as the last place on Earth without COVID-19. On King George Island, social events between the various national bases have been cancelled. And scientists wait anxiously to see what impact the global pandemic will have on their continuing research. (Reuters | 4 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Oncologist Charlie Swanton shares how researchers at one of Europe’s largest biomedical-research centres, the Francis Crick Institute, have retooled their cancer lab into a diagnostic-testing facility. (Nature | 4 min read)   Physicists have found the strongest evidence yet that neutrinos are fundamentally different from their antimatter counterparts. Researchers at the Super-Kamiokande detector in Japan found that one flavour of neutrino — muons — morphed into different types of particle at a different rate than did their antimatter twins. If confirmed, the results could help to solve the Universe’s greatest mystery: why there is more matter than antimatter. (Nature | 4 min video) Go deeper with the expert perspective from physicists Silvia Pascoli and Jessica Turner in the Nature News & Views. Reference: Nature paper Donald Knuth’s earliest claim to fame was at age 13, when he won a contest by finding 4,700 anagrams for ‘Ziegler’s Giant Bar’ — and was awarded chocolate for his entire class. It was only the first of many honours that would come to include the A. M. Turing Award, the most prestigious in computer science. Knuth describes his multi-volume opus-in-progress, The Art of Computer Programming, as a manifesto for writing code so beautiful that it can be read by humans like a story. “It describes the way I love to do math and the way I wish I had been taught.” Quanta | 10 min read Andrew Robinson’s pick of the top five science books to read this week includes how to stay healthy indoors, the past and future of power, and shoe libraries. Nature | 3 min read Following a Nature investigation that revealed thousands of coronavirus tests are going unused in US laboratories, researchers tell the Coronapod what they found when they dug deeper into what’s holding things back. Plus, the team discusses what a freeze on US funding means for the World Health Organization, and investigates the role of the immune system in the death of COVID-19 patients. Coronapod | 29 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Li Hua is a structural biologist and pharmacologist at Huazhong University of Science and Technology in Wuhan, China.Credit: Peng Nian for Nature “Since January, I have spent every day alone in my laboratory, urgently trying to find a cure for COVID‑19,” says structural biologist and pharmacologist Li Hua, who lives on campus at Huazhong University of Science and Technology in Wuhan, China, the epicentre of the pandemic. Li’s colleagues, who had left for the Lunar New Year holiday, could not return during the lockdown and he is currently the only one who can access the lab. Now the lockdown there has ended, Li is working with collaborators in Shanghai to test the efficacy of the drug candidates he identified. (Nature | 3 min read) A rarely seen view of the top of one of Stonehenge’s giant sarsen stones reveals familiar-looking tenons and holes. (Stonehenge on Twitter) We have a winner! Our roaming Rockhopper shall be known henceforth as Leif Penguinson. Let songs be sung to the glory of reader Nigeala Nigrath for the suggestion! This week, we look back on the time Leif Penguinson joined the Apollo 15 mission to the Moon. Good times! But can you spot the penguin? The answer will be in Monday’s email, all thanks to Briefing photo editor Tom Houghton. Please let me know how you think Leif survived on the Moon without a spacesuit (or any other feedback on this newsletter) at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '186'>17 April 2020</date>
<url id = '187'>https://nature.com/articles/d41586-020-01132-y</url>
<title id = '187'>June Almeida left school at age 16 but went on to a storied career — although her initial coronavirus paper was rejected as “just bad pictures of influenza”. Plus: Major League Baseball teams become guinea pigs for coronavirus research and a neutrino discovery offers clues to the great matter–antimatter mystery.</title>
<body id = '187'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here View of photomultiplier tubes arranged at the Super-Kamiokande neutrino detector facilityThe Asahi Shimbun via Getty Physicists have found the strongest evidence yet that neutrinos are fundamentally different from their antimatter counterparts. Researchers produced neutrinos and antineutrinos at an accelerator in Tokai, Japan, and shot them 295 kilometres through the Earth’s crust to the Super-Kamiokande detector. The team found that one flavour of neutrino — muons — morphed into different types of particle at a different rate than did their antimatter twins. If confirmed, the results could help to solve the Universe’s greatest mystery: why there is more matter than antimatter. Quanta | 5 min read Go deeper with the expert perspective from physicists Silvia Pascoli and Jessica Turner in the Nature News & Views. Reference: Nature paper Ice retreating from a Norwegian mountain pass has uncovered a wealth of artefacts dating back to the Viking era or earlier.Local archaeologists first took note of the pass in 2011, when an ancient woollen tunic was found that could be more than 1,700 years old. The thaw intensified in 2019, revealing everything from stone-built cairns and the remains of a small shelter, to dairy products and reindeer pelts. The ice left many of the items in an exquisite state. “One might find arrows with the fletching perfectly preserved, with the sinew still in place, the glue that glued the feathers to the shaft,” says medieval and environmental archaeologist James Barrett. The Guardian | 5 min read Reference: Antiquity paper Research from China is crucial to understanding the COVID-19 pandemic.Credit: Xinhua News Agency/Shutterstock • Scientists in China say COVID-19 papers are being vetted by universities and that studies on the origin of the virus need government approval before being published. Some scientists welcome this process, saying it could stop poor-quality studies getting attention. But others fear China is trying to control information about the pandemic’s source, and that the vetting process could delay the release of valuable insights to help to control the virus. Documents obtained by Nature suggest that the Ministry of Education told universities early last month that they needed government approval before announcing results related to COVID-19. (Nature | 6 min read) • Staff of 27 US Major League Baseball teams — “all the way from general managers to hot dog vendors” — have volunteered for a study of COVID-19 antibodies. The teams offer scientists the opportunity to gather data quickly from a wide demographic and geographic spread of people in the United States. Viral immunologist Daniel Eichner spotted the potential of the cohort because of his other gig: running a testing laboratory for performance-enhancing drugs. (NPR | 6 min read) • More than 180 clinical trials of proposed COVID-19 drugs are already recruiting participants, and another 150 are registered to start soon. John-Arne Røttingen, the chair of the steering group of the World Heatlh Organization’s SOLIDARITY mega-trial, says a more collaborative approach is needed. “It's encouraging in the sense that it is really important to do trials,” says Røttingen, but “the scale of these trials is too small, and the variation in terms of how they are being run is too large. They aren’t really designed to answer the questions that need to be answered.” (Nature Reviews Drug Discovery | 7 min read) • The first coronavirus was discovered in the 1960s by virologist June Almeida, who grew up in a Glasgow tenement in the UK and left school at age 16. Almeida mastered electron microscopy as a lab technician and went on to a PhD programme and storied career. Her discovery could have been recorded even sooner: an earlier paper featuring her images was rejected as “just bad pictures of influenza”. (BBC | 4 min read) • Sociologist Alondra Nelson has gathered a crowdsourced list of resources dubbed the #CoronavirusSyllabus. “This exact thing hasn’t happened before, but something not dissimilar has happened before,” says Nelson. “These are some resources to help you contextualize this moment even if it is in some ways unprecedented.” (Institute for Advanced Study blog | 8 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Total confirmed cases of COVID-19 worldwide as of 15 April. The milestone comes just two weeks after one million infections were recorded. The United States has the most cases — more than 600,000 — followed by Spain and Italy. Because many people are not tested, the true number of cases is probably much higher. (Nature | Continuously updated) Infectious-disease epidemiologist Helen Ward, of the influential Imperial College London School of Public Health, argues that decision makers in the United Kingdom must learn from past mistakes. (The Guardian | 8 min read) An engaging biography uses the journey of Louis Bromfield from Pulitzer-prizewinning novelist to farming pioneer to reinforce growing calls to rebuild healthy, fertile soil around the world. Nature | 4 min read There is a diversity–innovation paradox in science, finds an analysis of US PhD recipients and their dissertations across three decades. (PNAS paper) This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '187'>16 April 2020</date>
<url id = '188'>https://nature.com/articles/d41586-020-01110-4</url>
<title id = '188'>What little we know about COVID-19 immunity, what we can intelligently guess at — and how decision makers can work with it. Plus: the United States halts funding to the World Health Organization, and mathematician and ‘Game of Life’ designer John Conway has died from COVID-19.</title>
<body id = '188'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Methane gas flare and pump jack at an oil well in North DakotaRichard Hamilton Smith/Getty The concentration of methane in the atmosphere has reached an all-time high. Data from a network of sampling stations operated by the US National Oceanic and Atmospheric Administration (NOAA) show concentrations of the potent greenhouse gas are rising, with an accelerating rate of increase. Methane comes from natural sources, such as wetlands, and from human activities, including oil and gas extraction and livestock farming. “Here we are. It’s 2020, and it’s not only not dropping. It’s not level. In fact, it’s one of the fastest growth rates we’ve seen in the last 20 years,” says climate scientist Drew Shindell. Scientific American | 4 min read Read more: Methane leaks from US gas fields dwarf government estimates (Nature, from 2018) Reference: NOAA report Open-source machine-learning can now cut the time it takes to complete certain chemical analyses from eight hours to one minute. The software tool, called DP4-AI, takes nuclear magnetic resonance (NMR) data and automatically suggests the most probable structure from a set of candidates. The tool’s creators say chemists shouldn’t worry that automation will lead them to lose their skills in teasing out what their spectra show. “Calculators have not stopped people doing arithmetic, but rather have allowed people to perform complex arithmetic more quickly and accurately,” says synthetic-chemist Jonathan Goodman. Chemistry World | 4 min read Reference: Chemical Science paper India has been in lockdown because of the coronavirus pandemic since 25 March.Credit: Manjunath Kiran/AFP/Getty • India plans to use people power to control the coronavirus outbreak. It cannot test the majority of its 1.3 billion people, so the country has sent thousands of public-health workers into villages, towns and cities to trace and quarantine those who might have had contact with infected people. Epidemiologists say India’s approach could be relevant for countries facing similar challenges — but widespread testing is needed to suppress the virus, or cases will be missed. (Nature | 6 min read) • US President Donald Trump has criticized the World Health Organization (WHO) for its handling of the COVID-19 pandemic and said the United States will halt its funding pending a review. The US has contributed more than US$400 million per year to the WHO over the past two years, making it the largest contributor to its roughly US$2.8-billion yearly budget. (Nature | Continuously updated) • Bats are a key source of human viruses — the SARS-CoV-2 coronavirus probably jumped to humans from bats — but not because of anything unusual about the cave cuties. There are just a lot of types of bat. A statistical analysis found that animal groups with more species tend to have more viruses, and consequently, a larger number of viruses that can jump to people. Rodents were the most species-rich order of mammals in the study; they also had the largest number of viruses that had moved to people. (Nature | 4 min read) • “Immunity after any infection can range from lifelong and complete to nearly nonexistent,” notes epidemiologist and infectious-disease researcher Marc Lipsitch. He lays out the current understanding of how our immune systems respond to other coronaviruses, and how decision makers can work with what little we know about COVID-19 immunity. (The New York Times | 8 min read) • Online scams, hoaxes and lies are overwhelming valid recommendations and crucial health information on social media, says social-scientist Joan Donovan. She argues for researchers to demand that tech companies become more transparent, accountable and socially beneficial. (Nature | 5 min read) • Mathematician John Conway, who designed the iconic Game of Life, died aged 82 on 11 April, reportedly of complications arising from COVID-19. His work transcended the boundary between recreational and ‘serious’ maths, turning play into research and vice versa. (Nature | Continuously updated) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Source: Our World in Data; European CDC - Situation Update Worldwide. Some nations have managed to slow the spread of COVID-19, but our exit strategy remains unclear, says epidemiologist Marc Lipsitch. Decision makers will have to find the right mix of isolating people with SARS-CoV-2 and tracing their contacts, border restrictions and social distancing. (Science | 5 min read) BepiColombo snapped these images as it swung past Earth on 10 April. The spacecraft is using Earth’s gravity to slow down en route to Mercury. Coronavirus precautions meant that most of BepiColombo’s mission-control team had to watch from home. (Nature | 4 min read)ESA/BepiColombo/MTM, CC BY-SA 3.0 IGO Condensed-matter physicist Seyed Akbar Jafari is one of six scientist-parents who describe how they juggle science and childcare from home. (Nature | 12 min read) If you find yourself stuck inside with a lot of extra time (and toilet paper), get inspired by this epic TP-powered Rube Goldberg machine built by engineer Mathieu Carillat. I don’t support toilet-paper hoarding, but I do love to have a folder full of your feedback. Please send your opinions on this newsletter to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '188'>15 April 2020</date>
<url id = '189'>https://nature.com/articles/d41586-020-01102-4</url>
<title id = '189'>Some chemists have come to think of water as two distinct liquid phases that coexist in a mixture. Plus: uncertainty about how COVID-19 kills is hampering treatment and scientists will soon be able to publish open-access papers in Nature.</title>
<body id = '189'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Doctors administer an experimental vaccine at treatment centre in Beni, Democratic Republic of the CongoCredit: Andia/Universal Images Group via Getty A new case of Ebola has been recorded in the Democratic Republic of the Congo (DRC) just two days days before the World Health Organization was set to announce the official end of the outbreak — so the 42-day countdown to an official declaration of victory must begin again. Public-health workers in the DRC face a war on two fronts: COVID-19 and Ebola. Nature | 6 min read Scientists will soon be able to publish open-access papers in Nature. The journal’s publisher, Springer Nature, says it is committed to joining the bold open-access initiative known as Plan S, pending discussion of further technical details. The publisher will offer a route to publishing open access in Nature and most Nature-branded journals from January 2021 — though we don’t yet know what it will cost. Nature | 5 min read (Nature is editorially independent of its publisher, Springer Nature — as is this Briefing.) Nanobiologist Mauro Ferrari, the president of European Research Council (ERC), has resigned suddenly after only three months in the post. Ferrari said he was resigning because the European Union had not coordinated an appropriate response to the COVID-19 crisis, and in particular because the ERC had rejected his proposal to create a programme to combat the coronavirus. The prestigious funding agency’s governing body responded with an unusually strong statement, saying that it had unanimously called for Ferrari’s resignation and that he had largely neglected his responsibilities. Nature | 4 min read The airborne Stratospheric Observatory for Infrared Astronomy (SOFIA) is the second-most-expensive astrophysics mission that NASA operates — behind only the Hubble Space Telescope — but ranks near the bottom in productivity among major astronomical facilities, a Nature investigation has found. The US-German observatory has fallen short of its goal to produce more than 150 scientific papers per year. Instead, it produced an average of 21 papers per year between 2014 and 2018 — fewer than most major observatories — although that rate has picked up in the past year. An independent panel review obtained through a freedom-of-information request questioned the observatory’s return on investment. Nature | 5 min read COVID-19 ravages the lungs of patients.Credit: Sergei Krasnoukhov/TASS/Getty • How does COVID-19 kill? Uncertainty over whether the virus itself — or a person’s immune response — ultimately overwhelms the organs is making it difficult for doctors to determine the best treatments. Clinical data suggest that the immune system plays a part in the decline and death of people infected with the new coronavirus, and this has spurred a push for treatments, such as steroids, that rein in that immune response. But some of these treatments act broadly to suppress the immune system, stoking fears that they could hamper the body’s ability to keep the viral infection in check. (Nature | 6 min read) • The COVID-19 pandemic is putting weather forecasts and long-term climate studies at risk of significant data gaps. For some research, this might be the first interruption in 40 years. “That’s painful for the scientists involved,” says ecologist Frank Davis. (Nature | 5 min read) • Coronavirus lockdowns are creating a natural experiment by drastically reducing emissions. But tantalizing signs that the situation is making the air cleaner aren’t as straightforward as they seem. The seasons and the weather also affect how much dangerous pollution is in the air. In China and Italy, the difference is so pronounced that experts think the coronavirus lockdowns probably have an impact. In the United States, it’s too soon to say. (Nature | 5 min read) • Thousands of coronavirus tests are going unused in US laboratories, reveals a Nature investigation. Some labs have ramped up their facilities for testing. But lab leaders say they’re performing at half capacity or less because of bureaucracy and logistical barriers. “I show up in a magic ship,” says genetic scientist Fyodor Urnov, “with 20,000 free kits and [approval] and everything, and the major hospitals say: ‘Go away, we cannot interface with you.’” (Nature | 8 min read) • Some optimistic forecasts suggest that a SARS-CoV-2 vaccine could be available in 12–18 months. Experts are getting into the nitty gritty of how we can deploy it to stop the pandemic. If millions of people need it, and producers continue making crucial supplies of other vaccines, there could be a shortage. Philanthropist Bill Gates says his foundation will help to pay for production facilities to be built in advance, even if some of them are never used (different types of infrastructure are needed depending on the vaccine type). Hoarding by rich countries could limit supplies, and there is no agreement yet on how a vaccine should be shared equitably. (Nature | 8 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19.  The first results from a symptom-tracking app, which has recruited more than 1.5 million people in the United Kingdom, found that users who tested positive for COVID-19 were three times more likely to report losing their sense of smell and taste than were those who had symptoms of the virus but tested negative. (Nature | Continuously updated) Finland’s history and geography explains why it has an enviable stockpile of pretty much everything, explains Tomi Lounema, the chief executive of Finland’s National Emergency Supply Agency. The country has tapped into its supply of medical equipment for the first time since the Second World War. (The New York Times | 10 min read) Water is unlike most other liquids on Earth: it has at least 66 weird properties, including high surface tension, high heat capacity, high melting and boiling points and low compressibility. Some chemists have come to think of it as not being one liquid at all, but two distinct liquid phases that co-exist in a mixture. “We can’t fill a glass of two kinds of water types, but there have been some experiments which have indirectly seen these sorts of transitions,” says mathematician and modeller John Russo. Chemistry World | 11 min read Science reporter Brian Resnick finds solace in a paper, based on a decade of unfunded work, about the resilience of flowers. (Vox | 10 min read) On Thursday, Briefing photo editor Tom Houghton hid our intrepid Rockhopper penguin in a stand of dragon blood trees (Dracaena cinnabari) in Yemen. Can you find the penguin? When you’re ready — here’s the answer! I’m happy to be back after our short spring break. You can make me even happier by sending your feedback — whether positive or critical — to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips, Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '189'>14 April 2020</date>
<url id = '190'>https://nature.com/articles/d41586-020-01083-4</url>
<title id = '190'>There are 78 confirmed COVID-19 vaccine candidates, 5 of which have already entered clinical trials. Plus: an engineered enzyme that dissolves one of the world’s most commonly used plastics, and giant dinosaur footprints found on the roof of a cave.</title>
<body id = '190'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A scientist on a caving trip happened to spot dinosaur tracks in the ceiling of Castelbouc Cave in France.Credit: Jean-David Moreau et al./J. Vertebr. Paleontol. Dinosaur footprints measuring up to 1.25 metres long have been found deep inside a cave in France — on the roof. The tracks were made 166 million to 168 million years ago, when three dinosaurs traversed the shoreline of a sea. The site was then at the planet’s surface, but geological processes have buried and tilted the sediments, and the prints are now on the cave’s roof, 500 metres underground. Researchers say the footprints probably belong to an unknown species of titanosaur, a category of long-necked herbivorous dinosaur that includes some of the largest animals ever to walk on Earth. Nature | 4 min read Read more Research Highlights: short picks from the latest papers. Pedro Vilela/Getty • As of yesterday, there are 78 confirmed COVID-19 vaccine candidates, 5 of which have already entered clinical trials. Get a firm grip on this incredibly fast-moving field with a short, data-driven overview by seven experts at the Coalition for Epidemic Preparedness Innovations (CEPI), one of the world’s leading vaccine funders. (Nature Reviews Drug Discovery | 6 min read) • Scientists agree that the new coronavirus SARS-CoV-2 was very likely to have originated in bats. Other than that, there are few certainties about how the virus jumped to humans. Pangolins have been suggested as the possible intermediary, but that’s still unproven. It’s also not firmly established that the ground zero of the outbreak was definitely a live-animal food market in Wuhan, China. (The Guardian | 7 min read) • Members of the world’s largest particle-physics laboratory, CERN, are doing their part to help countries deal with COVID-19 by turning their skills to manufacturing hand sanitizer, 3D-printed masks and face shields. CERN researchers are also designing an open-source ventilator, leveraging their extensive experience in managing gas flows and controls systems. (Nature | continuously updated) • The Saudi-led coalition fighting in Yemen (supported by the United States and United Kingdom, among other countries) has announced a unilateral two-week ceasefire in the devastating five-year conflict. On 23 March, United Nations chief António Guterres called for a global ceasefire to focus on efforts to stop COVID-19. No cases of the virus have been reported in Yemen, but the battered nation is in no state to fight — or even report — an outbreak. (BBC | 5 min read) • COVID-19 infections range from asymptomatic to deadly — even among individuals in the same risk group. Infectious-disease researchers Arturo Casadevall and Liise-anne Pirofski explain the five variables behind very different outcomes: the number of viral particles that cause infection, (maybe) genetics, the route by which a virus enters the body, the virulence of the virus and which infections the immune system has fought in the past. (Bloomberg Opinion | 8 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Further progress in artificial intelligence will require having machines teach themselves, computer scientists say. Machine learning has made impressive breakthroughs in the last decade, in large part through ‘supervised’ training: algorithms go through vast reams of data that have been previously labelled by humans (for example, about pictures being of cats rather than dogs). But ‘self-supervised’ programs (such as AlphaGo Zero) and ‘unsupervised’ techniques can sidestep human labelling, and can perform better at tasks such as discovering new laws of nature or coordinating motion. After all, babies learn how to walk by stumbling around, not by being told which moves are correct. “Humans don’t need that much supervision,” says machine-learning pioneer Yoshua Bengio. New York Times | 6 min read The study of friction and lubricants, called tribology, is an often-neglected field at the confluence of physics, materials science, engineering, nanotechnology and other fields. (How friction works at the microscopic level is not completely understood.) Tribology’s economic impact is immense — for example, a large part of a car’s energy consumption comes from friction between its mechanical components. And a hot question is how to reduce friction in wind turbines to enable them to withstand stronger winds. Popular Mechanics | 7 min read Researchers have engineered an enzyme that dissolves one of the world’s most commonly used plastics. The enzyme breaks down polyethylene terephthalate (PET) into its constituent monomers so it can be reused in nearly-new form, researchers tell the Nature Podcast. Nature Podcast | 16 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Physics thrives on abstract thought and, often, an otherworldly detachment from reality — but up close, the all-too-human business of doing science is messy, writes Sabine Hossenfelder. She reviews an essay collection from physicist and historian of science David Kaiser that reminds us that physics, like any human activity, is influenced by the fears and fashions of history. Nature | 5 min read Andrew Robinson’s pick of the top five science books to read this week includes life on land, how we see faces and reforming capitalism. Nature | 3 min read Carole Mundell is the chief scientific adviser at the UK Foreign and Commonwealth Office and an astrophysicist at the University of Bath.Credit: Leonora Saunders for Nature Carole Mundell is the chief scientific adviser at the UK Foreign and Commonwealth Office (FCO) and an astrophysicist. “The building’s grandeur contrasts with the humility of the public servants that work here,” says Mundell. “This parallels the humility needed to be a good scientist.” (Nature | 2 min read) Zoologist Lucy Taylor, who continues to study the GPS data from her tracked wild elephants while under lockdown, has some tips for the rest of us. (Nature | 6 min read) Spring has sprung here in London, which means I’ll be spending the next four days socially distancing in a paddling pool in my back garden. The Briefing will be back on Tuesday. That gives you plenty of time to find our intrepid Rockhopper penguin, who is hiding this week in a stand of stunning dragon blood trees (Dracaena cinnabari) in Yemen. Can you find the penguin? (Briefing photo czar Tom Houghton will reveal the solution here on Tuesday.) Happy Passover, Easter, general Springiness or maybe Autumniness depending on where you are and what you’re into. Your e-mails are always welcome at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '190'>09 April 2020</date>
<url id = '191'>https://nature.com/articles/d41586-020-01070-9</url>
<title id = '191'>New bridges between apparently distant continents in the mathematical landscape. Plus: the Event Horizon Telescope reveals a spectacular image of a quasar and how mass testing can keep a lid on COVID-19.</title>
<body id = '191'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Astronomers were surprised to find a perpendicular structure in the region of the quasar’s accretion disk where the jets are ejected.J.Y. Kim (MPIfR), Boston University Blazar Program (VLBA and GMVA), and Event Horizon Telescope Collaboration The team that a year ago gave the world the first image of a black hole has now presented images of another cosmic behemoth — this time a quasar, lying 5.5 billion light years away. The Event Horizon Telescope collaboration, which uses a network of radio telescopes scattered around the Earth, used quasar 3C 279 as a calibrator when they imaged black hole M87*. The observations provided an unprecedented look at 3C 279 itself, including rapid changes in its powerful jets, which stream matter out into intergalactic space at close to the speed of light. BBC News | 4 min read Reference: Astronomy & Astrophysics paper The world’s most contagious virus has killed more than 6,500 children in the Democratic Republic of the Congo. Cases began to spike in October 2018, and have since mushroomed into what WHO experts say might be the largest documented measles outbreak in one country since the measles vaccine was developed in 1963. The situation will only worsen with the COVID-19 pandemic: more than 20 countries have already suspended measles-vaccination campaigns as health-care workers scramble to deal with coronavirus. Nature | 8 min read Credit: Jae C. Hong/AP Photo • From animals wandering deserted city streets to world leaders on a video call, Nature’s pick of the must-see science images this month focuses on the COVID-19 outbreak. (Nature | Leisurely scroll) • Austria and Denmark are the first nations in Europe to announce the dates they will cautiously ease their lockdowns. In Austria, small shops are slated to reopen on 15 April, and in Denmark, primary schools should reopen the day after. Leaders will be keeping an eye on their own statistics and on Singapore and Japan, where re-openings have been met with waves of new infections. (The Washington Post | 6 min read) • With science around the world grinding to a halt, the influential Intergovernmental Panel on Climate Change (IPCC) is struggling to keep the world’s most influential climate report on track. Climatologist Valérie Masson-Delmotte, co-chair of the working group that assesses the physical science of climate change, describes how the scientists involved are dealing with the crisis. (Nature | 5 min read) • South Korea and Germany have shown that the rest of the world needs to urgently ramp up testing to keep a lid on COVID-19, argues a Nature Biomedical Engineering editorial. The journal outlines how mass at-home serological testing for antibodies in those who have had the disease, and frequent point-of-care testing for those who might be currently infected, can restore livelihoods and economies until a vaccine is available. (Nature Biomedical Engineering | 7 min read) • Read more: Continuously updated list of the tests in commercial development and in-depth analysis from Nature Biotechnology (12 min read). Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Yes, Isaac Newton produced epic works of genius while he was isolating from the plague, writes science writer Thomas Levenson — but it’s a tale of perseverance, not reinvention. (The New Yorker | 6 min read) Two breakthroughs have established new bridges between apparently distant continents in the mathematical landscape. They are the most significant recent additions to the Langlands programme — a ‘grand unifying theory of maths’ that won its originator, Robert Langlands, the Abel Prize in 2018. The programme envisioned a correspondence between two classes of mathematical objects — one in the theory of tilings and the other in arithmetic. A preprint by ten authors — the result of a massive brainstorming session — has now extended the correspondence from rational to complex numbers. And another preprint created a new bridge using a class of geometric curves that previously seemed beyond reach of the Langlands programme. “People wanted to do this for a long time,” says mathematician Ana Caraiani, one of the authors of the first breakthrough paper. But, “we pretty much didn’t think it was possible.” Quanta | 12 min read Read more: Robert Langlands’s ‘grand unified theory of maths’ wins the Abel Prize (from 2018) Reference: arXiv preprint 1 and arXiv preprint 2 Maybe the present moment can be thought of as somehow thick, rather than a zero-width boundary between past and future, says theoretical physicist Jonathan Oppenheim. (Quanta | 13 min read) This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '191'>08 April 2020</date>
<url id = '192'>https://nature.com/articles/d41586-020-01053-w</url>
<title id = '192'>Examine the shaky evidence for the efficacy of a decades-old antimalarial against coronavirus. Plus: Canada begins the world’s largest clinical trial of survivors’ blood to treat COVID-19, and celebrating the life of astrophysicist Margaret Burbidge, who traced the origin of everything to the hearts of stars.</title>
<body id = '192'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Simon Dawson/Reuters • UK Prime Minister Boris Johnson is in intensive care, with worsening COVID-19 symptoms. The British leader is not on a ventilator, but was moved as a precaution in case he needs one. The foreign secretary, Dominic Raab, is standing in as head of the government. (BBC | 8 min read) • Canada is beginning the world’s largest clinical trial of a COVID-19 treatment that uses the antibody-laden blood plasma of people who have recovered from the disease. The study will involve 1,000 patients in at least 40 hospitals across the country. The convalescent-plasma approach dates back to the 1890s, but even the trial’s leader, haematologist Donald Arnold, calls it a “Hail Mary” plan with only a small chance of success. (The Globe and Mail | 5 min read) • The coronavirus dashboard put together by researchers at Johns Hopkins University in Baltimore has become a cornerstone of our knowledge about the outbreak. Ensheng Dong, a first-year graduate student in civil and systems engineering, and his thesis advisor Lauren Gardner share how they built the website in just a few hours that now receives more than a billion hits per day. (Nature Index | 6 min read) • Early in the spread of COVID-19, before it had an official name, many news outlets — including this newsletter — labelled the virus after Wuhan or China. I apologize, and so does a Nature editorial. The outdated practice of associating a virus and the disease it causes with a specific place is irresponsible and needs to stop, argues the editorial. (Nature | 4 min read) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. Debate rages over whether a decades-old antimalarial, hydroxychloroquine, offers genuine promise as a potential COVID-19 treatment. The World Health Organization has included the drug in its global trial, SOLIDARITY, but its efficacy is as yet unproven. • Pharmaceutical researcher Derek Lowe explores evidence from small trials and preprints that hydroxychloroquine is not effective against COVID-19. (Science Translational Medicine blog | 6 min read) • Microbiologist Didier Raoult’s controversial research is the wellspring of some of the enthusiasm for the drug. The study has now received a statement of concern from the society that publishes the journal in which it appeared. (Retraction Watch | 3 min read) • Attorney Olga Lucia Torres, who takes hydroxychloroquine to treat her lupus, describes her worries for people who rely on the drug. There have been some shortages as people snap up supplies in the wake of unsubstantiated recommendations from US President Donald Trump, prominent US media figures and Brazilian President Jair Bolsonaro. (The New York Times | 5 min read) The coronavirus outbreak might be peaking in some areas of the United States, says physician-scientist Brett Giroir, a member of the White House coronavirus task force. (Reuters | 5 min read) Pioneering astrophysicist Margaret Burbidge, who helped to explain the origin of all chemical elements, from beryllium to zinc, died on Sunday at the age of 100. Burbidge was the lead author, along with William Fowler, Fred Hoyle and her late husband Geoffrey Burbidge, on a milestone 1957 paper that explained how stars synthesize many elements heavier than lithium through nuclear fusion. The paper helped Fowler to earn a Nobel Prize, an honour that some say Burbidge deserved, too. Burbidge was a trailblazer for women in the field — early in her career, she was barred from using some of the best telescopes of the time because of her gender. New York Times | 7 min read A survey of PhD students and their supervisors in Australia has revealed an alarming gap in each other’s expectations. Students thought that publishing at least four papers and winning grants was the most important outcome of their candidature, but supervisors said critical thinking, written communication and discipline knowledge were the best indicators of success. The results show the importance of discussing expectations and goals early on, says the survey’s lead researcher Adam Cardilini. Nature Index | 5 min read Novelist Arundhati Roy thinks deeply about what COVID-19 is doing to India, and to the world. (The Financial Times | 13 min read) Buried in a very readable profile of comedy misanthrope Larry David, there is advice from lustrous fashion designer Tom Ford about how to look good on a video call. Which I reproduce here in full (you’re welcome): “Put the computer up on a stack of books so the camera is slightly higher than your head. Say, about the top of your head. And then point it down into your eyes. Then take a tall lamp and set it next to the computer on the side of your face you feel is best. The lamp should be in line with and slightly behind the computer so the light falls nicely on your face. Then put a piece of white paper or a white tablecloth on the table you are sitting at but make sure it can’t be seen in the frame. It will give you a bit of fill and bounce. And lots of powder, et voilà!” This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Nicky Phillips and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '192'>07 April 2020</date>
<url id = '193'>https://nature.com/articles/d41586-020-01044-x</url>
<title id = '193'>Trust, testing, treatment and a quirk of fate have kept Germany’s death rate an order of magnitude lower than those in nearby nations. Plus: step through the coronavirus’s full genome and rediscover the polio epidemic that invented intensive care.</title>
<body id = '193'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Researcher Mark Meekan diving with a whale sharkWayne Osborn Growth rings in whale-shark vertebrae have shown specimens to be up to 50 years old at the time of death — implying that some whale sharks could live for more than 100 years, and perhaps up to 150 years. Researchers have found spikes in carbon-14 concentrations corresponding to years when above-ground thermonuclear tests peaked, at the height of the cold war. “It suggests that these things are probably intensely vulnerable to over-harvesting,” says fish biologist Mark Meekan. BBC News | 3 min read Reference: Frontiers in Marine Science paper Sean Gallup/Getty • Germany has the fourth-highest number of confirmed coronavirus infections — more than 100,000. But its death rate is an order of magnitude lower than those in nearby Italy and Spain. One reason is a quirk of fate: many of the first to be infected were young, healthy people who caught the virus in Austrian and Italian ski resorts. Germany has also tested many people with few or no symptoms, lowering the death rate on paper. And the country has a robust free public-health system with lots of intensive-care beds, a trusted government whose social-distancing guidelines are widely observed and an aggressive approach to early testing and treatment. (The New York Times | 10 min read) • The genome of SARS-CoV-2 is less than 30,000 letters long — short enough that The New York Times can spell it out in this infographic-packed feature. Take a journey through the virus’s full genome, the proteins it encodes, what they do and a few mysteries. (The New York Times | 7 min read) • Staying at home is not an option for scientists working on potential vaccines or caring for research animals. Four scientists offer advice on the precautionary measures necessary to continue essential research in the face of the pandemic. (Nature | 9 min read) • The Copenhagen polio epidemic of August 1952 was the outbreak that invented intensive care — the approach so important in fighting COVID-19. It was one of the worst polio epidemics that the world had ever seen, and half the victims were children. A heroic community effort brought to life the idea of anaesthesiologist Bjørn Ibsen: that a new type of ventilator could breathe for a person for hours or days. (Nature | 7 min read) • A tiger at a zoo in New York City is the first known wild animal to test positive for the virus that causes COVID-19. Veterinarians guess that the big cat probably caught the coronavirus from a zookeeper. The finding adds to concerns that vulnerable wild animals, including great apes, could be at risk from the illness. (National Geographic | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Evolutionary computational biologist Pleuni Pennings chimes in on an opinion piece by computational biologist C. Brandon Ogbunu, which offers tips for spotting smart people who don’t necessarily know what they’re talking about when it comes to COVID-19. (Wired | 9 min read) What could US$35 billion — the amount NASA plans to spend to land on the Moon again — buy you in space? Ars Technica’s list of ten alternatives includes boosting the tracking of near-Earth objects and learning how to deflect the scariest ones; replacing NASA’s ageing fleet of Earth-observation probes to understand the impact of greenhouse-gas emissions; and launching a new generation of telescopes to study the cosmos. Quirkier ideas also made the list, such as developing nuclear space propulsion or sending people to Mars — although it’s unclear whether that would cost less than NASA’s Moon programme. Ars Technica | 15 min read Mentors must change their approach during an outbreak that has left many of us feeling frightened, worried and overwhelmed. Ruth Gotian, the assistant dean for mentoring at Weill Cornell Medicine in New York City, offers her tips for mentors: check in and chat (accepting that children, pets and pajamas might be there too), suggest a different type of ‘to do’ list and don’t forget to just listen. Nature | 5 min read  Marine ecologist Carlos Manuel Duarte led a review that found ocean habitats, populations and ecosystems could be restored within 30 years — if we choose to do it. (The Guardian | 5 min read) Reference: Nature review article On Friday, Briefing photo editor Tom Houghton hid our cunning Rockhopper penguin among the zebras in the great Serengeti migration. Can you find the penguin? When you’re ready — here’s the answer! Keep smiling with one of the most-loved Quirks of Nature cartoons ever: enter here for a chance to win a framed print featuring the poetry stylings of a ‘haemato-poetic’ stem cell. Good luck! Your feedback is always welcome at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '193'>06 April 2020</date>
<url id = '194'>https://nature.com/articles/d41586-020-01016-1</url>
<title id = '194'>Wastewater could help estimate the total number of infections — and be an early-warning sign if the virus returns. Plus: preliminary evidence of airborne transmission of COVID-19 and that masks might help, and a mega maths proof will finally be published.</title>
<body id = '194'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Mice move their ears, cheeks and eyes to convey emotion.Credit: Getty Researchers have used a machine-learning algorithm to decipher the seemingly inscrutable facial expressions of laboratory mice. For example, a mouse experiencing pleasure pulls its nose down towards its mouth, and pulls its ears and jaw forwards. By contrast, when in pain, it pulls back its ears and bulks out its cheeks, and sometimes squints. The work “is an important first step” in understanding some of the mysterious aspects of emotions and how they manifest in the brain, says neuroscientist David Anderson. Nature | 4 min read Reference: Science paper After a long and contentious struggle, mathematician Shinichi Mochizuki has had his 600-page proof of the abc conjecture — one of the biggest open problems in number theory — accepted by a journal. If confirmed to be true, the proof confirms a profound link between the addition and multiplication of integer numbers. Critics have complained that the proof is impenetrable, and it was largely deemed flawed after two authoritative mathematicians circulated a rebuttal in 2018. Some also lament the poor optics of publishing the paper in a journal of which Mochizuki is chief editor — although that is not unheard of in maths. Nature | 6 min read For the first time, superconductors have been found in meteorites. The tiny particles of alloys containing indium, lead and tin would have formed in extreme environments, such as during planetary collisions. Researchers analysed meteorites using magnetic-field-modulated microwave spectroscopy to find where tiny amounts of superconducting materials were hiding. They then used a ‘divide-and-conquer’ technique, progressively breaking the meteorites into smaller grains to pinpoint the superconducting particles. In the coldness of space, these particles might affect planetary formation. Chemistry World | 4 min read Reference: PNAS paper COVID-19 testing in South Africa: the country is participating in the SOLIDARITY trial. The World Health Organization says it will support other African nations to join.Credit: Alet Pretorius/Gallo Images/Getty • Sewage could reveal the true scale of the coronavirus outbreak. More than a dozen research groups worldwide have started analysing wastewater for the new coronavirus to estimate the total number of infections in a community. Wastewater testing could be an early-warning sign if the virus returns after it subsides. (Nature | 4 min read) • The World Health Organization says it wants many more African nations to participate in its SOLIDARITY trial, a global study of four potential COVID-19 treatments. Of the more than 300 clinical trials that have launched to find a treatment for the disease, most are in China and South Korea. And more are on the way in the European Union and the United States. But very few are taking place in Africa, Latin America and south and southeast Asia — where the virus could surge next. (Nature | 5 min read) • Nature is summarizing the must-read papers and preprints on COVID-19 from across all publications. The latest include how antibodies from llamas could be useful in neutralizing coronaviruses, and evidence that people seriously ill with COVID-19 experienced striking improvement after receiving infusions of blood from disease survivors. (Nature | Continuously updated) Read the latest coronavirus news, continuously updated on Nature. Read Nature’s continuously updated selection of the must-read papers and preprints on COVID-19. There has been a clear divide among nations on the public use of face masks — from commonplace in much of Asia, to strongly dissuaded (until recently) in the United States, to obligatory in the Czech Republic. But their use is gaining momentum — and evidence is growing that the benefits of wearing basic face masks in public outweigh the downsides. • Since early reports revealed that a new coronavirus was spreading rapidly between people, researchers have been trying to pin down whether it can travel through the air. Health officials say the virus is transported only through droplets from coughs or sneezes — either directly, or on objects. But some scientists say there is preliminary evidence of airborne transmission — in which the disease spreads in the much smaller particles from exhaled air. (Nature | 7 min read) • The United States — which today has a quarter of all the confirmed cases in the world — is set to announce a new policy recommending that people use basic face coverings when in public. (BBC | 5 min read) • A Nature Medicine paper published today found that surgical face masks could prevent transmission of human coronaviruses and influenza from people with symptoms. The 5-year study (which used this amazing-looking sneeze-detecting machine) is a significant addition to a debate that has struggled to make sense of conflicting evidence. But some worry that masks will create a false sense of security, or deplete supplies for front-line health workers. (The Atlantic | 13 min read) Governments across the world are relying on mathematical projections to help to guide decisions in this pandemic. But much information about how SARS-CoV-2 spreads is still unknown and must be estimated or assumed — limiting the precision of forecasts. Epidemiologist Neil Ferguson, a member of the Imperial College London team that has most influenced the British response to the outbreak, explains exactly how the process works. Nature | 12 min read Source: Ref. 1 Total confirmed cases of COVID-19 worldwide as of 2 April. Because many patients are not tested, the true number of cases is probably much higher. (Nature | Continuously updated) Physician and public-health researcher Ashwin Vasan responds to location data showing that many people with lower incomes don’t have the luxury of staying at home. (The New York Times | 9 min read) “As young scientists, we want to be involved in shaping the landscape that determines our day-to-day work, since we are the ones who will benefit most (namely, for the rest of our careers)”, write three early-career researchers. As members of a student-run think tank, Young SiT, they call on universities to implement fairer evaluation methods that promote professional diversity and growth, not just publication metrics. Nature Index | 5 min read This week, in the coronavirus-free Nature Podcast, we look at evidence from deep below the sea-floor that there was a verdant forest on Antarctica around 90 million years ago. In Nature’s new coronavirus podcast, Coronapod, we explore how low- and middle-income countries are preparing for the pandemic. Nature Podcast | 17 min listen Coronapod | 26 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. A 3D magnetic resonance imaging scan of the brain.Credit: Tom Barrick, Chris Clark, SGHMS/SPL Scientists facing the daunting task of describing the brain conventionally conjure up different kinds of metaphor. The problem comes when we forget that they are descriptors, and see them instead as natural properties, writes historian Stephen Casper. Casper reviews a new book by biologist and historian Matthew Cobb, which provokes contemplation about how some metaphors burden us with invisible assumptions made in the past. Nature | 5 min read Andrew Robinson’s pick of the top five science books to read this week includes the neuroscience of navigation, a world of ice, and a black inventor reconstructed. Nature | 3 min read Maria Josefa Verdugo is a PhD student in marine biogeochemistry at the Alfred Wegener Institute for Polar and Marine Research in Bremerhaven, Germany.Credit: Esther Horvath for Nature Marine biogeochemist Maria Josefa Verdugo spent last winter living and working on a ship locked in the ice as part of the first year-round expedition to explore climate in the far north. “I’m from Chile, and I didn’t grow up around a lot of cold, snow and ice. But I’ve learnt to embrace it,” says Verdugo. “Everything’s cold when you’re handling an Arctic Ocean ice core.” (Nature | 3 min read) This week, our cunning Rockhopper is hiding among the zebras in the great Serengeti migration. Can you find that penguin? The answer will be in Monday’s Briefing , all thanks to photo editor Tom Houghton. Your ideas for this penguin’s name were so good, we’re going to have to have a vote. We’ll do it next week, so speak now or forever hold your peace (at least on the subject of a name for this particular penguin). Suggestions in an e-mail, please — plus any other feedback on this newsletter — to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '194'>03 April 2020</date>
<url id = '195'>https://nature.com/articles/d41586-020-01004-5</url>
<title id = '195'>Deep-sea fish migrate too. Plus: tooth protein reveals a close relative of the last common ancestor of humans, Neanderthals and Denisovans, and how digital tools for contact tracing could help create an exit strategy for the coronavirus outbreak.</title>
<body id = '195'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A tripod fish (Bathypterois grallator), one of the deep-sea species identified by the study conducted in the waters off the coast of Angola, West Africa.Image courtesy of the NOAA Office of Ocean Exploration and Research, Windows to the Deep 2019 Sea-floor observatories located 1,400 metres deep, off the coast of Angola, have found the strongest evidence yet that deep-sea fish follow seasonal migrations. Cameras captured 502 shots of fish over more than seven years, and found that traffic peaked in late November and in June. Among the fish were grenadiers (Coryphaenoides spp.), which can swim long distances, although it’s unknown how far. The authors speculate that the migration patterns might follow seasonal variations in food availability in shallower layers of the ocean. The Guardian | 4 min read Reference: Journal of Animal Ecology paper The enigmatic hominin Homo antecessor has found its place in our family tree thanks to the 800,000-year-old proteins in fossil teeth. Researchers used mass spectrometry to analyse a sliver of enamel from a molar found in Spain’s Gran Dolina cave. The tooth reveals that H. antecessor was a close relative of the last common ancestor of humans, Neanderthals and Denisovans. “We see that antecessor falls as a sister group — close, very close — to the branch that leads to us,” says geneticist Enrico Cappellini. Science | 5 min read Reference: Nature paper Medical workers perform drive-through swabbing tests in Alessandria, Italy.Credit: Miguel Medina/AFP/Getty • The most important climate event since the signing of the Paris Agreement in 2015 has been postponed for a year. COP26 had been planned to take place this November in Glasgow. Countries were expected to update and strengthen their commitments to reducing carbon emissions. (Nature | Continuously updated) • Leaders in Europe and North America often point to Italy as the harbinger of things to come in this pandemic. What does that future hold for researchers? Four lab leaders who have been in lockdown in northern Italy since early March describe how they are getting through it. (Nature | 7 min read) • In Lagos, Nigeria, roughly two-thirds of its 21 million people live in informal shelters without electricity or running water — and in poorer countries that are now seeing the arrival of the pandemic, public-health officials must face these conditions. To prepare, authorities are taking steps to keep COVID-19 from getting a foothold: shutting down most activities with the threat of arrest, and using rapid, easy-to-use tests — even if they are not as accurate. (Nature | 11 min read) • ‘Coronavirus’ might be a new word for some of us, but microbiologist Susan Weiss has been studying such viruses since the late 1970s. She describes the field’s humble beginnings, reviews some of the major findings over the last 40 years and considers their relevance in the current outbreak. (Journal of Experimental Medicine | 10 min read) • Contact tracing — finding everyone a person with a newly diagnosed infection has been in touch with, and asking them to quarantine before they can spread the virus — is a cornerstone of epidemic response. But it is harder than ever to do in our populous, fast-moving and super-connected world. NextTrace is one nascent attempt to solve the conundrum. The app hopes to offer anonymous self-reporting that relies on location data from mobile phones to notify other users who might have been exposed. Discover how digital tools could help create an exit strategy for the outbreak. (STAT | 6 min read) • Leading South African HIV researcher Gita Ramjee has died from COVID-19. Colleagues have paid tribute to “a champion in the fight against the HIV epidemic”, particularly with women in disadvantaged communities. (BBC | 4 min read) Read the latest coronavirus news, continuously updated on Nature. Volunteer Jennifer Haller says she has been overwhelmed with love and support after she became the first person outside China to receive an experimental vaccine against COVID-19. (Science | 12 min read) Sesame, the Middle East’s first synchrotron light source, was founded to spur cooperation and halt the brain drain in a troubled region. Plans for Sesame took off in 2000, when a jogger crashed a meeting of scientists in Amman. He turned out to be a cousin of the king of Jordan, and was instrumental in having the country serve as the seat for the project. Sesame has now been running for two years, despite huge hurdles — including that member states often cannot pay their dues — and is offering lessons for scientists pushing for a first synchrotron in the African continent. Chemistry World | 11 min read It’s silly season aton the arXiv. If you’ve got room in your heart for just one more joke paper from the arXiv, make it this long-awaited solution to achieving room-temperature superconductivity. Your feedback — whether positive or critical — is always welcome at briefing@nature.com, and I read every e-mail. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '195'>02 April 2020</date>
<url id = '196'>https://nature.com/articles/d41586-020-00989-3</url>
<title id = '196'>A small study shows that cats can be infected with the coronavirus — but no cats had symptoms and there is no direct evidence that cats can pass it to people. Plus, how to get your writing projects done and how the outbreak is affecting the world's biggest physics experiments.</title>
<body id = '196'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Nora Volkow at her childhood home in Mexico City, now a museum to her great-grandfather, the Soviet revolutionary Leon Trotsky.Credit: Alejandra Rajal for Nature Psychiatrist Nora Volkow is leading the world’s biggest funder of addiction research — the US National Institute on Drug Abuse — while the country is grappling with a devastating surge in drug use. Volkow’s obsession with the biological effects of excessive drug use — fuelled in part by her own family’s past — has not only shattered dogmas in neuroscience, but has helped to mitigate the stigma people with addiction face. Nature | 14 min read US President Donald Trump’s administration has weakened automobile-emissions standards for cars and trucks. The policy will spare automakers from having to meet ambitious targets to reduce pollution harmful to the environment and public health. The rule might fall in the courts: the Trump administration is already locked in legal battles with California and other states that use California’s stricter emissions rules. The Los Angeles Times | 6 min read A woman looks out of her balcony in locked-down Rome.Credit: Antonio Masiello/Getty • For 3 nights last week, 6,000 people holed up in Italy went on their balconies to take part in an unprecedented citizen-science experiment to measure light pollution with smartphones. The effort was a rare opportunity to gather data on the ground — and to connect harried Italians with science unrelated to the outbreak. (Nature | Continuously updated) • If governments play their cards right, 2019 could be a turning point for climate change: the year that global emissions peaked. A key factor will be whether governments are able to advance climate goals as they roll out economic-stimulus plans in response to the outbreak. Former US president Barack Obama managed to do it after the 2008 financial crisis, but evidence indicates the current administration is missing opportunities to bolster green growth. (Nature | 4 min read) • From the United States to China, the COVID–19 pandemic is forcing some of the world’s largest projects to shut down or to delay construction. CERN’s Large Hadron Collider and the LIGO and Virgo gravitational-wave detectors are among them. For now, some experiments, including neutrino and dark-matter detectors, have been operating with a skeleton staff. (Nature | 6 min read) • There is new evidence that cats can be infected with the SARS-CoV-2 coronavirus and spread it to other felines, but dogs are not susceptible. Cat owners should not be alarmed just yet: there is no direct evidence that cats secreted enough virus to pass it to people. Researchers infected animals with high doses of the virus, which provided evidence for their susceptibility — but the study didn’t reproduce people’s real-world interactions with their pets. None of the infected cats showed symptoms of illness. (Nature | 4 min read) • The coronavirus outbreak has made us all into science communicators — particularly those who are a “nerd node of trust”: the person friends and family turn to for expertise. Science communicator Liz Neely explains how science-minded people can share information effectively with humility and understanding. (The Atlantic | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Science writer Meehan Crist ponders what the coronavirus means for climate change. (The New York Times | 11 min read) In the oceans’ twilight zone, 200–1,000 metres deep, currents and ‘marine snow’ mix water from the abyss and the sunlit region above, and many organisms migrate up and down daily between layers. These poorly understood processes provide nutrient exchange and carbon sequestration. But the twilight zone is about to suffer a triple blow from carbon emissions, excessive fishing and increased sea-floor mining. Sixteen oceanographers call for scientists to seize the upcoming United Nations Decade of the Ocean, which starts next year, as an opportunity to focus on this crucial, neglected ecosystem. Nature | 8 min read When their schedules are crammed with laboratory work, teaching or administration, scientists often delay writing. Discover how productivity coaches, boot camps and online meet-ups teach researchers to avoid distractions and negative thoughts to get their writing projects done. Nature | 8 min read Last October, a treatment that promised to transform the lives of 90% of people with cystic fibrosis was approved in the United States. The three-part drug, Trikafta, has been spectacularly effective for some people, but it’s expensive, only available in the United States and is not suitable for everyone. With the promise of more precision-medicine successes on the horizon, Trikafta offers a case study of what happens after a long-awaited drug becomes a reality. Elemental | 18 min read Popular Netflix documentary Tiger King is being embraced as an escapist guilty pleasure — but it shows the sinister reality of the US wildlife trade, says conservation biologist Imogene Cancellare. (NBC News | 7 min read) This newsletter is always evolving — tell us what you think! Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '196'>01 April 2020</date>
<url id = '197'>https://nature.com/articles/d41586-020-00982-w</url>
<title id = '197'>Marine biologist Alyssa Frederick successfully defended her thesis on Zoom — here's how she did it. Plus, celebrating the life of Nobel laureate Philip Anderson, and how scientists are watching for a resurgence of coronavirus infections in places where lockdowns have already worked.</title>
<body id = '197'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Michal Cizek/AFP/Getty • Cases in Hubei province in China — ground zero for the coronavirus outbreak — have dropped to practically zero. and the region has started easing its extreme lockdown. Health authorities are watching for a second wave of infections, while deploying widespread testing and monitoring to keep it suppressed. (Nature | 7 min read) • Researchers are making difficult — sometimes life-or-death — decisions about what to do about their research animals as laboratories are closed down owing to the coronavirus outbreak. Some are even taking them home — entomologist Maria Cramer and her partner are sharing their two-bedroom basement apartment with her two most important and most genetically diverse colonies of ladybirds. (Nature | 5 min read) • Governments need to think twice before they suppress messages related to COVID-19, argues anthropologist Heidi Larson, who studies vaccine rumors. The nature of misinformation is not always clear-cut, and authorities risk undermining public trust, says Larson. (Nature | 5 min read) • Didier Raoult is a prominent microbiologist who co-discovered gigantic ‘mimiviruses’, which are so large that they are visible under a light microscope. He’s also the source of controversial research touting the effects of chloroquine on the SARS-CoV-2 virus, which gained prominence when it was promoted by US President Donald Trump. Pharmaceutical researcher Derek Lowe raises questions about the bold nature of Raoult’s claims and the unconventional human trial of the drug. (Science Translational Medicine blog | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Source: Royal Observatory of Belgium Coronavirus lockdowns have changed the way Earth moves. Researchers who study Earth’s movement are reporting a drop in seismic noise — the hum of vibrations in the planet’s crust — that could be the result of transport networks and other human activities being shut down. The data shown above are from the Royal Observatory of Belgium in Brussels, and the effect might be a lot less apparent at stations that are purposefully located in remote areas or deep boreholes to avoid human noise. The estimated number of deaths that have been averted in 11 countries in Western and Northern Europe because of infection-control measures such as national lockdowns. (Nature | Continuously updated) As chief scientific adviser to the UK government, biologist Ian Boyd, participated in a practice run for pandemic that left him “shattered”. He shares what he learned from such dry runs. (Nature | 5 min read) A growing suite of tools allows teams of researchers to work collectively to edit scientific documents. Discover systems including Manubot, Overleaf, Authorea, Fidus Writer and Manuscripts.io that will help you to “walk the walk of open science”, in the words of computational biologist Olga Botvinnik. You can also explore an example Manubot project to get the ball rolling. Nature | 7 min read Back in November 2019, before the coronavirus outbreak began, marine biologist Alyssa Frederick successfully defended her thesis remotely on Zoom. Her advice is to have patience with others, manage expectations and practise putting all the pieces together ahead of time — and don’t forget to celebrate afterwards. Nature | 6 min read Philip Anderson, one of the most influential theoretical physicists of the twentieth century, died on 29 March, aged 96. The Nobel laureate specialized in condensed matter, and his concept of ‘Anderson localization’ of electrons explains everything from how fog reflects light to why non-crystalline metals are electrical insulators. Anderson was also a forceful opponent of reductionism, and he spelled out his vision in his influential 1972 Science essay ‘More Is Different’. “Anderson was the pre-eminent condensed-matter theorist of his day — a day that lasted for over 50 years — and his fingerprints are everywhere,” says physicist Nigel Goldenfeld. New York Times | 5 min read Read Philip Anderson’s review of Robert Laughlin’s book A Different Universe: “My message is this: buy the book.” (Nature, from 2005) The number of hadrons — particles made of two or more quarks — discovered by the Large Hadron Collider (LHC) at CERN, with the latest one, called Ξc(2923)0, added on 30 March. (Elementary particles are much rarer finds: the last one was the Higgs boson, which the LHC discovered in 2012.) Higher-education researcher Sharon Stein sums up a deep dive into how expropriated Indigenous land forms the foundation of the US land-grant university system. (High Country News | 23 min read) The Education Office at Fermilab is asking us to guess what particle physics is being illustrated using just some balls and sand (and some very fetching floral curtains). Let me know the science fun that’s keeping you entertained during these tough times — plus any other feedback on this newsletter — at briefing@nature.com. And if nothing’s making you smile, or you are facing difficulties, you have my very best wishes. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '197'>31 March 2020</date>
<url id = '198'>https://nature.com/articles/d41586-020-00966-w</url>
<title id = '198'>The COVID-19 pandemic could have infected an estimated 90% of the world’s population and killed 40.6 million people if no mitigation measures were put in place to combat it. Plus: meet the scientists who are redeploying to fight the virus, and what DNA might say about why some people don’t get very ill from COVID-19.</title>
<body id = '198'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Source: NASA Ozone Watch A rare, and vast, ozone hole — probably the biggest on record in the north — has opened in the skies above the Arctic. Cold temperatures and a strong polar vortex have allowed high-altitude clouds to form. These clouds include ozone-destroying chemicals that linger in our atmosphere, despite having been mostly banned in 1987. The hole will probably be short-lived, and it doesn’t threaten people’s health because the Sun is just starting to rise above the horizon in high latitudes. Nature | 4 min read Reference: Nature paper The University of Rochester in New York has agreed to pay a US$9.4-million settlement to researchers who sued the institution over how it handled allegations of sexual harassment against a cognitive-science professor. The settlement brings to a close one of the most prominent harassment cases at a US university. “We want other universities to know that when people come forward with sexual harassment complaints, it’s important to take them seriously and to find resolution for them that doesn’t require people to seek justice through the court system,” says cognitive-neuroscientist Jessica Cantlon, who was one of nine plaintiffs. Nature | 2 min read Michelle Cipicchio (right) trains two lab technicians at the Broad Institute to extract viral RNA from patient samples.Credit: Scott Sassone/Broad Institute • The COVID-19 pandemic could have infected 90% of the world’s population and killed 40.6 million people if no mitigation measures were put in place to combat it. Estimates from an influential modelling group at Imperial College London highlight the importance that acting early had on suppressing the outbreak. (Nature | Continuously updated) • As labs shut down around the world, researchers are finding creative ways to donate their time, supplies and expertise. Meet a few of the tens of thousands of scientists who are redeploying to fight coronavirus. (Nature | 4 min read) • Why do some people infected with the SARS-COV-2 virus feel OK, while others — even those who are healthy and relatively young — get very ill? Some researchers are gearing up to search huge national genetic databases, such as the UK Biobank for the DNA variations that might give the answers. Others are recruiting patients with COVID-19 directly within hospitals, who are willing to share their genetic and health data. (Science | 6 min read) • There is a clear divide among nations when it comes to the public use of face masks — from commonplace in much of Asia, to strongly dissuaded in the United States, to obligatory in the Czech Republic. There is only meagre evidence that mass mask-wearing will help to slow the spread of COVID-19, and there are downsides — whether from a false sense of security or a shortage of masks for health workers who sorely need them. Science outlines the arguments on both sides. (Science | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Memoir and science fiction have a role to play in illuminating our current situation. Here are some of Nature’s recommendations for the best books about outbreaks and the researchers who study them. • Peter Piot’s No Time To Lose is a passionate account of his leading roles in the discovery of Ebola and in the global response to HIV and AIDS. It was selected by Chikwe Ihekweazu, director of the Nigeria Centre for Disease Control, as his choice for a classic to catch up on. (Nature | 16 min read) • The medical autobiography of epidemiologist Mary Guinan, a pioneer in the field of HIV/AIDS research and global smallpox eradication, is a rip-roaring read, says reviewer Tilli Tansey. (Nature | 5 min read) • In 2013, Margaret Atwood concluded her sweeping trilogy about a dystopian world devastated by a global pandemic with MaddAddam. “Atwood's book is a warning but also, in its final accounting, a hopeful meditation on the cycle of life, death and the possibility of life anew,” says reviewer Paul McEuen. (Nature | 4 min read) • Science fiction authors including Lauren Beukes and Kim Stanley Robinson share the books they recommend for making sense of the world (circa 2017). “Science fiction is the realism of our time,” argues Robinson. (Nature | 14 min read) Last week we shared Nature’s selection of the best histories and analyses of past pandemics. Find that list here. Dermatologist Min Pok-kee, who leads the public health strategy in Daegu, South Korea, as a volunteer civil servant, outlines the country’s aggressive response to COVID-19 and his worries about the US and UK approach. (Wired) A new treaty will govern uses of organisms from the open ocean, such as this hydromedusa brought up from a depth of about 2,700 metres in the middle of the Atlantic Ocean.Credit: David Shale/NPL Some researchers are fretting that an upcoming international treaty could harm the development of novel drugs. The treaty will regulate for the first time the exploitation of the high seas — those beyond countries’ 200-mile-wide exclusive economic zones. But anti-biopiracy provisions currently under discussion could have unintended consequences, reducing access. Research on marine organisms has led to the discovery of several drugs, including for AIDS and cancer. Nature | 10 min read Science should move towards a more inclusive definition of replication, which is not a housekeeping activity but an “exciting, generative, vital contributor to research progress,” write Brain Nosek and Timothy Errington of the Center for Open Science. Any study that provides evidence about a previous claim should be considered replication, the authors propose. PLOS Biology | 13 min read Astrophysicist Daniel Reardon is good-spirited about his attempt to invent a face-touching warning device that ended up with him in hospital. (The Guardian) On Friday, Briefing photo editor Tom Houghton hid our imperturbable Rockhopper penguin in the coral reefs of Papua New Guinea. Can you find the penguin? When you’re ready — here’s the answer! Keep smiling with one of the most-loved Quirks of Nature cartoons ever: Enter here for a chance to win a framed print featuring the poetry stylings of a ‘haemato-poetic’ stem cell. Good luck! Your feedback is always welcome at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '198'>30 March 2020</date>
<url id = '199'>https://nature.com/articles/d41586-020-00953-1</url>
<title id = '199'>A ‘human challenge’ study would involve exposing perhaps 100 healthy young people to the coronavirus and seeing whether those who get the vaccine escape infection. Plus: UK prime minister Boris Johnson has tested positive for coronavirus, and Neanderthals enjoyed a nice bit of fish.</title>
<body id = '199'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Cracked-open and burnt fragments of pincers of the edible crab (Cancer pagurus)João Zilhão A pile of ancient kitchen rubbish shows Neanderthals had a highly varied diet. Digging in a seaside cave in Portugal, researchers found bones of seals, dolphins and many types of fish, including sharks. The 86,000- to 106,000-year-old remains contribute to showing how Neanderthals’ behaviour — and perhaps their cognitive abilities — were not too different from those of their contemporary modern humans. The cave was so cramped that only a maximum of three people could work inside at any given time. “I was in the fetal position every single day,” says archaeologist Filipa Rodrigues, a coauthor of the study. New York Times | 4 min read Reference: Science paper Love it or hate it, the H-index has become one of the most widely used metrics for measuring the productivity and impact of researchers. Jorge Hirsch, who invented it back in 2005, recently admitted that, although he considers the H-index to be among the best objective measures of scientific achievement, it can “fail spectacularly and have severe unintended negative consequences”. Nature Index | 5 min read TWITTER/@BorisJohnson/HANDOUT/EPA-EFE/Shutterstock • UK prime minister Boris Johnson has tested positive for coronavirus, as has the health secretary, Matt Hancock. Both politicians say their symptoms are mild and they will continue working — remotely — on the country’s response to the rising number of infections and deaths. Prince Charles tested positive for the virus earlier this week. (BBC | 27 min read) • The British government says that, within days, it will begin large-scale serological testing that will show whether a person has been previously infected with the coronavirus that causes COVID-19. The ‘finger-prick’ tests will be available to buy from Amazon and pharmacies, and can be performed at home. If the roll-out goes ahead as planned, the country could become the first to implement at-home testing on this scale. Many questions remain unanswered: how accurate the tests will be, who will make them and how they can be manufactured in sufficient amounts. (Nature | Continuously updated) • A radical proposal to infect healthy people with the coronavirus to test vaccines could dramatically speed up research. A ‘human challenge’ study would involve exposing perhaps 100 healthy young people to the virus and seeing whether those who get the vaccine escape infection. Bioethicist Nir Eyal, who co-authored a provocative preprint proposing such a study, tells Nature how the study could be done safely and ethically. Participants, he argues, might even be better off for it. (Nature | 6 min read) • The United States Environmental Protection Agency (EPA) announced yesterday that it will temporarily suspend its enforcement of environmental laws because of the outbreak. “During this extraordinary time, EPA believes that it is more important for facilities to ensure that their pollution control equipment remains up and running and the facilities are operating safely, than to carry out routine sampling and reporting,” said an EPA spokesperson. (The Hill | 6 min read) • As we approach the three-month mark since the COVID-19 outbreak began, it’s worth taking a breath to catch up on what we know and the many questions that still need to be answered. From tests to immunity, STAT does a quick round-up of the state of play. (STAT | 11 min read) Read the latest coronavirus news, continuously updated on Nature. The number of confirmed cases of COVID-19 in the United States — which now overtakes China as the country with the ighest number of confirmed cases. (Nature | Continuously updated) COVID-19 will take a huge toll on lives, livelihoods and the economy if social distancing is not maintained, says health-security researcher Tom Inglesby. Move the slider on this simplified interactive graph from The New York Times to understand the effect on infections, hospitalizations and deaths. Scientists affected by the shutdowns outline the tools they are using to run their research groups remotely, and share their tips for building a virtual lab remotely. “You can look at it in two ways: there’s either very little you can do from home, or a lot you can do,” says Federica Di Nicolantonio, who studies the epigenetics of colorectal cancer and mesothelioma in Italy. Nature | 10 min read A paper just published in Science claims to have falsified earlier hints of dark matter. But ‘not so fast’, says another group that has analysed the same data set — regions in the Milky Way mapped by a European Space Agency X-ray telescope. It is the latest chapter in a saga that started in 2014, when researchers saw X-ray emissions from other galaxies peaking at an energy of 3.5 kiloelectronvolts. Some attributed this to the decay of a novel elementary particle called a sterile neutrino, a candidate for the mysterious dark matter that appears to hold galaxies together. Science | 6 min read Reference: Science paper Andrew Robinson’s pick of the top five science books to read this week includes a lively history of smell, practical solutions for climate change and big cats on the prowl. Nature | 3 min read This week, the coronavirus-free Nature Podcast investigates an ultra-fast electrical switch and an algorithm that calculates the amount of blood pumped by the heart beat by beat. Nature’s new coronavirus podcast, Coronapod, examines why hospitals in New York are preparing to infuse patients with the antibody-rich blood of people who have recovered from COVID-19. Nature Podcast | 16 min listen Coronapod | 26 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Robin Bell is a geophysicist at the Lamont-Doherty Earth Observatory at Columbia University in New York.Credit: Beth Perkins for Nature This is the Ice Pod — stripped so you can see its guts — an instrument built by geophysicist Robin Bell and her team to fit in the military planes that take them to Antarctica and Greenland. “Often, groups of researchers study just ice, rock or the ocean,” says Bell. “The Ice Pod lets us look at the solid Earth, the ice and the ocean at the same time.” Former EPA analyst of clean-air policy Kathy Kaufman explains why, in some cases, scientists have included supporting data in new rules that roll back environmental regulations that make the orders vulnerable to legal challenges. (The New York Times) The Detroit Zoo has joined in on the idea of letting their penguins explore the place — and so have we! Here’s another chance to find our cheeky Rockhopper — this time he’s hiding in the coral reefs of Papua New Guinea. The answer will be in Monday’s Briefing, all thanks to Briefing photo editor Tom Houghton. We need to name this penguin. Suggestions in an email, please — plus any other feedback on this newsletter — to briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '199'>27 March 2020</date>
<url id = '200'>https://nature.com/articles/d41586-020-00932-6</url>
<title id = '200'>The COVID-19 outbreak on the Diamond Princess has given researchers the rare opportunity to study the virus in a highly controlled population, in which almost everyone was tested. Plus: families face heartbreak as non-coronavirus clinical trials are put on hold, and the healing ozone layer is getting the jet stream back on track in the Southern Hemisphere.</title>
<body id = '200'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Clinical research has been disrupted as hospitals devote more resources to caring for people critically ill with COVID-19.Credit: Alberto Pizzoli/AFP/Getty Scientists are rushing to launch clinical trials of experimental vaccines and treatments for the coronavirus — while clinical trials of therapies for cancer and other illnesses are being put on hold. Neena Nizar and her family were poised to harvest the fruit of a decade of hard work and sacrifice: a clinical trial of an experimental treatment for her two sons’ rare and painful genetic disorder. Then came COVID-19. “I feel like we were chugging along on a train and then somebody dropped a huge boulder on it,” says Nizar. Nature | 6 min read The healing ozone layer above Antarctica is shifting wind patterns around the globe — most notably, pausing the movement toward the pole of the summertime jet stream in the Southern Hemisphere. Satellite observations and climate simulations reveal that the jet stream shifted from about 49° S to 51° S between 1980 and 2000. After 2000, when the ozone layer began to recover as a result of the Montreal Protocol that effectively banned ozone-killing chemicals, the change has halted. New Scientist | 3 min read Get expert analysis of the research by meteorologist Alexey Karpechko in the Nature News & Views article. Reference: Nature paper Figure 1 | Changes in the summertime jet stream in the Southern Hemisphere. a, The jet stream in the Southern Hemisphere forms a near-circle around Antarctica in the troposphere — the lowest layer of the atmosphere. b, Banerjee et al.6 have reanalysed data that record the position of the summertime jet stream during the past few decades. The grey line shows the average position determined from their analysis; the envelope around the line indicates the minimum to maximum range. The blue line indicates the underlying trend, and shows that the jet stream shifted from about 49° S to 51° S between 1980 and 2000 — the years when the stratospheric ozone layer over Antarctica was becoming depleted. The trend alters after 2000, when the ozone layer began to recover as a result of the Montreal Protocol, which banned ozone-depleting substances; the position of the jet stream did not alter significantly during this period. Banerjee and colleagues show that ozone recovery is the cause of the pause in the jet-stream shift. (Graph from Fig. 1b of ref. 6.) Passengers quarantined on the cruise ship Diamond Princess.Credit: Eugene Hoshiko/AP/Shutterstock • The COVID-19 outbreak on the cruise ship Diamond Princess has given researchers the rare opportunity to study the virus in a highly controlled population. Almost all of the 3,711 passengers and crew on the cruise ship were tested, some of them more than once. Some 700 people were infected, with a substantial number — 18% — showing no symptoms (the passengers included a large number of elderly people, who are most likely to develop severe disease if infected, so the share of asymptomatic people in the general population is probably higher). In a separate preprint study that has not yet been peer reviewed, taking into account data from the ship and from China, researchers estimate a case fatality rate of around 1.1% — much lower than the 3.8% estimated by the World Health Organization. (Nature | 5 min read) • A group of 100 research societies, professional organizations and universities is calling on the US government to lift restrictions on the use of fetal tissue in research. They argue that the limits delay necessary work on potential treatments for COVID-19. The ban was brought in to appease those who object to the use of tissue from aborted fetuses. (Nature | Continuously updated) • Some epidemiologists and public-health researchers are watching with horror as the United States fumbles the COVID-19 outbreak within its borders. A lack of preparation, a shortage of equipment where it’s needed and a shortfall in testing mean that the country might “end up with the worst outbreak in the industrialized world”, says epidemiologist Seth Berkley, who heads Gavi, the Vaccine Alliance. The Atlantic takes a scathing look at what it will take for the country to overcome the pandemic — and thrive afterwards. (The Atlantic | 22 min read) • Lives depend on decisions that are made on the basis of the predictions of mathematical models. Looking at the differing responses of the Netherlands, Hong Kong and the United Kingdom, Science explores the influence and limitations of COVID-19 models. (Science | 9 min read) Read the latest coronavirus news, continuously updated on Nature. After being disappointed with drug companies’ achievements after the SARS and MERS outbreaks, biologist Stephen Burley considers how to avoid the same mistakes with COVID-19. (Nature | 4 min read) The Exclusion Zone around the wreck of the Chernobyl power plant, in present-day Ukraine, represents a glimpse of the past of civilization after its demise, the “final resting place of the future”. This has made the site of the 1986 nuclear accident into a morbidly fascinating tourist destination. “Immediately you are struck by the strange beauty of the place, the unchecked exuberance of nature finally set free of its crowning achievement, its problem child,” writes author Mark O’Connell. New York Times | 25 min read Two Nature stories from 2019 have won awards from the Association of Health Care Journalists. • Amy Maxmen won for her exclusive in which she joined WHO director-general Tedros Adhanom Ghebreyesus and the teams of African health workers giving their all to stop Ebola in the eastern Democratic Republic of Congo. Their moving stories of success and struggle illuminate the complex challenges of battling one of the most deadly pathogens known to humankind in a place tortured by war. (Nature | 18 min read) Also well worth watching is the video, narrated by Maxmen, telling her first-hand story alongside stunning photographs by John Wessels. • David Cyranoski won for a feature on Japan’s stem-cell free-for-all. He went undercover to get honest and frightening answers from clinics hocking unproven therapies in Japan, which are sanctioned and promoted at the top echelons of government. (Nature | 14 min read) A few modest adjustments to the planning and delivery of talks can help scientists to share ideas with their peers more effectively, say geographer Scott St. George and Nature climate-science editor Michael White. (Nature) This calendar of science livestreams, talks, virtual dance parties and general good times is thanks to neuroscientist and artist Christine Liu and the STEM Squad community. If you prefer a quiet night (maybe post dance party?), how about a soothing evening of transcribing the United Kingdom’s 200 years of handwritten rainfall data? Invite me to your virtual dance party (just kidding, I’ll be combing through Met Office records) — or send me any feedback on this newsletter — at briefing@nature.com. Flora Graham, senior editor, Nature Briefing With contributions by Smriti Mallapaty and Davide Castelvecchi An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '200'>26 March 2020</date>
<url id = '201'>https://nature.com/articles/d41586-020-00899-4</url>
<title id = '201'>Researchers hope that the antibody-laden blood of those who have recovered from coronavirus might reduce severe infections — but we don’t know yet if it will work. Plus: signs that the outbreak in Italy went undetected for weeks, and how running a quantum computer is like solving a Rubik’s Cube blindfolded.</title>
<body id = '201'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Credit: MBARI Humboldt squid (Dosidicus gigas) greet each other by turning on their internal bioluminescent organs. Researchers analysed hours of footage of roving groups of gregarious Humboldts and found that they sent signals to each other by combining bioluminescence and skin pigments — and dimmed the light show when pursuing prey. Nature | 4 min read Reference: PNAS paper Seawater has advanced by more than 5 kilometres under Antarctica’s Denman Glacier between 1996 and 2018. Satellite data from the Italian Space Agency show that the glacier rests on a trough extending to 3.5 kilometres below sea level — the deepest land canyon on Earth — and holds enough ice to raise global sea levels by 1.5 metres. The study found it to be at risk of irreversible retreat. “If I have to look at East Antarctica as a whole, this is the most vulnerable spot in the area,” says geoscientist Virginia Brancato. The Washington Post | 6 min read Reference Geophysical Research Letters paper Travel in India is restricted for 21 days.Credit: Arun Sankar/AFP/Getty • India has commenced a 21-day lockdown, after prime minister Narendra Modi ordered the country's 1.3 billion residents to remain in their homes from midnight last night. With the world's second largest population, the scale of India's lockdown will dwarf other nations. Currently, the country has roughly 500 reported cases, and 9 reported deaths, with many infections probably going undetected. (Nature | Continuously updated) • An epidemiological analysis of Lombardy, the epicentre of the outbreak in Italy, reveals that the first onset of symptoms in the country occurred weeks before the disease was reported there on 20 February. The preprint article, which has not yet been peer-reviewed, looks at nearly 6,000 laboratory-confirmed cases. The advice from infectious-disease modeller Michele Tizzoni: “Be prepared. Even if you don’t see much.” (Nature | Continuously updated) • Hospitals in New York City — now the US epicentre of the outbreak — will start treating COVID-19 patients with the blood of people who have recovered from the disease. Researchers hope the antibody-laden blood might reduce severe infections, and ease the burden on hospitals. A big advantage of the blood is that it’s available now and relatively safe, if it is screened for infections. China tried treating some patients with plasma from recovered patients, but results of its effectiveness are preliminary so far. (Nature | 7 min read) • Last week, NASA halted work on the world’s most expensive telescope — the US$8.8-billion James Webb Space Telescope. The outbreak also threatens to delay the agency's goal to send astronauts back to the Moon, and possibly a major mission to Mars this July. (Nature | 4 min read) Read the latest coronavirus news, continuously updated on Nature. Science also has something to teach us about living with uncertainty, argues biochemist Darren Saunders. (The Conversation) Universities are closing worldwide, forcing instructors to turn to remote teaching. Lecturers who have already tackled the challenge offer their advice on how to embrace the digital classroom. Mathematician Leonardo Rolla’s top tip: seek constant feedback from students. “I am the director of this movie,” he says, “but we are all in this together.” Nature | 5 min read Building a practical quantum computer — one that could solve intractable problems such as designing optimal batteries or predicting how certain proteins fold — will require a number of engineering feats, says quantum-computing system architect Richard Versluis. It will require sophisticated non-quantum systems to keep the quantum layer running correctly without actually observing its delicate quantum states, which would cause the computation to fail. “Controlling a quantum computer is a lot like solving a Rubik’s Cube blindfolded,” says Versluis. IEEE Spectrum | 13 min read Nature is determined to support research and — where we can — to help find a way out, says an editorial. (Nature) If you’re feeling a wee bit stressed, Doctor Who has a special message just for you. Let me know where or when you’d set the destination of your Tardis — and any other feedback on this newsletter — at briefing@nature.com. And if you’re feeling a LOT stressed, you have my most heartfelt best wishes. p.s. Don’t forget we’re giving away a framed Quirks of Nature cartoon: Enter here for a chance to win a print featuring the poetry stylings of a ‘haemato-poetic’ stem cell. Flora Graham, senior editor, Nature BriefingWith contributions by Nicky Phillips, Asia Pacific bureau chief and Davide Castelvecchi, senior physical sciences reporter. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '201'>25 March 2020</date>
<url id = '202'>https://nature.com/articles/d41586-020-00876-x</url>
<title id = '202'>Nature’s pick of the best histories and analyses of outbreaks from the Black Death to the 1918 influenza pandemic. Plus, a stunning fossil skull belongs to the oldest modern bird ever found, and our new coronavirus podcast.</title>
<body id = '202'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Artist’s reconstruction of the world’s oldest known modern bird, Asteriornis maastrichtensis, in its original environment.Phillip Krzeminski An extraordinary fossil skull belonged to the oldest modern bird ever found. The duck-sized Asteriornis maastrichtensis lived 66.7 million years ago, just 700,000 years before the asteroid impact that killed off all non-avian dinosaurs. Paleontologists were staggered to discover the skull when they used a computed tomography scan to look inside a rock found in Belgium in 2000 by an amateur fossil hunter. National Geographic | 7 min read Read more: Go deeper with the expert view in the Nature News & Views Reference: Nature paper Three-dimensional image of the skull of the world’s oldest modern bird, Asteriornis maastrichtensis.Daniel J. Field, University of Cambridge  • In the first of Nature’s new coronavirus podcast series, Coronapod, we explore why testing and contact tracing — which are key to controlling the coronavirus outbreak — are not being done around the world. (Coronapod | 21 min listen) • On Friday, the World Health Organization (WHO) announced a global trial of existing drugs that offer the most promise against SARS-CoV-2, the virus that causes COVID-19. Remdesivir was unsuccessful as a drug candidate for Ebola, but targets an enzyme in a way that might hinder SARS-CoV-2. Chloroquine and hydroxychloroquine are decades-old antimalarials that show some thin promise against the new virus. And a combination of three drugs — ritonavir, lopinavir and interferon beta — has shown an effect in animals against a different coronavirus, MERS. The trial, called SOLIDARITY, will not be double-blind, but it will be easy to enroll in and carry out. (Science | 11 min read) • Iceland has tested a huge number of people per capita for COVID-19 compared with other nations: as of today, more than 10,000 out of its population of 364,000. And, unlike in many other places, that includes many people who show no symptoms of the disease. The effort would be difficult to reproduce for more populous nations, but the resulting data reveals much about the disease — including “that about half of those who tested positive are non-symptomatic”, says Thorolfur Guðnason, Iceland’s chief epidemiologist. (BuzzFeed News | 8 min read) Read the latest coronavirus news, continuously updated on Nature. Immunologist Anthony Fauci, the director of National Institute of Allergy and Infectious Diseases and the face of the scientific response to COVID-19 in the United States, describes the reality of doing press briefings with US President Donald Trump. (Science | 9 min read) From the Black Death to the 1918 influenza pandemic, COVID-19 is not humanity’s first brush with outbreaks. Discover what we can learn from history with Nature’s pick of the best histories and analyses of past pandemics. • Historian Frank Snowden argues that infectious diseases have shaped social evolution no less powerfully than have wars, revolutions and economic crises. (5 min read, from 2019) • The story of the race to stop a plague epidemic in San Francisco that broke out in 1900 is a rich history of epidemiology, political wrangling and scientific denialism. (6 min read, from 2019) • The 1918 influenza pandemic probably infected 500 million people — one-third of the world’s population at the time. There was a wide-ranging failure of medicine and governments — which is one reason why its history has been long neglected. (5 min read, from 2017) • An analysis of the fraught campaign to contain the 2013–16 Ebola crisis reveals the common threads of dysfunction that run through responses to epidemics. (5 min read, from 2018) • Rediscover pioneering epidemiologist and anaesthesiologist John Snow and his meticulous mapping of the cholera epidemic in London. (5 min read, from 2013) A new book by groundbreaking field biologist George Schaller enumerates the rare delights and thorny political challenges of field work in Mongolia. Nature | 4 min read Male and female scientists have similar rates of publication and citation, when controlling for the difference in their career lengths. An analysis of the publishing careers of almost 8 million scientists from 1900 to 2016 finds that women tend to have shorter careers — and the gap is growing. “Women are pretty similar to men, as long as they stay in the system and do not drop out,” says computational social scientist Roberta Sinatra. Nature Index | 5 min read NASA astronaut Scott Kelly, who spent nearly a year on the International Space Station, shares his tips on living in isolation. (The New York Times) In case you missed it on Friday, Briefing photo editor Tom Houghton hid a Rockhopper penguin in the scientifically fascinating Škocjan Caves in Slovenia for you. Can you find the penguin? When you’re ready — here’s the answer! Keep the laughs going with one of the most-loved Quirks of Nature cartoons ever: Enter here for a chance to win a framed print featuring the poetry stylings of a ‘haemato-poetic’ stem cell. Good luck! Your feedback is always welcome at briefing@nature.com.Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '202'>23 March 2020</date>
<url id = '203'>https://nature.com/articles/d41586-020-00861-4</url>
<title id = '203'>Scientists exposed to COVID-19 are a striking example of US authorities failing to test people and notify their contacts, a cornerstone of outbreak response. Plus: understanding how many covert coronavirus cases might be unwittingly infecting others and the Turing Awards goes to Toy Story graphics wizards.</title>
<body id = '203'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Pixar's Toy Story, released in 1995, was Hollywood's first fully computer-animated feature filmBuena Vista Pictures/Everett Collection/Alamy Special-effects pioneers Patrick Hanrahan and Edwin Catmull won the US$1 million A. M. Turing Award “for fundamental contributions to 3-D computer graphics”. Beginning with Pixar Animation Studios’ 1995 Toy Story — the first feature-length film made entirely of computer animations — the two computer scientists have had leading roles in designing and applying software to create ever-more-realistic renderings and to simulate physics and materials. Wired | 5 min read Tonnes of ice the Greenland ice sheet lost over the hot summer of 2019 — enough to raise average sea level by 2.2 millimetres. (The Guardian | 5 min read) Reference: Geophysical Research Letters paper More than a third of all COVID-19 deaths have occurred in Italy.Credit: Alberto Lingria/Reuters • Research teams are racing to understand a crucial epidemiological puzzle — what is the proportion of infected people who have mild or no symptoms and might be unknowingly passing the virus on to others. Some of the first detailed estimates of these covert cases suggest that they could represent some 60% of all infections, implying the need for strong social-distancing measures. This is different from the groups who are trying to understand the number of unreported cases — those that are missed because authorities aren’t doing enough testing, or ‘preclinical cases’ in which people are incubating the virus but not yet showing symptoms. (Nature | 5 min read) • France will set up a €50-million (US$54-million) emergency fund to tackle COVID-19 and invest an extra €1 billion (US$1.1 billion) in research to prepare the country for future epidemics over the next decade. (Reuters | 2 min read) • Nationalism and scientific cooperation are in a tug-of-war over the fate of a future vaccine for the COVID-19 virus. Propaganda, pride and the prospect of treating your own citizens first sit on one side. On the other, unprecedented scientific collaboration — even among pharmaceutical companies that are ordinarily fierce competitors — and the desire to get the vaccine to where it can make the most difference epidemiologically. (The New York Times | 9 min read) • If you want to hear what software squillionaire and vaccine-funding philanthropist Bill Gates thinks about it all, he (and two of his foundation's scientific advisors) did an ‘ask me anything’ session on Reddit yesterday. No Earth-shattering insights, but a nice round-up of the current state of things (especially in the United States) from someone who has the ears of all the major players. (Reddit | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Molecular geneticist Alberto Bardelli described tears, resilience and a Friday evening virtual cocktail hour after shutting down his laboratory in northern Italy. (Nature | 4 min read) A new particle-accelerator experiment is testing whether some of the Universe’s carbon-12 — the building block of life — was forged during supernova blasts or the collisions of neutron stars. Most carbon is thought to have formed mainly inside the quietly burning cores of stars. But the process — in which three helium-4 nuclei come together to form one stable carbon atom — can theoretically happen more efficiently when enhanced by neutrons in more cataclysmic stellar processes. Scientific American | 5 min read This week, the coronavirus-free Nature Podcast speaks to actress Rosamund Pike about her experience portraying Marie Skłodowska Curie, and explores how science in Russia is changing after years of decline. For your hit of coronavirus news, listen to the Podcast Extra — this week epidemiologists, genomicists and social scientists discuss how they're working to tackle the coronavirus and what they've learned so far. Nature Podcast | 19 min listen Podcast Extra | 18 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Scanning electron microscope image of a human embryo in the early stages of cell division.Credit: P. M. Motta & S. Makabe/SPL In her new book, pioneering developmental biologist Magdalena Zernicka-Goetz reflects on an epic journey studying the start of life. Drafted over 15 years, the book’s main narrative is the remarkable transformation, in just a few days, of a single spherical mammalian egg cell to a tube containing all the types of stem cell needed for a full body plan. The addition of an honest and passionate depiction of the complexity of science as a vocation makes the book even more appealing, writes reviewer Sarah Franklin. Nature | 5 min read Andreia Martins is field coordinator for the metapopulation programme of the Golden Lion Tamarin Association in Rio de Janeiro, Brazil.Credit: Maria Magdalena Arréllaga for Nature People trust the facts more when an imaginary scientist told a story in first person about how they became interested in the topic, found journalism researcher Amanda Hinnan and her colleague, public-health researcher Lise Saffran. (Nature | 5 min read)Reference: PLOS One paper With its doors closed, the Shedd aquarium is letting its Rockhopper penguins explore the place. Nature media maestro Tom Houghton (who illuminates this newsletter) was inspired to enrich your day by hiding a Rockhopper in the scientifically fascinating Škocjan Caves in Slovenia. Can you find the penguin? The answer will be in Monday’s Briefing! Tell us where our Rockhopper should hide next — and any other feedback on the Briefing — at briefing@nature.com.Flora Graham, senior editor, Nature Briefing With contributions by Davide Castelvecchi, senior physical sciences reporter. An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '203'>20 March 2020</date>
<url id = '204'>https://nature.com/articles/d41586-020-00831-w</url>
<title id = '204'>The first human clinical trial for a potential coronavirus vaccine is under way — here’s what scientists are looking for. Plus: a nuclear-fusion reactor design that started as a science-fair project and how Russia aims to revive science after an era of stagnation.</title>
<body id = '204'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Visualization of how a stellarator’s plasma (orange) can be manipulated using a combination of permanent magnets (red and blue) and superconducting coils (grey rings).Credit: C. Zhu/PPPL Researchers have proposed a simplified design for nuclear-fusion reactors, based on powerful permanent magnets. The prototype reactors normally require sophisticated superconducting coils to confine plasma in a way that might one day offer enormous amounts of energy. While helping his son with a science-fair project (he wanted to build a rail gun), plasma physicist Michael Zarnstorff realized that neodymium–boron magnets are now powerful enough to be used instead of superconducting coils. Nature | 4 min read Reference: Physical Review Letters paper Medical teams from across China began leaving Wuhan this week, after infections there have dropped.Credit: STR/AFP/Getty • The first human clinical trial for a potential coronavirus vaccine, developed by the drug company Moderna, started in the United States this week. Inovio Pharmaceuticals plans to begin similar safety trials for its SARS-CoV-2 vaccine in April. These are the five key questions vaccine researchers will be trying to answer about people’s immune response to the virus and how to ensure any vaccine is safe and effective. (Nature | 6 minutes) • More people have now died of COVID-19 outside China than inside the country. The total number of new infections being reported each day in the rest of the world is also greater than those in China. (Nature | Continuously updated) • The US government’s crackdown on scientists who do not disclose the research that they conduct in China prompted chemist Weihong Tan to leave the University of Florida. Now at Hunan University in south-central China, Tan says he has co-developed a quick coronavirus test that produces results in 40 minutes. (ProPublica | 28 min read) • The risk of severe illness and death from COVID-19 is highest in older people, but young adults are not out of the woods. The US Centers for Disease Control and Prevention says that up to one-fifth of infected people aged 20–44 have been ill enough to be hospitalized; 2-4% of these required treatment in an intensive-care unit. No intensive-care admissions or deaths were reported among people younger than 20. (STAT | 7 min read) Read the latest coronavirus news, continuously updated on Nature.  The Nature Podcast speaks to epidemiologist Jamie Lloyd-Smith and other epidemiologists, genomicists and social scientists fighting the outbreak in Italy and beyond. (Nature Podcast | 18 min listen) Russia has been boosting funding and reforming its languishing scientific system in a bid to turn around the decline that followed the collapse of the Soviet Union. Nature’s analysis shows that the results so far are patchy, but many scientists in Russia feel things are slowly changing for the better. Nature | 12 min read As climate change forces animals to move away from their normal habitats, wildlife corridors will be essential to help them pass the human-made boundaries that criss-cross the landscape. In Wyoming, policymakers and scientists envision a network of bridges, tunnels and protected byways to help Rocky Mountain wildlife survive what’s coming. The Washington Post | 11 min read Biologist Paige Byerly helps keep us sane in quarantine with ‘enrichments’ based on the activities for animals at the Seattle Aquarium. (Twitter) This week we put together an issue of Nature with everyone working remotely, for the first time in our 150-year history. And unlike the Nature Podcast folks, most of us didn’t even have to work under a duvet — but thanks to them, we did learn how to make a recording studio out of an apple crate and a coat hanger. Tell me how you’re enriching your workspace — and any other feedback on this newsletter — at briefing@nature.com. And if you are out of work, or you are facing difficulties, I send you heartfelt best wishes. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '204'>19 March 2020</date>
<url id = '205'>https://nature.com/articles/d41586-020-00816-9</url>
<title id = '205'>Nature calls for leaders to follow World Health Organization advice, end secrecy in decision-making and cooperate globally. Plus: How to see the true geometry of the Universe, and the Abel prize goes to two mathematics pioneers who found order in chaos.</title>
<body id = '205'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Hillel Furstenberg (left) and Gregory Margulis were jointly awarded the 2020 Abel Prize.Credit: Yosef Adest, Dan Renzetti Two mathematicians who used randomness to cast new light on the certainties of mathematics will share the 2020 Abel Prize — one of the field’s most prestigious awards. Hillel Furstenberg and Gregory Margulis won “for pioneering the use of methods from probability and dynamics in group theory, number theory and combinatorics”, the Norwegian Academy of Science and Letters announced today. Each of them bridged gaps between diverse areas of maths, solving problems that had seemed beyond reach. Nature | 5 min read Researchers have found “a dream reaction” that should make it easier and cheaper to insert ‘magic methyl’ groups into many drugs. The magic methyl effect drastically increases the potency of bioactive molecules by replacing single hydrogen atoms with methyl groups. The resulting reshaped molecule interacts more easily with its biological targets. “This paper is just stunning,” says organic chemist Tim Cernak. “This is the wish [of] every drug hunter.” Science | 5 min read Reference: Nature paper Social distancing has been used to halt the transmission of the coronavirus in China.Credit: Getty • Countries with escalating outbreaks are eager to learn whether China’s extreme lockdowns were responsible for bringing the crisis there under control. The crucial question is which interventions in China were the most important in driving down the spread of the virus. In this explainer, epidemiologists discuss what happened after the lockdown, what China could have done better, whether travel bans were effective and the lessons for other countries. (Nature | 7 min read) • For the past month, South Korean residents have been receiving flurries of emergency text messages from authorities, alerting them to the movements of local people with COVID-19. Epidemiologists say that detailed information about infected people’s movements is crucial for tracking and controlling the epidemic, but some question the wisdom of making those data public. (Nature | 5 min read) • LitCovid is a curated list of research articles about the coronavirus in PubMed, updated daily. The list is maintained by researchers associated with the US National Institutes of Health and links to 1,200 (and growing) papers, case reports and news stories, organized into helpful categories. (Nature | 1 min read) • In many countries, including the United States and the United Kingdom, governments have been making crucial decisions in secret and making announcements before publishing the evidence on which their decisions are based. “This is not how governments should work,” argues a Nature editorial. “The secrecy must end.” Nature calls on governments and their science advisers to follow World Health Organization advice, end secrecy in decision-making and cooperate globally. (Nature | 5 min read) Read the latest coronavirus news, continuously updated on Nature. The number of sign-language names for the coronavirus — Brazil alone uses at least three. In a letter to Nature, researchers call on the World Health Organization to create an international signing convention for the virus and the disease it causes. Molecular biologist Nevan Krogan is one of hundreds of scientists in an ambitious international collaboration that has identified 50 drugs that might be effective treatments for COVID-19. (The New York Times | 7 min read) Is our Universe geometrically flat — as it appears to us humble humans at first glance when we gaze into the night sky — or are we mere ants crawling over a spherical or hyperbolic shape? An infographic-packed feature walks us through the fantastic possibilities. Quanta | 11 min read Crowdfunding is gaining momentum as a way to finance scientific research, particularly among early-career researchers and in fields where grants are sparse. Three scientists who have successfully crowdfunded their own research share some lessons. The researchers raised funds in just 30 days on the largest dedicated crowdfunding platform for scientific research, Experiment.com. Nature | 5 min read Ecologist Daniella Teixeira found blackened trees and melted nest boxes when she returned to the bushfire-ravaged site on Kangaroo Island, South Australia, where she studied glossy black cockatoos. She joins the call for better strategies to support mental health for climate scientists. (Nature Index) Astronauts know how it feels to be stuck inside for a good long while (though at least they don’t have to do it with their kids!). If you feel your fitness is flagging, the European Space Agency has us covered with fun, kid-oriented workouts to help us keep fit like an astronaut. Let me know how many squats you can do while holding two cans of beans — and any other feedback on this newsletter —at briefing@nature.com. And if you are not feeling up for a workout, or you are facing difficulties, you have my very best wishes. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '205'>18 March 2020</date>
<url id = '206'>https://nature.com/articles/d41586-020-00802-1</url>
<title id = '206'>The phase I trial of the vaccine from drug company Moderna is just the beginning of a long process to test safety and efficacy. Plus: a ‘completely accidental’ discovery hints at how to use standard silicon microchips in a quantum computer and a year without conferences raises the question of whether we need them at all.</title>
<body id = '206'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A study to prod an antimony nucleus (buried in the middle of this device) with magnetic fields became one with electric fields when a key wire melted a gap in it.S. Asaad et al./Nature Researchers have discovered “by complete accident” a way to control the nucleus of a single atom using only electric fields. Theorists predicted in 1958 that an oscillating electric field could flip a nucleus, but it had never been observed. The finding hints that it might be possible to use standard silicon microchips as the quantum bits, or qubits, in a quantum computer without messing around with difficult-to-constrain magnetic fields. Science | 6 min read Reference: Nature paper NIH • The first phase I clinical trial for a potential COVID-19 vaccine has begun in Seattle, Washington. Over the next 6 weeks, 45 participants will receive varying first doses of the vaccine, followed by a second dose 28 days later. They will then be assessed over a 14-month period. The experimental vaccine relies on messenger RNA, which directs the body to make a protein found on the new coronavirus's outer shell — hopefully eliciting an immune response that protects against infection. (Nature | Continuously updated) Read more: Safety must come first in the rush to develop vaccines and treatments for the coronavirus, argues virologist Shibo Jian. (Nature | 5 min read) • The decision to close schools to slow the spread of COVID-19 weighs heavily on the shoulders of governments who must consider the risk to teaching staff, the impact on students and how childcare needs might hinder essential workers or expose older family members to infection. The US Centers for Disease Control and Prevention says that evidence from other countries shows that places where schools were closed, such as Hong Kong, “have not had more success in reducing spread than those that did not”, such as Singapore. (The Washington Post | 8 min read) • The COVID-19 virus isn’t ‘airborne’ in the sense that it lingers infectiously in the air for some time, like measles. But the extent to which the coronavirus can be spread through the air, in the form of droplets from a sneeze or cough, is not yet known — and there is some disagreement among scientists about the very definition of airborne. (Wired | 10 min read) Read the latest coronavirus news, continuously updated on Nature. The number of new domestically acquired cases of COVID-19 reported today by China. The country also reported 20 new cases imported from abroad. (AFP) Planetary scientist Sarah Hörst is among the researchers rethinking how they network in a year without conferences. (Nature | 5 min read) As geopolitical tensions rise in nuclear-armed states, scientists are modelling the global impact of nuclear war. It’s the most comprehensive effort yet to understand how a nuclear conflict would affect the entire Earth system — from the initial firestorm and the spread of its smoke, to the impact on oceans, the atmosphere and wildlife. Nature | 9 min read A barrage of fake responses to her online questionnaire prompted quantitative psychologist Melissa Simone to learn how to quash survey-ruining bots. Her tips for protecting your own survey includes using unique, personalized links and including ‘honeypot’ questions that only bots can see. Nature | 3 min read Credit: Nicolas L'Heureux Researchers weave a yarn out of human extracellular matrix (ECM), the supportive network that normally surrounds cells in a living tissue. Scientists extracted ECM from lab-grown human cells and used it to make strands of tough yarn that can be knitted or woven into ‘human textiles’ that could someday be used for medical textiles. (Acta Biomaterialia paper) See more of the month’s best science images, selected by Nature’s photo team. (Nicolas L'Heureux) Lego bricks washed up on British beaches indicate that they could survive in the ocean for up to 1,300 years, says environmental scientist Andrew Turner. (Independent) Reference: Environmental Pollution paper Cellist Yo-Yo Ma has kicked off a campaign to share #SongsOfComfort on Twitter with performances including Bach’s Cello Suite No. 3 dedicated to healthcare workers. If pop is more your thing, here’s a BBC round-up of online ad-hoc concerts from bands including Christine and the Queens. Tell me your favourite working-from-home tunes — and any other feedback on this newsletter —at briefing@nature.com. And if you are not so comfortably ensconced, or you are facing difficulties, you have my very best wishes. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '206'>17 March 2020</date>
<url id = '207'>https://nature.com/articles/d41586-020-00788-w</url>
<title id = '207'>Alcoholics Anonymous is one of the most effective methods for people who want to give up drinking, finds a systematic review. Plus: All the Dead Sea Scroll fragments at the Museum of the Bible are fakes and how scientists map the hidden spread of COVID-19.</title>
<body id = '207'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Odisha, in eastern India, has closed theatres, swimming pools, schools and other public spaces in order to stem the spread of coronavirus.Credit: STR/NurPhoto/Getty • US President Donald Trump called the coronavirus outbreak a national emergency on Friday afternoon. This gives the administration broad authority in its response to the disease, including access to up to US$50 billion in federal funds to combat the epidemic. (Nature | Continuously updated) • Because COVID-19 testing isn’t available to everyone, the coronavirus is spreading to some extent under the radar. Public-health leaders from the World Health Organization, the US Centers for Disease Control and Prevention (CDC) and the major UK research funder Wellcome explain how officials and researchers estimate the size of local outbreaks from incomplete data. (Nature | 6 min read) • What does ‘flattening the curve’ really look like? A Washington Post article runs a new simulation every time you view the page to show, in simplified form, how quarantine and social distancing affect the spread of infectious disease. (The Washington Post | 5 min read) • Singapore, Taiwan and Hong Kong are very different places that each managed to quash COVID-19 transmission, in their own ways, without draconian measures. Epidemiologists Benjamin Cowling and Wey Wen Lim explain in detail how it was done. (The New York Times | 9 min read) • The United Kingdom’s strategy for suppressing COVID-19 has come under fire after the government announced it would focus on building up ‘herd immunity’. The government has now backed away from the phrase, which is usually associated with vaccination and seemed to imply the intention that the majority of the British population be infected. The communication misstep has caused confusion, say some scientists, who have called on the government to share the evidence behind its plans. (The Atlantic | 9 min read) Read the latest coronavirus news, continuously updated on Nature. We don’t know whether COVID-19 will ebb and flow with the seasons, says virologist Neal Nathanson. In fact, we know little about viral seasonality at all. (Science | 13 min read) Alcoholics Anonymous (AA) and other 12‐step programmes are some of the most effective methods for people who want to give up drinking. A systematic review of 27 studies found that AA and similar approaches also led to lower health-care costs for participants compared with other methods, such as cognitive behavioural therapy. “The fact that A.A. is free and so widely available is also good news,” says psychiatrist John Kelly. “It’s the closest thing in public health we have to a free lunch.” The New York Times | 5 min read Reference: Cochrane Library review article The fragments of the Dead Sea Scroll displayed at the Museum of the Bible in Washington DC are — as long suspected — fake. All but one of the 16 fragments were found to be made from leather, rather than parchment like the real scrolls, says an expert analysis commissioned by the museum. The fakes were purchased by the billionaire Hobby Lobby family that bankrolls the museum. The family has in recent years paid millions in fines related to smuggled artefacts. National Geographic | 15 min read Reference: Museum of the Bible report The UK government is racing ahead with plans for an Advanced Research Projects Agency, modelled on the high-risk, high-reward US Defense Advanced Research Projects Agency (DARPA). Such freedom comes with responsibility, argues a Nature editorial: you can’t reap the rewards of high-risk research without investing in meticulous preparation and verification. Nature | 4 min read Science has become a weapon in the battle between back-country sports enthusiasts and conservationists over the plan to close thousands of hectares of Idaho forest to snowmobiling. Both groups share a love of the wolverines that roam the snowy mountains, but few agree on what the evidence says about how to protect the enigmatic animals. High Country News | 9 min read Although the Monterey Bay Aquarium is closed, its webcams are still up and running, notes its executive director, Julie Packard on Twitter. (Get those live cams) For those stuck at home, maybe with kids, educational organization Skype A Scientist will set up a video call with a real live scientist (to talk about science). And you can always drop me a line at briefing@nature.com with your feedback on this newsletter — I read every email. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '207'>16 March 2020</date>
<url id = '208'>https://nature.com/articles/d41586-020-00778-y</url>
<title id = '208'>How life survives in one of the deepest layers of Earth’s crust ever explored, a vaccine on the horizon for devastating swine fever and how China is staying on track of its mission to Mars.</title>
<body id = '208'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here A thin slice of rock from oceanic crust at Atlantis Bank in the Indian Ocean, where slow-living bacteria have been found.Credit: Frieder Klein/WHOI In one of the deepest layers of Earth’s crust ever explored, researchers are finding life. An analysis of rock samples from the bottom of the Indian Ocean has revealed microbes adapted to life within nutrient-poor hairline fractures in the Earth. Researchers found several species of bacteria, fungi and archaea that live in the rocks and feed on carbon from fragments of amino acids and other organic molecules carried in deep ocean currents. Nature | 5 min read Reference: Nature paper The proportion of all microbes on Earth that are thought to live in places long deemed inhospitable, such as deep sediments under the oceans, the cold deserts of Antarctica and the stratosphere. The European and Russian space agencies have pushed back the launch of a rover that will search for signs of life on Mars. The Rosalind Franklin probe was meant to launch in July but will now depart in 2022 to allow key tests to take place. The space agencies also cited the global coronavirus pandemic as a reason for the delay. The epidemiological situation in Europe has “left our experts practically no possibility to proceed with travels to partner industries”, said Dmitry Rogozin, director-general of Roscosmos. Nature | 4 min read Researchers in China have developed an experimental vaccine that can protect pigs for life from a lethal virus that has wiped out 40% of the country’s 440 million pigs. Virologists say the vaccine still requires clinical trials and large-scale vaccine production. The country has suffered huge economic losses as a result of the virus since it appeared in 2018, although it is mostly under control now. Nature | 4 min read Heavy rains, war and a lack of funding have been hampering efforts to control the biggest locust outbreak in more than a quarter of a century. Locust monitoring in Africa is severely underfunded, with many countries facing unpaid bills to the organization that provides a locust early-warning system and helps to control outbreaks. In Yemen, the ongoing war and the humanitarian crisis it has caused put the country in no position to deal with the pests. The United Nations has appealed for urgent funding as scientists scramble to model outbreaks and find safe countermeasures. Nature | 4 min read China's Long March 5 rocket will carry its Mars probe into space.Credit: AFP/Getty • China’s first journey to Mars is on track for a July launch, say researchers. But with parts of the country in some form of lockdown because of the coronavirus, the mission teams have had to find creative ways to continue their work. Instead of risking the team members getting infected on a plane or high-speed train, 3 people drove 6 payloads in a car from Beijing to Shanghai — a journey that took more than 12 hours. (Nature | 4 min read) • Although the symptoms of COVID-19 are mostly respiratory, some patients suffer severe cardiovascular damage. Four cardiologists explore the effects and the underlying mechanisms, in hopes of improving treatment for people with underlying cardiovascular diseases. (Nature Reviews Cardiology | 7 min read) • Scientists working on mathematical models for COVID-19 are still in the early stages of characterizing the outbreak, says epidemiologist John Edmunds. Next will come forward-looking ‘scenario planning’ models — but we need better surveillance and testing data to make those calculations more accurate. (The Scientist | 7 min read) Read the latest coronavirus news, continuously updated on Nature China’s live-chicken trade might help to spread flu viruses The market for live poultry might be fuelling the spread of avian flu viruses in China, according to an analysis of viral genomes and models of the poultry trade. Some influenza viruses that infect birds can also infect people: the H5N1 virus, for example, which has an estimated fatality rate of 50–60% in humans. ‘Third-hand’ tobacco smoke fills non-smoking cinemas Even in a non-smoking cinema, film-goers can be exposed to hazardous tobacco-related pollutants, which waft off the clothing and bodies of smokers in the audience. Levels of smoke-related chemicals rise sharply during age-restricted action films, but less so during a children’s flick. Primeval roots for a key animal protein A vital structure found in the neurons of all animals — calcium-ion channels — might have originated in bacteria. Researchers identified a bacterial version of the specialized proteins in a hot-spring bacterium called Meiothermus ruber. This suggests that the channels originated in a common ancestor of bacteria and animals. Get more of Nature’s research highlights: short picks from the scientific literature. Rosamund Pike plays Marie Curie in Radioactive.Credit: StudioCanal and Amazon Content Services Who was Marie Skłodowska Curie, and how did she become the only person ever to win Nobel prizes in two scientific disciplines? A new biopic, Radioactive, gives only a partial answer, writes reviewer Georgina Ferry. By falling into the ‘lone heroes’ trap, the film gives little insight into Curie’s own motivations or her respected place in the international scientific community. Nature | 5 min read A skull found perfectly preserved in amber might belong to the world’s smallest dinosaur — and reveal a whole new lineage of birds. Hear more in this week’s Nature Podcast. Nature Podcast | 27 min listen Subscribe to the Nature Podcast on iTunes, Google Podcasts or Spotify. Rose A. Marks is a postdoctoral researcher in molecular and cell biology at the University of Cape Town in South Africa.Credit: Jennie Lyn Pretorius for Nature Biologist Rose Marks scales a 100-metre quartzite cliff in South Africa to measure and take samples of the ‘resurrection’ plant (Myrothamnus flabellifolia), which can survive for years in a completely desiccated state. “Understanding what makes these plants so resilient could help us to develop crops that will survive drought,” says Marks. (Nature | 3 min read) Epidemiologist Mike Ryan, the World Health Organization's head of emergencies, calls on governments to respond aggressively and systematically to the coronavirus pandemic. (NPR) For music lovers, disco icon Gloria Gaynor shows us how to achieve the full 20-second hand wash to the tune of her classic hit ‘I Will Survive’. And the Seattle Symphony is publishing free videos of “performances that provide strength, comfort and joy” while their hall is closed owing to the coronavirus outbreak in Washington state. Today I socially distanced myself by replacing public transport with a 15-kilometre bike ride, and it was pretty great. Wish me a pleasant pedal home — and please send me any feedback on this newsletter — at briefing@nature.com. (And if you’re not finding any silver linings, or you are facing difficulties, you have my very best wishes.) Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '208'>13 March 2020</date>
<url id = '209'>https://nature.com/articles/d41586-020-00756-4</url>
<title id = '209'>Research on a vessel that has been intentionally frozen in Arctic sea ice will be affected after a team member on land tested positive. Plus: Iron rain falls on an ultra-hot giant exoplanet and the emotional and professional toll of long, drawn-out peer reviews.</title>
<body id = '209'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Artist’s impression of the night side of WASP-76bESO/M. Kornmesser A gas-giant exoplanet orbits so close to its star that liquid iron might rain in its skies. Astronomers glimpsed the tell-tale signal of gaseous iron in the spectrum of light observable on the planet at the boundary where day turns to night. When night turns to morning, the signal is gone — hinting that the iron condenses into rainfall at night. New Scientist | 2 min read Reference: Nature paper Scientists who post their peer-review activity on the website Publons say they’ve reviewed papers for journals termed ‘predatory’, an analysis has found. The researchers who review most for predatory journals tend to be young, inexperienced and affiliated with institutions in low-income nations in Africa and the Middle East. Many of the titles have some editorial oversight, but the quality of reviews is in question. “Clearly I was naively deluded in thinking if you had proper reviews the quality of publications would rise,” says entomologist Ian Burgess. Nature | 4 min read The ship Polarstern is locked in Arctic ice for the MOSAiC mission.Credit: Sebastian Grote/Alfred-Wegener-Institut (CC-BY 4.0) • Research on a vessel that has been intentionally frozen in Arctic sea ice since last October will be affected after a team member on land tested positive for the COVID-19 virus. The rotating crew of some 300 polar scientists on the Polarstern get tested for coronavirus before they arrive on the ship. For now the missions leaders are confident that the infection was caught before it reached the vessel — but it will delay some research. (Nature | 4 min read) • “Aim to create a sweet spot between complacency and anxiety, as well as moderate disgust.” That’s just one of the behaviour-change principles — create the right level and type of emotion — that four behavioural-health researchers give in their guide to slowing down COVID-19. The other principles are: make a mental model of transmission, create new social norms, replace one behaviour with another (don’t touch your face — instead keep your hands below shoulder level) and make behaviours easy (grab your tissues when you grab your keys). (BMJ blog) • Pressure to rush out a vaccine for SARS-CoV-2 (the virus that causes COVID-19) risks making matters worse for some people. There is evidence, from decades of efforts to produce a vaccine against other types of coronavirus, of ‘vaccine enhancement’ — in which some vaccinated people experience worse-than-usual symptoms if they do become infected. The mechanism is not fully understood, and the risk is usually mitigated by extensive testing in animals. (Reuters | 7 min read) Read the latest coronavirus news, continuously updated on Nature. Chemist Palli Thordarson explains how good old soap and water dissolves the fat membrane of viruses to make them fall apart like a house of cards. (The Guardian) Nuclear physicist and polymath Freeman Dyson’s influence spanned physics, disarmament, politics, culture and even science fiction. While still a student, he provided the mathematical grounds for quantum electrodynamics, or QED, to explain the interactions of elementary particles. The design of a safe, popular nuclear reactor, the adaptive optics used in many telescopes and 50 years of US government advice were among his many legacies — as well as a contrarian view of the impact of climate change. Nature | 4 min read Neurophysiologist Nancy Wexler spent her career chasing down the gene, discovered in 1993, that causes Huntington’s disease — the genetic condition that killed her mother, uncles and grandfather. Now Wexler has gone public with the fact that she has Huntington’s herself. Much of the reason is to raise awareness of the poverty and stigma faced by some extended families in Venezuela with high rates of the disease. Thousands of people from these groups contributed samples that led to the discovery of the gene and to the promising treatments in the clinical pipeline — and Wexler wants these people to benefit, too. The New York Times | 11 min read The peer-review process can sometimes take years. During that time, lines of research are abandoned, grants are harder to win, and researchers feel unable to compete for fellowships and jobs. Four researchers across different fields share the emotional and professional cost of drawn-out peer reviews and how things could improve. Nature Index | 6 min read US president Donald Trump’s demands for a speedy SARS-CoV-2 vaccine jars with his history of science denialism and cuts to science funding, argues Science editor-in-chief H. Holden Thorp. Flattening the curve — slowing the rate of coronavirus infections so that the health-care system can cope — is super important. Cattening the curve is super important and includes added cats! Thanks to epidemiologist Anne Marie Darling for meme-ifying a graph we all need to see. Like many of you, efforts to slow COVID-19 mean I’m working from home. Let me know your favourite video conference backdrop — and guess which Nature editor prefers a life-sized cardboard cutout of Idris Elba — by email at briefing@nature.com. And if you’re in less cosy surroundings, or facing difficulties, you have my very best wishes. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '209'>12 March 2020</date>
<url id = '210'>https://nature.com/articles/d41586-020-00732-y</url>
<title id = '210'>The COVID-19 threat hasn’t changed, but “alarming levels of inaction” prompt an escalation in language. Plus: the complete skull of a tiny dinosaur preserved in amber and better treatments on the horizon for kidney failure.</title>
<body id = '210'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Oculudentavis khaungraae is had a skull that was less than 2 centimetres long.Credit: Lida Xing This bird-like skull, exquisitely preserved in amber for almost 100 million years, belonged to probably the smallest dinosaur ever discovered. The skull is less than 2 centimetres long — suggesting that the creature was the size of a bee hummingbird (Mellisuga helenae), the smallest living bird. “It reveals to us a whole new lineage of birds,” says palaeontologist Jingmai O’Connor. Nature | 3 min read • The World Health Organization (WHO) has characterized the COVID-19 outbreak as a pandemic amid “alarming levels of inaction” to stop the spread. WHO director-general Tedros Adhanom Ghebreyesus emphasized that describing the situation as a pandemic did not change the organization’s assessment of the outbreak or its approach to stopping it. “There’s been so much attention on one word,” said Tedros. “Let me give you some other words that matter much more, & that are much more actionable: Prevention. Preparedness. Public health. Political leadership. And most of all, People.” (Nature | Continuously updated) • The American Chemical Society, one of the world’s largest scientific societies, cancelled its meeting in Philadelphia, Pennsylvania, on Monday, 13 days before it was due to begin. The American Physical Society’s huge March Meeting was also cancelled last week — one of scores of scientific conferences that have been called off worldwide. (Nature | Continuously updated) • To what extent can isolating the sick, quarantining their contacts and encouraging social distancing slow the spread of COVID-19? Four epidemiologists dig into how countries can mitigate the spread, using data from coronavirus hotspots in China, South Korea, Italy, and Iran. How individuals respond to health advice will be as important as government actions, if not more so, conclude the researchers. (The Lancet | 12 min read) • The fatality rate for COVID-19 is a crucial number for understanding the disease. Epidemiologist Marc Lipsitch explains why the number is so difficult to pin down and the measures that will eventually bring clarity. He also offers “the consensus for now” on the risk of dying for those with symptoms: around 1 or 2%, for young healthy adults. (The Washington Post | 6 min read) Read the latest coronavirus news, continuously updated on Nature. Infectious-disease specialist Helen Chu, who co-leads a study originally set up to track influenza in Seattle, recalls her thoughts on discovering that the coronavirus was spreading undetected in Washington state. (The New York Times) After decades of slow progress, researchers are testing better treatments for kidney failure — which kills more people than HIV or tuberculosis. They hope to supplement dialysis machines, which still use much the same technology as they did 50 years ago, with artificial kidneys and miniaturized dialysis that could save millions of lives. Nature | 10 min read  For three years, part of DARPA has funded two teams for each project: one for research and one to replicate it. The investment is paying off, write three participating researchers. They report back on a herculean effort — and an expensive one — that will bring benefits beyond reproducing any individual project. Nature | 10 min read Biomedical scientist Darren Saunders is a Eureka prize-winning cancer researcher with his own lab — whose funding has just run out. With ever more scientists in the workforce competing for grants, and science becoming ever more complex and expensive, Saunders is not alone. “There are a lot of scientists in my predicament, or worse, at the moment. It’s our big, dirty secret,” says Saunders. “But we do need to talk about it.” The Sydney Morning Herald | 3 min read As a university student in Nigeria, theoretical physicist Omololu Akin-Ojo learned to write computer code by hand, without the benefit of a computer to run it on. The experience influenced his decision to focus the East African Institute for Fundamental Research (EAIFR) in Rwanda on research in his own discipline. “Theoretical physics is the cheapest advanced research that you can do because you don’t need equipment to get started,” says Akin-Ojo. “You just need paper, a pencil, a brain and maybe a little computer hardware.” Quanta | 10 min read Population geneticist Jedidiah Carlson looked at how groups on Twitter — especially white supremacists — interact with bioRxiv preprints. (Twitter) You might have heard that it’s a good idea to wash your hands. But what song do you sing while you’re doing it? Developer William Gibson’s simple website generates a helpful and accurate hand-washing poster with the lyrics of your favourite tune. Let me know what you’ll be humming while scrubbing up — plus any feedback on this newsletter — at briefing@nature.com. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '210'>11 March 2020</date>
<url id = '211'>https://nature.com/articles/d41586-020-00713-1</url>
<title id = '211'>A pizza hangs in the balance in a bet over the progress of quantum computing. Plus, how transgenic animal models are advancing the fight against the coronavirus and a chance to win one of the most-loved Quirks of Nature cartoons ever.</title>
<body id = '211'>Hello Nature readers, would you like to get this Briefing in your inbox free every day? Sign up here Rhesus macaques are one animal model for the new coronavirus.Credit: Neil Bowman/Flpa/imageBROKER/Shutterstock • Researchers want to study the coronavirus in transgenic animals to reveal how infections develop and to aid efforts to create drugs and vaccines. The first results are emerging: teams in China have reported initial findings after infecting monkeys and mice that express a human gene that the coronavirus uses to infect cells. Some animal models are in short supply: a mouse-breeding laboratory says it has been overwhelmed with requests for a transgenic mouse that was developed in response to the 2002–03 outbreak of severe acute respiratory syndrome (SARS). (Nature | 6 min read) • Near-real-time analysis of viral genomes gives insight into how COVID-19 is hopping between countries and spreading in communities — but scientists are urging caution in how results are interpreted. Gene sequences represent only a tiny fraction of cases, and the mutations that differentiate strains are still very minor. “As the outbreak unfolds, we expect to see more and more diversity and more clearly distinct lineages,” says computational biologist Richard Neher. “And then it will become easier and easier to actually put things together.” (Science | 8 min read) • Panic-buying of protective N95 respirators is putting at risk the very research that aims to stop deadly pathogens. To adapt, some high-level biosafety labs are switching to reusable air-purifying hoods — but these are far more expensive. “We can be creative,” says infectious-disease researcher Joan Nichols. “But at the end of the day, we cannot do this work unless we’re protected properly.” (Undark | 8 min read) Read the latest coronavirus news, continuously updated on Nature. Epidemiologist Nicholas Christakis, who writes about how human social skills build kind and successful societies, supports the argument that the government of the United States should be reacting more strongly to the coronavirus outbreak. (The New York Times) Weathered, algae-encrusted plastic has a food-like odour that is attractive to sea turtles. Researchers wafted the scent over the tank of 15 captive-reared loggerhead sea turtles. The turtles responded with the same intensity as they did to the smell of fish and shrimp. “This 'olfactory trap' might help explain why sea turtles ingest and become entangled in plastic so frequently,” says biologist Joseph Pfaller. BBC | 4 min read Reference: Current Biology paper Beef is is draining US rivers Almost one-third of the water used in the western United States goes to crops that feed cattle. The irrigation of alfalfa, hay, maize (corn), sorghum and other crops eaten by cattle is the largest consumer of water in the United States — and the leading cause of abnormally low river flows. Leaving land fallow for limited periods would help, say researchers. Ageing microscopes get a new lease of life Transmission electron microscopes (TEMs) can make films of events lasting just trillionths of a second, thanks to a simple retrofit. An upgrade breaks the steady stream of electrons emitted by the TEM into pulses, overcoming the limitation that the sample must stay still. Impact of mixing low-calorie sweeteners with carbs Consuming low-calorie sweeteners at the same time as carbohydrates seems to affect the body’s ability to regulate blood sugar. After a regimen combining carbohydrates and the low-calorie sweetener sucralose, healthy people showed changes in their insulin sensitivity and their brain responses to sweet flavours. Get more of Nature’s research highlights: short picks from the scientific literature. A particularly exhausting week in which family life and his own health fell by the wayside in favour of work deadlines prompted neuroscientist Hilal Lashuel to ponder the toll of overwork in academia. University leaders at all levels should pause and think about the costs of continuing business as usual, he argues. Nature | 7 min read The cold, fresh water of Lake Huron preserves in eerily pristine condition many of the ships that fell victim to its ‘Shipwreck Alley’. An autonomous boat called BEN is aiming to map the mysteries that lie beneath the waves, and prove its worth in the quest to fill in the huge holes in our knowledge of the ocean floor. The Verge | 13 min read On 1 March 2030, a physicist will be one pizza (and one beer) richer. Physicists Jonathan Dowling and John Preskill have set up a playful bet about whether someone will invent a topological quantum computer. The theory is that such a computer will function through the merry dance of clusters of electrons, known as non-Abelian anyons, swapping locations inside a material. “I would be really happy if I lost,” says Dowling. “That would mean somebody made a topological quantum computer.” Wired | 9 min read ESO/M. Zamani The night sky above the planned Extremely Large Telescope (pictured) will be only “moderately affected” by the numerous satellites being launched as part of communication ‘megaconstallations’. A European Southern Observatory study suggests strategies to mitigate the problem, such as closing the telescope shutter at the precise moment when a satellite crosses the field of view. The study assumes 26,000 constellation satellites in total will be orbiting the Earth, but this number could be higher. (ESO press release) Read more: SpaceX launch highlights threat to astronomy from ‘megaconstellations’ Reference: Astronomy and Astrophysics paper (ESO/M. Zamani) Atmospheric scientist Shipra Jain shares how she got over her hesitation, even years into her research career, to call herself a ‘scientist’. (Nature) One of the most-loved Quirks of Nature cartoons ever could be yours! Enter here for a chance to win a framed print featuring the poetry stylings of a ‘haemato-poetic’ stem cell. Good luck! If poetry-spouting stem cells aren’t for you, let me know which cartoon you’d like to win next time. Please send your feedback to briefing@nature.com. Flora Graham, senior editor, Nature Briefing An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '211'>10 March 2020</date>
<url id = '212'>https://nature.com/articles/d41586-020-01487-2</url>
<title id = '212'>Breakneck triage nails many diagnoses, but deeper treatment is needed.</title>
<body id = '212'>Garment workers in Bangladesh risk their lives in an industry devastated by the collapse in global spending.Credit: Zabed Hasnain Chowdhury/SOPA Images/LightRocket via Getty There has never been a harder time to be a political leader. The choices that must be made are enormous, the consequences potentially catastrophic, the science guiding those decisions uncertain — and there is no precedent. As a result, the COVID-19 pandemic has revealed some of the best and the worst in the world’s leaders: from opportunism and denial to compassion and clarity. It’s a shame that policymakers did not have books such as Joshua Gans’s Economics in the Age of COVID-19 to lay out the issues for them in January. It is remarkable that they do already. Gans completed this book at breakneck speed, by late March. His attempt to explain the economic thinking that should guide policy is useful, but inevitably limited. With the situation and knowledge changing daily, unfurling events will always render some aspects of such an analysis obsolete. In this sense, Gans, an economist at the University of Toronto in Canada, has taken a brave shot at an impossible task. Ultimately, economic thinking will need wider horizons.   The crisis has forced some politicians, especially on the right, to go against deeply held inclinations by implementing interventions and financial handouts that, in normal times, even most of their opponents would deem excessive. Countries have tried to freeze their economies and prop up the absence of liquidity and wages with eye-watering subsidies until the wheels start turning again. Therein lies the difference from the oft-cited comparison with wartime economics. In that situation, activity continues, but redirected. The present worldwide lockdowns have drastically shrunk the workforce. Aside from essential workers — in health, care, food and transport, say — only those jobs that can be done alone from home can safely continue (never have I felt luckier to be a writer). This has sometimes been presented, too simplistically, as creating a choice between saving lives or saving the economy. As many countries have now passed the (first?) peak of infections, discussion has turned to the dangers to health posed by an economy left too long in stasis. That discussion needs to happen, but it risks becoming facile, too. Presenting lives versus livelihoods as a dichotomy is used in defence of leaders who hesitated to impose a lockdown. That, Gans shows, is mistaken. The highly infectious nature and the fatality rate of COVID-19, which were both clear early on, even if exact numbers were not, meant there was never a gradual trade-off to be had: a dash more economy at the price of a few more deaths. “If you know you are going to shut down the country eventually, there are huge returns to doing it quickly,” Gans writes. It is the only way to keep choices open as more is learnt about the virus and its spread.   This is not hindsight: Gans was writing while the UK and US governments were procrastinating. Nor is it just about saving lives in the short term. “Pursuing public health can be consistent with superior long-run economic performance,” Gans writes. And to be effective, that decision to shutter must be made with “resolve, clarity [and] transparency”. If leaders downplay the enormity of the crisis, prevaricate or issue weak behavioural guidelines — rather than expectations with consequences — then individuals will “do as they often do and pursue their own interest”, and will “keep businesses open and keep engaging in social life”. Then there’s the question of how to manage the crisis in an economy on pause. Again, ideology might clash with reality. If you urgently need masks or ventilators, then there’s no time to put it out to tender and let market mechanisms make the choices about who gets the contract and the product. There must be centralized decision-making and allocation, even if that risks a degree of ‘inefficiency’. There has been a surge in COVID-19 cases in meatpackers in the United States.Credit: USDA/Alamy And how do you keep the economy in suspended animation, without the onset of necrosis? Governments have generally realized that they must help to cover lost wages, but the details are very tricky. The options of providing financial assistance to cover bills, such as rent and mortgages, suspending those costs or covering them directly are not equivalent. Gans says that the aim must be for payments, subsidies and loans to “ensure that people’s short-term disruptions are not translated into long-term breakups”. One solution, he suggests, is repayment of government loans over time through taxation.   Is the past any guide? Gans touches on the only comparable event in recent times, the 1918 influenza pandemic. The economic consequences of that were complicated because it occurred directly after a global war (mobilization of troops exacerbated that outbreak). Gans might also have mentioned the AIDS epidemic in Africa, which has in some regions been devastating enough to orphan generations, deplete the workforce and hamper economic development. Prioritizing the economy over health is not necessarily taking the long view. Gans does a good job fleshing out the requirements for an exit strategy. He says we now need to “invest in the testing economy”, for example to establish who can safely return to work and to monitor workplace safety. “Countries and regions that were able to test, trace, and then isolate the infected were able to contain the virus quickly and reopen their economies sooner”, he points out — in a ringing endorsement of World Health Organization policy. Even with a vaccine, he says, testing is likely to be a part of our daily lives for many years. He also offers a useful discussion of how to optimally allocate a vaccine when it has not been produced in sufficient quantities for all (although he does not go into the issue of people refusing it, which is likely to be a problem). And he considers how innovation in vaccine development can be motivated without reliance on market forces and patenting of what is so clearly a global public good. There is previous discussion he might have drawn on here about the development of urgently needed drugs and treatments that seem unlikely to generate profits for pharmaceutical companies, such as new antibiotics and treatments for tuberculosis.   As others have done after previous outbreaks, Gans advocates establishing a pan-national institution with a “set of resources to contain future pandemics and ensure an international, harmonized response”. More like the International Monetary Fund than the WHO in his vision, this would focus not just on drugs but also on innovations to enhance protection from infection at work and on public transport. He calls “hundreds of billions of dollars per year to mitigate substantially the risk of global pandemics” a no-brainer, echoing those after the first Ebola outbreak who drew parallels with defence spending. Here, the book stops short. Realistically, Gan’s word was always going to be the first, not the last. But he paints a picture of a post-COVID-19 world that is largely back to normal, with some inconveniences. The truth is that the pandemic throws much more into question. Whatever landscape emerges, it is unlikely to be same as that at the end of 2019. There is a moral case for rethinking inequalities in light of what we have learnt about who is truly essential for society’s functioning. Some aspects of neoliberal economic policy are fundamentally in conflict with the needs of a fragile world, with greater risks to come. The behaviour of some leaders has pointed out the dangers of an information economy that has become a ‘marketplace for truth’. And economics itself must incorporate the revision of past preconceptions and habits that will be demanded of the rest of us.   Latest on: Economics Correspondence 19 MAY 20 Correspondence 19 MAY 20 World View 19 MAY 20 Government News Q&A 22 MAY 20 Correspondence 19 MAY 20 World View 19 MAY 20 Institutions News 12 MAY 20 Nature Index 29 APR 20 Nature Index 29 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '212'>15 May 2020</date>
<url id = '213'>https://nature.com/articles/d41586-020-01317-5</url>
<title id = '213'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '213'>   Jathan Sadowski MIT Press (2020) ‘Smart’ technology in hands, homes and cities will “measure, monitor, manage and monetize all aspects of our lives”, shows Jathan Sadowski. One toothbrush, for example, uses sensors to record when, how long and how well users brush their teeth, sending the information to cloud servers owned by the manufacturer or a third party. Users might choose to share such data with their dentists — but surely not with dental insurers without express permission. This hype- and jargon-free warning deserves a wide welcome.    Laurence C. Smith Little, Brown Spark (2020) Many civilizations began beside great rivers: the Tigris and Euphrates in what’s now Iraq, the Nile in Egypt, the Indus in India and Pakistan, and the Yellow in China. Rivers run through almost all our great cities, notes Earth scientist Laurence Smith in his highly readable history, extolling their fundamental benefits: “access, natural capital, territory, well-being and a means of projecting power”. No wonder we still speak of ‘crossing the Rubicon’, a border river traversed by Julius Caesar’s army in defiance of republican law, triggering the rise of the Roman empire.    Mark Lynas Fourth Estate (2020) The average global temperature has risen 1 °C above pre-industrial levels. This update to environmental activist Mark Lynas’s 2007 book Six Degrees explores the likely effects of further rises. Its horrifying eloquence derives from its restraint, grounded in research, including a 2019 Nature assessment of future carbon dioxide emissions from existing infrastructure, which leads Lynas to forecast total emissions double those required to keep increases to the 1.5 °C agreed in Paris in 2015. “This really is our final warning,” he concludes.    Vinayak K. Prasad Johns Hopkins Univ. Press (2020) The cost of new cancer treatments in the United States is routinely US$100,000 per year per person; some are more than $400,000 per dose. Oncologist Vinayak Prasad’s insider study analyses how US scientific, industrial and regulatory policies — as well as financial conflicts of interest for physicians — “incentivize the pursuit of marginal or unproven therapies at lofty and unsustainable prices”. Aimed at general readers (including patients), oncology trainees and experts in health-care policy, it informs and disturbs throughout.    Clara Bosak-Schroeder Univ. California Press (2020) US natural-history museums haunt classicist Clara Bosak-Schroeder’s study of how ancient Greek historians Herodotus and Diodorus Siculus portrayed non-Greek peoples, such as Ethiopians and Persians. She begins with her childhood fascination with a diorama of a crouching African woman holding a pink grub to her mouth, and ends as she adopts a killer whale at Washington’s Whale Museum. Ancient ethnographies, she says, can help people “confront environmental degradation and transform their own relationships to other species”. </body>
<date id = '213'>06 May 2020</date>
<url id = '214'>https://nature.com/articles/d41586-020-01316-6</url>
<title id = '214'>With science denialism stronger than ever, who better to revisit? By Alison Abbott</title>
<body id = '214'>In the seventeenth century, Galileo faced persecution for his heretical views on astronomy.Credit: Hulton Archive/Getty Galileo and the Science Deniers Mario Livio Simon & Schuster (2020) Is there room in the crowded canon for a new biography of Galileo Galilei? Astrophysicist Mario Livio is betting so. His Galileo and the Science Deniers aims to stand out by placing the original Renaissance man and his discoveries in modern scientific and social contexts. In particular, he argues, the charges of heresy that Galileo faced for his scientific claims in the seventeenth century have their counterparts in science deniers’ condemnations today. Born in 1564 in Pisa, Italy, into an intellectual family of declining fortune, Galileo pursued medicine at the University of Pisa. But he soon abandoned his course to study mathematics, his enduring passion. The Universe, he famously wrote, “is written in the language of mathematics”. It was an argot that allowed him to break reliance on the Aristotelian cosmology prized by the Catholic Church, and to forge a new, quantitative study of nature. In Pisa, Galileo delved into mechanics, using his observations to question accepted ideas about motion (although Livio reminds us that Galileo probably never conducted the famous experiment in which he supposedly dropped spheres from the city’s leaning tower and found that they fell at the same speed regardless of their mass).   In 1592, he moved to the University of Padua in Italy, an intellectually liberal environment happily beyond the jurisdiction of the Pope. Here, he began to discuss the revolutionary theory, proposed by Polish mathematician Nicolaus Copernicus in 1543, that rather than being the Universe’s fixed centre as Aristotle had insisted, Earth was in fact orbiting the Sun. Livio structures his account partly around specific works, including Galileo’s 1610 The Sidereal Messenger, which described his major astronomical observations. While working in Padua, Galileo often visited the nearby port of Venice, where he was introduced to the ‘spyglass’, a new-fangled instrument from Holland that could be used to see ships approach. Galileo turned it to the heavens to make the discoveries that changed the course of astronomy, and launched his own fate. In his first observations, Galileo saw that the Moon was not a smooth sphere, but was mountainous. This contradicted the church’s view that the heavens were pristine and unchangeable, unlike the corrupt, mutable Earth. He also saw satellites orbiting Jupiter, shooting a hole in the geocentric argument that if Earth were to move, it would lose its Moon. In 1610, against his friends’ advice and in pursuit of more money, Galileo left the protective environment of Padua and moved to Florence to work for Cosimo II de’ Medici, the Grand Duke of Tuscany. Despite now living within the Pope’s sphere of influence, in 1632, Galileo published his book Dialogue Concerning the Two Chief World Systems, an imaginary debate between Salviati, an advocate of heliocentrism, and a witless geocentrist named Simplicio.   Putting the Pope’s view — that God’s universe is inherently unknowable — into the mouth of a fool was risky. Galileo’s fame and fortune rose, but so did the power and determination of his enemies, and the Holy Inquisition finally claimed him. In a riveting account of the trial, Livio describes how the Inquisition dismissed Galileo’s claim that the Dialogue was a balanced argument that ultimately rejected Copernicanism. On 22 June 1633, one of the world’s most venerated scientists was on his knees before its members, renouncing the errors and heresies inherent to Copernicanism. Livio parses the considerable, and often ambiguous, evidence about Galileo’s life and trial, and comments on the conclusions reached by various historians. The official summary of the trial proceedings, he writes, “revealed a clear intention to present Galileo in the worst possible light”. Like others before him, Livio doubts claims that Galileo left the court defiantly muttering about Earth, “and yet it moves”. Anthony Fauci and Donald Trump participate in the daily coronavirus task force briefing at the White House in Washington DC.Credit: Drew Angerer/Getty The non-chronological zigzagging of the book can be hard to follow, but allows Livio to focus on themes, such as Galileo’s polymathy. He highlights Galileo’s lifelong study of the great Italian poets Dante Alighieri, Torquato Tasso and Ludovico Ariosto. And he notes that the astronomer’s drawing skills and knowledge of perspective allowed him to understand that the shadings on the Moon were shadows cast by mountains, and to depict them in lovely watercolours. Livio is at his best when he discusses how Galileo’s scientific understanding compares with that of researchers today. Galileo suggested, for example, that comets might be optical phenomena caused by the reflection of sunlight by vapours released from Earth. We now know they are ‘dirty snowballs’ made of ice, rock, dust and frozen gases. Some of these components vaporize when they get close to the Sun, giving comets two tails: one of dust that reflects sunlight, and one of gas that glows as it ionizes.   And what of today’s science deniers? Livio briefly addresses how religion and business interests still conspire to attack evidence for evolution and anthropogenic climate change. In general, “processes that are not fully understood don’t constitute flaws”, he points out, but critics from creationists to Donald Trump discredit scientific arguments by exploiting gaps in knowledge. It’s a chillingly relevant theme, yet the parallels he draws between Galileo’s trial and contemporary science wars feel thin, and there’s a frustrating lack of examples to demonstrate the continuity of denialism through the centuries. Nonetheless, Livio has added to the canon an accessible and scientific narrative, in which a profound love for Galileo shines through. </body>
<date id = '214'>04 May 2020</date>
<url id = '215'>https://nature.com/articles/d41586-020-01222-x</url>
<title id = '215'>As COVID-19 exacerbates inequalities, Thomas Piketty’s analysis reads as timely, but inadequate. By Ingrid Harvold Kvangraven.</title>
<body id = '215'>The need for food banks has risen during the coronavirus pandemic.Credit: Mario Tama/Getty Capital and Ideology Thomas Piketty Harvard Univ. Press (2020) The COVID-19 pandemic is exposing and exacerbating inequalities around the world. Read against this backdrop, economist Thomas Piketty’s latest book is timely, but partial. In Capital and Ideology (first published as Capital et ideologie in 2019), Piketty documents the global rise of inequality and critiques ideas that legitimize it. He builds on his bestselling 2013 book Le Capital au XXIe siècle (Capital in the Twenty-First Century), which spurred a public debate on growing gaps between the haves and have-nots in Europe and the United States. His latest work is important, especially because — before the pandemic — the London-based magazine The Economist had raised doubts about the extent to which inequality has really been rising. But in downplaying the roles of material interests, structures of production and capitalist dynamics, Piketty’s analysis is concerning. His argument is that societies always try to justify their imbalances, and that the prevailing justification rests on shaky foundations. He argues that differences in wages today are often justified by a “meritocratic fairy tale”, in which people believe that the entrepreneurial earn wealth and those living in poverty simply need to work harder. But, of course, Western societies are not meritocratic. As Piketty demonstrates, discrimination is common — based on status, race, gender and religion. In the COVID-19 pandemic, could our obvious dependence on undervalued work in sectors such as nursing, care of children and older people, grocery provision and delivery shift perception of the extent to which these workers deserve the low wages of their jobs, which are often precarious? I hope so.   Piketty discusses what he sees as the success of the period of social democracy in Europe and the United States in the 1950s to the 1970s, when the gap between the richest and poorest was narrower. He notes that most people who voted for social-democratic parties between 1950 and 1980 were workers, but that the vote has since shifted to the educated and middle class. Uneducated workers have thereby largely been left behind, paving the way for phenomena such as the election of US President Donald Trump and the United Kingdom’s referendum on leaving the European Union. However, as economist Michael Roberts has pointed out, the social-democratic period rested on compromises between capitalists, organized labour and the state, not on a coherent set of beliefs. What’s more, Roberts says, the collapse of these alliances might have had more to do with plummeting profitability in the 1970s, which made it harder for social-democratic politicians to support workers. Strikingly, Piketty does not recognize the political battle over ideas in academia, although this could help him to explain shifts since the 1970s, including economics departments squeezing out Keynesian and Marxist perspectives. Instead, Piketty simply draws a sharp line between knowledge production and politics. He labels his own empirical work “rational” and “unbiased”, but his policy recommendations “ideological”. This is problematic. Economists’ perceptions of their own analyses as being free of ideology often hinder open and democratic debate. The behavioural-economics work suggesting that the United Kingdom should not enter lockdown, which might have guided the UK government at the beginning of its COVID‑19 response, is just one example. In that case, a particular way of seeing the economy — as composed of separate individuals responding rationally to incentives — was presented as an objective foundation for evidence-based policy that legitimized delays in social distancing. Yet such evidence cannot be considered purely objective, and in this case it contradicted World Health Organization recommendations. Farmers harvest wheat in India despite a nationwide lockdown.Credit: Manoj Dhaka/Hindustan Times via Getty Piketty makes sweeping statements: he sees ideologies as social constructs with lives of their own, independent of what stakeholders stand to gain or lose. For example, he argues that one of the stated justifications for colonialism was the colonizers’ idea of having a “civilizing mission”; this is true, but the prevailing motivation was without doubt the vast wealth to be acquired. Clarity here is essential for understanding the generation of massive global injustice. Similarly, Piketty does not provide convincing evidence that, as he claims, inequality in post-colonial countries such as South Africa is driven by ideas legitimizing chasms in opportunity, rather than, for example, the stubborn persistence of racist institutions. It is ironic that Piketty nods frequently to Karl Marx while simultaneously ignoring key Marxist insights about dynamics such as the profit motive, unequal access to and ability to develop technology, and labour-squeezing cost-cutting. At times, it seems that Piketty simply equates capital with wealth, because he focuses both his analysis and his policy recommendations largely on wealth transfers. For example, rather than interrogating how we as society work, produce and consume, his solutions are biased towards redistribution without changing the core of the system.   This limits his capacity to explain global phenomena. This is clear in his view on the effects of trade liberalization: rather than exploring how the removal of barriers to imports in the 1980s led to a collapse of industry in the global south, Piketty focuses on the loss of income from tariffs. In the same vein, his proposals shy away from discussing the massive rebalancing of global finance and production that is necessary; instead, he focuses on aid transfers to governments, and taxation. His policy proposals don’t challenge our reliance on capitalist growth. Rather, they involve adjustments to the existing order, such as redistribution and the inclusion of employees on company boards. Therefore, the worry articulated in the United Kingdom’s most right-leaning quality newspaper, The Daily Telegraph, that Piketty is back “more dangerous than ever”, because of his vilification of entrepreneurs and billionaires, is in my view unfounded. Despite its shortcomings, this book does have the potential to start an important debate about how to restructure society in a more egalitarian and ecologically sustainable way. If we are to exit the global depression brought about by the current pandemic with a system set for net-zero emissions, this will be more important than ever. But these debates must also involve more careful analysis of capitalist dynamics and the social relations of production. </body>
<date id = '215'>28 April 2020</date>
<url id = '216'>https://nature.com/articles/d41586-020-01127-9</url>
<title id = '216'>How to live in the face of death — Mark O’Connell’s personal journey. By Caspar Henderson.</title>
<body id = '216'>Many former military bunkers, such as this one near Edgemont, South Dakota, are being repurposed into doomsday communities.Credit: Jim Lo Scalzo/EPA-EFE/Shutterstock Notes from an Apocalypse: A Personal Journey to the End of the World and Back Mark O’Connell Doubleday (2020) Are we facing the end of civilization, or even the planet? It’s a question that attracted some serious scientific firepower even before the current pandemic. UK institutions such as the Centre for Existential Risk at the University of Cambridge and the Future of Humanity Institute at the University of Oxford are modelling the probabilities of various catastrophes, from a giant meteorite strike to a scenario in which criminals and psychopaths gain ‘easy nukes’ and incinerate a vulnerable world. Meanwhile, climate and Earth-systems scientists are amassing more evidence by the month that, barring rapid and profound reorganization in our societies, climate change will batter our world on at least the scale of a major war.   Rather than assessing the science itself, Notes From An Apocalypse explores how such threats affect individuals. Written before the COVID-19 crisis, it is an eerily prescient mix of confession, political critique, meditation and comic monologue on living in the face of death. It is the second such book from Mark O’Connell, the winner of the 2018 Wellcome Book Prize (for his first, To Be A Machine, which tackled the philosophy that humanity can evolve beyond its limitations using science). As the scientific and political responses (or lack thereof) to threats ranging from global heating to plastic dominate the headlines, O’Connell probes deeper into our personal psyches. In a tone somewhere between those of writer Samuel Beckett, film-maker Woody Allen and poet W. B. Yeats, he asks what happens when we’re faced with the prospect of both individual and global demise. A successful literary journalist living in Dublin with his young family, O’Connell is obsessed with doom. He sets his computer home page to an online forum dedicated to discussing civilizational collapse, and compulsively checks his smartphone for YouTube clips of emaciated polar bears, when he should instead be watching cartoons with his son. This fixation leads him into the shadowy worlds of ‘preppers’. These self-styled survivalists stockpile stores and weapons, readying themselves for civilization’s impending collapse, and feed endless online discussions about videos of the contents of their ‘bug-out bags’ — knapsacks containing items they consider essential for the end-time. The clips strike O’Connell as apocalyptic variations of ‘haul videos’, in which young consumers lay out the treasures of a recent shopping binge, with Kevlar socks in place of Superga shoes, and athleisure swapped for military-grade cordage. This converted nuclear missile vault in Glasco, Kansas, has a heated pool and water slide.Credit: Chet Strange/The New York Times/eyevine O’Connell hits the road, deploying his considerable gonzo journalism skills to seek out other doomsday obsessives, each caught up in their own dark, imagined futures. He visits high-end condos being built in a former weapons-storage facility in South Dakota that can withstand explosions of up to half a megatonne. He seeks out the luxury bolthole of Peter Thiel, billionaire entrepreneur and ‘sovereign individual’ — a person who controls vast resources and intends to redesign the government to suit their needs after collapse. Thiel is one of several Silicon Valley elites who have chosen to build their bunkers in New Zealand. And O’Connell attends a meeting of the Mars Society in Pasadena, California, where enthusiasts share dreams of a new, American-style frontier in the unspeakably harsh conditions on the red planet. In each case, O’Connell skewers what he sees as a central psychopathology or distorted value system, even as he acknowledges his own uneasy fascination and near-complicity. Preppers, he argues, are readying not for their fears but their fantasies; they contribute “nothing to the prevention or alleviation of suffering in others”. Thiel and the would-be Mars colonizers imagine a world somehow beyond politics — and taxes — that is almost exclusively white and male.   The book’s final two journeys explore a quieter response to the threat of catastrophe. O’Connell retreats to the Scottish Highlands with members of the Dark Mountain movement — artistic and mostly gentle souls who, knowing climate chaos is imminent, seek solace in reconnection with nature. Lastly, he takes part in an anti-stag party with a friend who is separating from his wife. They visit the Chernobyl Exclusion Zone in Ukraine, where a future imagined by the Soviets met its end. O’Connell is struck by the shards of “our own machine age” — shattered glass from broken screens and a heap of old television sets with “ancient circuit boards greened with algae”. He wonders if this might be a glimpse of our own future. Back in Dublin, O’Connell finds he has lost his taste for cosmic nihilism. In the radiance, joy and hilarity of his kids — the way in which they connect with, rather than retreat from, the world — he finds inspiration to shift his focus from how our lives might end, to what makes them worth living. Notes from an Apocalypse offers no scientific analysis of the existential threats we face or how we should respond. Instead, it illuminates the anxieties and delusions we share and oversights we commit, and shows how easily our fears (particularly when enabled by power, money and technology) can cause us to walk away from the disasters we create — to hide, flee, stockpile — just when we most need to engage. In this reflective, hilarious and disturbing page-turner, O’Connell makes a compelling case that connecting with nature and each other is the best way to calm our apocalyptic dread — and it might even increase our prospects of avoiding the worst. </body>
<date id = '216'>22 April 2020</date>
<url id = '217'>https://nature.com/articles/d41586-020-01126-w</url>
<title id = '217'>From the start, racism has shaped the care of people with mental illness in the United States. By Mical Raz.</title>
<body id = '217'>Patients at Milledgeville State Hospital, Georgia, in 1951.Credit: Robert W. Kelley/The LIFE Images Collection via Getty Administrations of Lunacy: Racism and the Haunting of American Psychiatry at the Milledgeville Asylum Mab Segrest The New Press (2020) How does a culture that enslaved people, encouraged lynching and developed racial segregation decide who is and is not sane? That is the question that frames Mab Segrest’s book on the legacy of slavery for US psychiatry in general and for what was in the 1940s and 1950s one of the largest psychiatric hospitals in the world. Combining archival research with fictionalized scenes, Segrest, a feminist and anti-racist scholar, recounts more than a century of custodial care at Georgia’s infamous Milledgeville hospital for people with mental illness. It opened in 1842 and Segrest weaves its history with the wider trajectory of US psychiatric care, the ravages of the American Civil War (1861–65) and the many manifestations of white supremacy and violence against women.   The book is organized chronologically, but includes multiple forays into the present, which can be distracting. Segrest begins her account by reconstructing a period when slavery was omnipresent and the asylum took in only white patients. After the war, psychiatry followed a racist trajectory that was by no means inevitable, argues Segrest. When Milledgeville started taking black patients in 1867, it — like other asylums nationwide — adopted racial segregation. As part of their treatment, white men worked as gardeners; black men had to labour on the institution’s farm. White women were seamstresses; women of colour worked in the laundry. Segrest uses the asylum’s archive to show that luxuries such as writing supplies, slippers, soap and carpets were allocated much more generously to white patients, whereas black patients faced daily discrimination and neglect. Many died soon after arrival, reflecting both their poor health and the deplorable conditions they had to endure. Iron markers displaced from graves in the grounds of the Milledgeville hospital.Credit: Jaime Henry-White/AP/Shutterstock Segrest also highlights what was left unexplored. Rather than asking how slavery might devastate an individual’s psyche, physicians treating newly freed African Americans discussed how their mental health might have been harmed by emancipation. What’s more, these patients often came from counties in which extreme racial violence, including “whippings, assaults, and murders”, was routine. Yet, in many cases, this history remained undocumented. Asylum psychiatry “maintained a vast silence about the bloodbath all around it”, writes Segrest, just as it had previously been silent about the violence of slavery. Milledgeville underwent several name changes and ultimately became the Central State Hospital before the main building closed in 2010. In her final chapters, Segrest examines how, when such hospitals began to close in the 1980s, penal institutions took their place. As welfare programmes were starved, the US prison population spiked, with people of colour and people with mental illness disproportionately incarcerated. Today, 90% of US psychiatric-care beds are in jails and prisons. Psychiatry will not be able to escape “the afterlife of slavery”, she argues, until it confronts its culpability in mass incarceration.   A newcomer to the history of psychiatry, Segrest’s approach is fresh and creative. She uses her imagination to flesh out the realities of life within the asylum walls. Describing Frances Edwards, a mother of seven taken to Milledgeville in 1856, Segrest imagines her arms feeling weirdly light and empty without her children, as her breasts “ached and leaked”. Segrest also finds connections between topics not always identified as part of psychiatry’s past. She calls attention to the high rates of infant mortality in the black community, exploring how such factors might have shaped — and still shape — black women’s mental health. Segrest’s is one of several books in the past few years that have foregrounded discussions of race in the history of psychiatry and of asylums. Her impressionistic style and convoluted structure contrast sharply with the more rigorous work of historians such as Martin Summers in his 2019 Madness in the City of Magnificent Intentions and Wendy Gonaver in The Peculiar Institution and the Making of Modern Psychiatry, 1840–1880 (2019). Segrest’s mixture of fact and fiction can also be confusing. But what is lost in clarity is perhaps gained in popular appeal. Uncomfortable reading at times, this valuable book helps to show how white supremacy shaped the definition and care of people with mental illness from the start, and how psychiatry remains in its shadow. </body>
<date id = '217'>21 April 2020</date>
<url id = '218'>https://nature.com/articles/d41586-020-01025-0</url>
<title id = '218'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '218'>   Joseph G. Allen and John D. Macomber Harvard Univ. Press (2020) “We shape our buildings and afterwards our buildings shape us,” said Winston Churchill in 1943. This is even truer now; people in the United States spend 90% of their time indoors, note health scientist Joseph Allen and business-school lecturer John Macomber. Yet we read little about indoor air pollution. In 2018, the US secretary of education publicly rejected the idea of investing in school buildings rather than students — to the authors’ horror. Their detailed, important study is welcomed by architect Norman Foster. But it speaks to everyone.    Tim Dee Jonathan Cape (2020) Tim Dee is a lifelong birdwatcher and former radio producer. “I have tried to make as much of this book about the birds and other natural manifestations of the spring as I can,” he writes, “but it is, of course, also about me.” His peregrinations between Europe and Africa — often accompanied by his South African-born wife, ornithologist Claire Spottiswoode — are vividly informative about many migratory species, and alive with literary and historical allusions. But some personal digressions might strike readers as self-indulgent.    Robert Bryce PublicAffairs (2020) Electrification is often deemed one of humanity’s greatest engineering achievements. Yet it dates only from the 1880s: a mere eye-blink compared with hominins’ 400,000-year history of using fire, as Robert Bryce observes in his enlightening history. Moreover, three billion people still have little or no access to electricity. Although not optimistic about the potential for renewable sources such as wind to increase electricity production, Bryce predicts continuing advances in batteries, generators, lights, microchips and motors.    Mark Kurlansky Patagonia (2020) Salmon was eaten by Neanderthals in the Caucasus Mountains about 48,000 years ago. The name derives from the Latin salmo, meaning ‘leaper’; Roman legions in the Rhine Valley observed this fish jumping over rapids and waterfalls while migrating upriver to spawn. Today, because of dams and development, a mere 1.5 million live wild in the Atlantic Ocean — many fewer than in the Pacific. The survival of the Atlantic genus is “highly questionable”, notes Mark Kurlansky in his comprehensive history, complete with historical recipes.    Tansy E. Hoskins Weidenfeld & Nicolson (2020) According to this pioneering, pugnacious study, our shoes “are the propulsion and the consequence of globalization”. In 2018, 24.2 billion pairs were manufactured, mostly in poor conditions, say industry experts interviewed by journalist Tansy Hoskins. Hazards include noxious fumes, toxic chemicals and poverty wages, as well as the suffering of animals intensively farmed for their hides. She proposes a move to plant-based, metal-free shoes made with non-toxic glues and dyes — and even, radically, “shoe libraries”. </body>
<date id = '218'>15 April 2020</date>
<url id = '219'>https://nature.com/articles/d41586-020-01024-1</url>
<title id = '219'>A biography digs into Pulitzer prizewinner and farming pioneer Louis Bromfield’s life. By David R. Montgomery.</title>
<body id = '219'>Louis Bromfield guides visitors around Malabar Farm, his land near Lucas, Ohio, in 1942.Credit: Alfred Eisenstaedt/The LIFE Picture Collection via Getty The Planter of Modern Life: Louis Bromfield and the Seeds of a Food Revolution Stephen Heyman W. W. Norton (2020) Saving the world’s agricultural soils is perhaps the most overlooked environmental challenge of this century. Driving through freshly tilled fields in rural Indiana a few years back, I was struck by how low points retained rich, black earth, yet on the hilltops, the khaki subsoil was completely exposed. I could see the land being shorn of fertility. We urgently need to pay attention to practices that can help to regenerate it. To that end, former columnist for The New York Times Stephen Heyman resurrects an obscure figure from US agricultural history in this engaging biography, The Planter of Modern Life. Heyman’s subject, Louis Bromfield, was a Pulitzer-prizewinning novelist before he became a prominent critic of industrialized farming. Today, Bromfield’s journey of discovery reinforces growing calls to rebuild healthy, fertile soil around the world.   Little about Bromfield’s life was conventional. Skipping over his childhood in rural Ohio, Heyman follows him through a series of colourful roles, including ambulance driver during the First World War; literary darling of post-war Paris; gardener in southern France; and eventually Hollywood screenwriter. In 1930s France, Bromfield copied his peasant neighbour’s compost-making and mulching process to convert a bare, rubble-filled patch into a fertile vegetable plot. Noting how French gardens had been farmed for centuries, whereas the United States’ soil was blowing away in the Dust Bowl, he realized something was amiss with the modern approach to managing land. Bromfield then visited the English botanist Albert Howard in India. Howard had travelled there to teach Western agricultural techniques, but ended up documenting traditional Indian methods and adapting them to colonial agriculture — which helped to found the organic movement. On Howard’s farm in Indore, India, Bromfield saw large-scale compost building in action, and absorbed advice to emulate nature to maintain fertile soils. Poor soil quality and lack of rainfall led to dust storms across the US Midwest in the 1950s.Credit: Joseph Scherschel/The LIFE Images Collection via Getty After moving back to Ohio to avoid the Second World War, Bromfield bought a farm with his book royalties. He put Howard’s ideas to work when spring snowmelt revealed that half his topsoil had been washed off into gullies that could swallow a cow. Bromfield composted and mulched, planted cover crops and practised rotation, and left steep slopes unploughed. He even grew several crops together in the same field. His neighbours thought his methods crazy. But his soil quickly improved, and he became convinced that what we now call conventional farming was heading for disaster. Bromfield used his literary prominence to spread Howard’s ideas. He wrote popular memoirs (Pleasant Valley in 1945 and Malabar Farm in 1948) about his experiences, and opened his farm up to tens of thousands of visitors so that they could see a model for an alternative approach to cultivating crops. Better biography than agricultural history, The Planter of Modern Life is packed with spats from Bromfield’s life as a writer, but thin on farming and the actual origins of the modern organic movement. Heyman pays more attention to how Bromfield hosted Humphrey Bogart and Lauren Bacall’s wedding in 1945 than to soil-building practices. In its descriptions of history, the book verges on simplistic. It presents Bromfield as a leader, when he added little new thinking about restoration of degraded land. Heyman divides farming into industrial and organic camps and casts Bromfield as a champion of the latter — a guru who romanticized a disappearing era and saw solutions to modern problems in the past. Yet techniques used in organic farming created serious problems long before the rise of modern agrochemicals. From classical Greece to colonial America, tillage eroded topsoil out from under civilization after civilization.   As Heyman notes, Bromfield was not an organic purist. He realized that mineral and chemical fertilizers could be needed to kickstart the growth of enough biomass to sustain soil fertility with green manures. Such pragmatic thinking hasn’t endeared him to proponents of either organic or conventional techniques. But many of the practices Bromfield embraced are now central to the burgeoning global movements of regenerative and conservation agriculture, which aim to maintain crop yields and farm profits while enhancing soil fertility and cutting environmental impacts. As I discussed in my 2017 book Growing a Revolution, these innovative farmers combine traditional practices, such as planting cover crops and rotation, with more modern developments, such as no-till farming, to build fertile soil and reduce reliance on diesel, fertilizers and pesticides (while saving money). Bromfield’s story is an inspirational glimpse into the roots of these growing movements. Bromfield’s original insight, then, was seeing the crucial importance of soil health before science really understood why this matters, or how to build it. Today we know that the future of farming lies not in replicating a mythical golden age, but in merging ancient wisdom with the technology of today. </body>
<date id = '219'>14 April 2020</date>
<url id = '220'>https://nature.com/articles/d41586-020-00995-5</url>
<title id = '220'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '220'>   Paul Kenrick Natural History Museum (2020) One of many wondrous photographs of plant fossils from around the world in this fascinating history enlarges a 1-centimetre film of carbon in 420-million-year-old UK sandstone to reveal a forked strand with trumpet-shaped ends. Spore extraction in the 1930s proved it to be the earliest known plant truly capable of living on land, notes palaeobotanist Paul Kenrick. He covers all aspects and major varieties of plants in scientifically detailed yet accessible prose. However, he concedes, “No one can say for certain when the first flower bloomed.”    Robert Skidelsky Yale Univ. Press (2020) Economists often compare themselves with natural scientists, argues economic historian Robert Skidelsky. They refuse to admit that their mathematics inadequately model individual and group behaviour, hence their failure to predict the 2007–08 financial crisis. This impassioned critique aims to show how economic laws have limited scope compared with the laws in natural science. To be effective, Skidelsky argues, economics must include institutions and their power, and move towards social sciences such as politics and sociology.    Neil Shubin Pantheon (2020) As a graduate student in 1986, palaeontologist Neil Shubin was inspired by a cartoon of a fish next to an arrow pointing to an early fossil amphibian. How could fishes have evolved into land creatures? Today, many non-biologists assume that feathers originated to help animals fly, or lungs and legs to help animals walk on land. But they are “entirely wrong”, says Shubin. His four-billion-year history from ancient fossils to DNA presents the true picture to the general reader, with engaging portraits of contributing scientists past and current.    Richard Masland Basic (2020) How do we recognize faces? So asks neuroscientist Richard Masland throughout his rich historical analysis of human vision, ranging from neurons to artificial intelligence and consciousness. He cites a study of monkeys raised from birth without seeing faces; brain scans revealed that the monkeys’ facial-recognition area responded to the hands of the experimenters who nurtured them. Yet after six months of facial exposure, the same area gradually became face-sensitive. Defective “face patches” explain Masland’s own face blindness.    Anne Case and Angus Deaton Princeton Univ. Press (2020) In this hard-hitting study of US capitalism, economists Anne Case and Angus Deaton note that median earnings for white men without a bachelor’s degree lost 13% of their purchasing power between 1979 and 2017, whereas national income per head grew by 85%. They tell of blue-collar despair and deaths from addiction, alcoholism and suicide, and a rapacious health-care sector. Hence, no doubt, the “gesture of frustration and rage” that elected Donald Trump president. The authors’ proposed solutions involve reforming capitalism. </body>
<date id = '220'>07 April 2020</date>
<url id = '221'>https://nature.com/articles/d41586-020-00994-6</url>
<title id = '221'>Up close, the all too human business of doing science is messy. By Sabine Hossenfelder.</title>
<body id = '221'>A NASA physicist in 1957, when recruitment in the field was booming in the United States.Credit: Smith Collection/Gado/Getty Quantum Legacies: Dispatches from an Uncertain World David Kaiser Univ. Chicago Press (2020) Physicists like to think their job is to uncover truths about nature. It is a profession that thrives on abstract thought and, often, an other-worldly detachment from reality. But with his essay collection Quantum Legacies, David Kaiser reminds us that, up close, the business of doing science is a mess. Physics, like any human activity, is rocked by the waves of history — its busts and booms, fears and fashions. Kaiser is a physicist and historian of science. Both sides of his expertise shine through the pages of Quantum Legacies, as in his previous books, such as 2011’s How the Hippies Saved Physics, about the field in the United States after the Second World War.   Here, 19 stand-alone essays, most of which have been published in some form before, are collected in sets loosely associated with the history of quantum mechanics; the role of physicists in US politics of the past century; the development of the standard model of particle physics; and cosmology. (Notwithstanding the title, much of the book does not deal with quantum physics. Perhaps the field’s biggest legacy —quantum computing — makes no appearance.) Kaiser is at his best when he condenses decades of history into a few pages, bolstered by ample references to literature that few scientists would otherwise stumble upon. An essay about the threat supposedly posed to the United States by the high number of Soviet physics graduates during the cold war is a masterpiece of historical analysis. He narrates how US newspapers and politicians took statistics out of context for years, feeding fears of Soviet scientific supremacy and culminating — after the 1957 launch of the satellite Sputnik — in legislation that hiked the US capacity to train physical-sciences graduate students by 70%. This policy soon waned; by 1968, young physicists looking for jobs outnumbered advertised positions by nearly four to one. Kaiser’s sombre assessment: “scarcity talk looped from hype to amplification to feedback”. One essay is about the “phone book” of gravity: Gravitation by Charles Misner, Kip Thorne and John Wheeler, published in 1973. Kaiser reveals much about the layperson’s fascination with mathematics, regardless of whether they understand it. The book was a voluminous and high-level collection of pretty much everything there was to say about the general theory of relativity in the early 1970s, packed with graphics, equations, problem sets and highly technical elaborations as well as passages that are sometimes colloquial, sometimes philosophical. Despite its density, it was a surprise hit — both with a popular audience and with physicists. Kaiser tells how Wheeler himself surmised that for many readers, “who don’t expect to and never will get deep into the mathematics”, the technical boxes and illustrations simply added to its mystique. The mid-century physics hiring boom in the United States was prompted by fears of Soviet competition.Credit: Walter Sanders/The LIFE Picture Collection via Getty Unfortunately, Kaiser does not sustain this level of clarity and insight. Several topics seem arbitrary and out of date, and he fails to explain why they are of special interest. This is one of the perils of republishing material that has appeared before, but in some cases at least a footnote to explain the historical relevance would have been helpful. In one essay, Kaiser writes about Higgs inflation: the idea that the Higgs field, which gives rise to the eponymous boson, also caused the early Universe to expand exponentially fast. The essay was originally published in 2006, six years before the Higgs particle was detected experimentally; today’s reader is left wondering whether there is any evidence for or against Higgs inflation, and why Kaiser focuses on this hypothesis about the early Universe out of so many others. Elsewhere, an essay about efforts to detect gravitational waves, published in 2017, fails to mention that the US Laser Interferometer Gravitational-Wave Observatory (LIGO) was joined that year by the Europe-based Virgo Interferometer. This risks giving the impression that no one outside the United States was ever interested in the hunt. An account of searches for extraterrestrial intelligence demotes the remarkable observation of thousands of exoplanets over the past few decades to just two sentences. It says nothing about the Breakthrough Starshot Initiative, announced in 2016 by physicist Stephen Hawking and tech giants Yuri Milner and Mark Zuckerberg, which aims to explore our Galactic neighbourhood using light-driven centimetre-scale spacecraft.   A reflection on the Big Bang doesn’t explain what physicists mean by this term or what evidence there is for it. And Kaiser’s claim that the number of physics PhDs granted by US institutions shows booms and busts like the stock market leaves aside the question: what can we learn from this insight? Quantum Legacies is skilfully written and a pleasure to read. But it feels like a missed opportunity. Those unfamiliar with the scientific side of Kaiser’s exposition might hope for more background on the sometimes highly speculative topics. And I wish he had elaborated on what lessons his historical analyses offer today’s researchers. If reality checks “could easily have been applied” to prevent cycles in physics hiring, are we applying those now? If a 1973 book full of equations on general relativity attracted so much interest from non-experts, are we underestimating how much maths people are willing to deal with? If the US government sank several billion dollars into a mega-collider that was ultimately cancelled (the Superconducting Super Collider in Texas, shelved in 1993), what can we expect for particle physicists’ present plans to build an even larger one? Kaiser is silent on these questions — but his collection did prompt me to mull them over. </body>
<date id = '221'>06 April 2020</date>
<url id = '222'>https://nature.com/articles/d41586-020-00914-8</url>
<title id = '222'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '222'>   This rewarding meditation on “how we find and lose our way” might have been called “Am I here?” — the tragic refrain of science writer Michael Bond’s grandmother after she developed dementia. The book astonishes as it ranges from the neuroscience of meandering rats to the deleterious effects of satellite navigation. A desert ant, we learn, can forage at least 100 metres from its nest, then scurry back in a straight line — equivalent to a human wandering for a day and a night, then heading straight home without help from GPS.    Quantum physicist, historian and science broadcaster, Jim Al-Khalili is well placed to summarize the past, present and future of physics for a lay audience, without using mathematics. After a tantalizing chapter on scale, he analyses space, time, energy, matter, quanta, thermodynamics and various attempts to unify the general theory of relativity with quantum field theory — although he never defines a black hole. On the debate between Niels Bohr and Albert Einstein, Al-Khalili sides with Einstein, who believed in an objective reality.    Science journalist Bjørn Vassnes’s brief book demonstrates how “life’s different revolutions have been intertwined with the history of the cryosphere”. He includes memories of digging tunnels to his house in the Norwegian Arctic during snowy 1970s winters, and experience of Bangladesh, which never sees snow yet survives on river water from threatened Himalayan glaciers. Vassnes discusses how reindeer grazing eradicates vegetation that reduces the Arctic’s heat-deflecting albedo effect; perhaps it could aid the fight against global warming?    “The best book of Natural History Travels ever published in England,” said Charles Darwin of entomologist Henry Walter Bates’s 1863 The Naturalist on the River Amazons, an 11-year journal inspired partly by Darwin’s diary of his 1831–36 journey on the HMS Beagle. This enchanting part-facsimile justifies his words. Bates writes grippingly on anacondas, bird-killing spiders and blowpipes. Although little-known now, his name endures in ‘Batesian mimicry’: a survival strategy based on apeing harmful species, which he observed in butterflies.    There are no references to Lucean Arthur Headen on Wikipedia; nor did he leave behind significant personal papers. Yet this black inventor and entrepreneur, born in racially segregated North Carolina in 1879 among formerly enslaved artisans, deserves study. Local historian Jill Snyder’s biography reconstructs him. By his death in 1957, 26 years after moving to Britain, Headen had spent almost 4 decades running US and UK companies making cars and products based on his patents — some of which are still cited. </body>
<date id = '222'>01 April 2020</date>
<url id = '223'>https://nature.com/articles/d41586-020-00913-9</url>
<title id = '223'>A history of the metaphors behind brain research faces a dark past and disquieting future. By Stephen Casper.</title>
<body id = '223'>A 3D magnetic resonance imaging scan of the brain.Credit: Tom Barrick, Chris Clark, SGHMS/SPL The Idea of the Brain: A History Matthew Cobb Profile (2020) The poet Emily Dickinson rendered the brain wider than the sky, deeper than the sea, and about the weight of God. Scientists facing the daunting task of describing this organ conventionally conjure up different kinds of metaphor — of governance; of maps, infrastructure networks and telecommunications; of machines, robots, computers and the Internet. The comparisons have been practical and abundant. Yet, perhaps because of their ubiquity, the metaphors we use to understand the brain often go unnoticed. We forget that they are descriptors, and see them instead as natural properties. Such hidden dangers are central to biologist and historian Matthew Cobb’s The Idea of the Brain. This ambitious intellectual history follows the changing understanding of the brain from antiquity to the present, mainly in Western thought. Cobb outlines a growing challenge to the usefulness of metaphor in directing and explaining neuroscience research. With refreshing humility, he contends that science is nowhere near working out what brains do and how — or even if anything is like them at all.   Cobb shows how ideas about the brain have always been forged from the moral, philosophical and technological frameworks to hand for those crafting the dominant narratives of the time. In the seventeenth century, the French philosopher René Descartes imagined an animal brain acting through hydraulic mechanisms, while maintaining a view of the divine nature of a mind separate from matter. Later authorities, such as the eighteenth-century physician and philosopher Julien Offray de Le Mettrie, secularized the image and compared the human to a machine. The Italian physicist Alessandro Volta rejected the idea of ‘animal electricity’, proposed by his rival Luigi Galvani as a vital force that animates organic matter. Volta was driven at least partly by his aversion to the mechanistic view. New metaphors came from nineteenth-century phrenology, evolutionary theory and the doctrine of inhibition in physiology — the idea that the nervous system could repress actions and behaviours. Then came the age of communication, and with it fresh language for the brain. The late-nineteenth-century discovery of neurons led to a clash of rival images. Reformers imagined separate components, comparable to the wires and signals of the nascent telecommunications infrastructure. Conservatives cast the nervous system as a continuous network (or reticulum) akin to the blood circulation, feeling that this explained how volition and mind might work; to them, discrete signalling implied heterodox notions of mind, perhaps even of the soul. The post-1940 proliferation of references to enchanted looms, ghosts in machines, logical circuits, reptile brains, parallel processors and uploaded minds grew from those foundations. Cobb notes, but only in passing, that we need new images to make sense of research developments ranging from artificial intelligence to mini-brains grown in the laboratory to brain implants. He doesn’t try to invent examples.   The narrative Cobb offers is familiar. The epistemic power of metaphors in science has long been recognized by historians and philosophers of science. Yet for the popular audience he targets, Cobb’s account is an important contribution: few have offered such accessible insights, with choice examples and clear explanations of the societal factors that lie beneath. Cobb also eloquently shows how figurative language does much more than simply distil or give shape to complex, intangible subjects. Metaphors change how science is done, by licensing new interpretations or inspiring new experiments. Cobb also reminds us that metaphors conceal as much as they reveal. The ideas that they so persuasively represent often ignore key elements. Comparing the brain to a computer is beguiling, but neglects that brains are also organs, and aware ones at that. Our existing images and language are desperately limiting when it comes to imagining a situation in which the mental, physical and embodied are so tightly enmeshed. Thus, despite their power, our metaphors have done little to bridge the divisions that emerge as scientists seek to understand what brains are. After centuries of research, including recent advances in exploring consciousness through imaging techniques such as functional magnetic resonance imaging, there’s still no answer to Shakespeare’s question in The Merchant of Venice — “Tell me where is fancy bred, Or in the heart or in the head?” We can’t stop using metaphors. Scientists depend on figurative language to organize and communicate thoughts and ideas. But whether the neurosciences can get closer to a compelling idea of the brain in the decades ahead might depend on a full reckoning of the role of metaphors. Top of the list: researchers should acknowledge that although certain word choices seem innocent, many carry malign overtones. Ideas of the brain have often embedded inequities and prejudices about race, class, gender, sexuality and agency.   On these matters, Cobb should have said more. The word ‘racist’ appears only a few times in his book, and then only in footnotes. But a little thought makes clear that seemingly innocent metaphors like ‘higher’ and ‘lower’ functions, or descriptions of specific anatomical structures as ‘primitive’, carry racialized baggage. When originally characterized, they spoke to the ghastly view that the nervous systems of white, upper-class men made them evolutionarily superior to those they subordinated at home and abroad. Similarly, it is discomfiting to realize that Broca’s area, linked to language processing, is named for the French physician Paul Broca, who believed in a hierarchy of peoples. That, in 2020, there are scientists who still talk about ‘female brains’, an idea Cobb rightly derides, is evidence that gender (a word that appears only in the bibliography) remains central to too many people’s ideas of how the brain is constructed. And he makes no mention of what neurodiversity advocacy might mean for figurative language. Whatever new metaphors are to come, ones that embrace differences inclusively will be more insightful and more profound. The Idea of the Brain puts our current predicament in context and synthesizes much that needs attention. It is a very good book. It could have done more in a time when science is coming to terms with the limitations of the straight, white, wealthy, Western, non-disabled, male perspective. But I hope it provokes contemplation about why certain metaphors linger, where they come from, how they persist, and in what ways they burden us with the invisible assumptions of past cultures. </body>
<date id = '223'>30 March 2020</date>
<url id = '224'>https://nature.com/articles/d41586-020-00835-6</url>
<title id = '224'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '224'>   Robert Muchembled (transl. Susan Pickford) Polity (2020) In 2014, a paper based on experiments with 26 people claimed that humans can discriminate between more than one trillion olfactory stimuli. Its mathematical model was heavily criticized; it’s more likely, as French historian Robert Muchembled says, that smell is “the only one of our senses to be acquired from experience”. Thus, European children take around five years to feel disgust at their own excrement. With pungent examples, this lively history charts the transformation of smells from the Renaissance to the early nineteenth century.    Sarah Besky Univ. California Press (2020) The Indian prime minister projects himself as the son of a railway-station tea-seller; tea is part of Indian identity. Yet the tea served there for mass consumption since the 1950s is shaped by machinery into tiny balls reminiscent of coffee granules, rather than the traditional Indian twists of hand-picked leaves — one of many intriguing observations by anthropologist Sarah Besky, from extensive experience of plantations and auction rooms. Her nuanced study of Indian tea, although overly academic, is a refreshing brew of botany, business and culture.    Alastair Gee and Dani Anguiano W. W. Norton (2020) The 2018 Camp Fire that almost destroyed the town of Paradise in northern California, killing at least 85 people, has become “a poster child for the climate crisis”, note journalists Alastair Gee and Dani Anguiano. Their account, based on interviews with residents, firefighters and academics, is horrendous, especially the section ‘Hell’, describing the fire minute by searing minute. It confirms how humans, not nature, are responsible for disasters — a spark from an electricity tower, well past its replacement date, triggered the inferno.    Solomon Goldstein-Rose Melville House (2020) Solomon Goldstein-Rose was elected to the Massachusetts state legislature in 2016, aged 22, on a platform focused on climate change. This keenly practical prospectus targets the 2020 US national elections. But he observes that any political will in industrialized countries to reduce carbon dioxide emissions can never overcome “basic economic realities” in developing countries, which produce two-thirds of global emissions. So industrialized countries, responsible for global warming, must also bear the brunt of reducing emissions.    Mark Hallett and John M. Harris Columbia Univ. Press (2020) Among the copious illustrations in this erudite history of big cats by naturalists Mark Hallett and John Harris is a photograph of a US cave floor showing beautifully preserved footprints of a jaguar that took its prey underground, became disoriented and died tens of thousands of years ago. An illustration by Hallett shows a resting pride of hungry, fly-ridden Asiatic lions in India — the remainder of a population that once ranged from western Asia to the Mediterranean. The authors argue for renewed efforts to preserve endangered carnivores. </body>
<date id = '224'>25 March 2020</date>
<url id = '225'>https://nature.com/articles/d41586-020-00834-7</url>
<title id = '225'>Two manifestos call for climate action on different scales — national and global. By Maria Ivanova.</title>
<body id = '225'>Christiana Figueres (second from left) and other global leaders celebrate adoption of the 2015 Paris climate agreement.Credit: Arnaud Bouissou/COP21/Anadolu Agency/Getty The Future We Choose: Surviving the Climate Crisis Christiana Figueres & Tom Rivett-Carnac Knopf (2020) Climate Change and the Nation State: The Realist Case Anatol Lieven Allen Lane (2020) Climate change demands action: humanity must shift from persistent destruction to intentional regeneration. So, how best to make that happen? Two new books give very different answers. In one, the solution lies exclusively with nation states and their protection of security and self-interest. The other expects a global-scale spirit of shared endeavour to harness the collective power of governments, corporations and individuals. The collaborative approach is set out insightfully in The Future We Choose. Its authors — Christiana Figueres and Tom Rivett-Carnac — had crucial roles in the Paris climate agreement of December 2015. Hailed both as a monumental achievement and as woefully inadequate, the agreement saw 197 parties commit to keeping the global average temperature rise to less than 2 °C above pre-industrial levels. Figueres is recognized as the person most responsible for that achievement: as executive secretary of the United Nations Framework Convention on Climate Change, she was at the heart of the process for six years, from the aftermath of the breakdown of negotiations in Copenhagen in 2009 to the success in Paris. Rivett-Carnac worked with Figueres at the UN and ran political strategy for the agreement. The book is an immersive journey through two very different visions of the year 2050. In one, the world has no fresh air and two billion people live with temperatures exceeding 60 °C for days on end. On the alternative, thriving, green planet, renewable energy powers smart grids and 50% of the land is covered in forest. Although such a stark choice could be criticized as simplistic, it brings into sharp relief the risk that humanity runs. “Systemic change is a deeply personal endeavor,” the authors write. The most effective way to get anyone to act — from the person on the street to the diplomat in the negotiation room — is, they argue, to shift mindsets towards a deep acknowledgement of humanity’s dependence on nature.   Figueres and Rivett-Carnac are in a prime position to unpick exactly how to do that. Their behind-the-scenes stories are captivating, including the little-known details of the 2014 climate negotiations in which China and the United States moved away from competition towards shared wins, or Figueres’s decision to keep the 2015 conference running after UN security services found a bomb in the underground station serving the conference centre. I yearned for more, however, on what it took to change the mood and get commitment that no one thought possible. What was the exact chain of events leading up to the strike of the green gavel at 7.25 p.m. on 12 December 2015? One of the duo’s major achievements was to bring civil society and business into the historically intergovernmental affair. Rivett-Carnac, for example, designed and led the Groundswell Initiative — a largely covert organization that mobilized support for the ambitious agreement from a wide range of stakeholders. Nation states are not the single or even most important driver in solving the climate crisis; it is, as the authors put it, an “everyone-everywhere mission”. Bolivian soldiers combat forest fires in 2019.Credit: Aizar Raldes/AFP via Getty By contrast, Climate Change and the Nation State argues for reframing the struggle in nationalist terms. Some 55% of all emissions come from four countries — China, the United States, India and Russia. What would compel these states to drastically change their behaviour and reduce emissions? For author Anatol Lieven, the answer lies in self-interest. A British journalist with significant expertise in Pakistan and Russia, and an academic post in international relations, Lieven weaves his first-hand knowledge and experience into a compelling narrative. The branch of international-relations theory called realism, which assumes that states act to maximize their power, has rarely considered environmental concerns. Lieven calls on his realist colleagues to “wake up” to climate change as a paramount security threat. The heatwaves across Europe in 2003 and in Russia in 2010, he points out, respectively claimed more lives than France lost in its eight-year war in Algeria and Russia lost in a ten-year conflict with Afghanistan.   The mass movement of people, Lieven argues, will be “the most dangerous indirect effect … on Russia and the West”. The confluence of climate change, migration and automation will be the perfect storm, rivalling the devastation of nuclear war. Displacement of large numbers of people will put strain on states’ ability to provide for their populations. When this comes on top of massive unemployment resulting from increased automation, the nation state will not be able to respond. That will lead to a world much like Figueres and Rivett-Carnac’s potential dystopia. Lieven makes a strong case for urgent action, especially by powerful states. He sees the armed forces, as experts on national security, as the logical first responders — not just for crowd control, but for large-scale infrastructure changes such as building flood defences. He advocates military support of the US Green New Deal, for example. But Lieven is wrong to disregard global governance. Climate change is the quintessential worldwide problem. No one state, no matter how powerful its economy or military, can resolve it alone. Despite the primacy of the nation state in international affairs, global agreements and institutions are indispensable in ensuring that commitments are fulfilled, action is supported and agreements revised — think of the role of the World Health Organization in the coronavirus pandemic. This underscores that human and planetary health are inextricably bound, that global action is imperative, and that effective international or supranational organizations are crucial. Lieven is convincing when he writes: “If there was ever an issue that demands prudence in judgment and courage in action, it is climate change.” But the judgement and action have to be global. Figueres and Rivett-Carnac might come across as overly optimistic in their conviction that a sense of global responsibility, for fellow humans and other species alike, will prove sufficient to spur the necessary action. But their short and simple book grabs you by the heart and makes you want to join the great adventure against overwhelming odds. </body>
<date id = '225'>23 March 2020</date>
<url id = '226'>https://nature.com/articles/d41586-020-00763-5</url>
<title id = '226'>Star field biologist and explorer George Schaller penetrates another isolated wilderness. By Tom McCarthy</title>
<body id = '226'>Wild Bactrian camels in Mongolia.Credit: De Visu/Alamy Into Wild Mongolia George B. Schaller Yale Univ. Press (2020) Eminent field biologist George Schaller is known for his ‘firsts’. He made groundbreaking studies of mountain gorillas in Central Africa. He was the first Westerner permitted to study China’s giant pandas. In Pakistan, he took the first photographs of snow leopards in the wild. His latest book, Into Wild Mongolia, chronicles yet more firsts — acquired through arduous expeditions into the isolated nation as it emerged from 70 years as a communist satellite state of the Soviet Union. From 1989 to 2018, in collaboration with Mongolian and international biologists, Schaller observed some of the region’s rarest species. Discoveries include surprisingly low numbers of young wild Bactrian camels (Camelus ferus); interbreeding between these and domesticated camels (Camelus bactrianus); the narrow diets of Gobi brown bears (Ursus arctos gobiensis), now supplemented at feeding stations; the number of Mongolian gazelles (Procapra gutturosa) lost to disease and commercial meat hunting; and the shocking disappearance of marmots. The ecological implications of these data are rarely positive. The findings are set against the trials and joys of remote fieldwork — from dodging street thugs and countryside bandits to eking out food. And Schaller contextualizes, giving a historical perspective on the land, its people, the once-dominant Buddhist religion, the tumultuous time under communism, and links to famed thirteenth-century ruler Genghis Khan. When Schaller first visited Mongolia in 1989, government and people were on the edge of financial ruin, with the collapse of the Soviet Union, essentially their sole trading partner. Fuel, when available, was tightly rationed. Trying to provision expeditions to the farthest corners of the vast country, Schaller faced shop shelves empty apart from the occasional box of matches or packet of cigarettes (and vodka, of course). He rejoiced when he found a few cans of peanuts — hardly adequate to keep a research team alive for weeks in the Gobi Desert. Thrifty and field-hardened, Schaller and his team followed the customs of Mongolia’s nomadic herders, who required little beyond mutton, flour, tea and a bit of rice. They benefited, too, from the nomads’ traditional hospitality towards travellers, relishing the soup, tea and rock-hard dried cheese curds insistently offered. A Gobi bear (Ursus arctos gobiensis) foraging. Less than 100 such bears remain in the wild.Credit: Eric Dragesco/Nature Picture Library The team did not experience such generosity from its local scientific counterparts, whose apparent lack of enthusiasm irritated Schaller and slowed research progress. Disbelief best describes his feelings when the Mongolian snow-leopard biologist does not get out of his sleeping bag on the morning they capture and radio-collar one of the rare cats. He is no less indignant when the national bear biologist passes up an opportunity to track the rare Gobi bears they had fitted with collars. He laments what he sees as a laissez-faire attitude in many biologists, rangers and managers of protected areas he meets, fearing that Mongolia cannot save its unique treasures unless things change. Schaller’s more engaged companions in the field often included his wife, Kay, who contributed practically and scientifically. His adult son, Eric, joined one trip, spending part of it huddled in a tent in sub-freezing temperatures listening to the pings of a distant snow leopard’s radio-collar to decipher its pattern of activity. At least he caught a rare glimpse of the enigmatic beast.   Schaller was also joined by biologists from the United States, Russia and Europe, and eventually by several young, bright and enthusiastic Mongolian scientists through whom he saw a positive future for conservation in the country. His relief was compounded when strong leaders started to emerge for new national conservation organizations and government agencies. Schaller — with whom I have worked at various times over the past three decades — is driven by his vision of what must be done if wild spaces and rare species are to persist. The vast, fragile eastern steppe is the part of Mongolia that he holds most dear, along with its seemingly endless herds of gazelles. He is eloquent in his condemnation of what he deems gross mismanagement by the nation’s current political leaders, who allow oil drilling, mining and road-building in crucial protected areas. Ultimately, he enjoins Mongolians to heed the rallying cry of their ancient rulers — that if the natural world is taken care of, it will take care of them. </body>
<date id = '226'>17 March 2020</date>
<url id = '227'>https://nature.com/articles/d41586-020-00762-6</url>
<title id = '227'>A pioneering developmental biologist reflects on an epic journey studying the start of life. By Sarah Franklin</title>
<body id = '227'>Scanning electron microscope image of a human embryo in the early stages of cell division.Credit: P. M. Motta & S. Makabe/SPL The Dance of Life: The New Science of How a Single Cell Becomes a Human Being Magdalena Zernicka-Goetz & Roger Highfield Basic (2020) Studying the early development of humans and other mammals takes colossal dexterity and stamina. Unlike, say, sea-urchin embryos — which are transparent — mammalian embryos are hidden from view. Investigating their development is also tricky because of the delicate timing and sequence of events that produce highly complex organisms. As a result, the story of how scientists have ingeniously deciphered many of the basic mechanisms of development over the past century is often as gripping as what they’ve discovered. Magdalena Zernicka-Goetz has been a key player in that story. A pathbreaking figure in developmental biology and stem-cell science for several decades, her work has reset the clock for the determination of cell fate in the early mammalian embryo. Among other discoveries, her lab perfected a culture system that can extend human embryonic development in vitro (M. N. Shahbazi et al. Nature Cell Biol. 18, 700–708; 2016), providing a powerful model for basic research with implications for improving understanding of pregnancy loss. Her book The Dance of Life — written with science journalist Roger Highfield — is a vivid first-hand account of epic technological changes and revelations in her field. It is also a personal tale of an ongoing scientific odyssey, replete with failure, exhaustion and tenacity as much as thrilling new vistas. Drafted over 15 years, the book’s main narrative is the remarkable transformation, in just a few days, of a single spherical mammalian egg cell to a tube containing all the types of stem cell needed for a full body plan (see go.nature.com/2vgrjpw). Until the early 2000s, it was thought that this diversity arose from cells that were initially identical. For example, at the four-cell stage, all cells were presumed to be equally capable of giving rise to all cell types — a state known as symmetrical totipotency. It turns out that they are not. Embryologist Martin Johnson’s group, working at the University of Cambridge, UK, in the late 1970s and 1980s, had developed the ‘polarization hypothesis’. This suggested that differences that developed in each individual cell of the early embryo could become the basis of distinct cell populations, or lineages. Zernicka-Goetz subsequently strove to understand the importance of asymmetry in establishing which cells become what, where, why and how. She confirmed that the early embryo’s famous plasticity derives not from uniformity, but from divergence.   Several tools enabled Zernicka-Goetz to track pattern formation at the start of development. One was green fluorescent protein, a jellyfish molecule that glows under ultraviolet light and can be used to tag individual cells so that their movements can be mapped. This tool enabled her team to follow the different fates of embryonic cells during the first few stages of cell division. The refinement of this laborious yet powerful methodology forms a major strand of the book. The other crucial instrument was the theoretical model of asymmetry as the origin of form, provided by mathematician Alan Turing. In his 1952 paper ‘The Chemical Basis of Morphogenesis’, written two years before his death, Turing described how two interacting chemicals with different diffusion rates can create a stable pattern — a process later known as reaction–diffusion theory (A. M. Turing Phil. Trans. R. Soc. B 237, 37–72; 1952). Could it be that minor differences between the earliest embryonic cells might be enough to give rise to separate lineages of brain cells, blood cells and so on? To find out, Zernicka-Goetz needed to compare cells that divided in different ways, to test their inherited biases towards specific developmental pathways. At the start of the millennium, in what she calls a “grueling” set of experiments, the author and her team manually assembled, disassembled and reassembled colour-coded chimaeras made up of first-division mouse cells with particular polarization taken from different embryos. They used these handmade models to track and compare subsequent patterns of cell division, again with fluorescent labels. By 2005, they had followed the fates of hundreds of cells in chimaeras and several thousand in embryos. Results indicated that distinct cellular identities influenced cell fate, from the splitting of the first cell into two, and two into four. Paper after paper was rejected by sceptical reviewers, but consensus finally began to shift as the evidence became irrefutable.   To explore the dance of life further, better tracking methods were needed, including live imagery. These have materialized over the past decade, thanks to the ability to film the very earliest stages of embryo development in vitro and to extend the period under study beyond day 11 or 12, right up to the 14-day legal limit enforced in many countries for human embryos in culture. Here, researchers can peer into “the black box of implantation in culture”, as Zernicka-Goetz puts it. These techniques are complemented by rapidly expanding knowledge of the key molecular-signalling pathways involved, and the use of increasingly sophisticated organoids. These embryo-like structures, grown in culture, are now so complex that they can be induced to undergo gastrulation, the process by which cells develop layers that in life give rise to the internal structure of the organism. With gene-editing tools such as CRISPR, the ability to model, redirect and control embryogenesis has ushered in a new age of creative biology. The power of this bespoke biology has not yet been translated into clinical treatments. And the social and ethical challenges of the field remain acute. Zernicka-Goetz writes compellingly about her own experiences of a pregnancy in which the fetus was diagnosed through chorionic villus sampling with a risk of severe chromosomal abnormality. (Her son was born healthy.) A key theme is the powerful influence of this experience on her desire to understand how early embryos sort and order their cells. She also reflects on the challenges of being a female scientist and balancing work and family life, as well as the opposition she faced from her peers over her results. Zernicka-Goetz’s honest and passionate depiction of the complexity of science as a vocation will have wide appeal. It is a chronicle of the intellectual excitement of basic science, the thrill of the chase, and the intensity of the emotional ups and downs along the way to transformational discoveries. </body>
<date id = '227'>16 March 2020</date>
<url id = '228'>https://nature.com/articles/d41586-020-00591-7</url>
<title id = '228'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '228'>   Hope Jahren Vintage (2020) In 2009, palaeobiologist Hope Jahren was required to teach climate change. Initially reluctant, she soon conceived a vocation. Her compelling book uses statistics brilliantly to provoke self-examination. In sections on ‘Life’, ‘Food’, ‘Energy’ and ‘Earth’, it illuminates subjects from population growth to melting glaciers. If the whole planet consumed resources on the US scale, carbon dioxide emissions would be more than four times higher, she observes: “Using less and sharing more is the biggest challenge our generation will ever face.”    Stefano Mancuso (transl. Gregory Conti) Other Press (2020) About 400 metres from ground zero in Hiroshima, a weeping willow and other plants regrew from their roots. Revered, they are labelled hibakujumoku, “trees that suffered an atomic explosion”, an elderly Japanese diplomat translates in flawless Italian for visiting plant neurobiologist Stefano Mancuso. Later, he confesses he is a hibakusha: he survived the strike because his classroom was protected by a curtain of trees. Such anecdotes enliven Mancuso’s quirky little global history, which argues that plants “are more sensitive than animals”.    David Farrier Farrar, Straus and Giroux (2020) Fossil footprints unmasked by a 2013 storm on the English coast revealed that hominins walked beside an estuary 850,000 years ago. Although quickly erased by the tide, they inspired David Farrier to consider modern civilization’s future footprints, including Neil Armstrong’s marks on the Moon and the nuclear footprint: a geological repository for Finland’s spent fuel. This is designed to be forgotten — unlike its US equivalent, which proposes to use warning signs modelled on Edvard Munch’s 1893 painting The Scream.    Jack Price MIT Press (2020) In 1996, neurobiologist Jack Price, then at a major pharmaceutical company, was invited to fund academic research into stem-cell therapies. He declined. Now an academic himself, he is more hopeful. In 2006, Shinya Yamanaka discovered how to make ‘pluripotent’ stem cells, enabling brain-like tissue to be generated in a dish — “albeit small, misshapen and underdeveloped”, as Price notes in his clear, honest but intellectually challenging account. Today, several therapies have entered clinical trials. But how to make them affordable?    John Kay and Mervyn King Bridge Street (2020) When Christopher Columbus sought a westerly route to the Indies, “whatever counted as cost–benefit analysis in the Spanish court took no account of the possibility of a New World”, say economists John Kay and Mervyn King. They refreshingly criticize their discipline for not recognizing that its use of ‘risk’, ‘uncertainty’ and ‘rationality’ doesn’t match that of lay people. Odd, then, that their far-ranging book on “radical uncertainty” mentions Max Planck’s dalliance with economics but not Werner Heisenberg’s uncertainty principle. </body>
<date id = '228'>04 March 2020</date>
<url id = '229'>https://nature.com/articles/d41586-020-00585-5</url>
<title id = '229'>Sixteen months become a springboard to a panoramic view of the twentieth century, thanks to Michael Gordin’s elegant prose. By Pedro Ferreira.</title>
<body id = '229'>Albert Einstein worked at a university in Prague in 1911–12.Credit: History and Art Collection/Alamy Einstein in Bohemia Michael D. Gordin Princeton Univ. Press (2020) Many people skip over the fact that, from early April 1911 to late July 1912, Albert Einstein lived in Prague. “It was, after all, such a short time, and quite early in the physicist’s career,” Michael Gordin explains at the start of Einstein in Bohemia. Historians have variously dismissed those 16 months as an interlude, a sojourn and a detour. So did I — before I read this improbably good book. Multiple biographies of Einstein agree that the Prague period is notable for one main reason. Free of the heavy teaching load that had burdened him as an associate professor at the University of Zurich in Switzerland, he focused on developing his general theory of relativity. It was in Prague that Einstein came up with the idea of gravitational lensing, the concept that the pull of stars, planets and other astronomical objects would distort light rays. That idea — tool — is now central to modern astronomy, used to determine, for example, how much dark matter is hovering around clusters of galaxies. Although Einstein had yet to conjure up a dynamical theory of space-time, it was his prediction of gravitational lensing that would demonstrate the theory’s chops in 1919, when physicist Arthur Eddington and his colleagues measured how light from stars in the Hyades cluster was deflected by the gravitational pull of the Sun during a solar eclipse. At a personal level, gaining the position of professor of theoretical physics at the German University in Prague pushed Einstein into the senior echelons of academia. Just three years before, he had been a patent clerk in Bern. Interestingly, Gordin tells us, he was not the first person to be offered the job: first refusal went to one Gustav Jaumann at the German Technical University in Brno (who remembers him?), who turned it down. It was also in Prague that Einstein’s marriage to fellow physicist Mileva Marić began to fall apart. She was miserable there: snubbed as a Serbian and resentful at being dragged around for her husband’s career, then left sitting at home while he travelled elsewhere to give talks and collaborate. It was during a trip to Berlin that he began an affair with his cousin Elsa Löwenthal, who eventually became his second wife.   But that is about it. Slim pickings on which to base a whole book, I feared. Yet Gordin does something ingenious. He uses Einstein as a MacGuffin, a device that propels the plot but has little significance to the story he wants to tell. He explodes the narrative out of what he calls the “spacetime interval” of 1911–12 to follow a host of figures who were involved with Einstein in Prague, in some cases very tangentially. In so doing, he careers through the history of ideas as well as the political turmoil of Bohemia (now part of the Czech Republic) during most of the twentieth century, touching on physics, philosophy, nationhood, anti-Semitism and the rise of Prague as a centre of intellectual life. There are quirky observations, almost worthy of playwright Tom Stoppard. For example, Einstein and writer Franz Kafka probably met at a 1911 cultural soirée in the house of Berta Fanta, a “philosophically ambitious” socialite who held a salon above her husband’s pharmacy in Prague’s Old Town Square. But what really grips are the people. Take Oskar Kraus, a philosopher at the German University. Originally trained in law, he took against Einstein, writing countless articles in philosophy journals unpicking what he saw as egregious internal inconsistencies in relativity. His writing and stance foreshadowed the anti-relativity strand of the Deutsche Physik movement, an eviscerating force in German academia during the rise of the Third Reich. Kraus, who had been born into a Jewish family but converted to Protestantism, was arrested by the Gestapo and ultimately fled to Oxford, UK. Inevitably, Gordin takes in Ernst Mach, who had been in a post similar to Einstein’s at the German University’s forerunner from 1867 to 1895. Mach had been “the most successful physicist in the university’s history” and played an important role as rector for part of his tenure. But, like Einstein, he had been the second choice for the post. Mach’s ideas shaped the work of important relativists after Einstein, such as Dennis Sciama and Robert Dicke.   Another pen portrait is of Einstein’s successor in the post, physicist and philosopher Philipp Frank. His journey through the turbulent Prague of the 1930s serves as spotlight on a place battered by historical forces. During the late 1920s and early 1930s, Frank was part of the Vienna Circle, a hugely influential group of scientists and philosophers that also included philosopher Rudolf Carnap and mathematician Kurt Gödel. In Prague, Frank did much to carry the flame of both Einstein and Mach’s ideas through books and journal articles, publicly sparring with Kraus whenever necessary. In 1938, he had to flee to the United States, where he ended up at Harvard University in Cambridge, Massachusetts, and wrote one of Einstein’s first and most notable biographies. This is a panoramic view of twentieth-century Bohemia, with a sprinkling of Einstein. But what really carries it through is the beauty and force of Gordin’s prose. </body>
<date id = '229'>02 March 2020</date>
<url id = '230'>https://nature.com/articles/d41586-020-00510-w</url>
<title id = '230'>After a century of digging, archaeologists are still tantalized by the secrets of the 7,000-year-old city of Megiddo. By Andrew Robinson.</title>
<body id = '230'>In the 1920s and 1930s, a hydrogen balloon photographed the Megiddo site from the air.Credit: Oriental Institute of the University of Chicago Digging up Armageddon: The Search for the Lost City of Solomon Eric H. Cline Princeton Univ. Press (2020) “Welcome to Armageddon,” say Israeli tour guides, as groups from many countries climb the steep incline of the archaeological mound Tel Megiddo, southeast of Haifa and close to Nazareth. Within it are the remains of at least 20 cities, piled up, dating from about 5000 bc to the fourth century bc. Passing through the current gate, the tourists often burst into hymns or prayer. “Our small group of archaeologists smile tolerantly,” recalls Eric Cline in Digging Up Armageddon. They have been digging since before dawn to avoid the heat. A chain-link fence around the excavations jokingly requests: “Please do not feed the archaeologists.” Archaeologists digging at Tel Megiddo in Israel in 2018.Credit: Amir Cohen/Reuters Thus opens an original and lively study that skilfully mixes archaeology with personalities, and politics with culture, science and technology — such as the pioneering 1929 use of a crewless hydrogen balloon to photograph an archaeological site from the air. Dry and detailed analysis of strata and objects mingles with heroic archival excavation of biographical information, personal anecdotes and interpersonal struggles, beginning with the first dig, in 1903–05. The whole benefits from Cline’s personal experience. Over ten seasons from 1994 to 2014, starting as a volunteer, he dug in most areas opened up by a Tel Aviv University expedition, rising through the ranks to become co-director with Israel Finkelstein.   The book, however, focuses firmly on 1925–39, the most revealing period of excavations. These were run by the Oriental Institute at the University of Chicago, Illinois, under its inaugural director, Egyptologist James Henry Breasted, who coined the phrase ‘Fertile Crescent’. Funded by business magnate John D. Rockefeller, the digs were set against the troubled political background of the British Mandate for Palestine, a territory established after the defeat of the forces of the Ottoman Empire at the Battle of Megiddo in 1918. Excavations ceased in 1939, when many team members enlisted in the armed forces, “trying to stop the modern world from heading down the road toward a new Armageddon”. In 1948, the year Israel was founded, the expedition’s dig house was looted and accidentally burnt down. The Tel Megiddo site holds the remains of cities stretching back 7,000 years.Credit: Jon Arnold Images/Alamy Breasted and Rockefeller were fired up by the legend of Armageddon in the Bible, which originally refers to the place as Megiddo, supposedly built by King Solomon in the mid-tenth century bc. Solomon is recorded only in scripture, but Megiddo is mentioned in many other ancient texts, such as the records of Egyptian pharaoh Thutmose III, whose armies captured the city in 1479 bc. ‘Armageddon’ derives from Hebrew Har Megiddo, meaning the mound or mountain of Megiddo. By medieval times, having passed through multiple languages, these two words had transformed into Harmageddon and thence Armageddon. In the New Testament’s Book of Revelation, Armageddon witnesses the ultimate battle between the forces of good and evil before the Day of Judgement — hence its modern use as a byword for the end of the world.   Thus the excitement in 1928, when the expedition’s field director cabled Breasted in Chicago: “BELIEVE HAVE FOUND SOLOMONS STABLES”. As evidence, the telegram cited the Old Testament, which states that Solomon had 1,400 chariots and 12,000 horsemen stationed in “chariot cities” Hazor, Megiddo and Gezer, and in Jerusalem. The New York Times ran an article that launched Megiddo into “the limelight of biblical archaeology, where it has remained ever since”, notes Cline. The telegram sent in 1928 claiming discovery of ‘King Solomon’s Stables’ cited Bible passages in its first line.Credit: Oriental Institute of the University of Chicago But, as so often in archaeology, debate continues. The layout matched stables, but no horse bones have been discovered; and although grain was supposedly found, no analysis of it was ever published. Nor do any inscriptions from Megiddo mention Solomon. Current excavators favour a construction date in the first half of the eighth century bc, during the reign of Jeroboam II; a few think the structure is not a stable, but storehouses or barracks. Overall, Cline cautions: “Solomonic Megiddo has been extremely difficult to find.” Another contentious issue arose from an older stratum that revealed fire-blackening and crushed skeletons, including that of a young girl lying where she had been hit by a falling wall. But what devastated Megiddo in this period? One 1930s excavator postulated a “violent siege and fire by the incoming Philistines, probably circa 1190 bc”. The existence of the Philistines is attested by archaeological evidence elsewhere. However, no arrowheads or other weapons were found in or near the Megiddo bodies; nor were there sword marks on the skeletons. In addition, the walls had been misaligned by forces greater than could have been exerted by humans, even humans with battering rams. Moreover, the layer belongs to the tenth century bc, according to twenty-first-century radiocarbon dating.   Cline and others suggest that there was a major earthquake. These certainly occur in the eastern Mediterranean — for example, at Jericho in 31 bc and ad 1927 — as Cline explored in his 2014 book 1177 bc: The Year Civilization Collapsed. However, ancient earthquakes are notoriously hard to authenticate without a contemporary written record. The only certainty, writes Cline, is that the destruction “was an Armageddon for the inhabitants, regardless of whoever or whatever caused it”. Megiddo was finally abandoned just before 300 bc. At least one scholar has proposed that Alexander the Great destroyed the city, “but there is no evidence for such a cinematic finale”, jokes Cline — despite the site’s long military history. It’s more likely that Alexander’s army marched past the towering unoccupied mound, unaware of its already historic significance. </body>
<date id = '230'>26 February 2020</date>
<url id = '231'>https://nature.com/articles/d41586-020-00509-3</url>
<title id = '231'>Public acclaim escaped one of the twentieth century’s most illustrious astronomers, Cecilia Payne-Gaposchkin; a new biography sets her in the firmament. By Giuseppina Fabbiano.</title>
<body id = '231'>Cecilia Payne-Gaposchkin investigated stellar composition for her PhD research.Credit: Science History Images/Alamy What Stars Are Made Of: The Life of Cecilia Payne-Gaposchkin Donovan Moore Harvard Univ. Press (2020) Male astronomers often achieve a popular fame that eluded one of the field’s most distinguished women: Cecilia Payne-Gaposchkin. That should be remedied by Donovan Moore’s engaging and accessible biography. It skilfully opens up Payne’s achievements and adventures by setting them in the global village of astronomy, against the turbulent social and historical backdrop of twentieth-century Europe and the United States. In 1925, Payne was the first person to be awarded a PhD in astronomy at Radcliffe College, at the time the women’s branch of Harvard University in Cambridge, Massachusetts. Her thesis on stellar atmospheres is her greatest contribution: she related the line patterns in the observed spectra of stars to their physical conditions. She also discovered that hydrogen is the main component of stars, followed by helium. Her discoveries and expertise were eventually recognized with prizes and honours, culminating in a life-achievement lectureship from the American Astronomical Society.   The brilliance of Payne’s thesis was acknowledged by the most prominent US astronomers of the early twentieth century: her supervisor, Harlow Shapley, director of the Harvard College Observatory; and Henry Norris Russell at Princeton University in New Jersey. But both disagreed that hydrogen is the main component of stars. She based her theory on painstaking analysis of the large cache of stellar spectra in the Harvard collection. It was informed by the predictions of Indian physicist Meghnad Saha’s theory of ionization, which relates the observed spectrum of a stellar atmosphere (assuming it is a gas in thermal equilibrium) to its temperature, pressure and composition. Her conclusion went against a view widely espoused by prominent astronomers, including Arthur Eddington: that stars are made up of essentially the same elements as Earth (silicon, carbon, iron and so on). In response to this criticism, and because she was anxious to get her results published, Payne downplayed her finding as a possible error. Russell was later credited with the discovery, having reached the same result by different means. Payne’s role stayed hidden from the wider scientific consciousness for several decades. Moore illuminates Payne’s development into a remarkable scientist. Her mother had extensive interests outside the home — a rarity in upper-class Edwardian England. Hours spent in the household’s library equipped Payne with a knowledge and appreciation of the classics, music and theatre. After the First World War, although shy, she won over mentors and sponsors to help her study physics at the University of Cambridge (which did not award degrees to women at the time). There, she was influenced by Eddington and atomic physicist Ernest Rutherford.   In 1923, Payne set sail for the United States at Shapley’s invitation, having secured funding to become a research scientist at Harvard College Observatory. Shapley promoted her talents, but also exploited her. He encouraged her research and at her request hired Sergei Gaposchkin, a Russian astronomer escaping Europe just before the Second World War, who eventually became her husband. But he paid her poorly, so that she and Gaposchkin could not afford childcare, and their three children were often seen playing at the observatory. He also kept her low in the professional pecking order, even advising another institution against hiring her to a prominent position. She seems to have accepted this as a fact of life, but I wonder how things would have turned out had she received proper professional recognition earlier. It was only in 1954, after Shapley was replaced as director by Donald Menzel, that she received a reasonable salary. Two years later, at the age of 56, she was awarded a Harvard professorship. Payne remained at the observatory until her death in 1979. The friends she amassed read like a who’s who of early-twentieth-century physics and astronomy. They included those responsible for pushing the new atom theory (Rutherford, J. J. Thomson) and astronomers who were instrumental, with her, in opening up the astrophysical approach to understanding the Universe and its components (Shapley, Russell, Menzel, Eddington). A reproduction of Payne’s portrait in oils now hangs prominently in the lobby of the Harvard–Smithsonian Center for Astrophysics — a belated tribute to her hard work and dedication.   I met Payne in the mid-1970s. I remember her as a stern, chain-smoking presence stalking the halls of the observatory: she scolded me for being late for a meeting (recently arrived from Italy, I regarded being precisely on time as impolite). After reading Moore’s well-researched book, I realized that she was a complex figure with whom I can empathize despite being two generations younger and from a different background. A committed scientist and mentor to a new generation, she successfully juggled career and family with a love of the arts and world travel. Her autobiography (published privately as The Dyer’s Hand in 1979, and publicly as Cecilia Payne-Gaposchkin in 1984), is worth a read for its personal view of her multifaceted life and her interaction with observatory colleagues, including the female ‘computers’ who processed astronomical data. I also recommend for its immediacy her 1968 interview for the American Institute of Physics oral-history programme, conducted by Harvard astronomer and historian Owen Gingerich (see go.nature.com/37nm0vr). It captures her essential briskness and rare ability to talk in complex and nuanced sentences. </body>
<date id = '231'>24 February 2020</date>
<url id = '232'>https://nature.com/articles/d41586-020-00445-2</url>
<title id = '232'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '232'>   Jeffrey Rediger Flatiron (2020) An experienced physician who is also a skilled, driven and compassionate writer is a winning combination. This pioneering book by psychiatrist Jeffrey Rediger analyses unexplained spontaneous recoveries from potentially fatal medical conditions, including cancer. From interviewing patients over nearly two decades, Rediger concludes that each recovery was “unique” and only partially explicable, but that all provide evidence of “a powerful link” between our identities and our immune systems.    Ilan Kelman Oxford Univ. Press (2020) Human choices cause disasters, but can also prevent them, argues Ilan Kelman in this grimly informative history. A specialist in disasters and health, he surveys earthquakes, epidemics, floods and more in a range of countries. Thus, in 2010, a magnitude-7.1 earthquake near Christchurch, New Zealand, caused not a single death. The same year, a magnitude-7.0 quake in Haiti caused at least 100,000 fatalities and a cholera outbreak — because of poor buildings and health care. Scientific foresight and political will are always key to resilience.    Matt Gaw Elliott & Thompson (2020) The Milky Way is invisible to 77% of today’s UK population because of artificial light, notes naturalist and journalist Matt Gaw: “Many adults and children, my own included, have never seen it.” Such thoughts inspired this poetically written but scientifically grounded study of darkness and its effect on humans and wildlife. Gaw describes night wanderings on English beaches, across Dartmoor and in central London. On the Scottish island Coll, a Dark Sky Community without a single street light, his children were entranced by the stars.    Peter Westwick Oxford Univ. Press (2020) In 1961, Dwight Eisenhower warned in his last address as US president that the “military–industrial complex” must be checked for the sake of “security and liberty”. Historian Peter Westwick is more positive in his incisive narrative of the top-secret 1970s invention and construction of the stealth plane F-117. Nearly invisible to Soviet-designed radar, it was used to crucial effect in the 1991 Gulf War. Westwick argues that it offered an alternative to nuclear weapons, but admits that “to defend American liberties, aerospace engineers gave up civil liberties”.    Syukuro Manabe & Anthony J. Broccoli Princeton Univ. Press (2020) The first global climate model, developed in 1896 by chemist Svante Arrhenius, included the warming effect of atmospheric carbon dioxide. In the 1960s, meteorologist Syukuro Manabe pioneered computer simulation of climate change. Manabe’s book written with atmospheric scientist Anthony Broccoli has evolved from his lecture notes, with chapters on, for example, general circulation models. Although technical, it should prove useful to those wishing to understand global warming’s future impact. </body>
<date id = '232'>19 February 2020</date>
<url id = '233'>https://nature.com/articles/d41586-020-00436-3</url>
<title id = '233'>As civilization seems to be lurching towards a cliff edge, historical case studies are giving way to big data in authors’ search for understanding. By Laura Spinney</title>
<body id = '233'>Monuments to resilience or collapse? The 800-year-old statues of Easter Island.Credit: Andia/Universal Images Group via Getty In case you missed it, the end is nigh. Ever since Jared Diamond published his hugely popular 2005 work Collapse, books on the same theme have been arriving with the frequency of palace coups in the late Roman Empire. Clearly, their authors are responding to a universal preoccupation with climate change, as well as to growing financial and political instability and a sense that civilization is lurching towards a cliff edge. Mention is also made of how big-data tools are shedding new light on historical questions. But do these books have anything useful to share? Any actionable points besides that on my coffee mug: “Now panic and freak out”?   The newest is Before the Collapse. In it, energy specialist Ugo Bardi urges us not to resist collapse, which is how the Universe tries “to get rid of the old to make space for the new”. Similarly, Diamond’s 2019 book Upheaval suggested that a collapse is an opportunity for self-appraisal, after which a society can use its ingenuity to find solutions. Both writers seem to accept that collapse is inevitable, but they take very different approaches to analysing it. Diamond zooms in to glean lessons from historical case studies; Bardi zooms out to view societies as complex dynamic systems that behave cyclically. Numerous books published in the past few decades chart how research has shifted from Diamond’s approach to Bardi’s. Collapse: How Societies Choose to Fail or Succeed Jared Diamond Viking (2005) Before the Collapse: A Guide to the Other Side of Growth Ugo Bardi Springer (2020) Upheaval: Turning Points for Nations in Crisis Jared Diamond Little Brown (2019) Questioning Collapse: Human Resilience, Ecological Vulnerability, and the Aftermath of Empire Edited by Patricia A. McAnany & Norman Yoffee Cambridge Univ. Press (2009) The Collapse of Complex Societies Joseph Tainter Cambridge Univ. Press (1988) Understanding Collapse: Ancient History and Modern Myths Guy D. Middleton Cambridge Univ. Press (2017) Why the West Rules — for Now: The Patterns of History, and What They Reveal About the Future Ian Morris Farrar, Straus and Giroux (2010) War and Peace and War: The Rise and Fall of Empires Peter Turchin Pi (2006) Revolution and Rebellion in the Early Modern World Jack Goldstone Univ. California Press (1991) Questioning Collapse, a 2009 collection of essays edited by archaeologists Patricia McAnany and Norman Yoffee, took Diamond to task for cherry-picking to spin a good yarn, for example in blaming such iconic societal failures as the population crash of Easter Island on its people’s destruction of their own environment. The story is not so simple, the authors argue. The Indigenous Rapa Nui society weathered a string of environmental crises — very few of its own making — yet thrived until the first Europeans arrived. Likewise, is it reasonable to claim that Mayan society collapsed around the ninth century, given that seven million people living in and around Central America speak Mayan languages today? These cases might be better viewed, say McAnany and Yoffee, as lessons in resilience. East Africa is currently battling the biggest locust swarms in decades.Credit: Njeri Mwangi/Reuters Scholars have long warned against peering down the ‘retrospectoscope’ at apparently neat examples of what not to do. In his influential 1988 The Collapse of Complex Societies, archaeologist Joseph Tainter argues that collapse — in the sense of the complete obliteration of a political system and its associated culture — is rare. Even the worst cases are usually better described as rapid loss of complexity, with remnants of the old society living on in what rises from the ashes. After the ‘fall’ of Rome in the fifth century, for example, successor states took more than 1,000 years to achieve comparable economic and technological sophistication, but were always recognizably the empire’s offspring.   Nevertheless, societies do go through rocky patches, from which some emerge transformed. It’s not surprising that scholars should want to understand why. In his thoughtful Understanding Collapse (2017), archaeologist Guy Middleton surveys more than 40 theories of collapse — including Diamond’s — and concludes that the cause is almost always identified as external to the society. Perennial favourites include climate change and barbarian invasions — or, in the Hollywood version, alien lizards. The theories say more about the theorists and their times, Middleton argues, than about the true causes of collapse. The pressing question, Tainter told a workshop on collapse at Princeton University in New Jersey last April, is why can a society withstand repeated external blows — until one day it cannot? For him, a society fails when it is no longer able to adapt to diminishing returns on innovation: when it can’t afford the bureaucracy required to run it, say. In Why the West Rules — For Now (2010), historian Ian Morris proposes a twist on this, namely that the key to a society’s success lies in its ability to capture energy — by extracting it from the ground, for example, or from nuclear fission once fossil fuels have run out. By contrast, Peter Turchin, author of the 2006 War and Peace and War, suggests that collapse is what happens when a society stops being able to deal with the strains caused by population growth, leading to inequality and strife.   Turchin has been compared to Hari Seldon, science-fiction writer Isaac Asimov’s “psycho-historian”, who studies the past to statistically predict the future. He belongs to a new breed of scientific historian taking a big-data approach, and argues — controversially — that societal spasms are cyclic. This idea itself comes and goes: the ancient Greeks took the cyclic nature of history for granted, but it has been unfashionable since the Enlightenment. Today, we tend to have a linear concept of progress, in which life generally improves for most people over the long term. Works such as Turchin’s see this trend as superimposed on an inherent cyclicity in the evolution of societies. This raises the question of whether collapse is essential to renewal. Without winter, can you have spring? Bardi says no. Whether you think this good or bad depends partly on your point of view. The mass extinction 66 million years ago was bad for dinosaurs, but good for mammals, sociologist Miguel Centeno observed at the Princeton workshop, which he convened. But if collapse could usher in not only a renewed world, but a better one, shouldn’t we dinosaurs embrace it? Close to 40% of Guatemalans are Mayan, yet historians talk about the collapse of Mayan societies.Credit: J. Emilio Flores/Corbis via Getty For Turchin and Jack Goldstone — on whose work on the demographic forces shaping history Turchin builds — this is good advice only if you understand what causes collapse. Then it might be possible to make the transition less violent or disruptive. Goldstone rigorously dissected upheaval in the sixteenth to the nineteenth centuries in his 1991 book Revolution and Rebellion in the Early Modern World. This convinced him that revolution is an inappropriate response to societal tensions, usually leading to tyranny. Solutions have come instead from deep, meaningful reform. Yet the idea that revolution removes obstacles to progress has “deluded literally billions of people”, he argues. An interdisciplinary community of researchers is now searching for patterns that have defined collapse throughout history, to determine what might be an appropriate response. If we can’t and shouldn’t prevent a future crisis, could we at least soften it — perhaps with the help of new technologies — so that renewal happens, but less is lost and fewer people suffer? Even if the mind-boggling complexity of human societies makes this a pipe dream, as some argue, it seems a sounder approach than sparring over case studies that might not have constituted collapse at all. Speaking as a dinosaur, whose only alternative is to panic and freak out, I’ll take it. </body>
<date id = '233'>18 February 2020</date>
<url id = '234'>https://nature.com/articles/d41586-020-00356-2</url>
<title id = '234'>A physicist and humanist takes us on a grand tour of all time. By Philip Ball</title>
<body id = '234'>A cloud of interstellar gas and dust, captured by NASA’s Hubble Space Telescope.Credit: M. Livio and the Hubble Team (STScI)/NASA/ESA Until the End of Time: Mind, Matter, and Our Search for Meaning in an Evolving Universe Brian Greene Penguin (2020) Brian Greene’s Until the End of Time sits within a tradition of grand, synoptic visions of the Universe, rooted in physics, that feels (to this British reader) distinctively American. Halfway through, I realized why. With its scepticism of religion but openness to humanistic wonder, awe of nature, celebration of the individual and recognition of the power of physical law, the narrative has a strong whiff of transcendentalism. There is an echo of philosopher Henry David Thoreau in Greene’s account of lying out at night, enraptured by the aurora borealis. And essayist Ralph Waldo Emerson’s declaration that the “sublime laws play indifferently through atoms and galaxies” could almost be this book’s epigraph. Such qualities lift this work above many accounts of the cosmic story spanning from the Big Bang to the end of time — whether that’s a big rip, heat death or cosmic bounce. Greene takes us from quarks to consciousness, and from the origin of life to the genesis of language. He draws from an impressive range of sources, such as poets William Butler Yeats and Sylvia Plath. In attempting to weave in the evolution of physical laws with that of the human mind and cultures, Greene’s aim vaults beyond that of his bestselling 1999 book, The Elegant Universe. Until the End of Time is packed with ideas; whether they come together as a convincing story is another matter.   This narrative features humanity as a brief moment when matter became self-aware. Current physical and cosmological theories imply that this state of affairs can’t last. Eventually proton decay, a dominance of dark energy or thermodynamic heat death will doom all matter and thought. Greene, however, suggests that intelligent beings could eke out their thought processes almost indefinitely by gradually slowing them to minimize their inevitable thermodynamic cost. He views this extinction of sentience as a cosmic tragedy. It’s poignant to see a modern physicist, however girded with string theory, the general theory of relativity and the equations of quantum mechanics, experience the same anguish that goaded ancient monarchs to defy mortality by commissioning monumental tombs. Greene finds the solace that religion typically provides in the idea that the “small collection of the universe’s particles” that constitutes humanity can evolve and “with a flitting burst of activity create beauty, establish connection, and illuminate mystery”. His grand tour is sometimes breathtaking, necessarily selective and occasionally superficial. It often lacks the space or rigour to do its vast range of subjects justice. Beyond fundamental physics, Greene is a lucid summarizer of other popular accounts, but little more. That can leave his story patchy, and even misleading at times. His explanation for why water is a special solvent required for life attributes it all to the molecule’s polar nature — in which case it would not be special at all. (Hydrogen bonding is left out, and although that does not tell the whole story, neglecting it means we get almost no story at all.) To explain the origin of myths, the book offers a bit of obsolete early-twentieth-century anthropology from the likes of folklorist James George Frazer, that is given a contemporary gloss of evolutionary psychology. When it comes to the human impulse to dance — as participants do here at an Indigenous arts festival in Toronto, Canada — are cosmology and quantum mechanics a meaningful part of the narrative?Credit: Steve Russell/Toronto Star via Getty The biggest shortfall is in the account of how biology works, which seems to be derived largely from physicist Erwin Schrödinger’s 1944 book What Is Life? and biologist Richard Dawkins’s 1976 The Selfish Gene. Life in Greene’s reckoning is all encoded in the genome, and once molecular replicators appeared on the planet, the rest was just evolutionary history. He adds that non-equilibrium thermodynamics can give us a head start: its tendency to create spontaneous knots and patterns of local order are a stepping stone towards life’s organization. But what’s missing — foreshadowing a wider lacuna in the book — is any sense that intermediate levels of that organization, particularly the cell, are equally fundamental.   When it comes to human behaviour — creativity, art, story, religion — Greene places a reductive faith in evolutionary psychology. He is probably right to say that many of our complex behaviours are underpinned by rather basic adaptive impulses, but he doesn’t adequately acknowledge how culture shapes them. For instance, he supports psychologist Steven Pinker’s notorious description of music as “auditory cheesecake”. This posits that music is enjoyable because it piggybacks on capacities that evolved for other reasons, such as the ability to separate our auditory experience into comprehensible chunks. This might or might not be true, but to appreciate what music really means, we need to consider its cultural, historical and social specifics, and not just attribute it to “our ancient adaptive sensitivity to sounds with elevated information content”. Whether in cell biology or a musical tradition, asking why any specific feature is the way it is demands that we consider a causal explanation. And therein lies the problem with Greene’s approach: where it seeks out cause. It’s true that when he enlists physics as the underpinning theory of everything (“Life is physics orchestrated”), this is not the physicist’s standard hubristic claim. Indeed, he points out that we need “overlapping narratives” for explanations of phenomena at different scales of size and complexity, from subatomic particles to galaxies, each of which must at least be consistent with the one below. And Greene acknowledges that an account of human behaviour at the level of fundamental particles would be pointless. But he still implies that causation flows upwards through the hierarchy of scales. We lack genuine free will, he says, because there is no such factor at play among the fundamental forces.   Thus, Greene remains wedded to the idea that the most reductive view has ultimate authority — that it all comes down to particles, entropy and evolution. “Perhaps one day we will invoke a unified theory of particulate ingredients to explain the overwhelming vision of a Rodin,” he writes. He doesn't recognize that in complex systems, new properties and causative mechanisms that arise at only the higher levels of the hierarchy are as real and fundamental as, say, the strong and weak nuclear forces. This is what physics Nobel laureate Philip Anderson argued in his 1972 essay ‘More Is Different’. If we accept Anderson’s position, we have to call into question the entire programme that Greene articulates here. By the time we get to, say, the human impulse to create stories, are Big Bang cosmology and quantum mechanics meaningful parts of the narrative? Perhaps, then, by setting out a vision of the world as seen by a thoughtful, humanistic fundamental physicist, Greene has offered not so much a state-of-play panorama as a tour showing where that view works spectacularly and where it falls short. It is an eloquent invitation to debate. </body>
<date id = '234'>10 February 2020</date>
<url id = '235'>https://nature.com/articles/d41586-020-00273-4</url>
<title id = '235'>From tobacco to food and fuels, industries use denial, deceit and doubt to corrupt. By Felicity Lawrence.</title>
<body id = '235'>Sugar: multinationals have used the tobacco-industry playbook to stymie legislation aimed at cutting consumption.Credit: James MacDonald/Bloomberg via Getty The Triumph of Doubt: Dark Money and the Science of Deception David Michaels Oxford Univ. Press (2020) In 2017, US presidential strategist Kellyanne Conway coined the phrase “alternative facts” to defend false claims about the size of the crowd at Donald Trump’s inauguration. Numerous commentators lamented that we were entering a new era of Orwellian doublethink. These are indeed upside-down times, as epidemiologist and former safety regulator David Michaels demonstrates in his excoriating account of the corporate denial industry, The Triumph of Doubt. Unwelcome news is automatically rebranded fake news. Inconvenient evidence from independent sources — say, about climate breakdown and fossil fuels, or air pollution and diesel emissions — is labelled junk science and countered with rigged studies claiming to be sound. But it would be wrong to see truth decay solely as the preserve of today’s populist politicians. Normalizing the production of alternative facts is a project long in the making. Consultancy firms that specialize in defending products from tobacco to industrial chemicals that harm the public and the environment have made a profession of undermining truth for decades. They hire mercenary scientists to fulfil a crucial role as accessories to their misrepresentations. Michaels was among the first scientists to identify this denial machine, in his 2008 book Doubt is Their Product. His latest work combines an authoritative synthesis of research on the denial machine published since then with his own new insights gleaned from battles to control the toxic effects of a range of substances. He takes on per- and polyfluoroalkyls, widely used in non-stick coatings, textiles and firefighting foams; the harmful effects of alcohol and sugar; the disputed role of the ubiquitous glyphosate-based pesticides in cancer; and the deadly epidemic of addiction to prescribed opioid painkillers. In each case, Michaels records how the relevant industry has used a toolbox of methods to downplay the risks of its products, spreading disinformation here, hiding evidence of harm there, undermining authorities — all tactics from the tobacco industry’s playbook.   The doubt in the title of both Michaels’s books derives from a now-notorious memo written in 1969 by an unnamed executive at a subsidiary of British American Tobacco. It outlined a strategy for maintaining cigarette sales: “Doubt is our product since it is the best means of competing with the ‘body of fact’ that exists in the minds of the general public. It is also the means of establishing a controversy.” By creating scientific disinformation about links between tobacco and disease, this malign strategy delayed regulation by decades and protected corporate profits. Michaels’s insider perspective on the doubt machine dates back to 1998, when he became chief safety officer for nuclear-weapons facilities at the US Department of Energy during the administration of US president Bill Clinton. Here, he had a ringside view of the tricks used by vested interests to dispute established science, intimidate the authorities and scupper regulation. In his first book, he described how the ‘product defence industry’ applied the tobacco template to asbestos, lead, plastics and toxic materials such as beryllium used in nuclear applications. From 2009 to 2017, Michaels served as a senior regulator, appointed by president Barack Obama, in the Occupational Safety and Health Administration (OSHA). Here, he gathered even more material to show how deceptions have infected the body politic. A man prays in foam caused by pollution in the Yamuna River in New Delhi.Credit: Dominique Faget/AFP via Getty The principles of scientific inquiry involve testing a hypothesis by exploring uncertainty around it until there is a sufficient weight of evidence to reach a reasonable conclusion. Proof can be much longer in coming, and consensus still longer. The product-defence industry subverts these principles, weaponizing the uncertainty inherent in the process. Its tricks include stressing dissent where little remains, cherry-picking data, reanalysing results to reach different conclusions and hiring people prepared to rig methodologies to produce funders’ desired results. Michaels acknowledges other doubt scholarship. This includes that of science historians Naomi Oreskes and Erik Conway in Merchants of Doubt (2010); nutritional scientist Marion Nestle’s numerous books on the food industry, such as Soda Politics (2015) and Unsavory Truth (2018); and journalist Jane Mayer’s 2016 Dark Money. That last book traced the funding that links climate-change denial to the libertarian right’s ideological drive to shrink the state and deregulate industry. Michaels names names fearlessly, pointing the finger at product-defence practitioners and the front groups and think tanks that masquerade as independent while taking industry’s shilling. Those wanting to check his allegations can find many previously unavailable source documents archived at the Triumph of Doubt Special Collection at https://toxicdocs.org. So much of his material outrages, but two episodes stand out. One is the German car manufacturer Volkswagen’s brazen malfeasance regarding its diesel engines. The company developed secret software so these engines could cheat emissions tests, allowing its vehicles to fraudulently pass stringent US checks on the disease-causing particulates in diesel exhaust. This was unintentionally uncovered in 2014 by students working on behalf of the campaign group the International Council on Clean Transportation in Washington DC, and confirmed the following year by the US Environmental Protection Agency. Michaels’s account of the scientists prepared to launder their data to abet this criminal activity is forensic.   The second standout is his description of his years-long battle at the OSHA to reduce workers’ exposure to silica particles from sand used in dozens of industries, from construction to steel manufacture and fracking. He and at least 50 staff members worked to collate evidence and counter a barrage of pseudoscientific objections and litigation. Michaels, no longer required to be a non-partisan government official, reserves special criticism for the Republican Party. He argues that corporate polluters and manufacturers of dangerous products have long depended on the party to neuter public-health and regulatory agencies with phoney rhetoric about liberty and free-market enterprise. He wants stronger regulation, not because he does not care about freedom, he says, but because we cannot be free without the state’s protection from harm. The Triumph of Doubt is at times dense with technical detail, of necessity as Michaels prosecutes his case against companies known to be litigious. It is a brave and important book, raising the alarm about the systemic corruption of science. </body>
<date id = '235'>03 February 2020</date>
<url id = '236'>https://nature.com/articles/d41586-020-00111-7</url>
<title id = '236'>Social context affects how we act on issues such as climate change. Policymakers ignore it at their peril, a book argues. By Thomas Dietz.</title>
<body id = '236'>US Army nurses in 1947. Shifting social norms have driven the swift rise and demise of smoking in many places.Credit: Underwood Archives/UIG/Shutterstock Under the Influence: Putting Peer Pressure to Work Robert H. Frank Princeton Univ. Press (2020) In 1989, just 12% of US adults favoured legalization of same-sex marriage; by 2015, that figure was around 60%. What triggered the transformation? In Under the Influence, economist Robert Frank reveals that peer pressure lies behind many such step changes. Once views began to shift, the process was self-reinforcing. As Frank drives home, we humans are especially adept at learning from our peers. Our decisions are strongly influenced by social norms — what we think others are doing, and what we think they think we should do. In some circumstances, we can be self-interested; in others, we can be altruistic. So it’s not surprising that much of social-science research focuses on social context in decision-making. Frank reviews extensive evidence from studies across a number of disciplines on how peer pressure shapes the dynamics of smoking, drinking, obesity, consumerism and many other important social issues. Because the tendency to emulate can lead to rapid social change, for better or worse, it is a key lever for policy. Yet, asserts Frank, that message has yet to reach many policy analysts and economists. Under the Influence offers a corrective through compelling arguments for incorporating social contexts into the design of policy on climate change, public health, the financing of public goods, social justice, taxation and beyond.   Among the cascades of change Frank examines are ‘arms races’, which can focus on anything from nuclear weapons to consumer goods. They are a type of commons dilemma or collective-action problem: the pursuit of narrow self-interest leads to overuse of a resource, and disaster. (If foresters, for instance, limit the number of trees they fell every year, the forest can regenerate, to the benefit of all; if they each boost their own short-term profits by maximizing their felling, the forest ecosystem might collapse.) But in an arms race, what matters is not your absolute measure of resources. It is what you have compared with what I have. Thus, everyone has an incentive to accumulate resources in a never-ending upward spiral. Frank points, for example, to the sharp increases in US housing prices that led to the bubble of the early 2000s. To ensure access to the best school districts, buyers competed to live in the most affluent neighbourhoods, bidding up housing costs inexorably. The result was unrealistic prices, unsustainable mortgage burdens and a slump in price that led to bankruptcies and the collapse of lenders — all of which contributed to the 2008 economic meltdown. For many people, wealth relative to others is more important than absolute spending power.Credit: Chris Ricco/Getty Frank examines another problematic arms race: the widespread opposition of the rich to increased taxation. This, he argues, hinges on what he calls the “mother of all cognitive illusions”: the belief that happiness is based on absolute wealth (and spending power), which higher taxes would slash. Frank counters that view, asserting that rich people’s well-being is based on relative wealth — their position compared with that of their peers. A tax affecting all top earners would maintain relative position, whatever the effect on absolute spending power. His analysis is timely, because low and declining US tax rates for the top income bracket have led to a loss of government revenue and, in turn, massive underinvestment in public goods such as education and infrastructure. Frank suggests a remedy: taxing consumption (income minus savings) for the wealthiest. One of the great strengths of Under the Influence is Frank’s use of research from across the social sciences, including psychology and political science. Yet he fails to engage with much that’s salient to his arguments here. For instance, regarding policy challenges such as climate change and obesity, he admits that his “deepest passion” is efficiency — that is, he favours taxation over regulation. Thus, he adopts a standard utilitarian approach to decision-making. To demonstrate the success of this approach, he cites the US policy that placed a price on sulfur dioxide emissions from 1995, significantly reducing levels of acid rain. But when he discusses the importance of in-depth deliberation in resolving conflicts, and in changing individual views on gay rights and environmental protection, he does not mention the extensive literature on how deliberative processes can underpin good decision-making, a theory complementary to his utilitarianism. Frank’s analysis would thus benefit from even deeper digging into findings on context, social structure, power and social inter-action, such as the critique of growth dynamics in environmental sociology or the 2017 book Beyond Politics, an analysis of private environmental governance by Michael Vandenbergh and Jonathan Gilligan. For example, Frank’s argument about the well-being of the affluent resting on relative status does not factor in the possibility that rich people might be seeking political power and influence on govern-ment instead. Among the richest, power might depend on absolute wealth. Similarly, his thoughtful chapter on climate change does not fully address opposition to climate policy from powerful fossil-fuel interests.   Moreover, Frank mentions only in passing issues such as the human tendency to associate with those like us (homophily) and to affirm what we already believe (confirmation bias). In the social networks of government officials, lobbyists and others who influence policy, these tendencies lead to polarization and a lack of action on serious problems. So although Frank urges us to consider context, he misses the need to pay more attention to the structure of contexts, including inequality and power. Of course, one book, however broad its compass, cannot cover everything. And even where I felt Frank had not tackled important lines of research, those gaps point to the need to think more deeply about human actions and the policies that shape them. At a time of multiple impending crises, Under the Influence will provoke your thinking in constructive ways. </body>
<date id = '236'>20 January 2020</date>
<url id = '237'>https://nature.com/articles/d41586-020-00054-z</url>
<title id = '237'>Barbara Kiser reviews five of the week’s best science picks.</title>
<body id = '237'>   We are deluged with billions of bytes of data, yet much crucial information goes unseen and unreported. So reveals statistician David Hand in this penetrating study of missing (‘dark’) data and its impacts on decisions — skewing stats, enabling fraud, embedding inequity and triggering preventable catastrophes. Advocating “data science judo”, Hand offers expert training, from recognizing when facts are being cherry-picked to designing randomized trials. A book illuminating shadowed corners in science, medicine and policy.    Humans are less discrete entities than mash-ups of microbiota and shifting beliefs, declares ecologist Tom Oliver in this rich, intriguing book. We are, he shows, so interfused with the environment that all life might be seen as a web of genes, and all minds a web of memes. Oliver reframes the self as a fleeting union of molecules, a target for manipulation by parasites, a cooperative co-creator who is also destroying the biosphere. But by recognizing our connectedness, he argues, we enable needed societal and environmental change.    Start-ups have long been seen as a geek-driven, idealistic antidote to corporate culture. Anna Wiener’s unsettling memoir may muddy that image. In 2013, a 20-something Wiener was drawn to the digital economy of California’s Silicon Valley. Soon enough she recognized it as a reckless, male-dominated world of barely regulated surveillance. She witnessed the boom in online abuse and political trolling from the inside, and the growing inequity in San Francisco fuelled by venture capitalists. An acute eye on a dystopia in the making.    Extreme longevity might seem a seductive concept to some. To a handful of prominent researchers, it’s an experimental goal. Venturing into that rarefied world, journalist Chip Walter interviewed stars such as biotechnologist J. Craig Venter and X Prize founder Peter Diamandis. Their eventful stories are woven through Walter’s tour of biotech research centres Calico and Celularity, and fields from cryopreservation to regeneration. Results remain broadly inconclusive, but this witty look at ‘curing’ death is worth the ride.    Astronomer Fred Watson is a science communicator par excellence. Here, with infectious enthusiasm, he plunges the reader into the science on sky-watching and space observation. Kicking off with a nuanced discussion of twilight — covering everything from crepuscular rays to the ‘green flash’ — he moves on to meteor showers, the potential contamination of the Solar System’s ice moons by earthly microbes, the mystery of a hypothesized Planet Nine and the real origins of the Moon. </body>
<date id = '237'>14 January 2020</date>
<url id = '238'>https://nature.com/articles/d41586-020-00055-y</url>
<title id = '238'>Three books warn of the perils of basing policy on untested ideas</title>
<body id = '238'>US president Ronald Reagan based his tax cuts of the 1980s on questionable ideas.Credit: Bettmann When the President Calls: Conversations with Economic Policymakers Simon W. Bowmaker MIT Press (2019) Arguing with Zombies: Economics, Politics, and the Fight for a better Future Paul Krugman W. W. Norton (2020) Good Economics for Hard Times Abhijit V. Banerjee and Esther Duflo PublicAffairs (2019) In November 2017, the economist Vera Shlakman died at the age of 108. Her 1935 Economic History of a Factory Town is a landmark in the field. Chronicling how textile manufacturing transformed Chicopee, Massachusetts, Shlakman zeroed in on working women’s lives, vaulting beyond analyses of data on wages and shift lengths to include the value of dowries and information in letters and diaries. Ousted from teaching economics during the McCarthy era of the 1950s, she never published another book. I thought of Shlakman, and how far we have strayed from such integrated analyses of economic realities, while combing through Simon Bowmaker’s 2019 When the President Calls. Over the past half-century, Bowmaker shows, economic advisers to US presidents from Richard Nixon to Donald Trump have enabled central bankers and treasury officials to implement untethered ideas. Often described in terms borrowed from mathematics or physics (such as the ‘velocity of money’), these concepts neither command an expert consensus nor are they necessarily reproducible. Two other new books, both by economics Nobel laureates, also capture the spirit of Shlakman’s diverse thinking: Paul Krugman’s Arguing with Zombies, and Good Economics for Hard Times by Abhijit Banerjee and Esther Duflo. Banerjee and Duflo’s book appeared a month before they were awarded the 2019 Nobel, which they shared with Michael Kremer. It encapsulates nearly two decades of research bringing field trials of policies in low-income countries into the mainstream — from improving educational outcomes, to uptake of vaccination. Krugman’s tome, meanwhile, mulls over the mistakes of the past two decades. The ‘zombies’ of his title are economic theories and policies that should have been killed by evidence, but keep coming back — such as the idea that inequality is necessary for growth. In a world still reeling from the impact of the 2008 financial crisis, Krugman (who was awarded his Nobel that year) has harsh words for practitioners clinging to the old ways. A prominent example is the ‘Laffer curve’, named after Arthur Laffer, who later became Ronald Reagan’s economic adviser. Bowmaker clearly relished the chance to quiz Laffer, who is reported to have sketched the idea out on a napkin over a 1974 lunch with two White House officials: Donald Rumsfeld and Dick Cheney. If governments raise taxes for people with modest incomes and cut taxes for the wealthy, Laffer argued, they can raise revenue and boost growth. The former, because lower-income earners far out-number the rich, thus amplifying their total tax contribution; the latter, because wealthy business owners are likely to use cash saved from tax cuts to invest in new products, more jobs or equipment, thus boosting growth. There is no consensus on whether the Laffer curve is accurate — even some leading conservative economists, such as Gregory Mankiw, are critical. Yet it became the basis for tax cuts, beginning with Reagan’s decision to slash the top rate of income tax from 70% down to 28% over the early to mid-1980s. Bowmaker found Laffer still buoyant. “That’s my baby and I just loved it,” Laffer said. “It was the best tax bill in US history.” Trump awarded him the Presidential Medal of Freedom in 2019. As these three books reveal, however, the ideas that Laffer and others came to represent are now under severe pressure, even from the centre-right parties in high-income nations that initially backed them. Decades of falling or flat public spending, unrestricted free trade, relatively light regulations on financial institutions and low taxes on businesses and top earners have not translated into across-the-board prosperity. That is seen especially in the United States and Britain, now among the most unequal countries in their peer group. In 2016, six out of northern Europe’s ten poorest regions, as measured by gross domestic product per person, were in the United Kingdom. Abhijit Banerjee and Esther Duflo advocate field trials of economic policies, notably in low-income nations.Credit: Jim Davis/The Boston Globe via Getty Economic nationalism has emerged from these trends, under slogans ranging from “America First” to the UK “Take Back Control”. The real results are the Trump administration’s public disavowal and renegotiation of the North American Free Trade Agreement — and Brexit. At the same time, the collapse of what were once seen as mainstream economic ideas by politicians right and left has opened up space for more conventionally green-left approaches to policymaking. That explains in part why the Italian American economist Mariana Mazzucato is being heard across political divides. In books such as The Value of Everything (2018), Mazzucato makes a strong case for the state as enabler in an economic policy that privileges well-being and sustainability. That is also the space into which Banerjee and Duflo enter in Good Economics for Hard Times. Like Krugman, they are critical of policies based on weak or non-existent evidence. But their approach is less argument than stepping back to let the research do the talking. The studies they cite probe hot topics such as climate change, immigration and the viability of continued economic growth. Banerjee and Duflo synthesize the literature on what is agreed and what is controversial in an accessible, often entertaining way. There are gaps, however. Mentioning the work of Kate Raworth and Tim Jackson on the environmental impacts of constant growth, and of Partha Dasgupta on the value of biodiversity, would have enriched and unified their thinking on the impact of consumption-fuelled growth on climate change, and on biodiversity and ecosystem services. Grand narratives, which smack of the old economic thinking, are not the goal of Banerjee and Duflo. The authors do, however, need to articulate their approach in narrative terms, or they will struggle to be heard. As Bowmaker demonstrates, US presidents want to hear happy endings, rather than sit through a menu of options communicated seminar-style. Harry Truman is reported to have said that he preferred one-handed economists, because he didn’t like hearing “on the other hand”. Barack Obama was an exception, making decisions after hearing arguments pro and con. Arthur Laffer popularized a theory of the relationship between tax rates and the government income dubbed the Laffer curve.Credit: AP/Shutterstock Sadly, neither Arguing with Zombies nor Good Economics for Hard Times tackles in depth what I feel is the defining challenge for newer generations of economic policy advisers. That is, how to mitigate the risks of expert-shopping by policymakers. If researchers with fringe ideas continue to validate untested theories, yet more zombies will invade the corridors of power. This happens to scientific advisers, too — although perhaps less often. In the 1990s, governments with significant oil and gas interests joined the powerful fossil-fuel industry lobby in seeking experts who could cast doubt on human influence on climate change. The consensus view of the Intergovernmental Panel on Climate Change — backed as it was by a huge number of heavyweight researchers — was essential in preventing such dubious ideas from penetrating the mainstream (although it has not stopped Trump’s withdrawal from the 2015 Paris climate agreement). Economists need to organize similarly across different schools of thought — and to include development economics as well as ecological and environmental economics and feminist economics. There is strength in numbers and robustness in diversity. That can go some way towards curbing the unworkable concepts that continue to emerge. As these three thoughtful, timely books demonstrate in their own ways, a space has opened up for new ideas in economics at a time of widespread inequality, social and cultural schisms, and environmental crisis. That is an opportunity to avoid another 50 years of theories that inform the highest levels of policy as if evidence didn’t matter. </body>
<date id = '238'>13 January 2020</date>
<url id = '239'>https://nature.com/articles/d41586-019-03958-7</url>
<title id = '239'>Barbara Kiser reviews five of the week’s best science picks.</title>
<body id = '239'>   Hugo Mercier Princeton Univ. Press (2019) We’re a hard species to dupe, argues cognitive scientist Hugo Mercier in this persuasive study on the science of belief. Drawing on research and historical case studies from the French Revolution to the Nazis, Mercier demonstrates that we have powerful cognitive mechanisms that enable us to weigh up a range of cues. Thus governmental propaganda succeeds by riding on existing opinion, not changing it. And vigilance, not credulity, seems to make false rumours (so often about threats) stick. A bracing book that might make you less gullible about gullibility.    Juan Du Harvard Univ. Press (2020) Just 40 years after then-Chinese premier Deng Xiaoping proposed to make Shenzhen a special economic zone, it has become a conurbation of 20 million, a technology hub and a symbol of China’s economic transformation. But as urban planner Juan Du shows in this deep dive of a history, the ‘instant city’ narrative is a myth. Sweeping aside slick origin stories, Du reveals a reality in which Shenzhen’s prosperity is driven by oyster fishers, vibrant night markets and the aspirations of millions, not just by the policymakers of Beijing.    Daniel J. Levitin Dutton (2020) Neuroscientist Daniel Levitin reveals the ‘dreaded’ age of 60 to be a developmental stage with specific advantages, and a launchpad for productive elderhood. Looking in turn at the latest neuroscience, behavioural studies and findings on longevity and cognitive enhancement, Levitin delves into the multiple-trace theory of memory, the ageing microbiome, fats and the brain, the impacts of neural implants, and the joys of non-retirement. A clear-eyed, insightful overview of the neurophysiological healthspan.    Guy Crosby Columbia Univ. Press (2019) This chronology of culinary science was part-inspired, nutritionist Guy Crosby tells us, by primatologist Richard Wrangham’s theories on how fire and cooking influenced human evolution. Those are contested. Nevertheless, Crosby’s historical tour of kitchen physics and chemistry is a sprightly delight. Here are invitations to peer into the architecture of “optimally kneaded dough”, goggle at the oldest known recipe (on a 3,750-year-old clay tablet from Babylonia), wonder at a seventeenth-century pressure cooker and contemplate the terroir of Vidalia onions.    Miguel Nicolelis Yale Univ. Press (2020) The human brain, contends neuroscientist Miguel Nicolelis, is a peerless “organic computing device” that has authored a singular cosmos: human culture, from cave art to robotics. Within a framework he dubs relativistic brain theory, Nicolelis launches a mind-bending journey through neurological evolution, electromagnetism in relation to the brain, brain–machine interfaces and beyond. He warns, however, that our compulsion to monetize, and our addiction to “digital logic”, jeopardize our future. </body>
<date id = '239'>08 January 2020</date>
<url id = '240'>https://nature.com/articles/d41586-019-03956-9</url>
<title id = '240'>A sumptuous book explores how the insect finds its way. By William A. Foster</title>
<body id = '240'>Cataglyphis ants use astonishing navigational techniques to find their way back to the nest after foraging.Credit: agefotostock/Alamy Desert Navigator: The Journey of an Ant Rüdiger Wehner Harvard University Press (2020) The fear of getting lost and being unable to find our way home is woven into the stories we hear as children: it can haunt us for years. Humanity’s navigational skills are poor and increasingly rarely used, leaving us to view feats of animal navigation with a mixture of envy and admiration. How do Atlantic salmon find their way back to the streams where they were born, after up to three years at sea? How do Arctic terns find their breeding sites in the far north after excursions of more than 70,000 kilometres to the Antarctic? Desert Navigator is the story of how a tiny ant (Cataglyphis spp.) became the ideal model organism for the study of animal navigation. It begins 50 years ago in a vast Saharan salt pan, where a lone, shiny black ant caught the eye of neuroethologist Rüdiger Wehner as it scuttled across the sand. Eventually, it discovered the corpse of a large fly, gripped it firmly in its mandibles, and then performed the manoeuvre that launched Wehner’s field of research.   The ant set off in a straight line, crossing more than 100 metres of the barren ground to disappear into an inconspicuous hole — the entrance to its underground colony. The only plausible explanation is that the ant knew all along exactly where it was in relation to its home nest. But how does Cataglyphis manage this, with a minute brain and no mobile phone? Wehner unspools the answer over the book’s seven chapters, describing the astonishing subtlety, intricacy and diversity of the techniques used both by the ants in finding their way home and by researchers in discovering how they do it. The ants plot their compass direction using patterns of polarized light and gradients of colour and light intensity in the sky, along with the position of the Sun, backed up by cues from Earth’s magnetic field and the wind direction. They know where they are by counting the steps they have taken and keeping track of the direction they were following at the time. They can memorize panoramic ‘snapshots’ of landmarks, such as boulders, around their goals. Somehow, their brains integrate all this information so that their foraging journeys can be optimally organized. Graduate student David Andel records an ant’s walking trajectory.Credit: Rüdiger Wehner The central message is that the ant’s sophisticated repertoire of navigation behaviour actually emerges from a large number of relatively simple elements, which Wehner calls the ant’s toolkit. These are combined in different ways, depending on the problem. This bottom-up, modular approach provides the key to understanding the ants’ navigational prowess and how it might have been honed during their development and evolutionary history. Wehner sees no need to introduce the further complication of a mental map, often invoked, for example, in studies on rodents: the ant can find its way around simply by cunning use of its toolkit’s components.   The toolkit concept underpins the investigative protocols used by Wehner and his team. The basic approach is to change a stimulus with surgical precision. The researchers carefully measure the resulting mistake the ant might make when performing a particular task, compared with an un-manipulated control ant. The exact behavioural consequence of the change in that particular stimulus is now revealed, although the physiology of how this effect is produced is usually not investigated, basically because the ant’s brain is so minuscule. For example, to prove that the ants could measure distance travelled by counting the number and length of strides taken, the team added stilts to the insects’ legs. An ant that arrived at the food source and then was given a set of stilts took the correct number of strides back to the nest. But it overshot its destination to the extent predicted by the increase in stride length. Even more astonishingly, the ants can allow for the extent to which they move up and down over bumps during a journey: they turn their 3D experience into an accurate 2D measurement. The researchers established this by forcing ants to take an outward foraging trip on a long ‘roller coaster’: a sawtooth track across the desert. On the return journey over flat ground, the ants predicted accurately where the nest should be. How they do this is not known. Atlantic salmon (Salmo salar) can return to spawning sites after years away.Credit: Michel Roggo/Nature Picture Library A hugely important aspect of this research is that the intricate experiments on the desert navigators are carried out under natural field conditions. This is unusual. Experimental studies on navigation in rats, for example, are routinely carried out in small-scale laboratory enclosures. Chapter 2, the longest in the book, is devoted to an absorbing account of the biology and ecology of the desert ants. This embodies a key lesson that Wehner would like to give to all students of animal navigation: “start out as a naturalist”, he says, and “let the animal guide your way of investigation”.   My only reservation is that this is a demanding book for its intended audience — the general reader. Wehner explains everything clearly and concisely, but only once. There are more than 100 detailed figures of the results of individual experiments, some of which might involve up to 10 variables, such as which part of an ant’s eye was covered; each figure requires several minutes of detailed concentration. A glossary, chapter summaries and a little more time spent on explanation would have helped. Wehner’s prose is precise and sometimes quite idiosyncratic, but his respectful admiration for these “elegant, skilled, and vivacious little runners” always shines through. This sumptuously produced book is a triumph both of natural history and of science, with lessons that reach well beyond the study of animal navigation. Understanding how this wonderful eusocial insect can accomplish its apparently miraculous feats of navigation has required imagination, intelligence and decades of disciplined application. Here, in one place, we can at last savour the full glory of this remarkable achievement. </body>
<date id = '240'>07 January 2020</date>
<url id = '241'>https://nature.com/articles/d41586-019-03957-8</url>
<title id = '241'>The long-awaited novel from science-fiction giant William Gibson forces us to think about what future we wish for. By Liesbeth Venema</title>
<body id = '241'>Credit: Getty Agency William Gibson Berkley (2020) With a breezy “Here we go,” Eunice enters the room — and the third page of William Gibson’s speculative novel Agency. I instantly liked her. Resourceful, fast-talking and street-smart, Eunice is on a serious mission: to give the world a fair chance to fend off the end of humanity. But she doesn’t waste time on existential angst. And she has kindness to spare (along with a handy infographic explaining the narrative structure of the 2010 film Inception). If she sounds too perfect, she is. Eunice is an autonomous personal artificial-intelligence assistant, developed with technology mysteriously ahead of her time and thrown into the lap of the novel’s real protagonist, beta tester Verity Jane. Agency is a feverish tour through worlds where the need to harness the latest technological advances to avert or mitigate global catastrophes overwhelms all else.   To understand and fulfil her mission, Eunice needs Verity. She must also evade meddling from the company that dispatched her. Eunice puts all sorts of high-tech measures in place to prevent the company from listening in on her conversations with Verity and finding out about her elaborate plans to free herself from its clutches. She arranges deep-faked artificial speech generation to mask discussions in the pair’s apartment. She hires freelancers to make state-of-the-art drones that track stalkers. When the time comes, she even disappears, distributing herself into many parts to ready herself for a reboot. Needless to say, all this leaves Verity perplexed — the only emotion she ever displays. It is hard to feel invested in a character so devoid of personality. But the alternative for Verity, we soon realize, might be constant terror. Her world is beset by daily climate catastrophes and is on the brink of nuclear war. She lives, we find, on a timeline that branched off from our ‘real’ one in 2015. Mindbendingly, Gibson sets our own timeline more than two centuries into the future. Agency shuttles between the two. With this, we realize that we’re back in the world of The Peripheral (2014), Gibson’s previous novel. In that story, he developed a clever take on time travel that avoids the usual paradoxes. Somewhere in our future, cybertechnology has advanced to a point where it is possible to send digital messages to the past. Once a message is received, a new timeline branches off. Gibson dubs these ‘stubs’. With a digital link established, it is even possible to operate, from one timeline, a physical telepresence — a machine, robot or cyborg avatar called a peripheral — in another. Who creates the stubs? It’s never fully clear. But we find that Verity’s was maliciously crafted to cause misery to humanity. Yet, in it, Donald Trump never became US president, and the United Kingdom’s referendum on whether to stay in the European Union was won by Remain. The irony is notable.   But our timeline, we find, has fared no better. Glimpses of a London in the twenty-second century reveal humanity barely surviving after severe political instability, climate change, rising poverty and organized crime. The only reason our species has hung on is that crucial technological advances arrived just in time. Through a peripheral, Verity — in her separate timeline — encounters characters in this future London attempting to steer our world towards a less disastrous outcome. After this unusual meeting, the action becomes less interesting. Gibson’s portrayal of a reality free from Trump and Brexit, yet beset by catastrophe, amounts to little. We occasionally hear about the television appearances of a female president, presumably modelled on Hillary Clinton, which show her as calm and capable in the face of every disaster. When Agency was trailed, much was made of this scenario. But despite the big questions it raises, the concept remains underdeveloped in the story. Overall, however, the narrative is complex and intriguing, even if the cast of dozens — brought together for niche specialisms from designing drones to organizing events — isn’t always. Along with Eunice, an exception is Ainsley Lowbeer. I desperately wanted to hear more about this semi-mythical special detective from future London. She owns a self-cloaking car and can make space empty itself around her. And she is the main force behind attempts to orchestrate small changes to other timelines to help stabilize them: for instance, by nudging Eunice to greater agency.   The dialogue is snappy, the technospeak relentless. Although character development is scant, the interaction between humans and advanced technology (such as Verity communing with Eunice through a set of smart glasses) is described in fascinating detail. In fact, Agency offers a refreshing variation on the usual limited artificial-intelligence trope. Eunice has a skill set based on that of an African American naval official, Marlene Miller. She declares herself an independent global citizen, and never bothers to ponder the question of whether she is sentient or not. All Eunice knows is that she is herself. Most of all, she has agency, the capability to act, and she doesn’t waste a moment in doing so throughout. And that is important. Gibson’s message seems to be: prepare for the worst, and get technology on your side. </body>
<date id = '241'>06 January 2020</date>
<url id = '242'>https://nature.com/articles/d41586-019-03876-8</url>
<title id = '242'>Reviews of the essential science reads this year.</title>
<body id = '242'>Frank Close, Trinity: The Treachery and Pursuit of the Most Dangerous Spy in History (Allen Lane, 2019) reviewed by Ann Finkbeiner. Angela Saini, Superior (Beacon, 2019) reviewed by Robin Nelson. Glass eyes of a type used in the twentieth century for ‘racial’ classification.Credit: David Harrison Daniel Kennefick, No Shadow of a Doubt (Princeton, 2019) reviewed by Peter Coles. Bathsheba Demuth, Floating Coast (W.W. Norton, 2019) reviewed by Sverker Sörlin. Paul Steinhardt, The Second Kind of Impossible (Simon and Schuster, 2019) reviewed by Sharon Glotzer. A model of a quasicrystal structure.Credit: Alison Forner/The Second Kind of Impossible, Simon and Schuster Kevin Walker, The Grand Food Bargain (Island Press, 2019) reviewed by Felicity Lawrence. Sarah Parcak, Archaeology from Space (Henry Holt, 2019) reviewed by Jo Marchant. David Spiegelhalter, The Art of Statistics (Pelican, 2019) reviewed by Evelyn Lamb. Tracking data can be baffling without a thorough knowledge of statistical approaches.Credit: solarseven/Getty Arthur Holland Michel, Eyes in the Sky (Houghton Mifflin Harcourt, 2019) reviewed by Sharon Weinberger. Sarah Dry, Waters of the World (Scribe UK, 2019) reviewed by Ruth A. Morgan. Latest on: Arts Technology Feature 08 MAY 20 Career Column 27 FEB 20 Books & Arts 08 OCT 19 Culture Obituary 23 MAY 20 Technology Feature 08 MAY 20 World View 14 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '242'>16 December 2019</date>
<url id = '243'>https://nature.com/articles/d41586-019-03757-0</url>
<title id = '243'>Barbara Kiser reviews five of the week’s best science picks.</title>
<body id = '243'>   L. S. Fauber Pegasus (2019) Four towering sixteenth-century scientists — Nicolaus Copernicus, Tycho Brahe, Johannes Kepler and Galileo Galilei — discovered heliocentrism at a time of sociopolitical tumult. As L. S. Fauber drives home in this dynamic science history, their intermeshed stories form a mighty “intergenerational epic” sweeping in the likes of Brahe’s sister Sophie and Galileo’s daughter Virginia. A wonderfully wrought explication of how a powerful thesis began its journey to becoming unavoidable fact, and seeded modernity in the process.    Charles Frankel Chicago University Press (2019) For this intriguing exploration of volcanism and viniculture, Charles Frankel scoured geologically active regions to trace how soils and landforms shape local wines. He begins with the 1620 bc eruption on Santorini that left the Greek island little more than a caldera, yet created ideal conditions for growing Assyrtiko grapes, used in unctuous Vinsanto. No less gripping are Frankel’s descriptions of the deep-time lava flows and flooding that formed Oregon’s Pinot-growing Willamette Valley. A gem for geologists and wine buffs alike.    Peter Wothers Oxford University Press (2019) Hydrogen, caesium, silver: how were elements named? In this stimulating chemical chronicle, Peter Wothers unravels tangled etymologies. Eighteenth-century chemist Antoine Lavoisier, for instance, named oxygen to signify ‘acid-former’, only to have the word construed as ‘‘the son of a vinegar merchant”. W, the symbol for tungsten, is a nod to its traditional moniker wolfram (derived from ‘wolf-foam’). From copper to californium, we discover how the sober face of the periodic table hides dramatic backstories.    Sonia Contera Princeton University Press (2019) Nanotechnology researcher Sonia Contera’s succinct study surveys the progress of nano-tools in biological and medical research. As she relates, there is much in development: DNA technology aimed at crafting nanoscale machines to target specific cancer cells; nano-antibiotics for fighting infection; and nano-approaches to tissue engineering. Contera frames this near-future transmaterial science, with its focus on human well-being, as an effort allied to social justice even as it probes existential questions of what it means to be human.    Sidney Harris Science Cartoons Plus (2019) After a year of bouts at the bench (and blasts of bad news), you may need relief. Science cartoonist Sidney Harris — whose work has graced the pages of many journals, including Nature, offers an ace antidote in this irreverent look at chemistry. Here is Lewis Carroll’s Alice, thwarted by a looking-glass made of Lexan polycarbonate resin; two chemists absorbing the news that “you can’t both be the ‘father’ of ammonium pentoxide phosphate”; the Institute for Advanced Hindsight; and oodles more. </body>
<date id = '243'>10 December 2019</date>
<url id = '244'>https://nature.com/articles/d41586-019-03694-y</url>
<title id = '244'>Thane Gustafson argues that fuel pipelines foster stability. By Andrew Moravcsik</title>
<body id = '244'>Russian natural-gas pipelines in northwestern Siberia.Credit: Alexander Nemenov/AFP/Getty The Bridge: Natural Gas in a Redivided Europe Thane Gustafson Harvard Univ. Press (2020) Many people imagine that geopolitics drives the energy trade between Russia and Europe. As the story goes, each side seeks to exploit gas and oil to influence the other in the big game of power politics — and Russia seems to have the upper hand. The European Union now imports nearly 40% of its natural gas from Russia. For decades, national-security specialists have recommended that Europeans reduce their dependence on these imports at any cost. Most recently, a fierce debate over Nord Stream 2 — a second Russian pipeline across the Baltic Sea to Germany — has led US congresspeople to threaten sanctions. Political scientist Thane Gustafson challenges this view in The Bridge. He argues that the trade in gas reflects slow-moving patterns of market demand and supply, which in turn stem from incremental changes in technology for pumping, piping and consuming fuel. The result is a pattern of remarkably stable economic interdependence that seems impervious to the geopolitical environment.   As extraction and pipeline technology opened up Soviet gas fields in the 1960s, and the ongoing postwar reconstruction of Europe stoked demand, East–West gas trade became all but inevitable. Ever since, Russia has wanted to supply gas and Europe has wanted to buy it. The past 50 years have seen energy shocks and gluts; major political crises from Poland to the former Yugoslavia; the fall of the Soviet Union and rise of Russian President Vladimir Putin’s authoritarian state; outright warfare in Ukraine and elsewhere; massive experiments in deregulation; and the rise of environmentalism. Yet relations between Europe and Russia in the natural-gas sector have remained nearly constant. This is because change is slow in three factors: proven reserves of gas, aggregate demand for energy and investment in physical infrastructure to link the two. The Bridge is an overview rather than a work of original research. Yet it offers a readable, intelligent, even-handed historical interpretation of this modern economic relationship. It divides East–West natural-gas relations neatly into three distinct periods. The first begins around 1960, with the spread of transport and use of natural gas in Europe, originally limited to small local networks in Italy and the Netherlands. Backed by US expertise, Europeans began to consider long-distance gas pipelines from Siberia, and made Western industrial equipment, investment and technical know-how available to the Soviet Union. The rigidity of the Communist system meant that production took almost a decade to come online. Eventually, the gas arrived, at first flowing through a terminal in Austria. The second period begins after 1970, when the quantity of Russian natural gas entering Europe increased. European consumption expanded quickly; gas proved cheaper and environmentally cleaner than coal or oil. Other countries, notably undersea-gas producers Norway and Britain, also created highly centralized systems for exploiting and piping the fuel. Yet the vast, low-cost Russian reserves enjoyed a comparative advantage, rising to provide almost half of consumption in European countries, prominent among them Germany and Italy. Workers building a pipeline in Germany, one of the biggest consumers of Russian gas.Credit: Krisztian Bocsi/Bloomberg via Getty This period, Gustafson argues, demonstrates the exceptionally stable nature of this type of international economic cooperation. Pipelines take decades to build, then tend to operate for decades more, often governed by just one or two long-term contracts. The physical, tangible linkage between producer and consumer “automatically creates a mutual dependence”, he writes. Moreover, because pipelines are centralized, they encourage domination of the market by monopolies — in the 1970s, these consisted of the Soviet Ministry of the Gas Industry, and European national or regional utilities. Natural gas, or anything else that travels through a fixed infrastructure, becomes a “relationship commodity”: investments, personal contacts and market shares follow the technology.   This, Gustafson avers, is why the East–West gas trade has remained impervious to geopolitical disruption. In 1968, shortly after the Soviet invasion of Czechoslovakia, Austria accepted the first Russian gas shipments into Europe. In 1981, when the pro-democracy Solidarity movement in Poland led to the Soviet-backed imposition of martial law, the US administration under president Ronald Reagan imposed sanctions on exports of pipeline technology. It could afford to do this because it was largely uninvolved in the East–West energy trade. Yet behind the scenes of these political upheavals, the real stakeholders acted differently. The Soviet Union developed home-grown alternative compressor and pipe technology — crucial for transporting gas — and Europe continued to sell technology that the Soviets could not produce at home. The third period began around 1990. Geopolitics grew more unruly. The Soviet Union collapsed in 1991. The gas ministry was turned into the massive state-owned corporation Gazprom, which was then largely privatized. Putin, who became president in 2000, brought Gazprom back under near-total state control. Russia also provoked a series of interventions and conflicts in Georgia, Moldova, Syria and Ukraine. The West responded by imposing sanctions — limits on investment and exports in sensitive military and civilian technologies, and even on energy investment. Russia’s countersanctions largely targeted Western agricultural exports. More recently, Russia has become involved in the disruption of elections in the West, and in cyberwarfare. Yet gas quietly continues to flow through the East–West pipeline.   Much of the book’s analysis of the most recent period focuses on another potentially disruptive change: new EU regulations. Gustafson makes much of the fact that, 30 years ago, the European Commission began pressing to open up the European energy market to greater competition. Directives render prices more transparent and uniform, and compel firms to supply gas across borders. At the same time, the commission is acting more forcefully to limit monopolies and cartels, and domestic deregulation has led to the rise of new corporate players. Overall, this concerted EU policy has further strengthened Europe’s hand. Russia cannot use embargoes or market segmentation to exploit individual countries. And Gazprom — which still has a near-monopoly on Russian exports, even though it is losing domestic market share — cannot acquire dominant positions in Europe. This is a significant develop-ment — and, from a Western perspective, a positive one. Yet it is difficult to discern how EU policies have altered Russia’s gas trade with Europe in any fundamental way. Exporting and importing nations alike have found ways to maintain overall control of their markets. If anything, Gustafson’s analysis would seem to show that the primary impact of EU consolidation has been to insulate a mutually beneficial economic status quo from disruption. Gustafson ends by considering long-term threats, which he introduces only to dismiss. For 20 years, conflict with Ukraine — first over energy pricing, then over politics — has led Russia to propose new pipelines that geographically circumvent its neighbour. Many worry that new lines, such as Nord Stream 2, might cut Ukraine out entirely. Yet Gustafson remains confident that if this occurs, Kiev, already transitioning away from Russian natural gas, will find new suppliers. Another threat comes from new technological options for transporting fuel as liquid natural gas, a more fungible form that would permit US imports to Europe. This might create an alternative to stable pipeline politics, although the transition would be slow because of the higher cost of the technology. Also, environmental-protection and climate-change concerns will continue to rise, reducing European demand in the long term. Yet, in the interim, natural gas will remain abundantly available, relatively inexpensive and still environmentally superior to oil or coal power. Gustafson’s overall conclusion is thus that Russian gas is likely to remain Europe’s major energy bridge to a future world of renewables. He even sees the next few decades as a “golden age of gas”. This is a soberly optimistic conclusion, not least because it suggests that commercial interests will induce modern countries to transcend ideological and geopolitical differences. </body>
<date id = '244'>02 December 2019</date>
<url id = '245'>https://nature.com/articles/d41586-019-03622-0</url>
<title id = '245'>Barbara Kiser reviews five of the week’s best science picks.</title>
<body id = '245'>   Richard J. King Univ. Chicago Press (2019) Herman Melville’s sprawling masterpiece Moby-Dick (1851) is a fictional feat studded with empirical evidence, reveals maritime historian Richard King in this invigorating study. King traces references to ethology, meteorology, marine microbiota and the oceans to Melville’s sailing experience in the Pacific and wranglings with the works of scientists William Scoresby, Louis Agassiz and others. Moby-Dick, King boldly avers, is a “proto-Darwinian fable” — and its beleaguered narrator, Ishmael, an early environmentalist.    Simon Parkin Sceptre (2019) Did gaming win the Second World War? In this stirring history, Simon Parkin recounts how eight mathematically minded members of the UK Women’s Royal Naval Service, with retired captain Gilbert Roberts, aimed to crack the tactics of Germany’s notorious U-boats through war games. Playing large-scale Battleship on the floor of a Liverpool office, the team’s ‘Operation Raspberry’ was decisive in winning the Battle of the Atlantic. Parkin’s account redresses a balance: none in this doughty sisterhood has ever been publicly honoured.    Naomi Oreskes Princeton Univ. Press (2019) As some sectors of society reject expertise on issues such as vaccination, science historian Naomi Oreskes explores what makes science trustworthy. This concise volume — comprising her 2016 Tanner Lectures on Human Values at Princeton University in New Jersey, along with commentary by experts — is a bracing exploration of philosophy of science and a demonstration of her vigorous engagement with the topic. We trust science, she reminds us, because consensus is a crucial indicator of truth — and “objectivity is maximised” through diversity.    Kenneth Paul Rosenberg Avery (2019) Psychiatrist Kenneth Rosenberg has been at the front lines of mental illness since the 1980s, when US psychiatric-hospital closures forced many people with serious mental conditions onto the streets or into prisons: some jails now ‘warehouse’ thousands. He meshes research with an analysis of systemic failures and personal stories, including those of psychiatrist Elyn Saks and his own sister, both diagnosed with schizophrenia. His ultimately hopeful study highlights key steps for patients, from details on integrated care to US legal advice.    Jonathan D. Moreno and Jay Schulkin Columbia Univ. Press (2019) That fatty mass in the skull is not all there is to the brain — neural tissue lurks all over the body. So bioethicist Jonathan Moreno and neuroscientist Jay Schulkin begin their guide to neurology. To “see the brain in its wholeness”, they examine the historical interplay of experiment and theory through lenses from comparative structure to evolution and imaging. The result is fascinating, whether on ‘brains in a dish’ or BrainGate technology to help people with paralysis control their limbs; but factual logjams impede the flow at times. </body>
<date id = '245'>27 November 2019</date>
<url id = '246'>https://nature.com/articles/d41586-019-03533-0</url>
<title id = '246'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '246'>   Claudia Hammond Canongate (2019) In 2014, journalist Claudia Hammond, presenter of BBC Radio 4’s All in the Mind, joined a group studying rest at London’s Wellcome Collection. She proposed a radio survey called the Rest Test. Responses from 18,000 people in 135 countries yielded a top ten of restful activities, and they inspire the titles of her informative chapters interlacing findings from dozens of studies. Intriguingly, the top five are largely solitary. Number one is reading, which “not only allows us to escape other people, but simultaneously provides us with company”, she notes.    Susan Schneider Princeton University Press (2019) Artificial intelligence (AI) technology will raise increasingly difficult ethical issues, argues philosopher, cognitive scientist and self-confessed technotopian Susan Schneider in this demanding dialogue between philosophy and science. How would you feel, she begins speculatively, about purchasing a “Hive Mind” — a brain chip permitting you to experience the innermost thoughts of your loved ones? That presumes, however, that future AI can capture consciousness with computation — which she argues is unlikely.    Matthew Gutmann Basic (2019) Anthropologist Matthew Gutmann has spent 30 years exploring concepts of masculinity across the United States, Latin America and China. “We place unreasonable trust in biological explanations of male behaviour,” he argues in this wide-ranging book, which discusses US mass killings by men, Donald Trump’s presidency and much more. Yet, he contends, there have been no major discoveries of a link between testosterone and aggression since 1990, despite a boom in scientific articles on the topic.    Erik Asphaug Custom House (2019) The days of the week are named after bodies in the Solar System and a diverse mix of Norse and Roman deities. So notes Erik Asphaug, a planetary scientist who is part of the team behind two lunar and planetary NASA missions. But if the planets were born out of material orbiting the Sun, like raindrops condensing from a cloud, why do they differ so much in structure and chemical composition? This detailed book assesses the astronomical and geological evidence on the origin of planetary diversity.    Adam Shoalts Allen Lane (2019) In 1967, the centennial of Canada’s confederation, ten teams of canoeists paddled from central Alberta to Montreal. In mid-2017, to mark the 150th anniversary, explorer, historian and geographer Adam Shoalts travelled across the Canadian Arctic by canoe and on foot. His journey took him from the Yukon to Nunavut: across the terrestrial world’s largest expanse of wilderness outside Antarctica. It proved less stressful than his normal “modern, hyper-connected world”, he avers in his engaging, hazard-strewn account. </body>
<date id = '246'>20 November 2019</date>
<url id = '247'>https://nature.com/articles/d41586-019-03531-2</url>
<title id = '247'>A hard-hitting study exposes the devastating effects of shame and discrimination. Julie Pulerwitz reviews.</title>
<body id = '247'>A person with HIV in the Mae Tao Clinic in Thailand.Credit: Alvaro Ybarra Zavala/Getty Lazy, Crazy, and Disgusting: Stigma and the Undoing of Global Health Alexandra Brewis & Amber Wutich Johns Hopkins University Press (2019) As a public-health researcher working on HIV around the globe, I have seen the devastation that stigma can cause. It leads to people being shunned and isolated, and discriminated against in health care, at work and at school. And it inhibits them from accessing life-saving services and medications. As a new book by medical anthropologists Alexandra Brewis and Amber Wutich convincingly argues, stigma strips people of dignity and exacerbates the already-difficult circumstances of the poorest and most vulnerable. It can itself have major impacts on health, such as depression and even suicide. Brewis and Wutich work in low-income countries. Their book’s title, Lazy, Crazy, and Disgusting, highlights their areas of focus: obesity, mental illness and community sanitation. The authors focus on detailed, qualitative case studies in diverse arenas. These demonstrate three things: how stigma arises and affects the most marginalized; why stigma is so difficult to combat; and how public-health efforts can unwittingly fuel it.   It is this third issue — the unintended consequences of big campaigns — that forms their main argument. And it is a rarely heard and compelling one. When, for example, the US public-health community framed smokers as putting others at risk of getting cancer from ‘second-hand’ smoke, the messages hit home. Social norms regarding whether it was acceptable to smoke changed, and many smokers were motivated — and managed — to quit. But there were also negative consequences. Smokers were blamed for their addiction, and people with smoking-related diseases (even those who had never smoked) were often castigated as bringing their conditions on themselves. Meanwhile, tobacco companies escaped criticism. Similarly, the authors demonstrate how concerns about the effects of obesity — such as diabetes and cardiovascular disease — have led to fat-shaming, depression and more. Yet obesity is strongly linked to socio-economic circumstances, and a lack of access to high-quality foods such as fresh fruit and vegetables. It is counterproductive and unfair to blame individuals. As obesity and associated conditions become increasingly prevalent across the global south, Brewis and Wutich caution against a snowball effect of harmful messaging and impacts. The book is less strong on ways forward. I second the authors’ calls for increased awareness among health practitioners, for tracking of stigma levels and for policy to be evidence-based. But, in my view, a more comprehensive and nuanced response is needed. There are important distinctions between, for example, public-health measures to reduce people’s internalized feelings of blame and shame, and legislative efforts to minimize ‘enacted’ stigma — that is, instances of discrimination. Internalized stigma might lead to depression, and those who experience it might benefit from counselling, say. By contrast, human-rights abuses must be countered with anti-discrimination policies and laws.   Moreover, Brewis and Wutich fail to explore an important concept: intersecting stigmas. For example, a person with HIV who works in the sex industry and injects drugs might experience compounded bias and discrimination. The authors use HIV as an example of a success story in which concentrated efforts from the global health community, such as health policies and mass-media campaigns, have greatly reduced stigma. But this is true only in some communities — especially in high-income countries where living with HIV has been transformed into a chronic illness through the use of antiretroviral medications. These medications are often not reaching the most vulnerable, and in many contexts — for example, where drug users are criminalized and struggle to access health care — intersecting stigmas remain rampant. This engaging book nevertheless fills a significant gap in the literature by providing a wake-up call to scholars and practitioners unfamiliar with the topic. And it reminds me that we should all be working together to avoid any unintended consequences of promoting health. </body>
<date id = '247'>19 November 2019</date>
<url id = '248'>https://nature.com/articles/d41586-019-03532-1</url>
<title id = '248'>An astronaut and an archaeologist explore the triumphs and disasters of space hardware. Meg Urry reviews.</title>
<body id = '248'>Astronaut Michael Good works on maintaining the Hubble Space Telescope.Credit: JSC/NASA Dr Space Junk vs The Universe: Archaeology and the Future Alice Gorman MIT Press (2019) Handprints on Hubble: An Astronaut’s Story of Invention Kathryn D. Sullivan MIT Press (2019) The space age erupted with a flurry of satellites. The first two Soviet Sputniks launched in 1957, soon followed by the US Explorer I and Vanguard I. In 1959, spurred on by cold-war tensions, NASA selected seven men as astronauts for its Project Mercury programme. (Thirteen women who passed the same hurdles, courtesy of a private, parallel programme, were vetoed.) Barely a decade later, NASA’s Apollo astronauts walked on the Moon. The dawn of space exploration was all about the now. It was daring and risky, punctuated by engineering miracles and an air of invincibility. It wasn’t, however, focused on the long term. Now, six decades on from Sputnik, old spacecraft are displayed in museums, robotic missions regularly reveal secrets from throughout the Universe and private companies such as SpaceX are planning colonies on Mars. And the rich, crowded future of space is the focus of two books, one by space scientist and oceanographer Kathryn Sullivan, the other by space archaeologist Alice Gorman. Both make us think more deeply about how we, as humans, ought to fit into the cosmos. In Handprints on Hubble, Sullivan, a former NASA astronaut who helped to launch the Hubble Space Telescope in 1990 and has been involved in updating its capabilities since, highlights the importance of planning for new instruments and infrastructure. Gorman, meanwhile, applies an archaeologist’s perspective to space-related materials and activities in Dr Space Junk vs the Universe.   Hubble has made more than one million observations of stars and galaxies, and probed dark matter and the history of the Universe itself, over nearly three decades. I worked for many years at the Space Telescope Science Institute in Baltimore, Maryland, which runs Hubble’s science operations for NASA. I was there for the launch, the discovery of a flaw in the primary mirror and the astonishing fixes that astronauts repeatedly pulled off. Yet Sullivan’s book makes clear how much I hadn’t known. Hers is a first-hand story, from conception to today, of the first space mission for which in-orbit maintenance and repair were integral from the start. Sullivan brings alive the strenuous challenges of space mechanics. Replacing entire instruments or — much harder — parts deep inside them during long, arduous spacewalks demands custom-designed tools. For example, Sullivan explains the evolution of the foot anchors that keep astronauts in place. Without these, turning a screw one way would make the astronaut and/or the spacecraft rotate in the opposite direction. This is the kind of detail that underscores the complexity of the job. Illustration of dangerous junk orbiting around Earth.Credit: Dotted Yeti adapted from NASA Every step needs forethought. Once removed, a screw will float away if not caught, creating dangerous space junk that could damage other craft, as Gorman discusses. Sullivan and her colleagues spent hundreds of hours testing tools and procedures on a simulated Hubble in an underwater tank, with scuba-diving gear standing in for unwieldy spacesuits. The meticulously planned servicing missions are what have kept Hubble at the leading edge. Its first set of instruments was selected in 1978. By today’s standards, the technology was impossibly crude and the computer storage limited. After the first servicing mission in 1993, new optics compensated for the flaw in the mirror, the workhorse camera had improved detectors and the spacecraft had new solar panels and other vital infrastructure. There have been four more such missions. The ultraviolet spectrograph (COS) installed in 2009 is up to 20 times more sensitive than the previous ones. That is equivalent to increasing the mirror diameter from 2.4 metres (the largest that could fit inside the Space Shuttle bay) to more than 10 metres (larger than any telescope yet built). Like Sullivan, Gorman was fascinated by space as a child, inspired by dark skies over the Australian countryside. But in common with many women at the time, she was discouraged from becoming an astrophysicist. Instead, she earned a PhD in archaeology and worked as a consultant documenting Indigenous heritage sites in her home country. But her gaze frequently turned skywards. Eventually, she applied her training to space exploration, regarding even the lowliest “space junk” as an important part of the historical record. As the “Dr Space Junk” of her book’s title, Gorman writes about how we should protect our space legacy. She describes the reef of orbital detritus around our planet, including satellites, alive and dead, embedded in a sea of discarded hardware and debris from space collisions (deliberate and otherwise), as well as receding planetary probes and equipment abandoned on the Moon. Hubble Space Telescope image of the super-massive star Eta Carinae.Credit: NASA/ESA/N. Smith (University of Arizona)/J. Morse (BoldlyGo Institute) She draws parallels between archaeological investigations that find ancient artefacts on Earth and missions to catalogue objects that are merely decades old circling above it. At times, her ideas can seem fanciful — as when she discusses shadows left by footprints on the lunar surface, or speculates about future civilizations finding spacecraft beyond the Solar System. But for the most part, the book made me think fresh thoughts. Gorman reminds us how fragile our access to space is. Orbital debris alone poses a risk to every newly launched spacecraft. She warns of the need for nations to cooperate in preserving and protecting the space environment, and points out the moral responsibility of spacefaring nations to deal on an equal basis with those that are not. Both Sullivan and Gorman envision a future in which astronauts, and possibly ordinary citizens, live and work in space regularly. In that world, it will be normal to site telescopes in stable orbits at L2 (the second Lagrange point, which circles the Sun in tandem with the Earth–Moon system) and to upgrade them regularly. Hubble has seen a few monster galaxies as they were early in the evolution of the Universe. In future, more sensitive instruments, such as the James Webb Space Telescope, might see much smaller early galaxies and possibly even the first stars. To read these two books is to marvel at what we have achieved in our nascent efforts to inhabit space, and to recognize that we have barely begun that quest. Many popular treatments of space travel, including the films Apollo 13 (1995) and First Man (2018), have framed it as competitive derring-do. Sullivan and Gorman focus more on our common interests, as humans, in knowledge and cooperation. They invite us to think anew about the legacy and the future of space. </body>
<date id = '248'>18 November 2019</date>
<url id = '249'>https://nature.com/articles/d41586-019-03458-8</url>
<title id = '249'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '249'>   Philip Goff Pantheon (2019) In this well-argued, but provocative, study, philosopher Philip Goff asserts that ”nothing is harder to incorporate into our scientific picture of the world” than consciousness. Goff harks back to 1623, when physicist Galileo Galilei adopted a dualist position: consciousness exists completely outside the physical realm. Today, materialists aim to explain it as purely physical. Goff opts instead for the 1920s ‘panpsychism’ view, which claims that all physical matter shares consciousness.    Jim Davies Pegasus (2019) Scientific books on creativity abound. But this deeply researched study of imagination — ranging from everyday practicalities such as planning a shopping list, to dreams and hallucinations — is not one of them. Cognitive scientist Jim Davies, who heads a Science of Imagination Laboratory in Canada, researches how to get software to replicate the processes our brains use to create visual scenes in our minds. But, as Davies admits, in psychology the jury is still out on whether “mental imagery exists as its own separate representation”.    Dan Hooper Princeton University Press (2019) In 1919, when general relativity was confirmed astronomically, science knew nothing of cosmic origins. Even in the 1950s, Albert Einstein joked that “Every man has his own cosmology and who can say that his own theory is right!” Today, the Big Bang is universally accepted, and evidence suggests that gravity started to behave much as it does now within about 1043 seconds. Yet much remains perplexing, explains astrophysicist Dan Hooper in this informed introduction to “the mysteries of our universe’s first seconds”.    Laura Trethewey Pegasus (2019) Three million US citizens work on the ocean — in fishing, oil and gas, tourism and other industries and services. The global figure is three billion. Journalist Laura Trethewey set out in 2015 on “an extended listening tour” to hear some of their stories. She describes a teenage Ghanaian refugee who crossed the Mediterranean, a ‘water-squatting’ Pacific Northwest community and a biologist who tracked the accelerating disappearance of the sturgeon. The vivid result — her debut — persuades us that “the ocean’s story is also our own”.    David Lindsay Roberts Johns Hopkins University Press (2019) This charming collection of 20 “unexpected stories of mathematical Americans through history” focuses not only on the greatest US mathematical minds, and includes just six career academics. Abraham Lincoln, self-trained as a surveyor, later studied Euclid — as demonstrated in his Gettysburg Address, “dedicated to the proposition that all men are created equal”. A pity, however, to exclude Tom Lehrer, mathematician-cum-satirist, who composed the classic 1965 song ‘New Math’. </body>
<date id = '249'>13 November 2019</date>
<url id = '250'>https://nature.com/articles/d41586-019-03457-9</url>
<title id = '250'>Andy Greenberg’s book Sandworm is trenchant on the mounting capacity of malware to wreak havoc. Brian Nussbaum reviews.</title>
<body id = '250'>Ukraine’s electricity grid has been hit by several cyberattacks (photo from 2019).Credit: ISS/NASA Sandworm: A New Era of Cyberwar and the Hunt for the Kremlin’s Most Dangerous Hackers Andy Greenberg, Doubleday (2019) In 2017, a piece of malicious software called NotPetya launched the first global data-destruction pandemic. It was probably the most expensive cyberattack in history. The culprit was Sandworm, an aggressive, malicious hacking group, which many analysts linked with Russian military intelligence. Technology journalist Andy Greenberg’s eponymous book tracks the group’s attacks, and the people and companies that chase them across computer networks worldwide. It also spells out the implications of the hackers’ destructive agenda for all of us. Greenberg recounts the details of the group’s record since at least 2014. He draws on his reportage for Wired on the 2015 and 2016 attacks on the Ukrainian electrical grid, which led to serious blackouts and left hundreds of thousands of Ukrainians without power. In addition to NotPetya, he examines other attacks conducted by, or affiliated with, Sandworm. These range from strikes against election infrastructure in several countries, including the United States, to the 2018 winter Olympic Games in South Korea, and international treaty organizations such as the Organisation for the Prohibition of Chemical Weapons in The Hague, the Netherlands.   Sandworm is particularly disturbing for several reasons. Its malware is sophisticated and capable. Its targeting illustrates troubling recent dynamics in cybersecurity — including escalating disruption, the ability to destroy data or create physical damage and the targeting of crucial civilian infrastructure. By contrast, most cyberattacks have focused on the theft or exposure of data, exploiting this for financial gain or strategic advantage. We know such data breaches all too well from criminal and spying activity, such as credit-card data being stolen from large retailers, or the mammoth 2015 breach at the US Office of Personnel Management in Washington DC. Whether what was taken was financial data or strategically important secrets for espionage, the impacts of cyberintrusions tended not to create immediate concerns about the safety of individuals or the stability of global economic or political structures. Many commentators thus downplayed the rhetoric that surrounded them. For many years, however, military and intelligence officials and other commentators in the United States often spoke of the possibility of a “cyber Pearl Harbor” or “cyber 9/11”. Some warned consistently of “cyberwar”, although many cybersecurity experts bristled at the suggestion that computer crime and espionage efforts warranted the term. Political scientist Thomas Rid aired that scepticism in his 2013 book Cyber War Will Not Take Place. Civilian targets such as electrical grids are increasingly vulnerable.Credit: Karl F Schofmann/Shutterstock NotPetya was a turning point. It deployed what looked to be ransomware, using a ‘back door’ in a Ukrainian tax-preparation software package. Ransomware encrypts a computer’s files and offers to sell users a decryption key for a ransom, often paid in cryptocurrency such as Bitcoin. Greenberg shows how, while seemingly targeting Ukraine, the malware spread rapidly around the world. And although it seemed to be conventional ransomware, it did not offer a real way to decrypt files. Greenberg details how NotPetya led to many firms incurring costs of hundreds of millions of dollars. They included pharmaceuticals giant Merck, shipping conglomerate Maersk, FedEx subsidiary TNT Express, French construction company Saint-Gobain and US food producer Mondelēz. Tom Bossert, former homeland-security adviser to US President Donald Trump, confirmed to Greenberg that global losses were estimated to top US$10 billion. Even Rid told Greenberg that if “anything comes close to cyber 9/11”, it is NotPetya.   In the past decade or so, physical systems — from stop lights to electrical grids to pacemakers — have become increasingly connected to, and controlled by, computers. Greenberg uses Sandworm to show how threats have grown from assaults on systems that collect and process information, to assaults on systems that control physical devices and processes. From insulin pumps, to dams, to entire transport networks, the increasing reliance on computers to run the things around us has made the potential impact of cyberstrikes much more grave. Malicious computer code might prompt a crucial industrial device to overheat and cause a fire, explosion or other damage — as happened to a German steel mill, according to a 2014 report by Germany’s Federal Office of Information Security. Sandworm was not the first group to damage physical infrastructure with malware, however. That was the team behind Stuxnet, which destroyed centrifuges at the nuclear power plant in Natanz, Iran, disrupting uranium enrichment. David Sanger at The New York Times attributed Stuxnet to a mix of US and Israeli intelligence and security agencies. But Sandworm, by hitting the Ukrainian electrical grid, has drastically ramped up concerns about protecting civilian targets. Greenberg documents how the group has created malware designed to manipulate and harm control systems across borders, software and hardware platforms, and industry sectors. In an era of fake news and disinformation, determining whether hackers are who or what they seem can seem a daunting task. Yet anonymity in cyberspace is often overstated. Despite the challenges in identifying culprits who have the capacity to hide and to leave false trails, many governments and, increasingly, private organizations, are capable of doing it. Greenberg shows how researchers, firms and agencies, from big software companies to private-sector actors, are responding to Sandworm and other cyberthreats. The result? Hackers have been subjected to exposure and publicity; security firms have blocked their tools; and their members and leaders have been indicted. Sandworm offers an important front-line view of the changing cyberthreats that are shaping our world, their creators and the professionals who try to protect us. </body>
<date id = '250'>12 November 2019</date>
<url id = '251'>https://nature.com/articles/d41586-019-03456-w</url>
<title id = '251'>A trawl through the global tide of cast-offs shows how we might avoid drowning in them. Edward Humes reviews.</title>
<body id = '251'>Shelves loaded with packets of goods are shifted around by robots inside an Amazon warehouse in New Jersey.Credit: Demetrius Freeman/The New York Times/Redux/eyevine Secondhand: Travels in the New Global Garage Sale Adam Minter, Bloomsbury (2019) A sprawling, insightful travelogue through the world of repair, reuse and waste, Secondhand takes readers deep inside the consumer economy’s back end. In exploring the vast global tide of used and discarded goods, Adam Minter, a journalist writing on technology and the environment, delivers a book as crammed with oddities and gems as the second-hand shops he loves to haunt. Manufacturing — the start of the expanding consumer pipeline — is environmentally damaging enough. The United Nations estimates that the fashion industry, for instance, is responsible for 10% of global greenhouse-gas emissions and 20% of waste water. Some 85% of textiles then end up in landfill, or are burnt. Our homes are filled with other products — furniture, kitchenware, shoes, décor, appliances — that meet similarly ignominious and unsustainable ends. But a significant portion of global consumer goods finds second and third lives through the reuse economy. It is the costs and benefits of this afterlife of stuff that Minter examines with a sense of wonder and cautious optimism in Secondhand. In effect a follow-up to Minter’s Junkyard Planet (2013), Secondhand is anecdotal rather than analytical. It journeys from Goodwill used-goods stores in Arizona to the textile-salvage importers of Nigeria and Pakistan, and the market stalls of an enterprising Mexican merchant known only as Shoe Guy. Sprinkled with sometimes counter-intuitive observations, the book delivers a key insight early on from a professional home de-clutterer: we are all hoarders. The extent to which that becomes problematic is a matter of degree.   This is a mass behaviour, Minter shows, that is unprecedented in human history: keeping more possessions than we need, or can even contain in our homes. Between 1967 and 2017, he notes, US spending on stuff, from sofas to mobile phones, increased almost twenty-fold. He asserts that similar patterns of overconsumption are gathering steam worldwide. In one startling discussion on how consumers have bitten off more than they can chew, he looks at the US mini-storage warehouse industry that sequesters domestic overflow. By 2017, Minter reports, there were more than 54,000 home-storage businesses, generating annual income triple that of Hollywood’s box-office revenues — which was US$12 billion in 2018. Ultimately, we have collectively failed to create a ‘circular economy’ — obviating waste by designing consumer products to retain value through use, reuse and recycling. In Minter’s view, that is a signature crisis of our age. These are disturbing issues with wide-ranging impacts. Yet simultaneously, Minter — the son of a junkyard owner — is fascinated by the inner workings of the second-hand world. He delights in exploring how cultural perceptions of used goods differ between countries. Japan is one of his most interesting case studies. Its ageing, shrinking population is leaving behind homes filled with uninherited possessions. The country has been particularly adept at turning the stigma attached to used goods into a virtue. Bookoff, a company that buys and sells used goods in brightly lit, fashionable boutiques, has led the charge. When it started in 1991, the company designed a machine that refurbished books by shaving away stains on pages; similar methods were applied to clothes. Gradually, consumers began to realize that keeping possessions in pristine condition retained their value, meaning that they fetched better prices when sold. Eventually, used goods began entering the store as good as new, and the machines were retired. Workers sort through bales of second-hand clothes in a recycling centre in Senegal.Credit: Darame/AFP/Getty Minter asserts that such attitude change is essential on a global scale. But this is only part of his prescription for creating a modified consumer economy that could save us from drowning in discards. This is a tall order, Minter admits. In most of his travels, he found that consumers almost always prefer new goods, and will select poor-quality, quick-to-wear-out, inexpensive new items over even the most pristine, high-quality used goods. Retailers such as Walmart in the United States, and ‘fast-fashion’ chains in the United Kingdom, sell many cheaply made, ephemeral goods at second-hand-shop prices. That simultaneously drives the impulse to buy higher, while creating goods with little resale value. Minter laments that even on eBay, the online auction house that began as a game-changing force in the sale of used goods, poorly crafted new products now comprise more than eight out of every ten sales. Assessing the net impact of the reuse economy is anything but straightforward, Minter notes. Although it has created jobs and new sources of affordable, high-quality products in low-income countries, the flood of used items can destroy industries. In Kenya, for example, home to some of the world’s largest used-clothing markets, a textile industry with 500,000 workers in the 1980s has dwindled to one-tenth of that workforce.   Secondhand offers a few possible solutions beyond the Sisyphean task of altering this ingrained consumer preference for the new. It suggests manufacturers are having something of a light-bulb moment in realizing that durability and longevity in products are good for business. One example is Dell’s strategy of building long-lived, upgradeable and future-proofed computers that can be leased for three years, then resold as economical alternatives to brand-new hardware. Minter is also a fan of right-to-repair laws as a counter to the trend of manufacturing unfixable products, as well as the growing Repair Café movement in Europe (notably absent as a significant force in the United States). Prescriptions aren’t Secondhand’s strong suit, however. Instead, Minter succeeds brilliantly in using the stories of those working in the world of waste to hold up a mirror to our out-of-control buying habits and incite us to join the reuse, repair and recycle economy. </body>
<date id = '251'>12 November 2019</date>
<url id = '252'>https://nature.com/articles/d41586-019-03355-0</url>
<title id = '252'>Andrew Robinson reviews five of the week’s best science picks.</title>
<body id = '252'>   David Owen Riverhead (2019) “For a deaf child, having hearing parents can be a serious handicap,” notes New Yorker staff writer David Owen in this sensitive study of hearing. (He is personally involved, as someone with tinnitus who saw his grandmother struggle with deafness.) Meshing the science with individual auditory experiences, Owen discusses hearing aids, cochlear implants, genetically deafened mice, sign language, Thomas Edison and noise levels in US cities and towns — all in absorbing, anecdotal detail, although regrettably with no diagrams.    Joel Levy Smithsonian (2019) This picture-packed volume by science journalist Joel Levy tours scientific advances sparked by ideas in science fiction. The title comes from a definition of sci-fi by Syd Mead, an industrial designer behind the look of futuristic movies such as Blade Runner (1982). But how prescient is sci-fi? Levy shows how H. G. Wells’s 1903 story ‘The Land Ironclads’ inspired Winston Churchill to promote the development of the military tank in 1915. But Wells did not envisage its key technical idea: caterpillar tracks, for added grip.    Tim Woollings Oxford University Press (2019) The jet stream — strong high-altitude air currents — was discovered in the 1920s. In this analysis of its complex impact on weather, physicist Tim Woollings relates how in 1944, the Japanese used the jet stream to launch trans-Pacific incendiary balloons. By strange chance, one hit the US plant that provided plutonium for the bomb that devastated Nagasaki in 1945. Today, argues Woollings, the jet stream is “very likely” to be threatened by another product of human activity: rising carbon dioxide emissions.    Stephen Wolfram Wolfram Media (2019) Computer scientist and businessman Stephen Wolfram, designer of the technical-computing system Mathematica, proffers good stories in this collection of autobiographical essays. In ‘Something I learned in kindergarten’, he recalls himself as a six-year-old spotting a bite taken out of the Sun: a solar eclipse, something unknown to the other children. In ‘My life in technology’, he recalls rejecting the Latin word mathematica, learnt at school, as too long and ponderous. Silicon Valley luminary Steve Jobs convinced him otherwise.    John C. H. Spence Oxford University Press (2019) Starting with Albert Einstein, scientific consensus holds that the speed of light is a universal constant. So writes physicist John Spence in his history of attempts to measure the speed of light. Spence considers the implications of its constancy for modern physics and technology. For instance, the aether — a theoretical space-filling medium rejected in Einstein’s relativity — is still “anything but empty”. Despite its appealing vignettes of great physicists, this is a challenging read. </body>
<date id = '252'>06 November 2019</date>
<url id = '253'>https://nature.com/articles/d41586-019-03269-x</url>
<title id = '253'>Barbara Kiser reviews five of the week’s best science picks.</title>
<body id = '253'>   Sarah Cole Columbia University Press (2019) H. G. Wells was, asserts scholar Sarah Cole, a pioneer adept at “rescaling the cosmos and humanity’s place in it”. He straddled the border between science and literature, but not all his complexities were benign: he both repudiated racism and for some time shamefully ascribed to ideas on eugenics. Cole adroitly captures Wells, from his mould-breaking books (such as the 1895 science-fiction classic The Time Machine and 1920 Outline of History) to his unlikely intellectual kinship with subtle modernists such as Virginia Woolf.    Henning Beck Greystone (2019) The human brain at work, notes neuroscientist Henning Beck, is sloppy — and that is precisely what makes us creative powerhouses. Beck’s coolly amusing narrative takes us through forgetting, pigeon-holing, distraction and deep into creativity. He explores how idle wool-gathering is more conducive to creativity than is ‘efficient’ thinking, and the uncannily similar way in which true and false memories are generated in the brain. His is a hopeful message, ultimately. If we don’t err, we don’t change. So: “stay fallible”.    Nathalia Holt Little, Brown (2019) The early hand-drawn animations of Walt Disney Studios remain a technological wonder. Few know, however, of the company’s female virtuosi, who from the 1930s on injected nuance into characters from Bambi to a panoply of princesses. In her gripping corrective, Nathalia Holt ushers these animators and story developers into the limelight: Bianca Majolie, Sylvia Holland, Retta Scott, Grace Huntington and Mary Blair. Particularly in the early years, Holt shows, they paid a high price to work, forced to battle harassment in mostly male teams.    Colin Stuart Michael O’Mara (2019) This compelling portrait of the Sun packs in facts while speculating on gaps in our knowledge. Astronomy journalist Colin Stuart traces the arc of discovery from the fourth-century bc heliocentricism of Aristarchus of Samos through solar spectroscopy, star formation and nuclear fusion, the “epic journey” of sunlight to Earth and more. The Sun is both bountiful and belligerent, he reminds. Solar power could make 87% of countries energy self-sufficient — but the next big solar storm could send our electrical infrastructure into meltdown.    Azra Raza Basic Books (2019) Each year, the United States spends US$150 billion on treating cancer. Yet as oncologist Azra Raza notes in this incisive critique-cum-memoir, the treatments remain largely the same. Raza wants to see change: eliminating the first cancer cell rather than “chasing after the last”, which is doable with current technologies. Meanwhile, she braids often-harrowing stories of patients, including her own husband, with insights gleaned from laboratory and literature on this complex, often confounding array of diseases. </body>
<date id = '253'>30 October 2019</date>
<url id = '254'>https://nature.com/articles/d41586-019-03084-4</url>
<title id = '254'>Gaia Vince takes an enjoyable sprint through human evolution — Tim Radford reviews.</title>
<body id = '254'>Shilluk people in Sudan gather in the shade for traditional storytelling.Credit: Eye Ubiquitous/Alamy Transcendence: How Humans Evolved through Fire, Language, Beauty, and Time Gaia Vince Allen Lane (2019) Gaze into a mirror. Reflected is a marvel of evolution: a weak-jawed, bipedal omnivore with a greedy brain, in which 100 billion neurons consume 20% of the body’s energy intake. Science journalist Gaia Vince urges us towards such reflections in Transcendence, a book tracing the journey of Homo sapiens through genes, environment and culture to what might be, she surmises, a new state of being. For her hugely enjoyable sprint through human evolutionary history, Vince (erstwhile news editor of this journal) intertwines many threads: language and writing; the command of tools, pursuit of beauty and appetite for trinkets; and the urge to build things, awareness of time and pursuit of reason. She tracks the cultural explosion, triggered by technological discovery, that gathered pace with the first trade in obsidian blades in East Africa at least 320,000 years ago. That has climaxed this century with the capacity to exploit 40% of the planet’s total primary production. How did we do it? Vince examines, for instance, our access to and use of energy. Other primates must chew for five hours a day to survive. Humans do so for no more than an hour. We are active 16 hours a day, a tranche during which other mammals sleep. We learn by blind variation and selective retention. Vince proposes that our ancestors enhanced that process of learning from each other with the command of fire: it is 10 times more efficient to eat cooked meat than raw, and heat releases 50% of all the carbohydrates in cereals and tubers. Thus Homo sapiens secured survival and achieved dominance by exploiting extra energy. The roughly 2,000 calories ideally consumed by one human each day generates about 90 watts: enough energy for one incandescent light bulb. At the flick of a switch or turn of a key, the average human now has access to roughly 2,300 watts of energy from the hardware that powers our lives — and the richest have much more.   Humans are more social than other primates. We can keep track of around 150 other people, which demands a large brain and might also help to expand it. To learn a fact stimulates one part of the brain; to hear a story activates many. That is why we find information 22 times more memorable in narrative form. Homo sapiens is a storytelling animal, and this adaptation ensures the transmission of skills and knowledge as fable, epic or cautionary tale. Vince, drawing on brain-scan studies, shows that neuroscientists have noted a synchrony, both spatial and temporal, between speaker and listener during storytelling, a phenomenon known as ‘neural coupling’. The human capacity for narrative, metaphor and pattern-matching can lead us to see meaning where there is none, however. In a US psychological experiment in 1944, students were shown a short animation of two triangles and a circle passing across a screen, while a rectangle remained stationary (F. Heider & M. Simnel Am. J. Psychol. 57, 243–259; 1944). Of the subjects, 33 out of 34 anthropomorphized the moving shapes, creating narratives of anxiety, concern, rage and frustration. Vince continually returns to the evolutionary triad of genetics, environment and culture to address our similarities and differences. Some human biological adaptations are part of cultural variety. The semi-nomadic Moken people of Thailand can see clearly underwater because they can constrict their pupils to the maximum limit of human capacity, increasing depth of field and changing the lens shape. This is a learnt capacity: in an experiment, Swedish children mastered it. Divers of the Bajau people in Indonesia, however, exemplify heritability and environmental selection at work. Their spleens are 50% larger than average, acting as a reservoir of oxygenated blood and endowing them with consummate endurance underwater. Our most profound cultural tool, language, is in some ways culturally selected. We owe our acrobatic way with words to a larynx that descends at three months of age. Thereafter, Vince notes, we can no longer swallow and breathe at the same time. Our languages shape our thinking and cultural identity in many ways, but environment also shapes speech. Languages in warm, wet, wooded regions tend to have more vowels and fewer consonants. Languages that emerged at altitude have more words with a strong expulsion of air in the consonants. Tonality in languages (in which a word has different tones that change meaning) is important. The emergence of non-tonal languages over the past 50,000 years — Homer’s Greek was tonal, modern Greek is not — might have influenced the spread of two gene variants involved in brain growth, according to a 2007 study (D. Dediu and D. R. Ladd Proc. Natl Acad. Sci. USA 104, 10944–10949; 2007). So words also shape our inheritance.   Vince has a lot to say about words. The average response rate between speakers during a conversation is 200 milliseconds. But it takes 600 milliseconds for the signal to go from ears to brain, to understanding, to the preparation of a response and its transmission. Thus, conversation must rely on a sophisticated prediction system that commits a large part of the brain to both speaking and listening. Language, writes Vince, “gives us an unparalleled ability to convey an infinity of ideas. We use it mainly to talk about ourselves.” Of course we do. Humans might not be so much Homo sapiens as Homo narcissus, the self-absorbed species. Yet all of our capacities together have, in their different ways, endowed us with the capacity to become a super-organism. We are now a globally connected urban species, outsourcing our brains to computers, increasingly to artificial intelligence and (so far) to nine billion robots. We have begun the Anthropocene, and our demands on the planet are not sustainable. That could usher in a new dark age, or a global order in a new shared civilization. We transcend our evolutionary beginnings. Vince dubs this emerging species Homo omnis, or Homni for short. Her chosen analogue for such a biological super-organism is not flattering: it is the slime mould, in which single cells coalesce as one to move on. The fortunate are protected at the centre; those on the margin become vulnerable to environmental change. Which sounds disturbingly like us. Many aspects of Transcendence have been explored before. And, with that wealth of palaeoanthropological and other research to draw from, most of the chapters become a mosaic of tersely introduced evidence. Read it anyway. It is at least 22 times more memorable than many textbooks, and a good story without — so far — a happy ending. </body>
<date id = '254'>30 October 2019</date>
<url id = '255'>https://nature.com/articles/d41586-019-03268-y</url>
<title id = '255'>Susannah Cahalan’s investigation of the social-psychology experiment that saw healthy people sent to mental hospitals finds inconsistencies — Alison Abbott reviews.</title>
<body id = '255'>David Rosenhan and his volunteers feigned symptoms to be admitted to psychiatric hospitals.Credit: Denver Post via Getty The Great Pretender: The Undercover Mission that Changed our Understanding of Madness Susannah Cahalan Grand Central Publishing (2019) From 1969 to 1972, an extraordinary experiment played out in 12 psychiatric institutions across 5 US states. Eight healthy people — including David Rosenhan, a social psychologist at Stanford University in California, who ran the experiment — convinced psychiatrists that they needed to be committed to mental hospitals. The ensuing paper, published in Science in 1973, opens with the words: “If sanity and insanity exist, how shall we know them?” It claimed that the psychiatric establishment was unable to distinguish between the two. Rosenhan’s study had far-reaching and much-needed effects on psychiatric care in the United States and elsewhere. By the 1980s, most psychology textbooks were quoting it. It also influenced society more widely, and not always positively: in the law courts, for instance, it undermined the value of expert testimonies from psychiatrists. Now, in The Great Pretender, journalist Susannah Cahalan turns a fresh, critical eye on the experiment and the shockwaves it sent through the field and beyond. Cahalan quotes a former colleague of Rosenhan’s, who notes that he was a good networker, an excellent lecturer and a generally charismatic character. “But some people in the department called him a bullshitter,” Kenneth Gergen says. And through her deeply researched study, Cahalan seems inclined to agree with them. She discovered that the man whom she had initially admired, and who had done so much to change how mental illness was perceived, was not all that he had seemed. And neither, she argues, was his famous experiment.   Cahalan began her investigation into Rosenhan’s experiment in good faith. Ten years ago, she developed paranoia, hallucinations and, eventually, seizures. She was dosed with antipsychotics before being correctly diagnosed with a very rare type of autoimmune encephalitis, an ordeal she describes in her first book, Brain On Fire. After it was published in 2012, a casual conversation with McLean Hospital psychiatrists in Boston, Massachusetts, alerted Cahalan to Rosenhan’s experiment. She immediately wanted to know more — about the experiences of those who volunteered, and the challenges that such a risk-laden experiment would have posed decades ago. Rosenhan was not the first to infiltrate a psychiatric hospital and report on conditions. Cahalan tells, for example, of the nineteenth-century journalist Nellie Bly, who deceived doctors to spend ten days in an overcrowded women’s asylum on Blackwell’s Island, New York. Bly’s reports of the appalling conditions there shamed politicians into increasing the asylum’s budget. But Rosenhan was the first to carry out a formal experiment involving a number of “pseudopatients”. All eight, including Rosenhan, reported the same symptom to different doctors: that they heard voices uttering “thud, empty, hollow”, denoting existential doom. Seven were diagnosed with schizophrenia; one with manic depression. Once admitted to hospital, the volunteers stopped simulating symptoms of abnormality. Rosenhan noted in the Science paper that genuine patients often realized that the pseudopatients did not have a mental-health disorder, and accused them of being undercover journalists or academics checking up on the hospital. Psychiatrists seemed less perceptive: it was several weeks before some of the pseudopatients got discharged. Although Rosenhan died in 2012, Cahalan easily tracked down his archives, held by social psychologist Lee Ross, his friend and colleague at Stanford. They included the first 200 pages of Rosenhan’s unfinished draft of a book about the experiment.   At first, it seemed that Cahalan’s research was going to be easy, even though Rosenhan had given fictitious names to the pseudopatients she wished to track down, along with the hospitals they went to. Ross warned her that Rosenhan had been secretive. As her attempts to identify the pseudonymous pseudopatients hit one dead end after the other, she realized Ross’s prescience. The archives did allow Cahalan to piece together the beginnings of the experiment in 1969, when Rosenhan was teaching psychology at Swarthmore College in Pennsylvania. The students complained that the course was too abstract, so Rosenhan suggested that they check into a psychiatric hospital to get to know people with schizophrenia personally. The superintendent of the local Haverford State Hospital was willing to take them on, but Rosenhan cautiously decided to check things out for himself first. He emerged humbled from nine traumatizing days in a locked ward, and abandoned the idea of putting students through the experience. But it set him thinking about a scientific experiment aimed at exposing the system’s travesties. According to Rosenhan’s draft, it was at a conference dinner that he met his first recruits: a recently retired psychiatrist and his psychologist wife. The psychiatrist’s sister also signed up. But the draft didn’t explain how, when and why subsequent recruits signed up. Cahalan interviewed numerous people who had known Rosenhan personally or indirectly. She also chased down the medical records of individuals whom she suspected could have been involved in the experiment, and spoke with their families and friends. But her sleuthing brought her to only one participant, a former Stanford graduate student called Bill Underwood.   Underwood and his wife were happy to talk, but two of their comments jarred. Rosenhan’s draft described how he prepared his volunteers very carefully, over weeks. Underwood, however, remembered only brief guidance on how to avoid swallowing medication by hiding pills in his cheek. His wife recalled Rosenhan telling her that he had prepared writs of habeas corpus for each pseudopatient, in case an institution would not discharge them. But Cahalan had already worked out that that wasn’t so. Comparing the Science report with documents in Rosenhan’s archives, she also noted many mismatches in numbers. For instance, Rosenhan’s draft, and the Science paper, stated that Underwood had spent seven days in a hospital with 8,000 patients, whereas he spent eight days in a hospital with 1,500 patients. When all of the leads from her contacts led to ground, she published a commentary in The Lancet Psychiatry asking for help in finding them — to no avail. Had Rosenhan invented them, she found herself asking? In recent years, other heroes of social psychology have been found to have misrepresented their data. The most prominent case is that of Dutch social psychologist Diederik Stapel, who was forced to retract 58 papers. Those who have followed these cases might be appalled by the Rosenhan story, but will not be surprised. Cahalan, whose life was saved by front-line medical science in the context of psychiatry, was shocked by what she found. She writes that she cannot be completely certain that Rosenhan cheated. But she is confident enough to call her engrossing, dismaying book The Great Pretender. </body>
<date id = '255'>29 October 2019</date>
<url id = '256'>https://nature.com/articles/d41586-019-03081-7</url>
<title id = '256'>Robert Shiller’s new book probes how social behaviour trumps statistics in determining the fate of economies — Tim Jackson reviews.</title>
<body id = '256'>A technician monitors cryptocurrency-mining rigs at a Bitfarms facility in Saint-Hyacinthe, Canada.Credit: James MacDonald/Bloomberg via Getty Narrative Economics: How Stories Go Viral and Drive Major Economic Events Robert J. Shiller Princeton University Press (2019) “Economists are tellers of stories and makers of poems,” wrote the economic historian Deidre McCloskey in 1990. It’s a curious observation for a profession that prides itself on hard-nosed, quantitative analysis and strives continually for predictive power. The Nobel-prizewinning economist Robert Shiller goes even further. Stories are more powerful than statistics, he claims. The irrationality inherent in financial exuberance (and despair) defies the neat territory of numbers and demands a deeper excursion into the decidedly unruly world of narratives. That is the declared aim of his book Narrative Economics. It’s a compelling hypothesis. Since the 1960s, we have known that science is socially constructed. Since the 1980s, sociologists have sought to understand the ‘social amplification of risk’ — in which people are drawn inexorably towards stories of disaster or triumph (rather than statistics or probabilities) as the lodestone for the perceptions of risk that guide their everyday decisions. Around the same time, philanthropist George Soros adapted the concept of reflexivity to explain how investors’ perceptions affect the social environment, which, in turn, informs their perceptions. This feedback loop allows speculative bubbles to arise with alarming speed, and then collapse again. The phenomenon reached its apotheosis in a now infamous remark from Citibank chief executive Chuck Prince that “when the music stops, in terms of liquidity, things will be complicated. But as long as the music is playing, you’ve got to get up and dance.” His prophetic words came just months before the 2007–08 financial crisis struck.   Shiller elevates these insights into a full-blown exploration of the multiple ways in which narratives influence economic behaviour. Much as he tracked the rise and fall of asset prices in his Nobel-prizewinning work, he now charts the flux of narrative memes using Google’s Ngram Viewer — which allows users to track the frequency of words and phrases in text over time — and Proquest’s database of news citations. It’s a quaint device, and there’s a deceptive similarity between the time-series graphics in Narrative Economics and those in his bestselling book Irrational Exuberance (2000). But the message is effective: the value of your story might go up as well as down. The empirical core of the book is a detailed exploration of numerous real-life case studies, ranging from bimetallism (an old-fashioned form of money) to bitcoin (a brand-new one), and from the Great Depression of the 1930s to the Great Recession of recent years. Along the way, his anecdotes form a fascinating subscript. A convincing case is made, for instance, that fears of a ‘singularity’ — a point of no return arising from technological advances — are perennial. He notes numerous viral outbursts of this meme (associated with cotton mills, electricity and computers, for instance) dating back to the nineteenth century. Today’s apocalyptic anxieties about a robot takeover are nothing new and should not be heeded, Shiller seems to imply. How that will turn out remains to be seen. We learn that the mechanism through which a memorable turn of phrase goes viral can be described as a form of contagion, mirroring models from epidemiology. But we are also persuaded that viral success depends inherently on the messenger. Few remember that the phrase “the only thing we have to fear is fear itself”, immortalized by US president Franklin D. Roosevelt during the Great Depression, was first uttered years earlier by economist Irving Fisher. It’s troubling, of course, to be reminded that the rewards for creativity are often misallocated — particularly in today’s plagiaristic world of social media, with its immense powers of narrative acceleration. But for me, this particular example raised a deeper concern. Fear is a rational response from people whose livelihoods are under existential threat. So why would a president inveigh against it? The answer is that Roosevelt was painfully aware of the implications of fear. He was addressing what the economist John Maynard Keynes (borrowing from another long-forgotten creative) called the “paradox of thrift”: the tendency of ordinary people to curtail their consumption in the face of economic uncertainty, and put their money into savings instead.   Such behaviour is sensible, admirable even, at the individual level. Perhaps it is so at the planetary level, too: lower consumption might benefit the environment. But economics has a problem with it. As people spend less, demand is suppressed, prolonging the recession. The same thing happened after the 2007–08 crisis. The paradox of thrift was the foundation for Keynes’s most famous proposal: that governments provide stimulus that could return the economy to growth when people would not. This was the rationale for Roosevelt’s New Deal package of reforms, and the inspiration for the proposed US legislation called the Green New Deal. Keynes was a pragmatist; his prescriptions were a response to the diseases of the day. But he was also a visionary. In his essay ‘Economic Possibilities for Our Grandchildren’ (1930), he foresaw a time when our society would move beyond growth. It hasn’t happened yet — in spite of economist Kenneth Boulding’s remark to the US Congress in 1973 that “anyone who believes exponential growth can go on forever in a finite world is either a madman or an economist”. Shiller is clearly not a madman. But in the course of an otherwise fascinating exploration of the power of story, he never once acknowledges that eternal growth is itself just a narrative. He notes that the logic of relentless expansion conflicts with the logic of human anxiety. But he assumes that it is people who are at fault. Narratives can have clear, moral and prudential foundations, it seems, but they might still be cast as irrational. Indeed, for Shiller, that memorable speech on the “fear of fear” shows government attempting to “lean against false or misleading narratives and establish a moral authority against them”. Roosevelt’s remark was designed to “create and disseminate counternarratives that establish more rational and more public-spirited economic behavior”. What Shiller seems to be saying is this: when ordinary human sentiment runs counter to the prevailing logic of capitalism, the state must override it. It is a deeply suspect, potentially dangerous conclusion. But it, too, demonstrates just how pervasive narrative is. Ultimately, Narrative Economics is an eloquent and accessible exposition of a seductive idea. It’s a particularly compelling hypothesis for Britain, a country still reeling from a public referendum whose outcome was determined by viral confabulations of the most pernicious kind. We are all “tellers of stories and makers of poems”. But neither economists nor politicians can claim moral authority over narrative truth. We must all choose carefully which stories we live by. </body>
<date id = '256'>23 October 2019</date>
<url id = '257'>https://nature.com/articles/d41586-019-03082-6</url>
<title id = '257'>The strange circumstances surrounding the invention of the world’s first PC are probed by a new book — Sharon Weinberger reviews.</title>
<body id = '257'>Engineer Adriano Olivetti in his typing machine factory in Ivrea, Italy.Credit: Keystone-France/Gamma-Keystone via Getty The Mysterious Affair at Olivetti: IBM, the CIA, and the Cold War Conspiracy to Shut Down Production of the World’s First Desktop Computer Meryle Secrest Knopf (2019) In the depths of the cold war, an Italian industrialist on the cusp of marketing the first personal computer dies on a train to Switzerland. Adriano Olivetti has had contact with Western spy agencies; his associates hint that his heart attack might not be what it seems. Such is the thriller-esque start to biographer Meryle Secrest’s The Mysterious Affair at Olivetti. At the heart of Secrest’s book lie two questions: how did the Italian typewriter company Olivetti produce the world’s first PC in the 1960s — long before its competitors — only to have its work fall into obscurity? And could Adriano Olivetti’s death be linked to the company’s disappearance from computer history? Secrest weaves a startling narrative around these events, involving a US intelligence agency and an information-technology multinational. The story goes back to Camillo Olivetti, the Jewish-Italian industrialist who founded the company in Ivrea, Piedmont, in 1908. His visionary son Adriano, who succeeded him as company head in 1938, was interested in architecture, politics and technology. He began to look beyond typewriters to machines combining the best aspects of form and function. More crucially, he started to expand from mechanical typewriters into electronics. When the Second World War broke out, Adriano Olivetti paid lip service to the fascists while secretly working to remove prime minister Benito Mussolini, all while keeping his factory going and his family alive. He survived the war, the company thrived, and he opened an electronics laboratory that drew on his experience in the United States. In the late 1950s, the company created one of the world’s first transistorized mainframes, the ELEA 9003. Olivetti’s table-top computer, ‘Programma 101’ in 1966.Credit: Ullstein Bild/Getty Olivetti’s death in 1960 threatened to derail the plans he had set out for the company to further expand into computers. Moreover, the firm was in a downward spiral, following his decision in 1959 to buy his main competitor, the US typewriter firm Underwood. Yet Mario Tchou, a key engineer who oversaw the company’s electronics work, was already thinking about shrinking mainframes into something that could sit on a desk. Adriano’s talented but less savvy son Roberto oversaw manufacturing of the Programma 101 (P101) desktop computer, which made its debut in 1965. It was the world’s first PC, and sold an astonishing 44,000 units over several years, including some to NASA. But the company’s computer manufacturing was eventually overtaken by its competitors, particularly in the United States. That sounds like the guts of a great technology history. The book’s subtitle, meanwhile, promises spy-versus-spy intrigue involving the CIA and US computer giant IBM. However, Secrest focuses more on the Olivetti family than its products. There is a bare-bones description of the P101 and how it was developed: the programming system, Secrest notes, “took an enormous amount of experimentation”. But there is little more on what must be an intriguing techie history.   Secrest thus also misses several opportunities to tease out intriguing storylines. For example, Adriano Olivetti’s insistence that something sitting in your office should be both functional and beautiful almost certainly inspired Apple co-founder Steve Jobs. The aesthetic similarities between Olivetti’s 1960s-era showroom on Fifth Avenue in New York City and today’s iconic Apple stores are uncanny. The book’s treatment of espionage is at times more detailed than its take on tech. Secrest describes fascinating wartime contacts between Adriano Olivetti and British and US intelligence agencies. While feigning loyalty to the Fascist Party, the industrialist was secretly meeting with the US Office of Strategic Services (OSS), the predecessor to the CIA, which dubbed him ‘Agent 660’. There is drama in this. But as Secrest makes clear, Adriano was no 007; the OSS never acted on his plans, and British intelligence seemed to dismiss him as a dreamer and deemed his convoluted scheme for toppling Mussolini unrealistic. The inside of an Olivetti Programma from the 1960s.Credit: Giorgio Perottino/Reuters The narrative takes a stranger turn around Adriano Olivetti’s death. It seems plausible that, saddled with mounting debt and Underwood’s outdated factories, a 58-year-old businessman might die of a heart attack. Instead, Secrest decides that the CIA murdered Olivetti — as well as Tchou, who died in a car accident in 1961. Gaining access to CIA records is certainly arduous, and Secrest describes her unsuccessful attempt to meet with the agency’s historian, David Robarge. In the absence of insider insights or access to fresh archival records, she turns to a car-repair shop owner in Rockville, Maryland, for confirmation of her theory that the CIA engineered the car accident that killed Tchou. The CIA, of course, really has attempted to assassinate certain figures, such as Patrice Lumumba, the first prime minister of the Democratic Republic of the Congo. But Secrest presents no evidence that US spies were involved in Olivetti’s death. She implies that IBM, too, was somehow implicated, citing cold-war competition and the company’s work for the US government and intelligence. (She reminds us that IBM, as documented by Edwin Black in his 2001 book IBM and the Holocaust, sold technology to the Nazis in the 1930s.) A link between that and the Olivetti affair is never aired, however. This conspiracy-mongering is a shame. Secrest does all the right research, and the clues to the company’s troubles (and Olivetti’s woes) are right in front of her. In an era of rampant conspiracy theories, such as bizarre allegations involving the Jewish Hungarian-American billionaire George Soros, we rely on scholarship to pull out the facts, not just the speculation. A more interesting historical question is why US computer science advanced so quickly during the cold war, leaving Europe behind for decades. It’s likely that this happened because the Pentagon and US intelligence agencies invested in companies and technologies that had no immediate commercial prospects, but served US strategic interests (see page 481). The relationship between spies, soldiers and computer scientists during and after the second half of the twentieth century is worthy of serious exploration. The Mysterious Affair at Olivetti does not offer that. Yet this book is, in other ways, a laudable attempt. It shines when describing Adriano Olivetti’s interest in architecture (Secrest authored the 1992 book Frank Lloyd Wright: A Biography). Secrest writes well on the aesthetics of Olivetti machines and Adriano’s attraction “to clean, boxy lines”, the signature of the Bauhaus movement. Her biographer’s instinct — choosing a visionary figure whose contributions have not been fully appreciated — is also to be applauded. As she laments, “the Programma 101 has not been well served by computer historians on or off the Internet.” She is right. That record remains to be filled. </body>
<date id = '257'>22 October 2019</date>
<url id = '258'>https://nature.com/articles/d41586-019-03080-8</url>
<title id = '258'>A look at hormone studies dissects fact from fake and questions interpretations — Randi Hutter Epstein reviews.</title>
<body id = '258'>Polarized-light micrograph of crystals of testosterone.Credit: Sidney Moulds/SPL Testosterone: An Unauthorized Biography Rebecca M. Jordan-Young, Katrina Karkazis Harvard University Press (2019) On 1 June 1889, renowned neurologist Charles-Édouard Brown-Séquard shocked his colleagues. Speaking at the Paris Society of Biology, the 72-year-old announced that a slurry made from the ground testicles of guinea pigs and dogs (injected under his skin ten times in three weeks) made him stronger. He also noted that his “jet of urine” lengthened by 25%. Brown-Séquard was ridiculed by his peers throughout Europe for disseminating results with no scientific basis and promoting quack youth-enhancing ‘cures’. Yet the bizarre elixir found favour with members of the public in the United States, United Kingdom and Europe — at least among men eager to recapture youthful sexual prowess. As the engaging book Testosterone explains, Brown-Séquard’s testimonial helped to shape future studies that linked the hormone to alleged ‘manliness’. Anthropologist Katrina Karkazis and sociomedical scientist Rebecca Jordan-Young did not write Testosterone to rehash familiar tales of wacky hormone experiments of yore, although this is one of a few that they include. Their contention is that many testosterone researchers — then and now, and intentionally or not — interpret data with blinkers on. When the facts do not fit the paradigm, the authors argue, findings are moulded into flawed dogma. Karkazis and Jordan-Young strive to comprehend how scientific practice around testosterone unfolds, and explore how the results “circulate and morph in the world”. Today, the biochemistry of this steroid hormone is well known, from its daily fluctuations to its synthesis from cholesterol and occasional conversion to oestradiol, a form of oestrogen. Testosterone is known to restore sex drive and muscle tone among men with ailments that reduce levels of the hormone, such as pituitary tumours. During puberty, a surge of testosterone in young men typically leads to enlargement of the muscles, penis, testes and prostate gland, and the emergence of secondary sex characteristics. In women, testosterone excreted by the adrenal glands and ovaries is generally important for ovarian function and bone strength. Portrait of Charles-Edouard Brown-Sequard in 1890.Credit: DeAgostini/Getty Like pathologists doing a post mortem, Karkazis and Jordan-Young dissect the remains of a selection of studies. They parse statistics and the cultural context that prompted the research and influenced how the data were analysed. (Full disclosure: I have served on a history of medicine panel with Karkazis and, as medical authors writing about endocrinology, our paths have crossed several times.) The authors delve first into testosterone’s role in ovulation. The hormone and its precursor, DHEA, have a role in the maturation of ovarian cells; DHEA might boost fertility directly or as a mediator of oestrogen production. There are chapters focusing on traits often assumed to be associated with testosterone, such as athleticism. The authors also scrutinize the brouhaha surrounding a small psychology study1 claiming that holding particular poses boosts testosterone production. There is a section on parenting, thanks to studies that created a fleeting media buzz by claiming that new fathers’ testosterone plummets when they change nappies and do other nurturing chores2,3. And the authors discuss athletes who take testosterone to boost their abilities. They do not dispute that injections, gels or patches that send testosterone levels skyrocketing above the norm build muscles when coupled with intense training. But they are sceptical about whether the hormone makes a large difference for every athlete. Some studies, they write, have found a correlation between high natural testosterone levels and speed and power; others show tenuous or no links. And a few studies link higher testosterone levels to worse performance. Jordan-Young and Karkazis challenge murky definitions. They show how researchers define risk-taking through “weirdly narrow and also wildly divergent” behaviours, such as riding a motorcycle without a helmet. They cite a team that surveyed business students about their entrepreneurial experience and used a saliva sample to gauge their testosterone levels4. On the basis of these dubious data, the investigators concluded that those who had the highest levels, coupled with family business experience, were the most entrepreneurial.   When it comes to testosterone and aggression, the authors say that some of the most rigorous studies (double-blind, placebo-controlled) show no connection. What’s more, they write that even the investigators of studies that tie testosterone to violence acknowledge that the link is inconsistent and weak. Yet the idea that testosterone drives violence remains widely accepted and “grossly overblown”. By setting the record straight, the authors build on their past record. Jordan-Young explored the evidence for putative neurological sex differences in the 2011 book Brain Storm; Karkazis demolished preconceptions about people who are intersex in her 2008 work Fixing Sex, which also explores the often disturbing history of ‘treatments’ for ‘ambiguous’ genitalia. Although often academic in tone, the book is leavened by a welcome informality. The authors describe the link between testosterone and violence as a zombie: “a fact that seemingly can’t be killed with new research”. They personify testosterone as “T” and characterize their book as an “unauthorized biography”. An authorized biography, they note, “sweeps away all kinds of details and smooths over contradictions”. Theirs intends to pull back a veil that has obscured the field. Still, I sometimes wanted more. In a chapter on ovulation, they quote a woman receiving fertility treatment who thinks that a therapy containing DHEA helped her to produce more eggs of higher quality. The authors note that the idea of testosterone aiding a woman’s fertility has been anathema to reproductive endocrinologists, but quote only one clinician. That left me wondering whether other clinicians were still reluctant, or if this were part of standard treatment. I wanted to hear from other fertility clinicians. In the opening of the chapter on athleticism, the authors refer to a 2012 meeting with an endocrinologist who explains that testosterone rises sharply in response to intense exercise, but that responses vary among athletes. Then, they describe an interview with a second expert who tells them the opposite, and also says that some types of sports training might lower testosterone. I wanted to know who these experts were. Moreover, although Jordan-Young and Karkazis are lively storytellers, every now and then an anecdote doesn’t jibe with the chapter’s content. For example, they start the discussion on risk-taking with a delightful account of 63-year-old Annie Edson Taylor, who in 1901 went over Niagara Falls on the US–Canadian border in a pickle barrel. That seems a literary stretch: we know nothing of Taylor’s hormonal state (except that because she was probably postmenopausal, her testosterone would have been low, and her oestrogen and progesterone certainly lower than before). These quibbles, however, are minor in a deeply researched and thoughtful book that adds a fresh perspective to a growing body of work aiming to debunk myths about hormones. </body>
<date id = '258'>22 October 2019</date>
<url id = '259'>https://nature.com/articles/d41586-020-00577-5</url>
<title id = '259'>How Nature reported efforts in 1970 to assess the economic benefits of scientific discoveries, and an update from 1870 on the search for the cause of malaria.</title>
<body id = '259'> A way to approach the problem of quantifying the economic benefits from curiosity-oriented research has recently been suggested by Byatt and Cohen. It consists essentially of identifying key discoveries which have had profitable applications and then estimating the economic effects of notional marginal delays in the timing of these discoveries; that is, of attempting to assess how much less wealth, suitably discounted to a common year, would have arisen if, because of a smaller scale of effort in particular areas of research, certain discoveries had been made later than they actually were … From the studies we have summarized, we conclude that Byatt–Cohen type innovations are quite difficult to find, let alone investigate. Interactions between science and technology are usually too complex for the method of notional marginal delay. Only rarely is it possible to pinpoint specific curiosity-oriented discoveries from which wealth-producing applications are derived. Even when this can be done, other factors affecting the timing of innovations, such as market factors or war-time pressures, obscure the effects of possible delays in discoveries. From Nature 14 March 1970 Since men of science … have proved by repeated and well-conducted experiments that there is life in the ocean, that there are moving, sensible, living creatures, of nearly every description, in its deepest recesses, it seems rather an idle question … which has been raised lately about their manner of living there; how they get their food where no plant of any description has ever grown; whether they take in their food by intussusception with a mouth … Yet these seemingly idle questions when treated by men of science and of experience may become the source of discoveries far greater and more important perhaps than they anticipated. Thus it is that the indefatigable Italian diatomist, Count Castracane, after having proved the very abundant growth of his puny protégés in the brackish waters of the Maremme and Paludi pontine, did not esteem it a bootless task to search for what they live upon, and also why they suddenly die away nearly all at once … [H]e goes on to state that nothing is so fatal to the life of marine or even brackish water diatoms as a sprinkling of pure fresh water … From this fact he comes to the very probable conclusion that the sudden dying away of myriads of diatoms, besides, perhaps, myriads of other living creatures, during the rainy season might be, if not the only, at least one of the most efficient causes of malaria. From Nature 10 March 1870 Latest on: History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Malaria Article 22 APR 20 News & Views 25 DEC 19 News 04 DEC 19 Research management Career Column 22 MAY 20 News 20 MAY 20 Career Column 15 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '259'>10 March 2020</date>
<url id = '260'>https://nature.com/articles/d41586-020-00636-x</url>
<title id = '260'>A brain structure called the cerebellum has mostly been associated with learning from errors. The discovery that the cerebellum is also involved in reward-driven learning in monkeys implies a previously unappreciated role in cognition.</title>
<body id = '260'>People and organizations alike use rewards, from snacks to salary bonuses and frequent-flyer miles, to shape behaviour through a process called reinforcement learning. For example, if a dog receives a treat for rolling over in response to a verbal command, the likelihood of that behavioural response to the verbal cue will increase. Writing in Neuron, Sendhilnathan and colleagues1 describe neuronal signals that could support such reward-driven learning. What is remarkable is where the authors found these signals — not in the brain areas that have long been implicated in reinforcement learning, but in the cerebellum, a brain structure historically associated with error-driven, rather than reward-driven, learning. The cerebellum is best known for its role in motor-skill learning — the process by which movements become smooth and accurate through practice. Fifty years of research2 supports the idea that when you practise a movement, such as your tennis backhand, the cerebellum uses feedback about errors to gradually refine the accuracy of the movement by weakening the neuronal connections that are responsible for those errors. It has been widely assumed that the cerebellum uses a similar, error-correcting learning algorithm to support cognition3, because the regions of the cerebellum that contribute to cognitive functions such as navigation4 and social behaviour5 have the same basic circuit architecture as those that control movement. In the past three years, however, there has been a flurry of studies showing reward-related neuronal activity in the cerebellum6–12. What are reward signals doing in an error-correcting part of the brain? Sendhilnathan et al. leveraged the rapid learning abilities of monkeys to gain fresh insights into reward-related signalling in the cerebellum. In each experimental session, the authors presented a monkey with two visual cues it had never seen before on a computer screen. One arbitrarily assigned cue would result in the monkey receiving a reward of fruit juice if the animal responded by lifting its left hand. The other cue would result in a reward if the monkey lifted its right hand. The researchers monitored the activity of neurons called Purkinje cells in the cerebellum as the monkeys learnt, through trial and error, to make the correct response to each visual cue (Fig. 1). Figure 1 | A rewarding choice. a, Sendhilnathan et al.1 examined neural activity in a brain region called the cerebellum during reward-driven learning. The authors presented monkeys with two visual cues. For one cue, the monkey needed to lift its left hand to receive a reward of fruit juice; for the other, lifting the right hand would lead to a reward. b, The monkeys performed a series of trials in which they were presented with one of the two cues. The authors monitored cerebellar neuronal activity while the monkeys learnt, through trial and error, which response would produce a reward for each cue. They found that a subpopulation of neurons carried information about the success or failure of the previous trial until the next trial was completed (not shown). (Figure adapted from Fig. S2 of ref. 1.) Sendhilnathan et al. found that the activity of cerebellar Purkinje cells carried information about the success or failure of the monkey’s most recent attempt at the task. One subpopulation showed high activity following a correct response to the cue; another showed high activity following a failed attempt. These signals arose a few hundred milliseconds after the end of a trial and persisted until the next trial was completed. As such, they seemed to provide a working memory that could enable the outcome of one trial to guide the next behavioural choice. These signals are reminiscent of those carried by neurons in frontal and parietal regions of the brain’s cerebral cortex, which encode the ‘value’ of different behavioural choices on the basis of reward history over multiple trials13. In the current study, the cerebellar neurons kept track of only the most recent trial’s outcome. But in this task, the outcome of a single trial provides sufficient information for the monkey to infer the correct response for the next trial — if a reward was not given when a monkey lifted its right hand in response to one visual cue, for instance, then the correct response to that cue must be to lift the left hand, and the correct response to the other visual cue would be to lift the right hand. It would be interesting to know whether cerebellar neurons can keep track of a more-extended history of rewards should the task require it, and whether the cerebellum interacts with the cerebral cortex in performing this computation.   Importantly, information about the previous trial’s outcome was present in the cerebellum only when a new set of cue–response associations was being learnt. As monkeys improved their performance over trials, the neuronal activity encoding each outcome waned. Moreover, the signal was not present when monkeys earned rewards by responding to a pair of visual cues that they had mastered through several months of training. These observations indicate that cerebellar neurons are not simply carrying information about rewards, predictions about rewards or the movements that animals make when anticipating rewards. Rather, the cerebellum seems to contribute specifically to learning about how to earn rewards in a new situation. The authors speculate that the cerebellum might enhance the rate of learning about rewards, a possibility supported by the recent discovery in rodents of direct, excitatory projections from the cerebellum to neurons in the brain stem that release the reward-associated neurochemical dopamine14. There are several intriguing parallels between the signals found by Sendhilnathan and colleagues and the signals involved in cerebellar control of movement. First, as with reward-driven learning, for some motor skills, cerebellar Purkinje cells contribute selectively to new motor learning and not to performing older motor skills15,16. Second, Purkinje-cell activity carries information that could guide both ongoing behaviour and the induction of learning during motor- and reward-based learning17. Third, the Purkinje cells carry signals that could support working memory in the form of activity maintained from one trial to the next in reward-based learning, and in the form of activity maintained during a delay period between a cue and the motor response to the cue, which seems to support motor planning11,18. Finally, during both types of learning, individual Purkinje cells are active for a specific time period of a few hundred milliseconds, with information seemingly passed from cell to cell over time19. These striking parallels raise the possibility that the cerebellum performs a similar function for error-driven motor learning and reward-driven reinforcement learning. We learn from both our successes and our failures. These two learning schemes were previously attributed to distinct brain structures, but the current results, along with those of others6–12, blur these mechanistic and conceptual boundaries. As such, the work highlights the need to consider how long-range interactions between brain areas support the shaping of behaviour by experience. </body>
<date id = '260'>09 March 2020</date>
<url id = '261'>https://nature.com/articles/d41586-020-00423-8</url>
<title id = '261'>A survey of tree establishment, growth and mortality shows that the rate at which Amazonian tropical forests take up carbon dioxide has slowed since the 1990s, whereas signs of a potential slowdown in Africa appeared only in 2010.</title>
<body id = '261'>Figure 1 | Taking an inventory of the Amazon rainforest. A researcher takes measurements of a tree trunk at a height of 2 metres above the ground. Long-term monitoring such as this can be used to estimate the amount of carbon stored by tropical forests.Credit: João Marcos Rosa The total area of the world that is covered by tropical forest is declining because of deforestation, land degradation and fires — a trend that has increased over the past few years1. At the same time, human-induced climate change is altering the functioning of tropical forests2. During the 1990s and early 2000s, structurally intact tropical forests actively removed carbon from the atmosphere (in the form of carbon dioxide) through photosynthesis, and stored it as biomass. Such forests have been responsible for about 50% of the terrestrial carbon sink3. Hubau et al.4 report in Nature that this globally crucial tropical carbon sink is becoming saturated in both Amazonian and African rainforests, but with different patterns of change.   Forests act as a net carbon sink when the amount of carbon gained through the establishment of new trees and tree growth is larger than the amount lost through tree mortality. In these circumstances, the quantity of carbon stored in the biomass increases over time. The interplay of carbon gains, losses and stocks determines the period of time for which carbon remains in the forest, which is known as the carbon residence time5. Hubau and colleagues monitored tree establishment, growth and mortality in 244 undisturbed old-growth forest plots in Africa across 11 countries, between 1968 and 2015, and compared their data with similar measurements from 321 plots in Amazonia6. Such long-term monitoring is essential for identifying trends and drivers of the carbon sink in forest biomass, but is highly challenging and costly in terms of coordination, labour and funding — particularly in the tropics, where access to field sites is difficult and working conditions are harsh (Fig. 1). The authors find that the carbon sink in African tropical-forest biomass was stable for the 30 years up to 2015, in contrast to the sink in Amazonian tropical forests, for which the annual net amount of accumulated carbon started to decline around 1990 (Fig. 2). So what drives the slowdown of the tropical carbon sink, and why are there differences between Amazonian and African tropical forests? Figure 2 | Estimates and projections of tropical carbon sinks. Hubau et al.4 have estimated the net amount of carbon that was absorbed from the atmosphere by tropical forests — the tropical carbon sinks — in Africa and Amazonia for the period from 1968 to 2015, using measurements of tree establishment, growth and mortality; only estimates from 1990 onwards are shown. The data show that the sink in Amazonia has declined since the 1990s, whereas the African sink was stable for the 30 years up to 2015. The authors also estimated the carbon sinks using statistical models, which they extrapolated to 2040. The extrapolations suggest that, by 2030, the carbon sink in Africa will be 14% lower than in 2010–15, whereas the Amazonian carbon sink will reach zero by 2035. Data shown are mean values; see Fig. 3 of ref. 4 for confidence intervals. The authors report a long-term trend of increasing carbon gains in the forests on both continents throughout the period studied, which correlates with the increase in atmospheric CO2 concentrations. They attribute the rising gains to CO2 fertilization — an increase in carbon uptake by plants that occurs as atmospheric CO2 levels rise. However, they find that increasing mean annual temperatures and drought since 2000 have reduced tree growth and thus offset the increase in carbon gains, with smaller reductions in Africa than in Amazonia. Hubau et al. go on to show that high carbon gains persisted for longer in Africa than in Amazonia because the warming rate was slower, there were fewer droughts and air temperatures were generally lower (because African forests are located at higher elevations). And, in contrast to an earlier study6, the authors were able to clearly attribute the decline of carbon gains in Amazonia to increasing temperatures and repeated extreme drought events, on the basis of a statistical analysis of their data. The researchers find no signs of the CO2-fertilization effect levelling off on either continent. Although the authors attribute the decline in carbon gains on both continents to climatic drivers, other limiting factors might be responsible — such as competition between trees for light and nutrients, and the general availability of nutrients on each continent. These factors were not considered in their statistical analysis, but might further constrain tree growth and weaken the sink as atmospheric CO2 concentrations continue to increase. Such limitations have been hinted at from experiments in which the atmospheric concentration of CO2 is enriched in a specific area of an ecosystem7, but no such experiment has been carried out in highly diverse, old-growth tropical forests such as those in Africa and Amazonia.   In addition to the trends in carbon gains, Hubau et al. find that carbon losses in Africa were stable from the 1990s until a decade ago, and then started to increase. By comparison, carbon losses in Amazonia had already started to increase in the 1990s. This continental difference seems to be because trees in Amazonia grow faster and have shorter carbon residence times than do those in African forests. Carbon dioxide fertilization might increase growth rate and carbon gains, but it also leads to quicker losses — CO2-fertilized trees grow fast and die young5,6, and therefore might not necessarily contribute to the carbon sink in the long term. The authors find that tree mortality associated with chronic long-term heat and drought leads to increased carbon losses, and that this effect is more pronounced in Amazonian than in African tropical forests as a result of accelerated warming rates in Amazonia since 2000. Data from the most intensively monitored African plots indicate that carbon losses in those forests began increasing from about 2010. The authors extrapolate their statistical models up to the year 2040, and thereby suggest that the carbon sink will decline on both continents. They estimate that, by 2030, the carbon sink in Africa will be 14% lower than in 2010–15, whereas the Amazonian carbon sink will reach zero by 2035 (that is, there will be no net carbon uptake from the atmosphere). These extrapolations need to be interpreted carefully, however, because they are in striking contrast to projections made by global models — which predict a strong, continuing carbon sink due to CO2 fertilization in intact tropical forests8. Recently reported models9 of vegetation growth that consider nutrient cycling show that the Amazonian-forest carbon sink is strongly constrained by the availability of phosphorus in soils. Hubau and colleagues’ findings underline the need to understand other factors that affect tree mortality and forest dynamics, in addition to such nutrient feedbacks, so that these can be integrated into global models2. So, what does a pan-tropical decline of the carbon sink in intact forests imply for the current climate crisis? Calculations of the maximum amount of anthropogenic carbon emissions that can be emitted to limit global warming to well below 2 °C — the goal of the 2015 Paris climate agreement — count on the continuation of a large tropical carbon sink10. Hubau and co-workers’ finding that tropical sinks are disappearing and could very soon turn into carbon sources suggests that, as well as strong protection of intact tropical forest, even faster reductions of anthropogenic greenhouse-gas emissions than those set out in the agreement will be needed to prevent catastrophic climate changes. </body>
<date id = '261'>04 March 2020</date>
<url id = '262'>https://nature.com/articles/d41586-020-00535-1</url>
<title id = '262'>Neurogenetic tools commonly used in model organisms have now been adapted to investigate feeding behaviour in the fly Drosophila sechellia. The experiments shed light on why this fly is such a fussy eater.</title>
<body id = '262'>Even closely related animals can behave in strikingly different ways. For example, the fly Drosophila sechellia feeds exclusively on the toxic noni fruit (Morinda citrifolia), whereas its closest relatives reject noni in favour of more-conventional options1. In these flies2–4, and in other animals5–7, researchers have observed intriguing neural differences between close relatives that might explain their differing behaviours. But until recently, it has been impossible to test these neural–behavioural correlations directly. Writing in Nature, Auer et al.8 adapt genome-editing approaches for use in D. sechellia. This allows them to probe the neural and genetic changes underlying the fly’s dietary preference. Our story begins with the arrival of D. sechellia’s ancestors on the Seychelles archipelago in the Indian Ocean, probably a few hundred thousand years ago9. Although at first glance a tropical paradise, the islands probably offered a harsh welcome. The noni fruit — nicknamed the vomit fruit for its pungent smell — might have been one of the only food sources consistently available to the ancient castaways10. At first, noni would have been unappealing and even deadly to the flies, but over time they evolved to tolerate the toxins and love the smell10. Present-day D. sechellia feed on the fruit exclusively1 (Fig. 1). By contrast, D. sechellia’s sibling species Drosophila simulans and more distant cousin, Drosophila melanogaster, retain their dislike for noni8. Figure 1 | Drosophila sechellia feeds and breeds solely on the noni fruit. Auer et al.8 explore the neural and genetic mechanisms that underlie this unusual behaviour.Credit: Benjamin Fabian What makes D. sechellia such a picky eater compared with its cosmopolitan, generalist relatives? In 2003, scientists studying the sense of smell in this group of flies uncovered an intriguing clue2. One class of sensory neuron that expresses the protein odorant receptor 22a (Or22a) was more abundant in D. sechellia than in any other fly species they analysed. And in both D. sechellia and D. simulans, these neurons were attuned to a class of compound prevalent in noni odour. Subsequent work revealed similar changes in further sets of noni-sensitive neurons3,4. Could these olfactory alterations underlie the specialists’ appetite for the smelly fruit? It seemed likely, but an inability to precisely manipulate D. sechellia’s olfactory system prevented scientists from moving beyond correlational evidence.   Now, Auer et al. have finally cracked the case using the genome-editing tool CRISPR–Cas9. This technology is commonly used in model organisms such as D. melanogaster and the mouse, Mus musculus, to manipulate genes at will. However, importing the technique into other species is not always straightforward. Other animals might take poorly to life in the laboratory, or it could be difficult to obtain enough viable embryos during the crucial time frame when genome editing takes place. The authors cleared these obstacles, thus gaining the precise genetic control necessary to begin rigorous causality testing. Auer et al. focused on how changes in Or22a contribute to D. sechellia’s selective diet. Inactivating the Or22a gene left the fly almost completely unable to locate its favourite fruit from just under one metre away. This result confirmed that neurons expressing Or22a process cues that help D. sechellia to target noni. But removing a receptor completely is a drastic manipulation — more extreme than the receptor ‘tuning’ that occurred as Or22a evolved greater sensitivity to noni compounds. It is similar to asking whether you can still perform a concerto on a violin missing a string. The missing string is clearly important, but you cannot tell how its tuning would have affected your performance. The authors therefore sought to explicitly test how tuning changes in Or22a affect noni-seeking behaviour. They substituted Or22a in D. melanogaster with the version of the receptor from D. sechellia, and vice versa. The two species’ receptors are nearly identical, harbouring just a few changes in amino-acid residues that tweak sensitivity to different compounds. To continue the musical analogy, we might compare this experiment to swapping strings between a violin and a viola and asking how the mismatched, differently tuned strings on each instrument affect the recital. Remarkably, the receptor swap gave D. melanogaster a slight taste for noni and diminished D. sechellia’s attraction to the fruit. This definitive test, made possible by the group’s cutting-edge toolkit, clearly showed that changes in Or22a tuning contribute to D. sechellia’s partiality for noni.   Of course, evolution of Or22a tuning is only part of the story. One of the most interesting aspects of Auer and colleagues’ study is just how many evolutionary changes might contribute to this apparently simple behavioural shift. The authors confirmed2 that, besides tinkering with Or22a’s tuning, evolution has amplified its ability to trigger downstream signalling in D. sechellia by doubling or tripling the number of Or22a neurons. Further receptor-deletion experiments strongly suggested that previously documented changes3,4 in two other key classes of sensory neuron are also involved. And remodelling of downstream circuits might play a part, too: Auer et al. discovered a structural branch on neurons deep in D. sechellia’s brain that could alter how the fly processes information about noni odour. Unfortunately, it remains difficult to directly test causality for many of these evolutionary changes. We can cleanly manipulate the activity of sensory neurons by altering the receptors they express, and can even modify the activity of neurons deeper in the brain — as demonstrated by recent work on the evolution of central brain circuits in two other non-model Drosophila species11,12. However, it is difficult, if not impossible, to precisely manipulate structural features such as the number of neurons in a circuit or the connections between them. This wiring is established early in an animal’s development, and has a genetic basis that is not yet well enough understood to allow custom manipulation. As our neurogenetic toolkits expand, it will be exciting to continue piecing together the puzzle of D. sechellia. We are entering an era in which genetic tools are available to alter precise targets in the nervous systems of diverse organisms. At the same time, we have countless observations of variations in animal behaviour at our disposal, gathered over the past century and more. By combining these two resources, as Auer et al. have done in D. sechellia, we can finally begin to test long-standing hypotheses about behavioural evolution across a diverse range of organisms. Even humble flies that love stinky fruit can provide powerful insight into how brains evolve to shape complex behaviours. </body>
<date id = '262'>04 March 2020</date>
<url id = '263'>https://nature.com/articles/d41586-020-00462-1</url>
<title id = '263'>A by-product of alcohol metabolism can damage the genome by crosslinking opposing DNA strands. The discovery of a safe mechanism that reverses such damage might open up avenues of research for drug discovery.</title>
<body id = '263'>Aldehydes are highly reactive molecules that can enter the body from the environment, and can be produced by cellular metabolic processes. One aldehyde relevant to human health is acetaldehyde, which is produced when cells process ingested alcohol. If acetaldehyde accumulates in cells, it reacts with DNA and can link two strands together, generating an extremely harmful form of damage known as a DNA interstrand crosslink1 (ICL). ICLs are also produced by many anticancer drugs, to kill tumour cells. Writing in Nature, Hodskinson et al.2 report the discovery of a mechanism for repairing acetaldehyde-induced ICLs that is safer than the commonly used route.   An inability to repair ICLs is linked to the rare genetic disease Fanconi anaemia (FA). This condition is caused by mutations in any one of 22 FANC genes, which encode proteins that participate in ICL repair3. People who have FA experience genomic instability, bone-marrow failure and premature ageing, and have a high risk of developing cancer. Since the 1970s, scientists have known that the cells of people who have FA are exquisitely sensitive to ICL-inducing drugs4, but it was not until 2011 that researchers found genetic evidence5 suggesting that acetaldehyde-derived DNA damage is a driving force of FA. How this damage is repaired was unknown. The need to clear acetaldehyde from cells to prevent DNA damage (lesions) became evident after the in vivo identification of a two-tier system in mice that protects against this highly reactive molecule5. The first tier of protection involves the enzyme aldehyde dehydrogenase 2 (ALDH2), which converts acetaldehyde to harmless acetate molecules (Fig. 1). Inactivation of this enzyme is common in members of Asian populations, and is associated with a higher incidence of alcohol-derived cancers6. The second tier is repair of the DNA damage generated by acetaldehyde. Figure 1 | Cellular defences against acetaldehyde. When humans ingest alcohol, it is converted in the liver to toxic acetaldehyde by the enzyme alcohol dehydrogenase (ADH). Acetaldehyde can also be formed by other metabolic processes, or come from the environment (not shown). The compound is detoxified by another enzyme, aldehyde dehydrogenase 2 (ALDH2), but still sometimes accumulates in cells, in which it forms interstrand crosslinks (ICLs) between bases in DNA molecules. This damage can be repaired by the Fanconi anaemia pathway, in a process that involves the formation of DNA breaks either side of the ICL. However, DNA breaks are potentially dangerous, and can lead to harmful chromosome rearrangements and cancer. Hodskinson et al.2 report a second pathway for ICL repair in which the crosslink, rather than a DNA strand, is cut. This completely restores one of the bases that was crosslinked, and leaves an adduct on the other. This repair process prevents chromosomal rearrangements. Because the combined inactivation of FANC and ALDH2 genes recapitulates the characteristics of FA in mice, it is suspected that ICLs are the cytotoxic (cell-killing) lesions generated by acetaldehyde5. Consistent with this view is the observation that FA severity correlates7 with the presence of an ALDH2 mutation in Japanese people who have FA. However, direct investigation of these crosslinks is not possible in cellular or in vivo systems using available technologies. Hence, whether acetaldehyde-induced ICLs accumulate in people who have FA remains a crucial unanswered question. A previously reported cell-free in vitro system derived from frog eggs8 has been widely used to study the mechanisms underlying repair of ICLs induced by other agents, including the anticancer drug cisplatin9. This system allows DNA molecules that contain a single, site-specific DNA lesion to be analysed. In the case of cisplatin-induced ICLs, the cell-free system revealed a sophisticated repair mechanism that depends on FANC proteins9,10. This mode of repair requires DNA replication and cuts DNA strands to ‘unhook’ and remove the ICL (Fig. 1). In their work, Hodskinson et al. undertook the enormous challenge of synthesizing a DNA molecule containing a single, site-specific acetaldehyde ICL, and then investigated how the lesion is repaired in the cell-free system. They found that this repair process requires an active FA pathway (a mechanism that involves FANC proteins). This is consistent with genetic evidence that FANC proteins are required in the two-tier system that protects against acetaldehyde damage. However, the authors unexpectedly discovered that about half of the crosslinks are fixed by a second, faster mechanism. Further investigation revealed that this second route also involves DNA replication, but is independent of the FA pathway.   Surprisingly, in the fast repair route, no cuts are made to the DNA strands; instead, the ICL is probably cut within the crosslink. This mode of repair results in the reversion of the crosslink to an undamaged base on one of the DNA strands, but leaves an adduct on the other strand (Fig. 1), which specialized DNA-replication enzymes can bypass to complete repair. This mechanism is reminiscent of the one that fixes ICLs generated by the drug psoralen11, but involves different enzymes. By avoiding DNA breaks — which are associated with genomic rearrangements, one of the hallmarks of cancer and ageing — the fast repair mechanism has an important advantage over the FA pathway. Taken together, Hodskinson and colleagues’ findings provide a holistic glimpse of how acetaldehyde-derived crosslinks are cleared from DNA, and support the idea that these lesions contribute to FA. The authors do not identify a protein that cleaves the crosslinks in the newly described repair route. One can therefore only speculate as to whether cleavage occurs spontaneously as a consequence of mechanical forces generated during replication as the DNA unwinds, or is the result of enzymatic activity. If it is indeed an enzymatic process, identifying the components of the pathway will be a challenge, but could open up opportunities for therapies: stimulation of the pathway might alleviate the symptoms of FA, or reduce the incidence of alcohol-derived cancers. The identification of the protein(s) involved in the crosslink cleavage would also allow in vivo experiments to test whether impairment of the alternative repair route increases acetaldehyde toxicity, especially under conditions in which this molecule is not detoxified by metabolism. Furthermore, mutations in the genes encoding proteins involved in this pathway might reveal the existence of a new group of people who have an FA-like disorder. In the meantime, Hodskinson and colleagues’ study underlines the need to develop better assays to study ICLs and other types of DNA damage in cells. By studying the repair of specific DNA lesions induced by compounds such as acetaldehyde, or other mutagens that arise in the body, we are likely to uncover other cellular defence mechanisms against cytotoxic DNA damage. </body>
<date id = '263'>04 March 2020</date>
<url id = '264'>https://nature.com/articles/d41586-020-00552-0</url>
<title id = '264'>Cellular stress can result in dysfunction and disease, and mechanisms exist to combat this. Previously unknown steps have been uncovered in a pathway that signals when mitochondrial organelles are dysfunctional.</title>
<body id = '264'>Organelles called mitochondria are responsible for storing energy derived from the food that we eat, in the form of molecules called ATP. Although a mitochondrion has its own genome, 99% of this organelle’s proteins1 are encoded by nuclear genes and imported from the cytosol (the liquid part of the cytoplasm) into the mitochondrion. To function effectively, this process requires coordination and communication, and it must be able to respond to any mitochondrial dysfunction that might occur. Environmental toxins2 and disease-causing agents3, as well as diverse age-associated conditions, including Alzheimer’s disease4 and Parkinson’s disease5, are associated with mitochondrial dysfunction. Writing in Nature, Guo et al.6 and Fessler et al.7 report a previously unknown mechanism that is used by mitochondria to send a signal of their dysfunction to the cytosol and nucleus, enabling the cell to adapt to mitochondrial stress.   Studies of the nematode worm Caenorhabditis elegans indicate that coordination between the nucleus and mitochondria during stress is regulated by a combination of remodelling of chromatin (the complex of DNA and protein in the nucleus) and activity of a transcription-factor protein that responds to mitochondrial dysfunction8,9. Mammalian studies paint a different picture and implicate a process called the integrated stress response (ISR), which causes an overall reduction in protein production but an increase in the production of several transcription factors. The ISR is activated in response to diverse cellular stresses, including those that don’t involve mitochondria. In 2002, researchers discovered10 that mitochondrial perturbations drive the synthesis of a component of the ISR — a transcription factor called CHOP — and induce the expression of two types of mitochondrial protein that aid the ISR. These are chaperones, which aid protein folding, and proteases, which are enzymes that cleave proteins. One enduring mystery has been whether mitochondrial dysfunction also directly regulates kinase enzymes in the cytosol that are needed for the ISR, and that act by adding a phosphate group to proteins. The ISR is regulated by such phosphorylation of the protein eIF2α (Fig. 1), which is involved in initiating the translation of messenger RNA during protein synthesis. eIF2α phosphorylation is mediated by four kinases — GCN2, PERK, PKR and HRI — that each phosphorylate eIF2α in response to different stressors. GCN2 is stimulated by depletion of amino acids; PERK responds to the presence of unfolded proteins in an organelle called the endoplasmic reticulum; PKR acts when double-stranded RNA accumulates in the cytoplasm during viral infection; and HRI is enlisted when the molecule haem is depleted11,12. The phosphorylation of eIF2α results in a reduction of total protein synthesis, but promotes production of the transcription factors ATF4, ATF5 and CHOP. These transcription factors harbour regulatory elements in their mRNA that facilitate translation when eIF2α is phosphorylated11,13. Figure 1 | A mitochondrial signal triggers a response that combats stress. When organelles called mitochondria become dysfunctional in mammalian cells, this can activate a pathway called the integrated stress response (ISR). Guo et al.6 and Fessler et al.7 have identified some previously unknown steps in this process. The authors report that mitochondrial dysfunction caused the protein OMA1, which is located on the inner mitochondrial membrane, to cleave the protein DELE1, a fragment of which enters the cytosol and binds to the enzyme HRI, activating it. HRI adds a phosphate group (P) to the protein eIF2α, and this phosphorylation slows the synthesis of most cellular proteins from messenger RNA (mRNA), mediated by the ribosome complex. This decrease in protein synthesis is one hallmark of the ISR. However, the production of the transcription factors ATF4, ATF5 and CHOP increases11,13 (Guo et al. and Fessler et al. did not study ATF5). These transcription factors are required to initiate the ISR. To understand how mitochondrial stress triggers an ISR, Guo et al. and Fessler et al. took similar experimental approaches using mammalian cells grown in vitro. Fessler and colleagues studied cells engineered to express a fluorescent version of CHOP that could be used to monitor the induction of an ISR. The authors induced random mutations in cells, and so identified genes that encode proteins needed to trigger the ISR. Guo and co-workers used cells engineered to express a fluorescent version of ATF4, and applied the gene-editing tool CRISPR to interfere with gene expression. Both teams identified genes that, when inhibited, altered the production of CHOP or ATF4 in their respective systems. One gene that both groups focused on encodes HRI. The authors discovered that mitochondrial dysfunction caused HRI to phosphorylate eIF2α even when haem was plentiful, which was surprising, given that HRI activation had been thought to depend on haem depletion11,12. The result revealed a previously unsuspected form of haem-independent regulation of HRI.   The two teams also identified another gene implicated in triggering an ISR — one that encodes the protease OMA1. OMA1 is located on the inner of the two mitochondrial membranes that surround the organelle, and is activated by a change in the electrical charge (depolarization) on the mitochondrial membrane that occurs during dysfunction14. The hunt was on to find a protein that is cleaved by OMA1 to activate HRI and the ISR. One gene of interest identified by Guo et al. and Fessler et al. encodes DELE1, a little-studied protein that resides in the space between the two mitochondrial membranes and is associated with the inner membrane. Inhibition of DELE1 prevented the phosphorylation of eIF2α in response to mitochondrial stress, as did inhibition of OMA1. These results are consistent with a model in which the two proteins function upstream of HRI activation. Both groups report that mitochondrial dysfunction causes a fragment of DELE1 to accumulate in the cytosol through an OMA1-dependent process. The studies reveal that the portion of cleaved DELE1 that enters the cytosol binds to HRI and activates it. Consistent with this model, both groups demonstrate that expression of this cleaved form of DELE1 in the cytosol is enough to stimulate HRI and increase CHOP and ATF4 expression in the absence of mitochondrial dysfunction. These studies clearly establish a previously missing link between mitochondrial dysfunction and the ISR. However, the consequences of ISR activation in response to mitochondrial dysfunction are still not fully understood. Diverse forms of cellular stress activate an ISR, raising the question of whether the ISR is tailored downstream of eIF2α phosphorylation to match the specific stress conditions. And how the ISR protects the cell during mitochondrial dysfunction isn’t clear. Is the reduction in overall protein synthesis the main protective function, or is protection mainly mediated through the action of the transcription factors associated with the ISR? It would be interesting to learn whether ATF4, CHOP or ATF5 are regulated by a post-translational modification in response to mitochondrial dysfunction, because this might offer a way to tailor their action to the type of stress that initiates the ISR. In these conditions, cells lacking components of ISR signalling fared better than did cells that had such signalling components. Findings by Guo, Fessler and their respective colleagues suggest that activation of the ISR can be protective or maladaptive, depending on the mitochondrial perturbation involved. Guo et al. report that DELE1 and HRI promote the survival of cells in which the mitochondrial protease LONP1, which degrades damaged proteins in mitochondria, was impaired. And some unknown aspect of the ISR can be detrimental to cells. This was observed, for example, when Guo et al. treated cells with oligomycin, a molecule that inhibits the mitochondrial enzyme ATP synthase (which helps to make ATP), and when Fessler et al. treated cells with carbonyl cyanide m-chlorophenyl hydrazone (CCCP), which depolarizes the inner mitochondrial membrane. Both studies used cultured mammalian cells that mainly rely for their energy production on a metabolic pathway called glycolysis, which occurs in the cytosol, rather than depending on energy production from mitochondria. It will therefore be interesting to know whether this newly discovered pathway acts in certain mammalian tissues, such as muscle and nerves, that are particularly dependent on energy produced from mitochondria rather than by glycolysis, and whether this pathway is involved in the diverse diseases that can result from mitochondrial dysfunction. Such investigations should further illuminate how cells monitor and regulate mitochondria, and the ways in which these systems might fail during ageing and disease. </body>
<date id = '264'>04 March 2020</date>
<url id = '265'>https://nature.com/articles/d41586-020-00592-6</url>
<title id = '265'>An image-sensor array has been developed that acts as its own artificial neural network to capture and identify optical images simultaneously, processing the information rapidly without needing to convert it to a digital format.</title>
<body id = '265'>Sight is one of our most vital senses. Biologically inspired machine vision has developed rapidly in the past decade, to the point that artificial systems can ‘see’ in the sense of gaining valuable information from images and videos1,2, although human vision remains much more efficient. Writing in Nature, Mennel et al.3 report a design for a visual system that, rather like the brain, can be trained to classify simple images in nanoseconds.   Modern image sensors such as those in digital cameras are based on semiconductor (solid-state) technology and were developed in the early 1970s; they fall into two main types, known as charge-coupled devices and active-pixel sensors4. These sensors can faithfully capture visual information from the environment, but generate a lot of redundant data. This vast amount of optical information is usually converted to a digital electronic format and passed to a computing unit for image processing. The resulting movement of massive amounts of data between sensor and processing unit results in delays (latency) and high power consumption. As imaging rates and numbers of pixels grow, bandwidth limitations make it difficult to send everything back to a centralized or cloud-based computer rapidly enough for real-time processing and decision-making — which is especially important for delay-sensitive applications such as driverless vehicles, robotics or industrial manufacturing. A better solution would be to shift some of the computational tasks to the sensory devices at the outer edges of the computer system, reducing unnecessary data movement. And because sensors normally produce analog (continuously varying) outputs, analog processing would be preferable to digital: analog-to-digital conversion is notoriously time- and energy-consuming. To mimic the brain’s efficient processing of information, biologically inspired neuromorphic engineering adopts a computing architecture that has highly interconnected elements (neurons, connected by synapses), allowing parallel computing (Fig. 1a). These artificial neural networks can learn from their surroundings by iteration — for instance, learning to classify something after being shown known examples (supervised learning), or to recognize a characteristic structure of an object from input data without extra information (unsupervised learning). During learning, an algorithm repeatedly makes predictions and strengthens or weakens each synapse in the network until it reaches an optimum setting. Figure 1 | Computing within a vision sensor for intelligent and efficient preprocessing. a, In conventional artificial-intelligence (AI) vision sensors, signals are collected from light-responsive sensors, converted from analog to digital form (ADC, analog-to-digital converter), amplified and then fed as inputs to an external artificial neural network (ANN) — layers of interconnected computational units (circles) whose connections can be adjusted, allowing the network to be trained to perform tasks such as classifying images. An input layer of the ANN receives signals encoding simple physical elements (represented here by dots and lines); in subsequent layers, these are optimized to mid-level features (simple shapes); and refined images are formed at the output layer (3D shapes). The overall response can be slow and energy-hungry. b, Mennel et al.3 report a system in which interconnected sensors (squares) on a chip not only collect signals, but also work as an ANN to recognize simple features, reducing movement of redundant data between sensors and external circuits. Mennel and co-workers implement an artificial neural network directly in their image sensor. On a chip, they construct a network of photodiodes — tiny, light-sensitive units, each consisting of a few atomic layers of tungsten diselenide. This semiconductor’s response to light can be increased or decreased by altering an applied voltage, so that the sensitivity of each diode can be individually tuned. In effect, this turns the photosensor network into a neural network (Fig. 1b) and allows it to carry out simple computational tasks. Changing the light responsivity of a photodiode alters the connection strength — the synaptic weight — in the network. Thus, the device combines optical sensing with neuromorphic computing. The authors arrange the photodiodes into a square array of nine pixels, with three diodes to each pixel. When an image is projected on to the chip, various diode currents are produced, combined and read. The hardware array provides a form of analog computing: each photodiode generates an output current that is proportional to the incident light intensity, and the resulting currents are summed along a row or column, according to Kirchhoff’s law (a fundamental rule of currents in circuits). The array is then trained to perform a task. The discrepancy between the currents produced by the array and the predicted currents (the currents that would be produced if the array responds correctly to the image, for a given task) is analysed off-chip and used to adjust the synaptic weight for the next training cycle. This learning stage takes up time and computing resources, but, once trained, the chip performs its set task rapidly. Using different algorithms for the neural network, the authors demonstrate two neuromorphic functions. The first is classification: their 3 × 3 array of pixels can sort an image into one of three classes that correspond to three simplified letters, and thus identify which letter it is in nanoseconds. This relatively simple task is just a proof of concept, and could be extended to recognizing more-complicated images if the array size were scaled up.   The second function is autoencoding: the computing-in-sensor array can produce a simplified representation of a processed image by learning its key features, even in the presence of signal noise. The encoded version contains only the most essential information, but can be decoded to reconstruct an image close to the original. There is more to be done before this promising technology can be used in practical applications. A neuromorphic visual system for autonomous vehicles and robotics will need to capture dynamic images and videos in three dimensions and with a wide field of view. Currently used image-capture technology usually translates the 3D real world into 2D information, thereby losing movement information and depth. The planar shape of existing image-sensor arrays also restricts the development of wide-field cameras5. Imaging under dim light would be difficult for the device described by the authors. A redesign would be needed to improve light absorption in the thin semiconductor and to increase the range of light intensities that can be detected. Furthermore, the reported design requires high voltages and consumes a lot of power; by comparison, the energy consumption per operation in a biological neural network is at the sub-femtojoule level (10−15 to 10−13 joules)6. It would also be useful to expand the response to ultraviolet and infrared light, to capture information unavailable in the visible spectrum7. The thin semiconductors used are difficult to produce uniformly over large areas, and are hard to process so that they can be integrated with silicon electronics, such as external circuits used for readout or feedback control. The speed and energy efficiency of devices that use these sensors will be dominated not by the image-capturing process, but by data movement between sensors and external circuits. Moreover, although the computing-in-sensor unit collects and computes data in the analog domain, reducing analog-to-digital conversions, the peripheral circuits still suffer from other intrinsic delays. The sensors and external circuits will need to be co-developed to decrease the latency of the entire system. Mennel and colleagues’ computing-in-sensor system should inspire further research into artificial-intelligence (AI) hardware. A few companies have developed AI vision chips based on silicon electronics8, but the chips’ intrinsic digital architecture leads to problems of latency and power efficiency. More broadly, the authors’ strategy is not limited to visual systems. It could be extended to other physical inputs for auditory, tactile, thermal or olfactory sensing9–11. Development of such intelligent systems, together with the arrival of the 5G fast wireless network, should allow real-time edge (low-latency) computing in the future. </body>
<date id = '265'>04 March 2020</date>
<url id = '266'>https://nature.com/articles/d41586-020-00536-0</url>
<title id = '266'>How Nature reported the response to early research into in vitro fertilization in 1970, and the classification of ancient burial sites in 1870.</title>
<body id = '266'> The test tube baby has struck again. Several years, perhaps even decades, in advance of it being even theoretically possible to grow mammalian embryos to full term outside the uterus, the test tube baby has been forcibly delivered and morally agonized over. “The era of the test tube baby has begun” was the annunciation made by one newspaper to its readers … The genesis of the affair was the announcement last week … that a technique of in vitro fertilization was shortly to be attempted at Oldham General Hospital as a treatment for infertile patients … A curious feature of the public debate is that the letter writing segment of the public, at least, seemed to believe that human life was about to be created from nothing in the test tube … These are indeed dark atavistic fears which have been nurtured … that scientists have usurped the creative powers and should assume the moral responsibilities formerly attributed to gods … There is always the danger that lack of information or misinformation may convert legitimate public concern about new knowledge into a paranoia that impedes research. From Nature 7 March 1970 The chief result of the examination of the Ancient British barrows of the south-west of England is their division into two great classes — (1) the Long Barrows, the primary interments of which have yielded implements of stone and bone only, and which are, therefore, confidently assigned to the Stone Age; and (2) the Round Barrows, affording implements of bronze as well as of stone, and occasionally, though rarely, of iron. The round barrows vary considerably in form, and Dr. Thurman thinks that these variations are not to be attributed to the individual fancy of the builders. He recognises three primary forms of round barrow — the bowl-shaped, the bell-shaped, and the disc-shaped … Remains of oxen … are often found in long barrows not far from the human remains … It would appear that oxen were slaughtered at the funeral feasts, and that the heads and feet (the bones of which parts are more frequently found), not being used for food, were buried in the barrow, perhaps as offerings to the gods or to the spirits of the dead. From Nature 3 March 1870 Latest on: Archaeology News 19 MAY 20 Article 11 MAY 20 Research Highlight 23 APR 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Medical research News 25 MAY 20 News 22 MAY 20 News 21 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '266'>03 March 2020</date>
<url id = '267'>https://nature.com/articles/d41586-020-00564-w</url>
<title id = '267'>A gene has been found that controls the conversion of the parasite Toxoplasma gondii into a form that chronically infects the human brain. The discovery could aid the design of therapies to eliminate this currently untreatable infection.</title>
<body id = '267'>It is estimated that around one-third of the global human population1 is infected with the single-celled organism Toxoplasma gondii, a parasite that can be ingested in food or picked up from activities such as gardening2. The parasite needs to differentiate into a chronic-stage form to establish a permanent infection in brain and muscle tissue, but how this parasite conversion occurs has been a mystery. Writing in Cell, Waldman et al.3 report the identification of a gene that encodes a master regulator of this differentiation event. Toxoplasma gondii can infect any warm-blooded animal. Human infection can occur through eating undercooked meat from infected livestock or by ingesting contaminated food or water. Within a couple of weeks of entering its host, T. gondii is converted from a form called an acute-stage tachyzoite into a bradyzoite, which establishes a chronic infection (Fig. 1). A bradyzoite forms a cyst that resides in host cells and is surrounded by a thick wall of proteins and sugars. The wall is a formidable barrier that makes the cyst inaccessible and thwarts its elimination by drugs or the host’s immune system. Figure 1 | How Toxoplasma gondii parasites differentiate to cause a chronic infection. a, Toxoplasma gondii infects humans, and can be life-threatening. During the initial stages of infection, the parasite exists in the bloodstream in a form called an acute-stage tachyzoite, which is in a vacuole. It is taken up by a host cell (not shown) and the cell and vacuole subsequently burst. The parasite enters the brain and gives rise to a chronic infection. Such infection occurs when the parasite differentiates into a form called a chronic-stage bradyzoite. b, Waldman et al.3 report that the gene BDF1 is required for this differentiation step. In both the acute and chronic stages of infection, this gene is transcribed into messenger RNA. However, the encoded protein BFD1 is preferentially made during chronic infection. BFD1 is a transcription-factor protein that can drive the expression of genes needed for the formation of bradyzoites. Although T. gondii infection is widespread in human populations, it is often harmless, being in the relatively quiescent state of bradyzoites that have not reverted to the activated tachyzoite form associated with disease. However, T. gondii infection can be life-threatening for unborn fetuses or for people whose immune systems are compromised. Moreover, in the United States, 2% of T. gondii infections result in sight problems or blindness owing to ocular damage caused by treatment-resistant parasites4. To uncover the signal that controls the formation of bradyzoites, Waldman and colleagues engineered T. gondii to express a green fluorescent protein if such cysts formed. Monitoring the fluorescent protein using microscopy and cell-sorting technologies offered a way of assessing whether the parasite had differentiated into the form associated with chronic infection. Exposure to stress-inducing treatment in culture conditions, such as an alkaline pH, made the parasite differentiate into bradyzoites. The authors used the gene-editing tool CRISPR to disrupt selected genes to assess whether any of them affected differentiation. The results were stunning and clear. The disruption of only one targeted gene — which the authors call bradyzoite-formation deficient 1 (BFD1) — prevented the formation of bradyzoites. BFD1 encodes a transcription-factor protein belonging to a family known as Myb-domain-containing proteins. Waldman et al. demonstrate that the Myb domain of BFD1 protein drives T. gondii differentiation. This is particularly intriguing because another Myb-domain-containing protein controls chronic-stage cyst formation in the parasite Giardia lambia5. Moreover, a related member of the Myb-domain-containing protein family enables Plasmodium parasites to develop in red blood cells6. In addition to BFD1, T. gondii encodes 13 other Myb-domain-containing proteins. Identifying their functions and determining whether any of them aid the infection process should be a priority.   Waldman and co-workers report that T. gondii lacking BFD1 fail to establish a chronic infection in mice. When investigating the regulation of BFD1 expression, the authors made the counter-intuitive discovery that the messenger RNA that encodes BFD1 was expressed at a similar level during both the acute and chronic stages of infection. The presence of BFD1 is sufficient to drive parasite differentiation into a bradyzoite, and the mRNA encoding BFD1 is preferentially translated into protein during the chronic stage of infection. Leveraging this finding, Waldman et al. engineered T. gondii to express a form of BFD1 that is unstable unless a specific compound is also given. Consistent with the authors’ model, the compound-mediated stabilization of BFD1 caused the parasite to form a bradyzoite. This discovery raises the question of how the translation of the mRNA that encodes BFD1 is regulated, possibly in response to stress, to trigger chronic infection. As expected, the authors observed that T. gondii parasites differentiated into bradyzoites after several rounds of replication in host cells in vitro under stressful conditions (in vivo stress arises, in part, from the host’s mounting immune response). This process was not synchronous across all parasites being cultured or even for those in one host cell. The researchers therefore used single-cell RNA profiling of wild-type and BFD1-deficient parasites to assess gene-expression profiles associated with the differentiation event. They also investigated the regions of the parasite genome to which BFD1 binds. Gratifyingly, as expected for a transcription factor, BFD1 bound to gene regions called transcription start sites, and, in particular, to those in a large set of genes that the authors had identified as being expressed at higher than normal levels during differentiation.   Many questions remain unanswered regarding BFD1’s regulation of differentiation, and how it might act upstream of a group of previously identified transcription factors called ApiAP2s, which are important, but not sufficient, for differentiation7. Considering that BFD1 is probably regulated by translational control, approaches that determine the RNA content of single cells might not be enough to identify the full cohort of factors driving differentiation. Another way to investigate translational control is to profile RNAs bound to the translational machinery of the ribosome complex. This method has already been used for T. gondii8,9, and should be enlisted to study bradyzoites. Bradyzoites can now be maintained in host cells grown in vitro without adversely affecting the host cells, opening many vistas for future experiments. Particularly exciting is the possibility of analysing bradyzoites during brain infection by using an approach that harnesses stem-cell technologies, such as those that produce neuronal stem cells. CRISPR provides a way of testing the role of host genes, and this method can also target T. gondii both in vitro and in vivo10–12. The availability of these tools sets the stage for new discoveries about the interplay between the parasite, host and immune system throughout the acute and chronic stages of infection. The development of artificial-intelligence methods that enable computer-driven assessments of complex and subtle differences in images of T. gondii offers another way of assessing the infection process13. Given that bradyzoites are the most relevant and challenging stage of the T. gondii life cycle to tackle for the treatment of the human disease, targeting BFD1 shows real potential for making progress in the development of drugs or vaccines. The discovery of one gene that can rule them all moves us closer to solving the riddle of this chronic infection. </body>
<date id = '267'>02 March 2020</date>
<url id = '268'>https://nature.com/articles/d41586-020-00472-z</url>
<title id = '268'>The longitudinal arch has long been considered a crucial structure that provides stiffness to the human foot. Now the transverse arch is stepping into the spotlight, with a proposed central role in the evolution of human foot stiffness.</title>
<body id = '268'>Humans evolved to walk and run effectively on the ground using two feet. Our arched foot, which is not a characteristic of other primates, is a unique feature crucial for human bipedalism. The arch provides the foot with the stiffness necessary to act as a lever that transmits the forces generated by leg muscles as they push against the ground. The arch also retains sufficient flexibility to function like a spring to store and then release mechanical energy. Writing in Nature, Venkadesan et al.1 present a new view of how foot stiffness is regulated. Their finding not only has exciting implications for understanding foot evolution, but also provides a possible framework when considering foot health and how to design better footwear. The foot’s longitudinal arch (the arch that runs from the heel to the ball of the foot; Fig. 1) is often credited2,3 with the leading role in foot stiffening. The ligaments spanning this arch, including the plantar fascia (or plantar aponeurosis), act like a bowstring to resist arch collapse when force is applied. Moreover, the spring-like mechanical properties of these ligaments contribute substantially to the foot’s ability to store and return energy4. Figure 1 | Human foot arches. The longitudinal arch of the human foot has been proposed2,3 to have a key role in providing stiffness for the foot, an attribute that enables humans to walk on the ground on two feet. Venkadesan et al.1 report that another foot arch — the transverse arch, which is in the vicinity of the metatarsal bones — makes a major contribution to foot stiffness. However, Venkadesan and colleagues present the idea that another arch component, the transverse arch (the part of the arch that curves across the foot at the base of the metatarsal bones; Fig. 1) is at least as important for foot stiffness as is the longitudinal arch, if not more so. The authors provide evidence for how transverse-arch curvature might help prevent foot bending and therefore increase foot stiffness. An analogy for this proposed stiffening mechanism is the way that a pizza slice becomes less floppy if the slice’s outer crust is curled up.   Venkadesan et al. initially took a theoretical approach to investigate the role of transverse curvature in stiffening the foot. Modelling an elastic shell, the authors demonstrated that, if the transverse curvature of the shell increased, this increased the stiffness of the shell in the longitudinal direction. Venkadesan and colleagues derived a parameter for curvature and longitudinal stiffness (independent of other factors such as shell size and thickness), and show that a distinct transition point exists beyond which the amount of curvature directly influences the longitudinal stiffness. A similar relationship exists for a physical model consisting of discrete rigid elements (analogous to the metatarsal bones) connected by springs (corresponding to ligaments). To test whether this model might be relevant to the stiffness of the human foot arch, the authors examined human cadaver specimens (frozen after death and then thawed to combat stiffening due to rigor mortis) and cut ligaments in the transverse arch that are expected to be crucial for coupling the curvature of this arch to foot stiffness. Venkadesan et al. then assessed the foot’s vertical deformation when loads were applied. Cutting the transverse ligaments reduced foot stiffness by the remarkable value of more than 40%. By comparison, previous research4 indicates that cutting the foot’s plantar fascia, which spans the longitudinal arch, reduces stiffness by just 23%. Venkadesan and colleagues’ data therefore suggest that transverse ligaments make a substantial contribution to overall foot stiffness. When bearing a load, the foot’s transverse ligaments are presumably stretched by the resultant spreading out of the metatarsals at the ball of the foot. The authors suggest that this ligament stretching is a direct result of transverse-arch curvature. Venkadesan and co-workers examined the evolution of the transverse arch across different primates, including various species of extinct hominin (those species more closely related to humans than to chimpanzees). As in other work5 investigating foot evolution, Venkadesan et al. focused on the amount of torsion (twist) in the fourth metatarsal bone. They estimated the curvature of the transverse arch and determined which species would probably have had sufficient curvature to induce stiffening of this arch to an extent similar to that of modern humans. For example, the authors examined the species Australopithecus afarensis. This species existed more than three million years ago, and whether it walked upright in a human-like fashion is debated6–8. Venkadesan et al. report that the transverse arch of A. afarensis was less curved than that of a human foot and thus, according to their model, probably less stiff. However, the authors correctly emphasize that such curvature alone cannot be used reliably to infer movement capabilities, and other mechanisms might stiffen the foot sufficiently to allow a human-like gait.   The curvature of transverse arches in human populations probably spans a wide range of values. Some people have noticeably flat feet whereas others have a high arch. Perhaps those with flat feet have less curvature of their transverse arch and thus potentially reduced stiffness in their feet compared with those whose feet are less flat. But it is also possible that people with flat feet have sufficient transverse-arch curvature to compensate for their low longitudinal arch, thereby maintaining sufficient stiffness for effective walking and running. Given that Venkadesan and colleagues’ work did not directly test whether there is a relationship between transverse- arch curvature and the stiffness of the human foot, it remains to be determined whether the range of differences in human transverse-arch curvature is a crucial functional parameter to explain foot stiffness. The range of curvature of the arch of human feet suggested by Venkadesan et al. would indicate that a nearly twofold change in stiffness is possible as a result of natural variation in curvature of the trans verse arch from one person to the next. However, any relationship between transverse-arch curvature and stiffness is probably not enough to completely explain the regulation of foot stiffness, and other factors will also need to be considered — for example, the stiffness of the plantar fascia or the potential for muscles to actively regulate arch stiffness. As such, caution is necessary before relying on this curvature parameter alone as the key variable in assessing human foot stiffness. The fields of evolutionary biology, sports science and medicine have largely neglected the transverse arch when trying to explain the managements of loads applied to the foot. Venkadesan and colleagues’ research suggests a new mechanism that links foot form and function and sets the scene for a possible shift in how the human foot is considered. More research will be needed to better understand how the transverse arch contributes to human locomotor performance, including determining what its contribution is to an individual’s foot stiffness and whether this provides any mechanical or energetic benefits. It is conceivable that new treatments that take advantage of transverse-arch curvature to modulate foot stiffness could be developed for various foot disorders. Perhaps even more exciting are the implications of this work for efforts to mimic a human foot when designing prosthetic limbs or legged robots. </body>
<date id = '268'>26 February 2020</date>
<url id = '269'>https://nature.com/articles/d41586-020-00468-9</url>
<title id = '269'>Metallic glasses are much stronger than conventional metals, but form certain instabilities under stress that lead to fracture. A process known as rejuvenation has been shown to solve this problem.</title>
<body id = '269'>Metallic glasses are formed by cooling melted alloys under conditions that prevent the melt from crystallizing1. They have remarkable mechanical properties — in particular, they can be subjected to high forces and undergo a large amount of deformation before they stop behaving elastically and start to deform permanently (plastically). However, they have one key weakness: they are prone to catastrophic failure under stress because they soften during plastic deformation, rather than hardening, as crystalline metals do. Writing in Nature, Pan et al.2 report a method for preparing metallic glasses that causes them to harden during plastic deformation, thereby avoiding the instabilities that lead to failure.   If you take a paper clip and bend it, you’ll find that more force is needed as you bend it to an increasingly sharp angle. This is an example of work, or strain, hardening — the strengthening of a material through plastic deformation. At the atomic scale, the plastic deformation of metallic crystals in the wire is caused by the motion of ‘dislocations’. These linear defects in the crystal structure multiply, intersect and entangle as deformation proceeds, thereby getting in each other’s way and strengthening the material3. This makes work hardening one of the most complex problems in science: it needs to be understood at many length scales, from the atomic-scale lengths of the dislocation cores, through the nano- and micrometre scales involved in dislocation interactions and structures, to the macroscale lengths associated with crack propagation and the structural stability of bulk materials. The mechanical behaviour of metallic glasses is fundamentally different. Because their atomic structure is not periodic, there are no dislocations. Plastic deformation instead occurs through shear, a mode of deformation that affects small groups of atoms (known as shear transformation zones; STZs) throughout the glass4. This shearing loosens (dilates) the atomic structure, and the resulting increase in volume facilitates the formation of new STZs. If the rate of deformation is sufficiently high, the atomic structure does not have time to relax and densify again. As a result, the local deformation rate continues to rise and finally becomes unstable, forming a narrow zone of intense shear strain (a type of deformation) known as a shear band.   Shear bands are macroscopic phenomena. They cause steps to form on the surfaces of materials and can therefore be suppressed by applying appropriate constraints — for example, by sandwiching metallic-glass layers between conventional hard metals5. Pan et al. accomplish this suppression by cutting a deep, narrow notch around the circumference of a cylindrical glass bar, and compressing it in the direction of its axis (see Fig. 1a of the paper). The central region of the bar near the notch undergoes extensive plastic deformation, during which shear bands are suppressed by the constraints exerted by the outer parts of the bar. The authors then cut out the central part and deformed the unconstrained sample under tension or compression. Remarkably, the resulting material exhibits properties similar to those of conventional crystalline metals: it undergoes work hardening and does not form shear bands. The mechanism responsible for this hardening, however, is far from conventional. To explain why, let’s consider the ground states of crystals and glasses. A crystal in its undeformed ground state has the lowest possible flow stress (a measure of the force needed to sustain plastic deformation). The introduction of dislocations during deformation costs energy, and their entanglement raises the flow stress3. A glass in its ground state, however, has the highest possible flow stress because it has the lowest number of STZs. Deformation of this state costs energy, but through shear-induced dilatation introduces new STZs that lower the flow stress (see ref. 6, for example). All glasses are in non-equilibrium states. When they are heated (annealed) to a temperature at which their atoms can move, the process tightens up their atomic packing and lowers their energies towards a ground state7. This process is called structural relaxation, or ageing, and it changes the properties of glasses8. For example, it can increase the density by a few tenths of a per cent; raise the elastic stiffness by a few per cent; increase viscosity by many orders of magnitude; and sometimes cause ductile glasses to become brittle.   Reversal of this ageing process is called rejuvenation, and can be achieved in several ways. The simplest is to heat a glass until it becomes a liquid again, and then rapidly cool it1. Another approach is to ‘shake up’ the structure, for example by ion irradiation9 or plastic deformation10. By heavily deforming samples of metallic glasses under constrained conditions, Pan et al. raise the energies of the glasses far above the energy of the ground state, rejuvenating them and loading them up with STZs. When the authors then deform them under the less-constrained conditions of a tensile or compressive test, structural relaxation sets in: the atomic packing increases and the volume introduced by the earlier deformation disappears; the number of STZs drops, causing the flow stress to increase; and work hardening is achieved. The practical implications of this work are clear: if metallic glasses can be treated so that the threat of shear-band failure is greatly reduced, then they can be more fully exploited for structural applications. However, this will require the development of methods for rejuvenating large volumes of metallic glasses — Pan and colleagues’ rejuvenated samples are only 3 millimetres long and 1.5 mm in diameter. Large-scale rejuvenation will require the deformation of large quantities of alloys under constrained conditions, which could be achieved using methods such as confined cold rolling10 or equal-channel angular extrusion11. The authors’ rejuvenation technique might also advance glass science. Because glasses are not in equilibrium, their properties depend on the processing path by which a particular state is reached. For example, in their experiments, Pan et al. measured the heat of relaxation of their glasses (a measure of the glasses’ internal energy) after rejuvenation and after various stages of subsequent deformation. It would be interesting to know how the structure and other properties of their glasses compare with those of glasses that have the same heats of relaxation, but which were obtained by the cooling of melted material and annealing. In other words, what makes the authors’ rejuvenation technique attractive is that it opens up many more paths for exploring the complex relationship between structure and properties in glasses. </body>
<date id = '269'>26 February 2020</date>
<url id = '270'>https://nature.com/articles/d41586-020-00469-8</url>
<title id = '270'>The dense soup of matter in the core of neutron stars is hard to model, but particle-accelerator experiments in which energetic electrons scatter off atomic nuclei could help to explore this high-density regime.</title>
<body id = '270'>Modelling the fundamental strong force between protons and neutrons — collectively called nucleons — is tricky. But in a paper in Nature, Schmidt and collaborators1 demonstrate a way to explore these interactions in atomic nuclei, and compare experimental measurements against calculations that use various models of the strong nuclear interaction. They do so at the shortest inter-nucleon distances yet probed, by poking nucleon pairs using high-energy electrons and focusing on a previously unexplored regime of short-distance, high-momentum interactions in a nucleus.   Quantum chromodynamics (QCD) is the fundamental theory of the strong interaction, one of the four forces in nature. In that theory, the nucleon–nucleon (NN) interaction that binds protons and neutrons into atomic nuclei is largely determined by the underlying dynamics of quarks and gluons (quarks being the elementary particles that combine to form protons, neutrons and other, less stable particles; gluons are the carriers of the strong force that ‘glues’ the quarks together). However, because the unwieldy nature of QCD makes it impossible to model atomic nuclei computationally, we still lack a truly quantitative understanding of the NN force from QCD. Instead, modellers have resorted to approximations known as effective NN interactions for use in models of nuclear properties2,3. These treat nucleons as point-like objects. Some effective interactions are phenomenological — they are based on experimental data obtained by scattering nucleons from each other3. Others2 are derived from first principles and exploit symmetries manifested in QCD. Because of the way they were developed, we can be fairly confident that the effective NN interactions accurately represent the actual interactions at typical inter-nucleon distances in nuclei, but not necessarily at the tiny distances that are relevant, for example, when describing the high-density cores of neutron stars. We know that the NN interaction is attractive down to about 1 femtometre (10−15 metres). At smaller distances, very strong repulsion sets in2,3. In atomic nuclei, nucleons consequently position themselves close enough to take advantage of the attraction, but shy away from the notoriously hard core of their neighbours at the shortest distances. For the description of most nuclear properties, nucleons can be approximated as independent particles subjected to a mean field created by the other nucleons. But about 20% of the time4, as a result of density fluctuations in nuclei, two nucleons come close enough to form a short-range correlated pair that defies the mean-field description. According to Heisenberg’s uncertainty principle, such large local density fluctuations are associated with large fluctuations in momentum5.   Schmidt and collaborators have now tested the details of effective theories of the nuclear force — that is, theories based on effective NN interactions — using the particle detector system known as the CEBAF Large Acceptance Spectrometer (CLAS) at the Thomas Jefferson National Accelerator Facility in Newport News, Virginia. They scattered energetic electrons off pairs of nucleons that were separated by very small distances (and which have characteristically high momenta) in nuclei to study the NN repulsion, and used the data to test the accuracy of effective NN interactions at these distances. Their work pushes the investigation of such pairs to the highest momenta yet attained. In optics, the resolving power of an instrument is the smallest distance at which two closely spaced objects can be separated. This distance is typically proportional to the wavelength of light used: the smaller the wavelength, the better the resolving power. In nuclear physics, to resolve nucleons in a nucleus, a resolution of about 1 fm is required. The high-energy electron scattering used by Schmidt et al. achieves this resolution because high-energy electrons have a tiny wavelength (the de Broglie wavelength) and a high momentum, which they impart to the nucleon systems being studied. From data taken using the CLAS detector, Schmidt and collaborators picked out reactions in which a scattered high-energy electron (e,e′, where e is the incoming electron and e′ is the scattered electron) liberated a proton (p) from a target nucleus (A); these reactions are described as A(e,e′p) events. More specifically, the authors selected scattering reactions in which a property of the liberated proton known as the missing momentum was measured to be more than 400 megaelectronvolts per c (where c is the speed of light). For the high-energy conditions studied, this missing momentum is approximately equal to the initial momentum of the struck proton inside the nucleus. A proton momentum of 400 MeV c−1 is almost twice that of ordinary mean-field protons, which indicates that the knocked-out proton was part of a short-range correlated pair. Out of these A(e,e′p) events, Schmidt and collaborators then selected a subset in which a second, high-momentum ‘recoil’ proton was released (Fig. 1). These reactions — which are described as A(e,e′pp) events — are the telltale signatures of short-range correlated proton pairs in which the two protons have high and opposite momenta. Figure 1 | High-energy electron scattering probes the strong nuclear interaction. Schmidt et al.1 studied nuclear reactions in which high-energy electrons (e) scattered off systems of nucleons (protons and neutrons; blue and pink, respectively), and in which high-momentum protons were liberated. The resulting data were used to investigate the interactions that occur between nucleons separated by very small distances, and to show that current models of nucleon–nucleon interactions might be valid at these short distances. The reaction yields for the two types of event can be numerically modelled such that the reaction dynamics and associated nuclear structure factorize (they can be separated out from each other). This allows the initial structure of the nuclei in the experiments to be reconstructed from the observed reaction yields. By using the nuclear structure calculated from effective NN interactions to predict yields, and comparing these predicted yields with the experimentally observed ones, Schmidt et al. tested whether effective interactions accurately describe NN interactions at very short inter-nucleon distances. The team calculated the reaction-yield ratios A(e,e′pp)/A(e,e′p) for different nuclear targets and for proton momenta ranging from 400 MeV c−1 to 1 gigaelectronvolt (109 eV) per c. These momenta far exceed the energetics of the nucleon systems that were previously studied experimentally and which were used to develop some of the effective NN interactions. They also greatly exceed the range for which some of the theoretically derived effective NN interactions — including those derived from a much-used approach known as chiral effective field theory — might be expected to be reliable. Nevertheless, the team found that both types of effective NN interaction — the phenomenological and chiral ones — could still be used in models to reproduce the new experimental data astonishingly well. In particular, as the momentum between two nucleons in a pair increases (and the distance between them consequently decreases), the data reflect subtle but important changes in the details of the force, as predicted by the models. The results demonstrate the potential of using such data to dissect the nuclear interaction at short distances. They might justify the use of effective interactions that were optimized at low energies to describe nuclear systems up to densities well above the central density of nuclei. Thus, Schmidt et al. have opened an exciting path to exploring NN interactions at the distances relevant, for example, for neutron stars. Interestingly, the effective NN interaction derived from chiral effective field theory fares less well than the phenomenological effective NN interaction that was tested — the chiral interaction is not as good at predicting absolute yields as it is at predicting ratios. As methods and computational approaches for modelling many-body systems (such as nuclei) advance, it will be possible to examine experimental data using the diverse tools of the trade but with fewer approximations. The ball is in the court of nuclear theory now. </body>
<date id = '270'>26 February 2020</date>
<url id = '271'>https://nature.com/articles/d41586-020-00424-7</url>
<title id = '271'>A key DNA-repair enzyme has a surprising role during the early steps in the assembly of ribosomes — the molecular machines that translate the genetic code into protein.</title>
<body id = '271'>Every minute, each human cell constructs up to 7,500 ribosomes — essential intracellular factories that decode instructions from genes to make all the proteins in the body. Ribosomes are assembled from four distinct ribosomal RNA (rRNA) molecules and 80 different proteins, which form small and large subunits, in a complex process involving more than 200 assembly factors. A better understanding of the underlying mechanisms might help to explain the devastating consequences of genetic mutations known as ribosomopathies that affect this assembly pathway. Writing in Nature, Shao et al.1 identify an unexpected role for the enzyme DNA-dependent protein kinase (DNA-PK) — a core component of the machinery for repairing DNA double-strand breaks (DSBs) — in the early steps of ribosome assembly. Cells must repair DSBs promptly, because they threaten genomic stability and can lead to cell death or cancer. Non-homologous end joining (NHEJ) is a main pathway for DSB repair. A dimeric protein complex called KU initiates this process by binding to the broken DNA ends, then recruiting the DNA-PK catalytic subunit (DNA-PKcs) to form the active DNA-PK enzyme (Fig. 1a). DNA-PK, through its kinase activity, adds phosphate groups to the side chains of serine and threonine amino acids in other proteins, and heavily regulates itself by phosphorylating a cluster of amino acids near its serine 2056 (S2056) and threonine 2609 (T2609) residues. This activity leads to the recruitment of other enzymes, such as Artemis, that process and join the broken DNA strands. Figure 1 | Two roles for DNA-dependent protein kinase. a, To repair DNA double-strand breaks, the DNA-dependent protein kinase catalytic subunit (DNA-PKcs) is recruited to DNA ends by the KU protein dimer. DNA-PKcs phosphorylates itself (P) on an amino-acid cluster near its threonine 2609 (T2609) residue. This enables the DNA-cleaving enzyme Artemis to access broken DNA ends, which are processed and joined. b, Shao et al.1 have found another role for DNA-PKcs: in the synthesis of the cell’s protein-producing factory, the ribosome. Precursor ribosomal RNA (pre-rRNA), which contains a region dubbed 18S, forms part of the ribosomal small-subunit processome. The authors find that KU recruits DNA-PKcs to another RNA molecule in the processome, U3. Self-phosphorylation might trigger an RNA-dependent conformational change in DNA-PKcs, regulating access of an RNA-cleaving enzyme such as UTP24, which cleaves the pre-rRNA to produce mature 18S rRNA that forms part of the ribosome. In a comprehensive series of genetic experiments, Shao and colleagues established that both the kinase activity of DNA-PKcs and phosphorylation at its T2609 cluster are crucial for blood development (haematopoiesis) in mice. Mice that entirely lacked both DNA-PKcs and the tumour-suppressor protein p53 developed a type of blood cancer and died. By contrast, animals that did not have p53 and carried a mutant form of DNA-PKcs lacking kinase activity survived. However, they developed a disease of the bone marrow reminiscent of a blood cancer called myelodysplastic syndrome. Moreover, mice in which amino-acid residues in the T2609 cluster were replaced by alanine residues (which could not be phosphorylated) died at four weeks old and had severe p53-dependent anaemia associated with reduced protein synthesis. This condition was reminiscent of the ribosomopathy Diamond–Blackfan anaemia (DBA), which is caused by mutations in any one of 18 different ribosomal proteins2. Shao et al. showed that deletion of the KU protein completely restored haematopoiesis in mice that had mutations in the T2609 cluster, ruling out defective DNA repair alone as the explanation for the blood disorders. What, then, might DNA-PK be doing in this context?   The first precursor of the small ribosomal subunit, known as the small-subunit processome, is assembled around an RNA called U3 (ref. 3; Fig. 1b) in a subcellular compartment called the nucleolus. Shao and colleagues confirmed previous reports4,5 that a proportion of KU and DNA-PKcs resides in the nucleolus. These observations suggested a link between KU, DNA-PKcs and ribosome assembly. The authors provided evidence that supports this link by using U3 as ‘bait’ to identify components of the small-subunit processome, which included DNA-PKcs and KU, but not other NHEJ factors. The small ribosomal subunit is partly comprised of an rRNA called 18S. The researchers found that unprocessed precursors of 18S rRNA accumulated in cells that lacked DNA-PKcs kinase activity, but did not accumulate when KU was deleted, too. Moreover, mice and cell lines lacking DNA-PKcs kinase activity showed reduced global protein synthesis. The authors used a technique called infrared crosslinking immunoprecipitation (irCLIP) to track down DNA-PKcs and KU to a specific location of the processome, near U3. Finally, they found that a structured fragment of U3 drives the assembly of DNA-PK and stimulates its catalytic activity in vitro, although does so much less efficiently than can DNA. Taken together, these observations suggest a model in which KU recruits DNA-PKcs to the small-subunit processome. In the case of kinase-defective DNA-PK, the mutant enzyme’s inability to regulate its own activity gives the protein a new function, blocking the processing of precursor rRNA into mature 18S rRNA in the small-subunit processome. The resulting defect in global protein synthesis drives a p53-dependent loss of red-blood-cell precursors — a cell type that has an especially high physiological demand for protein synthesis. The parallels with NHEJ are intriguing: in that pathway, the complete deletion of DNA-PKcs results in only a minor reduction in repair fidelity, and the joining of broken DNA ends is retained. By contrast, the kinase-inactive DNA-PKcs mutant is wholly unable to carry out end joining.   The specific role of DNA-PK in precursor rRNA processing, and how it recognizes precursor rRNA in vivo, remains unclear. However, structural analysis of the yeast small-subunit processome6 has revealed that U3 acts as a molecular guide that docks the processome onto the precursor rRNA by forming four evolutionarily conserved duplexes (hinges) between the two components: two hinges in a highly branched region of the precursor rRNA, and two in a region that will become the mature 18S rRNA. These hinges are a prerequisite for three cleavage events, mediated by an RNA-cleaving nuclease enzyme, that release the 18S rRNA ready to make the small subunit. Shao et al. show that DNA-PK and KU primarily interact with U3 at this hinge region. Thus, much as DNA-PKcs recruits the DNA-cleaving enzyme Artemis during the NHEJ processing of DNA ends7, with U3, DNA-PKcs might also help to recruit specific RNA-cleaving nucleases (such as UTP24) to the small-subunit processome to cleave the precursor rRNA for ribosome construction. Structural studies suggest that the binding of DNA-PKcs to KU and DNA could regulate the activation of DNA-PKcs kinase activity allosterically, that is, by changing the conformation of the enzyme8–10. In the future, it will be interesting to compare RNA- and DNA-dependent conformational changes in DNA-PKcs. The physiological relevance of the broad array of RNA partners identified by Shao et al. in their irCLIP analysis also remains to be dissected. Shao and colleagues’ study has identified an interesting player in ribosome assembly that might efficiently couple DNA DSB repair with processing of precursor rRNA, which is highly transcribed from the naturally unstable ribosomal DNA template. Broadly, the findings encourage us to critically evaluate how dynamic redistribution of DNA-PK might allow the cell to couple DSB repair with the regulation of protein synthesis. And, although further studies are required, we might have taken a step closer to deciphering the mysterious ribosomopathies. </body>
<date id = '271'>26 February 2020</date>
<url id = '272'>https://nature.com/articles/d41586-020-00481-y</url>
<title id = '272'>Blood cells called myeloid cells can facilitate metastasis — the spread of a tumour to distant organs. Taming these cells with drugs that alter the chemical structure of their DNA limits metastasis in mice.</title>
<body id = '272'>Many tumours are successfully treated with surgery. But cancer might recur at the surgical site or in distant organs, so surgery is often followed by treatment termed adjuvant therapy. This limits the risk of relapse by killing cancer cells remaining at the surgical site or those that have already moved elsewhere. However, adjuvant therapy is not always effective. Moreover, it might not prevent certain processes that aid cancer resurgence, such as the recruitment of blood cells called myeloid cells to distant organs, where they can lay the foundation for cancer cells to settle and thrive1. Writing in Nature, Lu et al.2 reveal how the chemical structure of the DNA in the nucleus of myeloid cells is a vulnerability that can be harnessed to target these tumour-promoting cells and limit cancer spread.   Tumour spread from its primary site to distant organs, which is called metastasis, involves complex interactions between cancer cells and the surrounding healthy tissues. Evidence is growing that primary tumours can produce signals that modify normal cells to generate a ‘soil’ in distant organs — termed a pre-metastatic niche — that permits subsequent ‘seeding’ and establishment of cancer cells at this secondary site3. Such secondary tumours, or metastases, are often lethal. Several types of cancer metastasize from their primary site to the lung. Efficient metastasis of breast cancer cells to the lung in mice requires the participation of a type of myeloid cell called a monocyte1,4. These cells normally function to fight infections, but they can also supply metastasizing cancer cells with factors that help them to get established and grow at a secondary site1,4. Certain molecules produced by the primary tumour alter the properties of monocytes and increase their numbers in the bloodstream, fostering the monocytes’ tumour-supporting functions in the pre-metastatic niche1,4. Therefore, blocking such ‘tumour-educated’ monocytes might inhibit metastasis. Previous work1,4 indicates that neutralizing a protein called CCL2, which is produced by the tumour and promotes monocyte accumulation in the lungs of mice, impairs metastasis in mouse tumour models. However, this approach was unsuccessful in clinical trials owing to difficulties in effectively neutralizing CCL24. Lu et al. studied mice that were given transplants under the skin of a type of tumour that metastasizes to the lung. Consistent with previous studies4,5, the authors found that two types of myeloid cell — monocytes and neutrophils — accumulated in the lung before metastases were detectable there. These tumour-elicited myeloid cells are collectively called myeloid-derived suppressor cells (MDSCs) owing to their ability to suppress the immune response against a tumour5. Confirming the metastasis-promoting capacity of MDSCs, the authors report that elimination of MDSCs delayed the metastasis of cancer cells to the lung and extended the animals’ survival.   Gene expression can be regulated by changes in the nucleus termed epigenetic modifications. These include processes that add or remove either methyl groups on DNA or acetyl groups on DNA-binding proteins called histones. Such epigenetic modifications often alter the expression of many genes in cancer cells, which might lead to loss of expression of genes that function as tumour suppressors, or to high expression of tumour-promoting genes. Anticancer treatments that use drugs to modulate epigenetic changes (termed epigenetic therapies) are gaining momentum6. Some of these drugs have been tested in clinical trials and approved for the treatment of certain blood tumours6. Epigenetic therapy is usually aimed at cancer cells, but earlier work7,8 indicates that such treatment can also target tumour-promoting myeloid cells. Lu and colleagues investigated the effect of using low doses of two such drugs: 5-azacytidine (which inhibits the addition of methyl groups to DNA) and entinostat (which inhibits the removal of acetyl groups from histones). This treatment did not kill cancer cells in vitro, nor did it inhibit the growth of primary tumours in mice. However, when mice that had undergone tumour-removal surgery were treated with the drugs, the migration of myeloid cells to the lungs and the formation of lung tumours were reduced (Fig. 1) compared with the effects in mice that did not receive the drugs. Lu and colleagues report that a similar treatment is well tolerated by people who have lung cancer, and preliminary evidence presented by the authors also suggests that it might limit tumour recurrence after surgery. Figure 1 | Drug therapy that blocks cancer spread in mice. Even if a tumour is surgically removed, remaining cancer cells might spread to a distant organ such as the lung. a, Non-cancerous cells called monocytes and neutrophils, which express the proteins CCR2 and CXCR2, respectively, arise in the bone marrow. Their numbers increase in the bloodstream if the cells are mobilized by tumour-derived factors (factors not shown). The cells can travel through blood vessels to reach the lungs and form a site called a pre-metastatic niche. This site aids the establishment of cancer cells there (called a metastasis). b, After animals had undergone tumour-removal surgery, Lu et al.2 treated them with epigenetic therapy, which consisted of two drugs that target the chemical structure of DNA. This treatment decreased the expression of CCR2 in monocytes and CXCR2 in neutrophils, and fewer of these cells reached the lung. The epigenetic therapy also caused some monocytes to differentiate into a cell called an interstitial macrophage. The impaired recruitment of tumour-promoting monocytes and neutrophils to the lungs was accompanied by a decrease in the number and size of metastases. Myeloid cells are made in the bone marrow and released into the bloodstream. Lu and colleagues report that epigenetic therapy modulated the gene expression of myeloid cells in the bone marrow of mice. By altering the chemical structure of DNA, epigenetic drugs can modify the ability of transcription-factor proteins to bind DNA and promote the expression of certain genes. Epigenetic therapy suppressed the activity of a transcription factor called NF-κB in the marrow-resident myeloid cells. NF-κB controls the expression of CCR2 (which is the receptor for CCL2) and CXCR2 (which is the receptor for another tumour-derived protein called CXCL1). After treatment, monocytes expressed lower levels of CCR2, and neutrophils expressed lower levels of CXCR2. CCL2 and CXCL1 drive the recruitment and retention of myeloid cells in the lung4,5. Therefore, drug-induced inhibition of CCR2 and CXCR2 expression probably impaired the process that enables a pre-metastatic niche in the lung to be populated by tumour-promoting myeloid cells. Lu et al. observed that the drug-mediated epigenetic reprogramming of myeloid cells was maintained during their exit from the bone marrow and when they reached the lung. Indeed, in addition to NF-κB, the activity of several transcription factors known to orchestrate the generation of a pre-metastatic niche3 was suppressed in the myeloid cells that made their way to the lung. Intriguingly, the authors found that the epigenetic therapy caused monocytes to differentiate into a type of cell called an interstitial macrophage, which might lack the ability to promote tumour establishment at a secondary site. The drugs also caused this type of cellular conversion in monocytes that were experimentally transferred from the bone marrow of untreated mice to the bloodstream of drug-treated mice. How the drugs drive this change is unknown. Perhaps their ability to rewire a cell’s transcriptional network underlies this phenomenon. Lu and colleagues’ findings indicate that the re-education of tumour-hijacked myeloid cells using epigenetic therapy might be a promising strategy for thwarting their metastasis-promoting capacity. These cells have several tumour-promoting functions, such as boosting the formation of blood vessels that aid tumour growth or suppressing antitumoral T cells of the immune system5,9. Thus, rather than having to individually target factors that aid different processes needed for tumour growth, epigenetic therapy might be a way to neutralize the broad and multifaceted capacity of myeloid cells to aid cancer. Drugs such as 5-azacytidine and entinostat are being tested in the clinic for use in combination with other anticancer therapies, both before tumour-removal surgery and in the setting of adjuvant therapy after surgery10. These drugs are not approved yet for treating solid (non-blood-cell) tumours in the clinic. High doses of such drugs are associated with severe toxicity, so using low doses that suffice to reprogram myeloid cells, together with other drugs that target cancer cells, might provide an effective anticancer treatment that avoids unwanted toxicity. </body>
<date id = '272'>26 February 2020</date>
<url id = '273'>https://nature.com/articles/d41586-020-00480-z</url>
<title id = '273'>How Nature reported a protest against the practices used to identify fungal species in 1920, and an ancient ceremony for testing the British coin of the realm in 1870.</title>
<body id = '273'> W. B. Brierley protests against the practice of mycologists in describing as species the forms which are presented to them in Nature or as pathological growths, especially on cultivated plants … But the laboratory and field experience of the experimentalist shows that under changes in the environment the whole structure and facies of the organism may be transformed, while under identical conditions there is considerable evidence that the morphological variation of a particular fungus is definite and constant … Two precisely similar fungi growing on a potato and a decaying tree stump respectively may really be different species, though the systematic mycologist would consider them identical … The only exact method of determining species is by means of quantitative data derived from cultural treatment under standardised physico-chemical conditions, for this method alone reveals the physiological condition of the organism. From Nature 26 February 1920 The trial of the Pyx is the formal testing of the coin of the realm, to ensure its being of the requisite weight and fineness. The name is derived from the Pyx, or chest, in which the coins selected for the purpose are contained. The first trial of the Pyx took place in the ninth and tenth years of Edward I. And as the last observance of this ancient ceremony was held during the past week, a few brief notes may not be without interest … The court now consists of several members of the Privy Council, under the presidency of the Lord High Chancellor and a jury selected from the Hon. Company of Goldsmiths. Last week the high officers of the Mint assembled at the Treasury, and in their presence the Lord Chancellor charged the jury to examine the coin of the late Master of the Mint, Thomas Graham, F.R.S., and to ascertain whether it was within the latitude of “remedy” allowed by law. This remedy amounts to 12 grains on each troy pound of gold coin … In the present instance … the verdict of the jury being, that the coin both as to weight and fineness was within the remedy. From Nature 24 February 1870 Latest on: Biological techniques Article 21 MAY 20 Article 20 MAY 20 Article 20 MAY 20 Culture Obituary 23 MAY 20 Technology Feature 08 MAY 20 World View 14 APR 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '273'>25 February 2020</date>
<url id = '274'>https://nature.com/articles/d41586-020-00479-6</url>
<title id = '274'>A method for directly converting connective-tissue cells into neurons opened up a new branch of research into cell-based therapies and called into question long-held beliefs about how development affects a cell’s identity.</title>
<body id = '274'>Our bodies rely on specialized cell types: brain cells compute information, red blood cells bind oxygen, and so on. Because almost all our cells have identical DNA, different patterns of gene and protein expression are needed to define these cell types. The selection and maintenance of these expression cascades were once thought to be irreversible after development. Over time, it emerged that cell identity could be changed, but it was often assumed that a cell could be converted into another cell type only if the two had a similar developmental origin. Ten years ago, Vierbuchen et al.1 overthrew this idea, by showing that connective-tissue cells called fibroblasts could be converted into functional neurons — which have a very different developmental origin — if they were engineered to express just three extra transcription factors.   This achievement was built on almost a century of visionary experiments in manipulating cell identity. In 1927, Hans Spemann showed that it was possible to change the fate of cells in a salamander embryo. The embryologist grafted ‘organizer’ cells (which drive early development of the body plan) from a donor embryo into a host embryo2, triggering the formation of a second embryo from the host cells. In 1962, the biologist John Gurdon showed that development can also be returned to the start3 — the nucleus of an adult cell can reacquire a state similar to that of cells in the earliest stages of development, and in this state it can give rise to an entire embryo. In the 1980s, it became clear that cells can also be directly converted from one specialized cell type to another (Fig. 1a). The first example4 was the conversion of fibroblasts into muscle cells by inducing the cells to express the transcription factor MyoD. Some years later, a different transcription factor was used to turn non-neuronal cells of the brain called glia into neurons in vitro5. The first demonstration that this type of conversion could also occur in vivo in mice6 opened up a potential new branch of therapy based on converting reactive glia into new neurons after brain insults or neurodegeneration7. Figure 1 | Breaking developmental barriers using direct reprogramming. Three early embryonic tissues called the germ layers (endoderm, mesoderm and ectoderm) give rise to all the body’s different cell types. a, Early experiments in cell reprogramming revealed that cells called fibroblasts can be converted into muscle cells in vitro through forced expression of one transcription factor4, and that glia (non-neuronal brain cells) can be converted into neurons by another transcription factor5. It was assumed that these conversions were possible only because the cell types had a shared developmental origin: fibroblasts and muscle both arise from the mesoderm; glia and neurons from the ectoderm. b, In 2010, however, Vierbuchen et al.1 demonstrated that co-expression of three transcription factors could induce fibroblasts to become neurons. c, The discovery led to many insights into cell identity. More conversions have been achieved8(examples indicated by arrows). We now know that cells cannot be converted into every cell type from within the same germ layer (skin cells cannot become neurons17, for instance). We can also hypothesize about other conversions that might be possible (dashed arrows) using different cocktails of transcription factors. In vitro, a wealth of other conversions was documented8, but all involved either reversion to an embryonic state9 or transformation into another cell type from within the same ‘germ layer’. Germ layers are the three layers of embryonic tissue (endoderm, mesoderm and ectoderm), which give rise to different organs and cell types. For instance, the gut tube and liver derive from the endoderm; muscle and connective tissue from the mesoderm; and neural tissue and skin from the ectoderm. It was assumed that cells could be converted only to other cell types from the same germ layer, owing to their closely related developmental origins. This dogma was shattered by Vierbuchen and colleagues, who converted mesoderm-derived fibroblasts from mice into functional neurons by co-expressing the three transcription factors Brn2, Ascl1 and Myt1l in the fibroblasts (Fig. 1b). By showing that developmental barriers are not an unsurmountable hurdle to cell-type conversion, the paper had a tremendous impact. First, it sparked a wave of interest in direct reprogramming to produce neurons. All of a sudden, fibroblasts — which are relatively easy to isolate from mouse embryos and are easy to grow in vitro — could be converted into a cell type of great therapeutic interest. The year after the paper’s publication, human fibroblasts were directly converted into neurons10, although this required more transcription factors than were needed for the conversion of mouse cells. It was only a few more years before transcription-factor cocktails had been defined to generate diverse neuronal subtypes11–13. In 2015, it emerged that the ‘induced neurons’ produced using Vierbuchen and colleagues’ method retain their cellular age — if the fibroblasts come from a 60-year-old donor, the reprogrammed neurons show a corresponding cellular age14. Thus, direct neuronal reprogramming is well suited for obtaining neurons to study age-related neurodegenerative diseases such as Alzheimer’s disease, Huntington’s disease or motor neuron disease15,16.   Beyond these key impacts on translational research, the paper raised questions about how developmental origin affects the maintenance of cell identity. For instance, the work called into question whether sharing a germ layer would always ease direct reprogramming between cell types. The answer is no: skin cells, derived from the ectoderm, cannot be readily converted to neurons17 (Fig. 1c). Moreover, neurons can switch between subtypes only during development18. These findings called for reconsideration of models of cell-identity maintenance. Perhaps, instead of depending on developmental origin, the key factors in how easily cell types can interconvert relate to similarities and differences in gene regulation between the mature cell types19. If this is the case, it should be possible to ascertain rules for interconverting cell types by altering specific gene-regulation parameters. However, no systematic studies to explore the potential of a given starter cell type to convert into different target cells have yet been carried out. This means that it is not yet possible to identify common rules for reprogramming — or, conversely, for the maintenance of identity. Filling this gap is now an important task. The ease of direct programming hints at the fragility of the mechanisms that maintain cell fate. So what keeps cells stable over decades? Researchers are starting to investigate the mechanisms (passive and active) that regulate expression of the transcription factors involved in switches of cell type, and to ask whether long-lived cells are more difficult to convert because they have developed more-elaborate fate-maintenance mechanisms. This could also be the reason that human cells are much harder to convert into other cell types than are mouse cells. The identification of these mechanisms would not only be a conceptual breakthrough, but would also help to overcome conditions in which cell identity becomes altered as cells deteriorate during ageing. For example, proteins that repress gene expression are involved in maintaining some aspects of cell identity in mature neurons20. However, little is known about whether or how these factors are depleted in ageing and neurodegeneration, and whether the loss of cell identity is a key contributor to ageing-related diseases. Direct reprogramming has revolutionized the concept of what defines a cell type, and has allowed us to explore fascinating questions about development. It has also triggered a revolution in disease modelling. That this has taken place in just one decade is testament to the impact of Vierbuchen and colleagues’ discovery. </body>
<date id = '274'>24 February 2020</date>
<url id = '275'>https://nature.com/articles/d41586-020-00411-y</url>
<title id = '275'>The first 3D structure of a full-length G-protein-coupled receptor whose natural activator is unknown has been determined, providing insights into an unusual mode of activation and a basis for discovering therapeutics.</title>
<body id = '275'>G-protein-coupled receptors are the largest class of membrane protein in the human genome, and represent the most abundant pharmaceutical targets. More than 800 such receptors are known in humans, of which perhaps 100 are orphan receptors — those for which the naturally occurring (endogenous) ligand molecules that bind to and activate them have yet to be identified1,2. This lack of understanding of orphan G-protein-coupled receptors (oGPCRs) impedes our ability to exploit their potential as therapeutic targets. Writing in Nature, Lin et al.3 close this gap in knowledge by reporting the first 3D structure of a full-length oGPCR, GPR52, in multiple states.   GPR52 is a potential drug target for treating several neuropsychiatric disorders, including Huntington’s disease and schizophrenia. When activated, it selectively binds to the Gs family of G proteins inside cells, and thereby stimulates the production of cyclic AMP (cAMP) signalling molecules, which regulate various cellular processes. Efforts to find drugs that target GPR52 would benefit from a greater knowledge of how the receptor couples to Gs and its activation process. Lin et al. began their investigation of the structural basis for GPR52 activation using X-ray crystallography. In their initial studies, the authors used a variety of strategies, including extensive protein engineering, to both stabilize the receptor and enable its production in sufficient quantities to produce high-resolution crystal structures. The researchers thus obtained the structures of human GPR52 in the ligand-free (apo) state and in complex with c17, a synthetic molecule that acts as an agonist (that is, it activates the receptor). Not unexpectedly, GPR52-apo adopts the GPCR architecture that has been seen in many other structures, involving seven transmembrane domains. Surprisingly, a region of the receptor known as extracellular loop 2 (ECL2) folds into what would normally be the binding site for an endogenous ligand (the orthosteric binding site), where it acts as a lid that blocks the entrance to this site (Fig. 1). Lin et al. observed that the activity of GPR52 is significantly diminished when ECL2 is mutated or deleted, indicating that the loop is essential for signalling activity in the receptor’s native environment. Meanwhile, the crystal structure of the receptor in complex with c17 suggests that this agonist binds to a ‘side pocket’ that has not been observed in previously reported structures of GPCRs. The authors therefore speculate that c17 acts allosterically — at a site remote from the orthosteric binding site — to potentiate GPR52’s activity. Figure 1 | Binding sites in the receptor GPR52. Lin et al.3 report structures of the membrane receptor GPR52, a potential drug target for which the putative naturally occurring agonist — the ligand molecule that activates the receptor — is unknown. The authors find that a region of GPR52 known as extracellular loop 2 (ECL2) binds to a site in the receptor that is analogous to the agonist-binding site in other receptors from the same family. ECL2 seems to activate the receptor, removing the need for an external agonist. The authors also find that the synthetic molecule c17, which activates GPR52, binds to a different region next to the site bound by ECL2, and might therefore be an allosteric modulator (a compound that potentiates the activity of the receptor but does not bind at the agonist-binding site). Remarkably, the authors were then able to form a stable complex of GPR52 with a modified Gs protein in the absence of an agonist, and to obtain the structure of the complex using cryo-electron microscopy. The receptor in this complex has the structural hallmarks of previously visualized, active GPCRs captured in complex with G proteins1. The arrangement of ECL2 in this active-state structure is the same as in the crystal structure of GPR52-apo, implying that ECL2 acts as a ‘tethered agonist’ under physiological conditions to facilitate signalling pathways in the absence of an endogenous agonist — similarly to the behaviour of some other GPCRs, such as the PAR1 protease-activated receptor4. Most GPCRs have some basal (constitutive) activity wherein they spontaneously couple to their particular G proteins. The constitutive activity of GPR52 is exceptionally high5. Indeed, Lin and colleagues find that GPR52’s basal activity is so great that the receptor’s ability to signal by increasing cAMP levels is only slightly augmented by the addition of c17.   The authors report that this high level of constitutive activity is achieved by at least two structural features that are unusual for GPCRs: the lack of a binding site for sodium ions, and the occupation of an apparent agonist-binding site by the tethered agonist in ECL2. The sodium-binding site of GPCRs is known to be important for damping constitutive activity6, and so the observation that a GPCR that lacks such a site has a high level of basal activity is not entirely surprising. By contrast, the discovery of a tethered agonist that helps to maintain GPR52 in the active state in the absence of an external agonist is truly striking. The new findings raise the intriguing possibility that, for at least some oGPCRs, the incorporation of agonists within the receptor itself obviates the need for external ligands. Indeed, several other oGPCRs that have high constitutive activities5 have been identified, along with others that don’t have sodium-binding sites6. It should be kept in mind that — as with all structural studies — Lin and colleagues’ work has provided only a few snapshots of the receptor structure. Further biochemical and biophysical studies will be essential to work out the details of GPR52’s dynamic behaviour under physiological conditions. Nevertheless, the authors’ high-resolution structures should aid the development of drugs that selectively target GPR52, but avoid other potential drug targets — for instance, by enabling computational studies7 in which ultra-large libraries of potential ligands are docked into the binding site revealed by the structures. Moreover, if the approaches used by Lin et al. for the structural elucidation of GPR52 are applied to other oGPCRs that have high constitutive activity5,6, they might transform our understanding of oGPCRs and accelerate their therapeutic exploitation. </body>
<date id = '275'>19 February 2020</date>
<url id = '276'>https://nature.com/articles/d41586-020-00426-5</url>
<title id = '276'>Analysis of a unique global data set reveals how the species diversity of birds is affected by the properties of archipelagos and offers a way to test an influential theory. Has this improved our understanding of island biodiversity patterns?</title>
<body id = '276'>The thousands of islands in the Aegean Sea between Greece and Turkey have inspired countless myths and works of literature. This region is also where the word archipelago, which means a group of islands, has its roots. Archipelagos and their constituent islands have long been viewed as natural ‘laboratories’ for developing and testing theories that aim to answer key questions about biodiversity1–5. Writing in Nature, Valente et al.6 report an impressive analysis of birds on archipelagos worldwide that provides some of these long-awaited answers.   In the 1960s, the biologists R. H. MacArthur and E. O. Wilson proposed the theory of island biogeography7,8, which is commonly used to explain observed patterns of species richness (the number of different species) on islands. This development marked the dawning of a renaissance for biogeography (the study of species distributions over space and time) that advanced this field from a largely descriptive endeavour to a quantitative and predictive science1–5. The theory of island biogeography was inspired by two well-established patterns of species diversity. One pattern is that species richness increases if a greater area is sampled. The other pattern is that the species richness of an island is lower the greater the isolation of the island — the farther away the island is from a potential source of species, such as the closest mainland. The theory of island biogeography predicts that the species richness observed on an island is the result of the interplay between three fundamental processes — extinction, colonization (the dispersal and establishment of species from the continental landmass to an island) and speciation (the generation of new species) — and that these processes depend on island area and isolation. This theory has had a wide-reaching influence on researchers in fields including ecology and conservation biology, and has underpinned the emergence of subdisciplines in these fields, such as macroecology and metapopulation biology1–5. Yet despite a multitude of studies3,5 testing the theory of island biogeography, few have sought to use molecular phylogenies to directly test on a global scale the dependency of extinction, colonization and speciation on island area and isolation. Valente and colleagues provide such a test. They focused on terrestrial birds, excluding migratory species, and gathered an impressive data set of 491 species across 41 archipelagos worldwide.   Building on their previous work investigating mechanisms that generate island biodiversity9, the authors applied an innovative modelling approach that combined molecular phylogenetic data with information on the spatial distribution of birds. The authors obtained genetic data from 90 species across different archipelagos, including 110 island populations not previously sampled. Valente and colleagues also sampled genetic data for the closest mainland-dwelling relatives of several of these island species. After combining their data with pre-existing data, the authors built phylogenetic trees showing the evolutionary relationships between species. Using these phylogenies, they were able to estimate colonization, extinction and speciation rates. The authors also included species known to have been driven to extinction by humans, because excluding such species impedes our understanding of natural processes and biodiversity patterns9,10. The authors’ models, which used rates estimated at the archipelago level, have high explanatory power and confirm several key predictions of the theory of island biogeography — namely, that extinction rates decline with increasing island area, colonization rates decline with increasing distance from the island to the continent, and speciation rates increase with the area and isolation of islands. The authors studied two types of speciation (Fig. 1) separately: anagenesis (in which a new species arises when an island population diverges from its ancestral species on the continent to become a different species3) and cladogenesis (in which an ancestral species splits into two or more different species3). They found that anagenesis increases with island isolation, and cladogenesis increases on larger, more isolated islands. These findings will help future studies that attempt to answer long-debated questions, such as why only certain animal and plant groups speciate extensively, and whether there are upper limits to the species richness and speciation rates in specific regions of the globe3. Figure 1 | Bird biodiversity. The theory of island biogeography7,8, proposed in the 1960s, is a milestone in our understanding of how biodiversity is established and maintained. Valente et al.6 tested this theory on a global scale using data for island-dwelling birds from 41 archipelagos. Their results confirm key predictions of this theory. a, The authors report that two-thirds of the birds native to archipelagos arose from a process of species formation called anagenesis, which typically occurs on isolated islands (those far from the mainland). Anagenesis has given rise to birds such as the Bolle’s pigeon (Columba bollii) of the Canary Islands. b, Another process of species formation, called cladogenesis, is most common on large, isolated islands. The authors report that of the birds they studied, a group called Hawaiian honeycreepers had the greatest number of species (33 in total) that arose by cladogenesis. One example of such species is Hawaii’s iiwi, or scarlet honeycreeper (Drepanis coccinea).Credit: a, FLPA/Alamy; b, Getty Valente and colleagues have not only advanced our understanding of the laws governing species richness on islands, they have also confirmed several predictions of the theory of island biogeography. As the authors mention, the next step will be to apply their analytical framework to other island-dwelling species, particularly those, such as snails or reptiles, that have less ability to disperse than birds do. These analyses could be further informed by incorporating into this approach species’ functional traits11, such as body size and diet. The implications of Valente and colleagues’ results extend beyond the field of island biogeography. For example, characterization of the relationship between island area and extinction rate contributes to the discussion in conservation science about how to assess the effects on biodiversity of habitat loss and fragmentation during the Anthropocene (the name proposed for the current phase of planetary history, in which human activity has a dominant influence on the environment). This is relevant to today’s world, in which natural habitats are becoming increasingly isolated12,13. An important aspect of Valente and colleagues’ study is their approach of considering an archipelago as a unit, rather than focusing on individual islands. This aligns with the idea14 that archipelagos might be the most appropriate units in which to frame analyses of biodiversity at large spatial and temporal scales. Analysis of large spatial units in biogeography is not a new approach; however, these units generally take the form of geometric shapes, such as grid squares, that do not directly correspond to ecological boundaries (for example, those defined by vegetation type) and their associated communities. By contrast, archipelagos represent natural units. It is likely that substantial strides will be made in our understanding of island biogeography from further analyses of ecological patterns and processes undertaken at the archipelago scale, especially if geological dynamics are incorporated. To paraphrase E. O. Wilson15: it is archipelagos that are “the logical laboratories of biogeography and evolution”. </body>
<date id = '276'>19 February 2020</date>
<url id = '277'>https://nature.com/articles/d41586-020-00384-y</url>
<title id = '277'>The breaking of a property of nature called charge–parity–time symmetry might explain the observed lack of antimatter in the Universe. Scientists have now hunted for such symmetry breaking using the antimatter atom antihydrogen.</title>
<body id = '277'>One of the greatest mysteries in modern physics is why the Universe seems to contain mostly matter and almost no antimatter. This observation could be explained if a property of nature called charge–parity–time (CPT) symmetry is violated. Under CPT symmetry, the physics of particles and their antiparticles is identical. A tiny violation of CPT symmetry during the Big Bang could, in principle, be responsible for the lack of antiparticles in the Universe. In a paper in Nature, the ALPHA Collaboration1 reports high-precision spectroscopic measurements of antihydrogen — an atom comprising an antiproton and a positron (the antiparticle of an electron). The authors find that the gaps between energy levels in antihydrogen are in excellent agreement with those measured previously in ordinary hydrogen2–4, placing strong constraints on potential CPT violation.   Tests of CPT symmetry using individual particles — such as neutral kaons5, positrons6 and antiprotons7,8 — have shown no sign of CPT violation. However, studies of antihydrogen might probe the influence of factors that were not explored in previous tests. Hydrogen is the simplest atom, and its properties can be calculated with impressive precision. For more than a century, the study of this atom has been the driving force behind groundbreaking ideas about the structure of matter. The optical spectrum of hydrogen was measured with great accuracy in the 1880s, before being quantitatively explained in the 1910s. The structure of the atom was then at the heart of the formulation of quantum mechanics and in the generalization of this theory to relativistic (fast-moving) particles in the 1920s. And it was the unexpected discovery9 of an energy gap between the 2S and 2P1/2 excited states of hydrogen by the physicist Willis Lamb in 1947 that motivated the development of quantum electrodynamics — the theory that describes the interactions between particles and light. This energy gap, known as the Lamb shift, exists in both hydrogen and antihydrogen. It originates mostly from quantum fluctuations, whereby particle–antiparticle pairs spontaneously emerge in empty space and then instantly annihilate each other. However, its magnitude is subtly affected by, for example, the charge radius (the spatial extent of the charge distribution) of the proton or antiproton, the weak nuclear force and, potentially, currently unknown phenomena that could be the source of the matter–antimatter asymmetry in the Universe.   The current work was carried out using the ALPHA experiment at CERN, Europe’s particle-physics laboratory near Geneva, Switzerland. A facility called the Antiproton Decelerator delivers antiprotons to this experiment, with a source of radioactive sodium providing positrons. Every few minutes, 90,000 cold trapped antiprotons and 3 million positrons are mixed in a sophisticated charged-particle trap. This process yields about 20 cold antihydrogen atoms that are then confined to a neutral-atom trap made from superconducting magnets. These antihydrogen atoms can be stored10 for at least 60 hours, and production cycles can be repeated to obtain hundreds of such atoms. The aim of the present study was to measure the energy differences between the 1S ground state and the 2P1/2 and 2P3/2 excited states of antihydrogen (Fig. 1). The ALPHA Collaboration used an approach called laser spectroscopy, which involved injecting pulses of laser light into the antihydrogen trap. This injection caused atoms to transition from the 1S state to the 2P1/2 or 2P3/2 state and to subsequently decay back to the 1S state. Atoms that ended up in a different magnetic substate of the 1S state from the one in which they started were expelled from the magnetic neutral-atom trap. These antihydrogen atoms then annihilated on contact with regular atoms in the walls of the ALPHA apparatus to produce particles called charged pions. Figure 1 | Lowest-energy states of antihydrogen. The ALPHA Collaboration1 carried out high-precision spectroscopic measurements of antihydrogen — the antimatter counterpart of hydrogen. Specifically, the team determined the energy differences between the 1S ground state and the 2P1/2 and 2P3/2 excited states of antihydrogen. They used these results to estimate the fine-structure splitting (the 2P1/2–2P3/2 energy gap). They also combined their previous determination11 of the energy gap between the 1S and 2S states with their current measurement of the 1S–2P1/2 energy difference to infer the Lamb shift (the 2S–2P1/2 energy gap). The authors found that all of these results are in agreement with the corresponding ones for ordinary hydrogen. (Drawing not to scale.) The ALPHA Collaboration plotted the number of observed charged pions as a function of the frequency of the laser light. They then used the positions of the two peaks in these plots to infer the 1S–2P1/2 and 1S–2P3/2 energy differences in antihydrogen. These differences agree with the ones measured in ordinary hydrogen at the level of 16 parts per billion. The authors used their results to estimate the fine-structure splitting (the 2P1/2–2P3/2 energy difference) in antihydrogen, with an uncertainty of 0.5%. This value is again in good agreement with the one for ordinary hydrogen. In 2018, the ALPHA Collaboration measured the energy gap between the 1S and 2S states in antihydrogen11 to one part in 1012. In the current work, the authors combined this result with their measurement of the 1S–2P1/2 energy difference to provide an estimate of the Lamb shift in antihydrogen. This value has an uncertainty of 11% (or 3.3%, when the fine-structure splitting in ordinary hydrogen is used in the analysis). Over the past few years, high-precision laser spectroscopy of antihydrogen has become possible, and the ALPHA Collaboration has achieved spectacular progress. An examination of several transitions in antihydrogen would enable targeted tests of CPT symmetry, quantum electrodynamics and the standard model of particle physics. For example, a measurement of the Lamb shift with an uncertainty of less than one part in 104 would allow the antiproton charge radius to be determined12. Moreover, improved measurements of the energy gap between magnetic substates in antihydrogen would provide detailed information about the magnetic structure of the antiproton13. The laser used for spectroscopy in the current work will, in the future, be used for cooling of antihydrogen by inducing 1S–2P1/2 and 1S–2P3/2 transitions. Such cooling would greatly improve the achievable precision in all spectroscopy experiments on antihydrogen. In addition, ultracold antihydrogen can be used to study the effect of gravity on these atoms14. Cold antihydrogen thus promises many cool results. </body>
<date id = '277'>19 February 2020</date>
<url id = '278'>https://nature.com/articles/d41586-020-00403-y</url>
<title id = '278'>The discovery of a sensor that detects hydrogen peroxide at the surface of a cell provides insights into the mechanisms by which plant cells perceive and respond to environmental stress.</title>
<body id = '278'>Chemically reactive, oxygen-containing molecules called reactive oxygen species (ROS) are central to cell function. Plant cells generate various ROS, including hydrogen peroxide (H2O2), which has a key role in cell signalling. It is produced in an extracellular space between the plasma membrane and cell wall called the apoplast, in response to a range of factors, including stressors, plant hormones such as abscisic acid, and physical or chemical changes outside the cell1. But whether and how this extracellular H2O2 (eH2O2) is sensed at the cell surface is unknown. Writing in Nature, Wu et al.2 identify the first known cell-surface H2O2 receptor in plants.   The apoplast and cell wall act as a dynamic interface between plant cells and the outside world, with all its threats, challenges and opportunities. Some eH2O2 moves from the apoplast into the cytoplasm through channel proteins called aquaporins3. However, unlike the cytoplasm, the apoplast contains relatively few molecules that counteract oxidation1 — and so ROS, including H2O2, can survive for much longer in the apoplast than in the cytoplasm. This is a compelling reason to suspect that there is a sensor for eH2O2 in the apoplast. Although little is known about the initial target of eH2O2, the consequences of its production are much better defined4. It is clear that eH2O2 triggers an influx of calcium ions (Ca2+) into the cell, which then leads to the systemic transmission of signals between cells in waves, activating processes such as pathogen resistance or acclimation to stress across the entire plant5. In addition, eH2O2 signals regulate the polarized growth of pollen tubes and root hairs6, and control the opening and closing of stomata3 — pores on the outer layer of the leaf formed by two guard cells. Stomata enable the free passage of molecules such as carbon dioxide and oxygen into the plant when open, and can close to prevent water loss from the plant. Wu et al. set out to identify cell-surface receptors for eH2O2 that trigger Ca2+ signalling, using a ‘forward’ genetic-screen approach. They treated seeds of the plant Arabidopsis thaliana with a chemical that induces DNA mutations, then screened the resulting plants to identify mutants that showed low Ca2+ influxes in response to H2O2. They named these mutant plants hydrogen-peroxide-induced Ca2+ increases 1 (hpca1). The authors then identified the HPCA1 protein. They report that HPCA1 is a membrane-spanning enzyme of a protein family known as leucine-rich repeat (LRR) receptor kinases. The group also showed that HPCA1 has two special pairs of cysteine (Cys) amino-acid residues in its extracellular domain. The thiol groups of Cys residues are known7 to be a target for oxidation by H2O2. The authors demonstrate that the presence of eH2O2 leads to oxidation of the extracellular Cys residues of HPCA1 in guard cells. This modification activates HPCA1’s intracellular kinase activity, triggering Ca2+-channel activation and Ca2+ influx, followed by stomatal closure (Fig. 1). Figure 1 | The HPCA1 protein. Wu et al.2 have identified the first extracellular sensor of hydrogen peroxide (H2O2) in plants, HPCA1. The protein has an intracellular kinase enzyme domain, and an extracellular domain that protrudes into the apoplast — the compartment between a plant cell’s plasma membrane and the cell wall. HPCA1 has two special pairs of cysteine (Cys) amino-acid residues. The authors demonstrate that H2O2 oxidizes thiol groups (not shown) on these residues, forming sulfenic acid (SOH; not shown) and disulfide bonds. This oxidation triggers a conformational change and kinase activity, which, through unknown mechanisms, lead to the opening of calcium-ion (Ca2+) channels and Ca2+ influx into the cell, triggering intrinsic and systemic signalling pathways. In the absence of eH2O2, the hpca1 seedlings showed no differences from wild-type seedlings. However, their guard cells were less sensitive to eH2O2 than were those of the wild-type seedlings, showing lower than wild-type levels of Ca2+ influx in response to eH2O2. HPCA1 is therefore required to convert the eH2O2 signal into a physiological response. Moreover, the abscisic acid-dependent production of eH2O2 by guard cells was defective in the hpca1 mutants. Of note, the function of HPCA1 in eH2O2 signalling was not limited to guard cells, and the authors provided evidence that eH2O2 signalling helps to transmit environmental signals to the nucleus of various cell types to regulate gene expression. Oxidation of Cys by H2O2 leads to the formation of a sulfenic acid (SOH), which is at the heart of reduction–oxidation (redox) signalling. Sulfenic acids are rather unstable intermediates that can be further oxidized to sulfinic (SO2H) and sulfonic (SO3H) acid, or can undergo ‘exchange reactions’ to form disulfide bonds. For HPCA1 to function properly as a receptor for eH2O2, the Cys oxidation process must be readily reversible, re-forming thiol residues that can be oxidized again. However, the factors that mediate reduction of the oxidized HPCA1 are unknown. One candidate is a membrane-bound electron-transport system, such as the one that reduces an oxidized form of the antioxidant molecule ascorbic acid in the apoplast8. Membrane-bound and apoplastic thioredoxin-like proteins are also putative candidates, given that thioredoxin is a well-characterized reducing agent for oxidized Cys residues of proteins.   Wu and colleagues have uncovered a receptor-kinase-mediated eH2O2 sensing mechanism that does not resemble any known eH2O2 receptors or sensors reported in other organisms. Nonetheless, HPCA1 might be part of a much wider portfolio of sensors used by plants to perceive and respond to environmental changes through ROS signals. The identification of such receptors has proved challenging, not least because likely candidates are members of very large protein families. Sophisticated screens, such as that used by Wu et al., will be required to tease out the family members that have ROS sensing and signalling roles. Once these sensors have been identified, it should be relatively easy to manipulate their properties to produce model plants and crops that have, for example, increased or depressed sensitivity to environmental H2O2 signals, and so show altered tolerance to environmental threats. Stomatal closure is not regulated just by H2O2; it is also a response to elevated atmospheric CO2 levels3,9. It will be intriguing to see how proteins such as HPCA1 function in redox signalling networks that are likely to prepare plants for life in a future high-CO2 world. High CO2 levels can stimulate photosynthesis and depress photorespiration; changes in the photosynthesis:respiration ratio have a wide-ranging impact on cellular redox balance, because photorespiration generates a molecule of H2O2 in one organelle, the peroxisome, for every oxygen molecule assimilated in another organelle, the chloroplast, during photosynthesis. Perhaps other H2O2 sensors act together with HPCA1 to transmit organelle-specific redox messages to the nucleus, along with messages from the external face of the plasma membrane. </body>
<date id = '278'>19 February 2020</date>
<url id = '279'>https://nature.com/articles/d41586-020-00404-x</url>
<title id = '279'>How Nature reported a stamp that celebrated the 150th anniversary of a learned society in 1970, and a call for nasal hair to be more widely appreciated, from 1870.</title>
<body id = '279'>Credit: Stamp Design Royal Mail Group Ltd (1970) The black-and-white print released by the Post Office fails to do justice to the 1s 9d stamp which will be on sale from April 1 to celebrate the 150th anniversary of the founding of the Royal Astronomical Society in 1820. Designed by Marjorie Saynor, a freelance designer making her first venture into stamp design, the stamp shows from left to right Sir William Herschel, Francis Baily (of Baily’s beads fame) and Sir John Herschel against a delightful pink background. The telescope represents the reflector completed by William Herschel at Slough in 1789 which apparently was never a technical success because of mounting problems. But the inclusion of Herschel’s telescope is appropriate because it features on the seal of the society. A first-day cover service is being provided by the Royal Astronomical Society at a cost of 6s 6d. For 17s 6d, however, the deal includes an authentic signature of the president, and Sir Bernard Lovell claims to have made £40 for the society already. From Nature 21 February 1970 Dr. Tyndall, in his lecture upon Haze and Dust, says “that if a physician wishes to hold back from the lungs of his patient, or from his own, the germs by which contagious disease is said to be propagated, he will employ a cotton-wool respirator;” … May I ask if there is any necessity for the unsightly respirators one sees over the mouths of people during the winter months and cold evenings? Has not Nature already provided us with an efficient one — one which, on experiment, will doubtless prove to be quite as trustworthy as the artificial one, without any of its inconveniences? I refer to the hair-sieve with which the sinuosities of the nasal passages are supplied; the hairs besetting its path freeing the indrawn air from contaminating particles of dust, whilst it is effectually warmed in its inward passage … Apart from the use of respirators, en passant, I may perhaps be allowed to echo the opinion of our best medical men in saying that the mouth is not the organ for respiration; if it were, should we not find the olfactory nerves developed there also? ... It is a well-known fact, that people who habitually breathe through the nose are less liable to infectious diseases and pulmonary complaints, one very common benefit derived by such who sleep with the mouth closed, is that they never awake with the painful and disagreeable sensation produced by a parched throat and cracked lips. This may be a small matter, but I think it is deserving of attention. When we break Nature’s laws we must pay the penalty. From Nature 17 February 1870 Latest on: Astronomy and astrophysics News & Views 20 MAY 20 Article 20 MAY 20 News Q&A 19 MAY 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Physiology Article 20 MAY 20 Article 20 MAY 20 News 07 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '279'>18 February 2020</date>
<url id = '280'>https://nature.com/articles/d41586-020-00393-x</url>
<title id = '280'>In 1990, an oceanographer who had never worked on climate science proposed that ice-age cooling has been amplified by increased concentrations of iron in the sea — and instigated an explosion of research.</title>
<body id = '280'>Thirty years ago this month, John Martin proposed a solution to one of the biggest mysteries of Earth’s climate system: how was nearly one-third of the carbon dioxide in the atmosphere (about 200 gigatonnes of carbon) drawn into the ocean as the planet entered the most recent ice age, then stored for tens of thousands of years, and released again as the ice sheets melted? These large natural cycles in atmospheric CO2 levels (Fig. 1a) were revealed in 1987 by an analysis of ancient air bubbles trapped in the first long ice cores taken from the Antarctic ice sheet1. Martin recognized that iron was a key ingredient that could have transformed the surface ocean during glacial times. His landmark iron hypothesis2, published in Paleoceanography, described a feedback mechanism linking climatic changes to iron supply, ocean fertility and carbon storage in the deep ocean. Figure 1 | The anti-correlated data that inspired the iron hypothesis. a, Measurements of air bubbles trapped in cores drilled from the Antarctic ice sheet show that atmospheric levels of carbon dioxide were significantly lower during the coldest periods (shaded regions) than during modern times (data from ref. 16; CO2 concentrations are shown in parts per million by volume; p.p.m.v.). b, The ice-core records also reveal that more iron was transported to the Southern Ocean in wind-blown dust during the coldest periods than during warmer times (data from ref. 17; iron flux is measured in micrograms per square metre per year). In 1990, Martin2 hypothesized that the increased levels of iron in the Southern Ocean during the coldest periods fertilized the growth of photosynthetic microorganisms in the surface Southern Ocean, which therefore produced more biomass from CO2. This, in turn, would have increased the strength of the biological pump, a mechanism that sequesters some of the biomass (and the carbon within it) in the deep ocean. Martin proposed that the stronger biological pump explains why so much atmospheric CO2 is drawn into the ocean during cold times. Two hundred gigatonnes is a lot of carbon to periodically withdraw from and release to the atmosphere. In the 1980s, a handful of models (see ref. 3, for example) had shown that an increase in biomass production in polar ocean regions was the most effective process for removing so much atmospheric carbon. Photosynthetic organisms in the surface ocean convert CO2 from the atmosphere into biomass, much of which is subsequently broken down into CO2 again by other organisms and returned to the atmosphere. But part of the biomass sinks into the deep ocean, which therefore effectively serves as a large storage reservoir of dissolved CO2. This mechanism of CO2 removal is called the biological pump. However, biomass production requires not only CO2, but also other nutrients to build lipids, proteins and enzymes. Researchers were struggling to ascertain how the ocean’s abundance of key nutrients, such as nitrates or phosphates, might have increased during glacial times to fuel a stronger biological pump. Martin argued that iron is another nutrient that limits the biological pump. He suggested that the modern marine ecosystem of the Southern Ocean around Antarctica is starved of iron, and therefore relatively low in biomass, despite having abundant nitrates and phosphates. But during glacial times, strong winds over cold, sparsely vegetated continents could have transported large amounts of iron-bearing dust into this ocean (Fig. 1b). Martin reasoned that this dust could have fertilized marine ecosystems and strengthened the biological pump, so that more carbon was transferred into the deep ocean, lowering atmospheric CO2 levels.   Around the time of publication, evidence for high dust delivery during glacial periods had just emerged from studies of deep Antarctic ice cores4. But there were no reliable measurements of dissolved iron in the Southern Ocean that could confirm that its surface waters are iron-starved in modern times, or data supporting the proposal that delivery of iron-rich dust would make a difference to ocean productivity. It was clear, however, that large patches of the world’s ocean had much lower quantities of biomass than would be expected on the basis of the concentrations of key nutrients such as nitrates and phosphates. But many researchers argued that this was due to natural overgrazing of algae by herbivores5. The idea that modern algal growth is limited by iron availability had, in fact, been proposed6 in the 1930s, but had been incorrectly discounted by oceanographers — who had measured plenty of iron in seawater samples collected from the waters around their iron ships7. Martin was one of the first oceanographers to implement painstaking procedures to avoid the contamination of samples and to determine that iron concentrations in the north Pacific Ocean were extremely low7, certainly low enough to curtail biomass production. Despite the initial scepticism that greeted the iron hypothesis, 12 separate experiments8 were carried out between 1993 and 2005 in which around 300–3,000 kilograms of dissolved iron were injected into small patches of the Southern Ocean, the equatorial Pacific Ocean and the north Pacific. The biomass of algae increased wherever iron was added, as biological production surged. Unfortunately, Martin died mere months before the first of these experiments, and did not witness the ocean-scale confirmation of his hypothesis, nor the internationally coordinated campaign to measure iron geochemistry throughout the world’s oceans9 — which confirmed iron limitation and revealed the intricate strategies used by marine ecosystems to acquire and recycle iron10. Earth scientists also tried to test the iron hypothesis computationally using simple ocean models. They used the changes in the dust-accumulation rate recorded in ice cores as input to simulate changes in iron delivery to the Southern Ocean, and data from the experimental iron fertilizations to calculate how this iron could affect algal growth and the biological pump. Such models could reproduce the timing and magnitude of about half of the observed decrease in atmospheric CO2 levels during glacial periods11. Iron fertilization is therefore clearly an important process that causes atmospheric changes, but might not be the only one.   Finding data to prove that biological production had been higher during glacials was a harder task — after all, the ecosystem during the most recent glacial period (about 20,000 years ago) is long dead. One possible solution was to extract cores from sediments piled on the sea floor, to see whether the mineral skeletons of algae accumulated faster during glacial times than in the modern era. However, the results were often ambiguous12, for several reasons: many algae don’t produce a preservable skeleton; numerous factors determine what proportion of biological remains is preserved on the sea floor; and the location of biological production changes through time as ocean fronts and sea-ice positions migrate. Fortunately, Martin2 and others13 had anticipated an alternative, global-scale test of the biological pump during glacial times. If more biomass reached the deep ocean during glacials, then deep-sea microorganisms would use up more oxygen as they consumed it, decreasing the concentration of oxygen in deep waters. Evidence of deep-ocean oxygen depletion would therefore be indicative of a strong biological pump. Martin recognized that the presence of certain microfossils in glacial-age sediments meant that the deep ocean had not become completely devoid of oxygen during glacials. But although this evidence crudely constrained estimates of the degree to which iron fertilization might have enhanced productivity during glacials, it could not be used to determine whether levels of deep-ocean oxygen were lower than during modern times. Since then, analysis of more-sensitive geochemical records indicates that the oxygen concentration in bottom waters did decrease during glacial times14. This provides the strongest confirmation yet of the large-scale accumulation of carbon in the deep ocean during glacial periods owing to a stronger biological pump. Slower rates of mixing between the deep and shallow oceans could also have enhanced the biological pump during glacials. The latest generation of climate models in which the ocean and atmosphere are coupled can test the contribution of the multiple processes that could have resulted in a reduction in bottom-water oxygen levels. Such models indicate that mixing rates can account for only half of the observed deep-ocean storage of CO2 during the glacial period, and that iron fertilization of the Southern Ocean is the major cause of the extra CO2 storage observed15. Martin concluded his paper by saying that iron availability “appears to have been a player” in strengthening the biological pump during glacial cycles, but that the size of its role remained to be determined. Thirty years later, the evidence convincingly shows that iron fertilization of the Southern Ocean was indeed a leading actor in this global-climate feedback. </body>
<date id = '280'>17 February 2020</date>
<url id = '281'>https://nature.com/articles/d41586-019-03952-z</url>
<title id = '281'>Most surfaces are rough at many length scales. Simulations show that this characteristic originates at the atomic level in metal-based materials when smooth blocks of these materials are compressed.</title>
<body id = '281'>Almost all solid surfaces are rough. This roughness occurs at length scales that encompass 13 orders of magnitude — from the kilometre-scale peaks of mountains, down to atomic-scale bumps. Roughness seems to emerge regardless of what is done to a surface. Yet there is little understanding of how this roughness comes about, and especially why it is often self-affine, meaning that a surface looks similar on different length scales. Writing in Science Advances, Hinkle et al.1 show that self-affine roughness has its origin at the atomic level. As anyone who has ever slipped on a wet floor will have noticed, the roughness of surfaces can have a crucial role in practical situations. Smooth surfaces are slippery when wet, but are also easier to lubricate inside moving machinery than are rough surfaces. By contrast, we sand surfaces before painting them to make them rougher, and thereby to increase the adhesion of the paint. The effects of roughness are less straightforward in other situations: for example, the roughness of the surfaces of skis and snowboards affects their friction on snow differently depending on the temperature and humidity2. Engineers have therefore developed many techniques to control surface roughness, such as grinding, polishing and so on. Hinkle and colleagues’ results help us to understand better how roughness emerges, and thus might provide new ideas for how to control it.   The authors carried out computational simulations of three materials: a single, perfect gold crystal, an alloy and a metallic glass. These materials have very different amounts and types of disorder, which means that roughness might be expected to develop through different mechanisms or to have different characteristics for each of them. Because the deformation of a material is likely to contribute to the formation of roughness, the researchers simulated the compression of flat blocks of these materials beyond their elastic limit — that is, at forces that cause irreversible (plastic) deformation. Because the length scales of the effects the researchers were looking for span several orders of magnitude, the simulations had to be quite large, containing tens of millions of atoms. Such simulations are computationally extremely expensive. Hinkle and colleagues investigated how fluctuations in the roughness produced in the simulations change when the size of the area being observed is increased. They observed that the roughness profiles of all three materials seem to obey a power law — that is, they do indeed display self-affine scaling, over nearly two orders of magnitude (from about 1 nanometre up to the size of their simulation ‘box’, which was approximately 70–100 nm; Fig. 1). Figure 1 | Roughness on a simulated gold surface. Hinkle et al.1 carried out molecular-dynamics simulations of tens of millions of atoms in smooth blocks of three materials, including gold (shown here), and observed how surface roughness develops when the blocks are compressed. Colours represent atomic positions perpendicular to the surface, measured relative to the surface’s mean height: red indicates high topography; blue, low. The highest features are 8.8 nanometres above the lowest point on the surface. The authors found that roughness emerges that is similar across nearly two orders of magnitude of length scales. Similar triangular features and variation of topography are visible in a (a region 80 nm across) and b (a region of a expanded to four times its original size). The same is also true at magnifications of 8 and 64 (not shown).Credit: Ref. 1 In addition to simulating millions of atoms, the authors simulated a continuum model of compressive deformation in which the material is not treated as being composed of individual atoms, but as a continuous medium. In these simulations, there is no sign of self-affine roughness. The authors therefore conclude that the development of self-affine roughness is related to atomic-scale fluctuations in plastic flow that are missing from the continuum model. Hinkle and colleagues’ results are convincing across the observed length scales, but the scaling behaviour of the roughness will need to be demonstrated across three orders of magnitude to confirm that it truly obeys a power law. This will require the atomic simulations to be extended to even larger scales. Modelling techniques (see ref. 3, for example3) are available at mesoscale lengths (which range from a few nanometres to several hundred micrometres), and provide a link between atomistic and continuum simulations. These approaches take flow into account in more detail than does the continuum model used by Hinkle et al., and would allow for increased atomistic detail and fluctuations in simulations. This could help to provide the extra order of magnitude needed to convincingly show the power-law statistics of the roughness.   It remains to be seen how universal the reported behaviour is. All of the materials investigated by Hinkle and co-workers are based on metals. After undergoing plastic deformation, they are all homogeneous (there is only one type of solid phase in the material) but disordered, and the dynamics and energy scales involved in atom displacement are all comparable. It would be interesting to see whether similar scaling behaviour emerges from the compression of other types of material that have different mechanisms of plasticity and deformation, such as polymers. If so, are the scaling exponents — the key scaling parameters in the power-law equation — the same for all materials? If roughness profiles can be extended to include one or more extra orders of magnitude, it would enable a reliable comparison of the scaling exponents. This, in turn, would help to determine whether these exponents vary with strain, deformation mechanisms, or even time. Power-law behaviour is common in plastic deformation. For example, ‘avalanches’ of plastic deformation occur in metals4, and in fibrous materials a power law describes the size distribution of avalanches when these materials deform plastically under tensile stress5. Given that Hinkle et al. simulate the formation of rough surfaces in response to plastic deformation, and also observe scale-free roughness in the bulk of the modelled materials, it seems likely that there is a link between the development of self-affine roughness and the power-law behaviour of plastic deformation events — as the authors also note. It would therefore now be interesting to study the emergence of roughness in a more dynamic way, by investigating the formation of roughness features during compression, and relating the changes in the surface profile to plastic events. </body>
<date id = '281'>14 February 2020</date>
<url id = '282'>https://nature.com/articles/d41586-020-00329-5</url>
<title id = '282'>Prussian blue analogues are archetypes of coordination solids, in which metal ions are bridged by ligands to form extended network structures. An analysis reveals a surprising ordering of the gaps found in their crystal lattices.</title>
<body id = '282'>The centuries-old pigment Prussian blue and its analogues are archetypes of compounds known as coordination solids, and have had an unparalleled role in advancing our understanding of inorganic chemistry and materials1,2. The wide-ranging structural, electronic, magnetic and optical properties of Prussian blue analogues (PBAs) have been repeatedly leveraged towards applications that include energy storage3, catalysis4, ion trapping5 and gas storage6. However, studying the surprisingly complex atomic-scale structures of PBAs remains a long-standing challenge. In a paper in Nature, Simonov et al.7 report that they have successfully grown single crystals of PBAs, which have previously been notoriously elusive. By coupling X-ray measurements of the crystal lattices with a simple but effective theoretical model, the authors reveal an unexpected ordering of vacancies — absent nodes in the lattices that correspond to missing metal–anion units. This structural insight could enable yet another means of adjusting the properties of these extraordinary materials.   Prussian blue (Fe4[Fe(CN)6]3·14H2O) was first reported8 in 1710 and was widely used as a deep-blue pigment. The eventual determination of its crystal structure greatly expanded the conceptual boundaries of inorganic chemistry. X-ray diffraction experiments performed on powders9, and later on single crystals10, of Prussian blue revealed the parent structure shared by all PBAs: a cubic framework in which two different types of metal cation act as ‘nodes’ linked in three dimensions by cyanide anion (CN) ‘struts’ (Fig. 1a). PBAs therefore have the general formula M[M′(CN)6], in which M and M′ are chemically distinct metal ions; the [M′(CN)6]3/4 complex ion unit (Fig. 1b) is known as a hexacyanometallate ion, and carries either three or four negative charges. The study of the PBA parent structure enriched our fundamental understanding of the coordination chemistry of transition metals (how ligand molecules or ions bind to transition-metal ions such as iron, cobalt and copper), and demonstrated that coordination solids that have multidimensional connectivity can act as porous framework materials through which molecules and ions can move. Figure 1 | Vacancies in Prussian blue analogues. a, Compounds known as Prussian blue analogues (PBAs) have the formula M[M′(CN)6], where M and M′ are two chemically distinct metal atoms. The idealized crystal structure of a PBA is a cubic framework in which M and M′ ions act as ‘nodes’ connected by cyanide ions (CN–), which act as ‘struts’. b, The actual crystal structures contain vacancies — gaps in the lattice that correspond to missing [M′(CN)6]3–/4– units. Networks of vacancies can form pathways that allow molecules or ions to be transported through PBAs, a potentially useful characteristic. Simonov et al.7 have used X-ray measurements of single crystals of PBAs and numerical modelling to reveal the hidden order of vacancies in PBAs. The idealized crystal structures of PBAs correspond to the cubic framework described above, but belie a hidden degree of complexity that is crucial in determining their physical properties. The true atomic-scale structures contain vacancies corresponding to absent hexacyanometallate ions (Fig. 1b), which form pores that are typically filled with water molecules. The concentration and ordering (networking) of vacancies control the pathways through which mass can move within the materials, and can therefore tune the ability of PBAs to reversibly transport ions or small molecules. Insight into how vacancy ordering is affected by the chemistry of PBAs, or by the conditions used to synthesize them, can thus provide guidelines on how to tailor the properties of these compounds for applications. X-ray-scattering measurements on PBA powders, beginning with the early diffraction studies on Prussian blue9, yielded structural information for these compounds. But the random orientation of millions of crystallites in powders leads to loss of information that is retained if measurements are performed on single crystals. To gain this extra insight and illuminate vacancy behaviour, Simonov et al. sought to produce crystals of a series of PBAs that contained different metal-ion combinations. Growing single crystals of PBAs is challenging because of the rapidity with which microcrystalline powders precipitate when solutions of PBA precursors are combined. However, the authors found that controlled mixing of these solutions over the course of weeks produced single crystals suitable for X-ray-scattering analysis. Simonov and co-workers observed clear indicators of non-random ordering of vacancies in the scattering data for their PBA crystals. This ordering depends on each crystal’s chemical composition and the conditions used to crystallize it. To understand the diversity of the vacancy networks, the authors developed a simple two-part model to simulate vacancy ordering. The model considers only the trade-off between the preference of these compounds to adopt a uniform vacancy distribution, and the preference for lattice sites to have a certain local symmetry, yet it effectively reproduces the experimental X-ray scattering results.   Notably, the authors’ insights enable the vacancy-network architectures of PBAs to be predicted by considering only a few factors that depend on the two model parameters, such as the choice of metal, precursor concentrations and the temperature of crystallization. Some networks turn out to have relatively direct pathways through which a molecule or ion could move, whereas other networks’ pathways are more tortuous. By selecting PBAs that have direct pathways facilitating mass transport, these materials can be optimized for use as battery electrodes, catalysts or ion-exchange materials. Simonov and colleagues’ work addresses a long-standing lack of detailed knowledge about the structural vacancies that determine the physical properties of Prussian blue and its analogues. But numerous challenges remain before the predictive potential of their results can be fully realized. Although remarkably effective, the modelling analysis does not consider further possible complexities, such as the effects of ionic species that dwell in the PBA pores. Extrapolation of the findings from these single-crystal studies to powder samples, which are more technologically relevant, will require further challenging experiments and enhanced modelling that considers the surface structure and chemistry of microparticles. Great care will also be needed to work out how each of the variables in a PBA synthesis correlate with the resulting vacancy ordering and material properties. Although these challenges necessitate substantial further work, they also represent an opportunity to exert even greater control over the properties of PBAs, guided by a deeper understanding of structure–property relationships. Refinement of more-complex models will dictate how to take advantage of the many variables of a PBA synthesis. Not only has this work resulted in new-found control over the optimization of PBAs for applications in energy storage, ion capture and catalysis, but it also represents a platform on which to build a similar understanding of other framework materials, such as zeolites11 and metal–organic frameworks12, which have their own sets of challenges and promising applications. </body>
<date id = '282'>12 February 2020</date>
<url id = '283'>https://nature.com/articles/d41586-020-00325-9</url>
<title id = '283'>A process termed ubiquitination mediates the regulated destruction of cellular proteins, thereby preventing disease or infection. Structural data now reveal how a crucial regulator of ubiquitination enzymes coordinates this process.</title>
<body id = '283'>Many cellular functions that occur in eukaryotes (organisms whose cells contain a nucleus) are regulated by targeted protein destruction. This targeting is often achieved by a process called ubiquitination (or ubiquitylation), in which a protein selected for destruction is tagged with the protein ubiquitin. Ubiquitination is aided by enzymes known as E3 ligases, a subset of which are called cullin–RING ubiquitin ligases (CRLs)1. CRLs help to transfer ubiquitin from an E2 conjugating enzyme, to which it is bound, onto the target protein1. By default, CRLs are inactive, and they are activated when a protein called NEDD8 (which is similar in sequence to ubiquitin) becomes attached to the cullin subunit of the CRL2–5. But how this activation happens has been a mystery. Writing in Nature, Baek et al.6 report structural data obtained using a technique called cryo-electron microscopy (cryo-EM) that fills in some of the blanks. CRLs contain a banana-shaped cullin subunit (one of five cullin proteins, CUL1 to CUL5). This binds (Fig. 1) at one end to a substrate-receptor subunit — which recruits the protein targeted for ubiquitination — and at the other end to what is termed a RING-finger protein, which is either RBX1 or RBX21,7. The RING-finger protein recruits a ubiquitin-attached E2 enzyme and stimulates the transfer of its ubiquitin to the target protein1. Previous structural analysis demonstrated that the attachment of NEDD8 to CUL5 enhances the potential of RBX2 and its ubiquitin-bound E2 enzyme to move towards the region adjacent to the substrate receptor and its bound target protein3. However, that work used a truncated version of CUL5 bound to RBX2, and lacked both a target protein bound to the substrate receptor and a ubiquitin-attached E2 enzyme, thus leaving to the imagination the mechanism by which NEDD8 stimulates ubiquitin transfer to the target protein. Figure 1 | Structural basis for how ubiquitination is stimulated by the NEDD8 protein. Baek et al.6 used cryo-electron microscopy to analyse how the ubiquitin protein (Ub) becomes attached to a protein that is thereby marked for degradation. a, Ubiquitin binds to the enzyme UBE2D. The protein IκB is a ubiquitination target, and binds to a substrate receptor called β-TRCP. This receptor also binds to a protein complex consisting of SKP1–CUL1–RBX1, called CRL1, to form a complex termed CRL1β-TRCP. The transfer of ubiquitin from UBE2D to IκB is aided by CRL1β-TRCP. The NEDD8 protein tags the WHB domain of CUL1, thereby increasing the flexibility of the complex and enhancing ubiquitin transfer. b, The authors describe a transition-state complex consisting of three modules: an activation module (the NEDD8-bound WHB domain), a catalytic module (ubiquitin, UBE2D and the adjacent part of RBX1) and a substrate-scaffolding module (the remaining components). They report that extensive rearrangements of these modules occur after NEDD8 binds to CRL1, a finding that helps to explain how NEDD8 enhances ubiquitination. (Image based on Extended Data Fig. 2 of ref. 6.) Baek and colleagues therefore sought to capture a human NEDD8-attached CRL in the act of transferring ubiquitin to its target protein. To achieve this goal, the authors made a ‘tribrid’ molecule comprising three components. One component was a stretch of amino-acid residues derived from the protein IκB, which is a ubiquitination target that binds to a substrate receptor called β-TRCP. The second was an E2 enzyme termed UBE2D, and the third was ubiquitin. This tribrid provided a stable mimic of how the molecular components are arranged during the transition state, when ubiquitin is being transferred from the E2 enzyme to the target protein. Using cryo-EM, the authors obtained structural data for the complex that formed when the tribrid and β-TRCP assembled with the proteins CUL1, SKP1 and RBX1 (this complex is called CRL1β-TRCP). The type of structural information that can be obtained using X-ray crystallography is constrained by technical issues (packing forces in the crystals) that affect data collection. The cryo-EM approach taken by the authors avoids these constraints and enables multiple conformations of a structure to be obtained. The authors confirmed an earlier finding3 that CRL1β-TRCP shows modest conformational flexibility in the absence of NEDD8, but that this flexibility increases when NEDD8 is attached. Furthermore, on addition of the tribrid, the ensemble of conformations converged to form one structure of a transition-state intermediate.   Baek and colleagues’ structural data are nothing short of spectacular. Previous work3 suggested that the active site of the E2 enzyme, where ubiquitin is transferred to the target protein, might not be fixed in location relative to the NEDD8-attached CRL because of the mobility of the RING-finger protein’s RING domain. The transition state presented by Baek et al. shows the precise 3D relationship of three modules that form the whole complex: a catalytic module, an activation module and a substrate-scaffolding module. The catalytic module comprises ubiquitin-bound UBE2D and the RING domain of RBX1, and this module moves when NEDD8 becomes attached to CUL1. The activation module consists of a mobile domain in CUL1 called the WHB domain, to which NEDD8 attaches. The substrate-scaffolding module includes β-TRCP and portions of CUL1 and RBX1 that have a fixed spatial relationship to β-TRCP and IκB. In Baek and colleagues’ proposed activated structure, the catalytic module projects directly towards the substrate-scaffolding module, such that UBE2D touches β-TRCP (Fig. 1). The activation module coordinates the architecture of the transition state, with NEDD8 forming multiple contacts between UBE2D in the catalytic module and CUL1 in the substrate-scaffolding module. These interactions stabilize the configurations of the WHB and RING domains and bring UBE2D’s active site into close proximity with β-TRCP and its bound target protein. To confirm these findings, the authors performed extensive and sophisticated kinetic analyses comparing wild-type complexes with those containing mutant proteins designed to disrupt interactions between the modules. All complexes containing a single mutant protein showed strongly reduced enzymatic activity compared with those of wild-type complexes, and complexes containing two mutant proteins had potent synergistic defects, which is consistent with the authors’ model for how the complex functions.   This structure provides information that explains many previously confusing or contradictory observations. For example, it now makes sense why, during a bacterial infection, there is a catastrophic effect on CRL function when bacterial enzymes target the glutamine 40 amino-acid residue of NEDD88. This is because modification of this residue would destabilize the activation module. In addition, the structure shows clearly how direct contacts between NEDD8 and UBE2D that occur away from UBE2D’s catalytic site9 work together with RBX1 to optimally position the catalytic module relative to the β-TRCP-bound target protein. These structural insights pose new questions. Most notably, why does the transfer of the first ubiquitin to some CRL substrates require an extra RBX1-interacting complex of E3 and E2 enzymes (ARIH1 and UBE2L3, respectively10), given the extraordinary catalytic efficiency of the complex reported by Baek and colleagues? Moreover, how is the observed rapidity of ubiquitin transfer achieved, given the proposed requirement for the complex to undergo substantial structural rearrangements to reach the transition state? And what might the transition state look like for the NEDD8-stimulated process of chain elongation (the attachment of further ubiquitin molecules to the initial ubiquitin tag on a target protein), considering that ubiquitin-chain elongation is mediated by different E2 enzymes11 from those that add the initial ubiquitin tag? With cryo-EM now firmly part of the toolkit for investigating ubiquitination, the answers might arrive sooner than we thought. Importantly, these new structural data might help in the design of drugs known as proteolysis-targeting chimaeras (PROTACs), some of which can redirect specific CRL enzymes to ubiquitinate and thus destroy targets of clinical interest that are outside the enzymes’ natural repertoire12. These drugs work by simultaneously binding substrate receptors of CRLs and a target protein. However, the formation of such complexes is not always sufficient to stimulate ubiquitin transfer13. The reason for this might become clear from the deeper understanding of CRL-mediated ubiquitin transfer gained through the work of Baek and colleagues. </body>
<date id = '283'>12 February 2020</date>
<url id = '284'>https://nature.com/articles/d41586-020-00323-x</url>
<title id = '284'>Devices known as quantum cascade lasers produce useful terahertz radiation, but are typically highly sensitive to fabrication defects. This limitation has now been overcome using a property called topological robustness.</title>
<body id = '284'>Electromagnetic waves with frequencies in the terahertz range (300 GHz to 10 THz) have applications in many areas, from imaging and security screening to the atmospheric and biological sciences. Semiconductor devices called quantum cascade lasers (QCLs) provide the most compact and efficient way to generate terahertz radiation. In QCLs, electrons cascade down in energy through a series of discrete quantum energy levels, emitting a photon at each step1. But, as with all compact semiconducting lasers, QCLs are notoriously sensitive to fabrication imperfections, which results in device-to-device variability of the laser output frequency. Now, writing in Nature, Zeng et al.2 report the realization of a terahertz QCL that is insensitive to such disorder. This achievement opens the door for terahertz lasers and optoelectronics that have unprecedented stability and fabrication reproducibility.   Lasers use a process known as optical feedback to build up light intensity and stimulate electrons to emit photons. A common way to introduce this feedback uses a structure called an optical cavity, which is typically composed of mirrors that reflect the emitted light back into the device. Compact lasers, however, use more-complex structures such as photonic crystals — materials that have a periodically varying refractive index. If this periodicity is carefully engineered, photonic crystals can be used to reflect light waves of only the desired frequency, and so achieve lasing3. But this approach is highly sensitive to disorder, because any imperfections in the photonic crystal cause reflections that result in waves of unwanted frequencies. These compete with the desired waves, leading to unstable light intensity and poor laser efficiency. In the past few years, ‘topological’ photonic structures have emerged as a way to make photonic devices that are insensitive to disorder. This area of research originated from concepts developed in condensed-matter physics. Over the past two decades, condensed-matter physicists have been able to use the mathematical descriptions of symmetries and topology to characterize different forms of matter. Of particular relevance to the current work are exotic materials known as topological insulators4. As the name suggests, these materials are insulators — that is, they do not conduct electricity in their interior. However, they host conducting electronic states at their boundaries. Such edge states can carry current in only one direction and are therefore robust against disorder that would otherwise scatter charge carriers. This robustness of edge states is a manifestation of the overall topological properties of the material. Topological insulators are so insensitive to disorder that they were previously used to define the unit of resistance: the ohm.   Although topological physics originated in the field of electronics, it has begun to inspire photonics5. Disorder and scattering are even more problematic in optics than in electronics, because photons exhibit strong interference effects that can lead to complicated, difficult-to-control laser behaviour. Translating topological protection into the optical domain opens up the possibility of making robust optical systems. In particular, topological lasers can emit light in a way that is robust against scattering and other consequences of imperfections. But previous realizations of topological lasers6–8 have operated at frequencies above the terahertz range. Zeng and colleagues overcame this limitation by incorporating topological protection into a QCL. To achieve this, they used a topological model known as the valley Hall effect, which relies on breaking the spatial-inversion symmetry of a crystal lattice9 (its symmetry under the combination of a 180° rotation and a mirror reflection). Specifically, the authors used a gallium arsenide–aluminium gallium arsenide substrate as the gain material — the medium in which light is amplified. This substrate contained layered semiconductor structures called quantum wells that were designed to support quantum cascade lasing. The authors drilled a triangular lattice of holes in the gain material (Fig. 1). The symmetries of this lattice resulted in the emergence of two valleys in the energy–momentum band structure — the relationship between the energy and momentum of photons in the material. The authors made the holes quasi-hexagonal so that they broke the spatial-inversion symmetry of the lattice and rendered the two valleys topologically inequivalent. This led to the formation of topological edge states at the interface between two such crystal lattices in which the orientation of holes (and valleys) was flipped in one lattice with respect to the other. Figure 1 | Design of a topological laser. Zeng et al.2 have made a laser in which terahertz radiation is emitted from the interface between two triangular crystal lattices that consist of quasi-hexagonal holes in a substrate material. The crystal lattices are topologically inequivalent because the orientation of the holes is flipped in one lattice with respect to the other, and this leads to the emergence of exotic photonic states called edge states at the crystal-lattice interface. The topological nature of these edge states renders the laser robust against fabrication imperfections. Zeng and co-workers used these topological edge states to design and make a robust ring resonator (a type of optical cavity that traps light at certain ‘resonance’ frequencies) in the form of a triangle (Fig. 1). It is this triangular cavity that, along with the light amplification from the substrate material, forms a topological laser. The laser produces light of many frequencies that are separated by similar frequency gaps. These frequencies correspond to the resonance frequencies of the triangular cavity and fall within the frequency range of the QCL gain material. The authors measured light emission from different points along the perimeter of the cavity and discovered that the emission at each point had the same resonance frequencies. This indicates that these waves travelled through the length of the cavity, traversing the sharp (60°) bends at the corners of the triangle. Furthermore, Zeng et al. found that the lasing frequencies did not change when they introduced defects, in the form of extra holes, around the cavity, demonstrating the robustness of the QCL. Another key feature of this laser is that energy is ‘pumped’ into the device electrically. Previous topological lasers6–8 have been optically pumped, which means that they require a second laser source to drive the topological laser to generate light. This pumping scheme severely limits practical applications. However, similar to many commonly used lasers (such as laser pointers), Zeng and colleagues’ QCL can be directly driven by an electrical current, allowing it to be powered, in principle, by a battery or a wall outlet, rather than by another laser. Robustness against defects and disorder is one defining characteristic of topological physics, but another important feature is a type of asymmetry called chirality. In particular, in the valley Hall effect, the two valleys are associated with photons of opposite circular polarization in the plane of the material. If right-circularly polarized photons travel to the left, then left-circularly polarized ones would travel to the right. Realizing this chirality represents a crucial future step towards terahertz topological lasers in which light waves flow around a ring resonator in only one direction. The chirality could be incorporated either by explicitly breaking time-reversal symmetry (a symmetry in which reversing the direction of light waves is equivalent to running time backwards) or by introducing directional light amplification in the cavity. Zeng and co-workers’ results pave the way for studying topology in a previously inaccessible part of the electromagnetic spectrum. One area of great interest for future research is the application of other topological models, such as exotic (higher-order) topological insulators, to make robust terahertz lasers that have other geometries. For example, these lasers could emit light at the corners, rather than at the edges, of a triangular cavity. Another fascinating prospect is the exploration of non-Hermitian (open) physical systems at terahertz frequencies, in which the presence of light amplification and loss can lead to the emergence of features such as parity–time symmetry (symmetry under the combination of a mirror reflection and time reversal) and exceptional points (spectral features that correspond to coalescing resonances)10. The realization of topological photonics in the terahertz range could therefore serve as a catalyst for the development of practical devices, and also enable a better fundamental understanding of topological physics and complex (nonlinear) optoelectronics. </body>
<date id = '284'>12 February 2020</date>
<url id = '285'>https://nature.com/articles/d41586-020-00328-6</url>
<title id = '285'>Tumours often grow entangled among neurons, which makes the cancer difficult to treat. The finding that cancer cells hijack neighbouring neurons to promote tumour growth suggests new therapeutic targets.</title>
<body id = '285'>Malignant tumours are a complex, yet organized, diverse ensemble of cells. Tumour cells are surrounded by other types of cell, which collectively form the tumour microenvironment. Components of this microenvironment include fibroblast cells, which can promote the growth and spread of tumours to distant sites, and immune cells. The latter have antitumour functions that are often suppressed by cancer cells; indeed, therapies that boost such immune cells are revolutionizing the treatment of certain cancers. By contrast, the interactions between cancer cells and neurons in the tumour microenvironment are less-well understood. Writing in Nature, Amit et al.1 reveal how tumours influence neurons to promote tumour growth, and show how this discovery might lead to new anticancer therapies.   The interplay between cancer and neurons has negative clinical consequences for people with prostate tumours2. Individuals who have a higher number of new neurons (in structures called nerve fibres) in the tumour microenvironment tend to have more-aggressive tumour features, such as further tumour growth and migration to distant sites, and a decrease in survival time2. Studies last year found that cancer cells and neurons can interact directly with each other through connections called synapses, that these connections aid the growth of brain tumours3–5 called gliomas, and that this interaction is associated with lethal cancer spread5. These and other findings3–6 contribute to a growing body of evidence that neurons are crucial components of the tumour microenvironment. However, what prompts the formation of neurons in the microenvironment had not been understood until now. Amit and colleagues took on this challenge by focusing on tumours known as head and neck cancers, which can arise in the oral cavity. In humans, these tumours are often characterized by mutations that inactivate the gene TP53. This gene encodes a protein (p53) that functions as a tumour suppressor and that can modulate the tumour microenvironment7. By analysing four different mouse models of this disease and data obtained from biopsies of people with head and neck cancer, the authors found that tumours with mutant versions of p53 have a higher number of associated newly formed neurons than do those with wild-type p53. Moreover, an increased number of such neurons correlated with a shorter survival time. To try to determine whether cancer cells with mutant p53 might stimulate neurons to form, Amit et al. analysed the factors released by human cancer cells that have mutant or wild-type p53. Both types of cell secreted vesicles that contained small RNA molecules called microRNAs (miRNAs). The vesicles in the two cell types were of a comparable number and size, but their contents differed (Fig. 1). Only the vesicles secreted from tumours with mutant p53 were devoid of an miRNA termed miR-34a, which is a tumour suppressor. When vesicles from tumours lacking p53 were injected into mice with tumours that had wild-type p53, the tumours with wild-type p53 grew larger and had more surrounding neurons than normal, indicating that the contents of these vesicles drive the formation of new neurons. This is the first report showing that miR-34a, the main function of which is to keep in check the proliferation of normal and cancer cells8, is important in counteracting the formation of neurons in the tumour microenvironment. Figure 1 | Tumours manipulate neighbouring neurons to boost cancer growth. Amit et al.1 analysed head and neck cancers using clinical data and mouse models. a, Tumour cells that expressed wild-type p53 protein released vesicles containing small RNA molecules called microRNAs (miRNAs) that were taken up by neighbouring neurons. An miRNA known as miR-34a blocks neuronal proliferation, and the neurons were maintained in their current state. By contrast, tumours that had a mutant version of p53 released vesicles that lacked miR-34a. In this case, neurons increased in number in the vicinity of the tumour, and these cells were reprogrammed as adrenergic neurons that express the molecule noradrenaline. These neurons had more axonal branches than did those near tumours that expressed wild-type p53. Interactions between adrenergic neurons and the tumour aided cancer growth. b, When mice received a transplant of p53-deficient tumour cells, treatment with a drug (carvedilol) that blocks adrenergic signalling pathways slowed tumour growth. This might provide a new therapeutic tool for targeting tumours that need neighbouring adrenergic neurons for their growth. Amit and colleagues analysed how these newly formed neurons promote tumour growth. The authors examined the neurons present in tumours with mutant and wild-type p53. Intriguingly, in the former set, the neurons (presumably including those already present in the area where the tumour formed) had undergone a functional change to become a type of neuron known as an adrenergic neuron — which uses the adrenergic signalling pathway and is activated in the ‘fight-or-flight’ response. This adrenergic feature (which has hallmarks including expression of the molecule noradrenaline) was crucial for sustaining cancer growth.   Interestingly, previous epidemiological analysis9 revealed that the use of the drug carvedilol, which blocks adrenergic signalling and is prescribed for conditions such as high blood pressure, is associated with a reduced risk of cancer onset. Now, Amit et al. raise the question of whether carvedilol’s anticancer properties might be due to its ability to target adrenergic neurons, given the effectiveness of the drug in treating mice with p53-deficient tumours (Fig. 1). The authors’ findings are of particular interest because these insights might offer a way to combat the tumour-driven formation of adrenergic neurons and to counteract their tumour-promoting effects. It will be important to establish whether adrenergic neurons’ contribution to tumour growth is limited to just head and neck cancers that have mutant p53, or whether this phenomenon could also be a feature of other types of tumour, as suggested by the epidemiological evidence for carvedilol use9. Mutant versions of the gene encoding p53 are among the most common alterations in certain human cancers, occurring in approximately 60% of colon cancers, 50–80% of lung cancers and 95% of ovarian tumours10. Given the high prevalence of p53 abnormalities in cancer, numerous efforts have been made to design compounds that target mutant p53 to force it to act like wild-type p53, and promising results have been obtained in early-phase clinical trials of such drugs11. It would be worth testing whether using both carvedilol and a drug that targets mutant p53 is more effective than either compound alone in treating these lethal forms of cancer. Amit and colleagues’ discovery that the absence of functional p53 influences the formation of neighbouring neurons might have relevance for interpreting reports showing that fluctuations in the levels of wild-type p53 are observed in nerve regeneration12. Thus, the authors’ findings might have repercussions that reach beyond the field of cancer research to regenerative medicine. Perhaps therapies that modulate the activity of p53 will have a future role in aiding the repair or regeneration of neurons, an outcome that would make a profound difference to the lives of people who have neurodegenerative diseases or other types of nerve injury. </body>
<date id = '285'>12 February 2020</date>
<url id = '286'>https://nature.com/articles/d41586-020-00314-y</url>
<title id = '286'>Tumours are often stiffer than normal tissues and show abnormally fast metabolism of glucose. It emerges that the link between these two traits involves tension in a network of protein filaments in cells.</title>
<body id = '286'>Tension in cells and tissues has crucial roles in an organism’s development and maintenance1. Tension-modulated processes such as cell growth, migration and differentiation have high energy demands, and so it is perhaps not surprising that cell tension also regulates cellular metabolism2 — but exactly how has been unclear. Writing in Nature, Park et al.3 describe a mechanism by which stiffness in the extracellular matrix (ECM) around cells promotes the reorganization of a filamentous cellular protein called actin to enhance glycolysis, a key metabolic pathway that generates energy from glucose. The authors also show how changes in this pathway can lead to elevated glucose metabolism in lung tumours4.   Cells sense changes in the ECM through transmembrane receptor proteins such as integrins1, which can trigger changes in the network of actin and myosin protein filaments, known as the cytoskeleton, that defines a cell’s shape. For instance, in response to a stiff ECM, integrins assemble into clumps called focal adhesions, around which scaffolding proteins such as α-actinin gather1. These proteins, in turn, facilitate the assembly of thick, cable-like, oriented arrays of actin–myosin filaments called stress fibres. Park et al. set out to examine this process using cells from the lining of the human lung, which are exposed to mechanical changes during breathing. The authors found that, as expected, cells placed on soft substrates in a culture dish did not spread across the dish or assemble actin stress fibres. However, compared with cells placed on stiff substrates, these cells showed a downregulation of several molecules that was consistent with reduced glycolysis. The enzyme phosphofructokinase (PFK) has a rate-limiting role in glycolysis. The researchers noted that levels of all isoforms of PFK were reduced when the cells were placed on soft substrates. But when PFK levels were increased in these cells, glycolysis was not restored to normal, arguing against the idea that mechanical cues regulate glycolysis by modulating PFK gene transcription. Instead, Park et al. hypothesized that the loss of PFK on soft substrates was due to protein degradation. A major pathway for this involves enzymes called E3 ubiquitin ligases, which tag proteins with ubiquitin molecules, marking them out for destruction by a molecular machine called the proteasome. In agreement with their prediction, the authors found that, in cells on soft substrates, levels of the PFK isoform PFKP could be restored — and glycolysis enhanced — if they inhibited proteasome activity or mutated lysine amino-acid residues in PFK that are required for ubiquitination. The group performed database searches to find the E3 ubiquitin ligase that mediates PFKP degradation, and identified TRIM21 as a possible candidate — this was confirmed by experiments in which the gene that encodes TRIM21 was mutated. Next, the authors manipulated levels of myosin or actin and found changes in TRIM21 positioning within the cell. They went on to demonstrate that TRIM21 associates with stress fibres. Their data further suggested that the actin in the cytoskeleton was sequestering TRIM21 and rendering it inactive (and thus maintaining PFKP activity) in response to stiffness outside the cell, promoting glycolysis (Fig. 1). The researchers confirmed this finding by increasing actin-bundle assembly using a mutant form of α-actinin and observing a subsequent increase in TRIM21 levels around actin. Figure 1 | A force for change in normal, but not cancer, cells. Park et al.3 have uncovered a pathway by which tension in the extracellular matrix (ECM) around cells governs the rate of glucose metabolism (glycolysis). a, If normal lung cells are surrounded by a soft ECM, the filamentous form of the structural protein actin (f-actin) does not form bundles, and the protein TRIM21 is active. TRIM21 adds ubiquitin molecules (Ub) to the glycolytic enzyme phosphofructokinase (PFK), tagging it for degradation. This leads to low glycolysis rates. b, By contrast, normal lung cells surrounded by a stiff ECM assemble a thick bundle of stress fibres (made of actin and another protein, myosin), which trap TRIM21. These fibres also form in lung cells engineered to express cancer-promoting oncogenes, regardless of the stiffness of the ECM. Trapping of TRIM21 prevents the degradation of PFK, leading to high rates of glycolysis. Together, Park and colleagues’ data add to a growing body of evidence demonstrating how the composition of and changes in the ECM can modulate glucose metabolism5,6. The data also offer a potential explanation for the aberrant glucose regulation reported in metabolically dysfunctional fatty breast tissue of people who are obese, in which the connective tissue, called the stroma, is often stiff7. Park and colleagues’ work is consistent with, and extends, another study that has linked cytoskeletal organization to glycolysis8, as well as work showing how the application of forces between cells can increase cellular metabolism9. One of Park and co-workers’ most compelling observations is that tension-mediated sequestration of TRIM21 might explain why tumours exhibit abnormally high levels of glycolysis10. The group’s unbiased computational assessment showed high levels of PFKP in tissue from people who had lung cancer. The authors engineered their human lung cell line to express some of the cancer-promoting proteins that are frequently overexpressed in lung tumours. PFKP levels were consistently higher in these cells than in the normal lung cells, regardless of the substrate on which they were grown. By contrast, the enzyme’s levels were variable in healthy lung tissue, and the authors’ lung cell line showed varying PFKP levels in response to changes in substrate stiffness.   Tumours often undergo a desmoplastic response — an increase in production, remodelling and crosslinking of components in the ECM, causing a tissue-scarring process called fibrosis that stiffens the stroma. The desmoplastic response promotes tumour-cell growth, survival and invasion11. This might suggest that a stiff tumour ECM fosters tension-induced actin bundling, which acts to sequester TRIM21 and stabilize PFKP, thereby promoting unfettered glycolysis. If this hypothesis is correct, treatments that prevent fibrosis would be expected to normalize tumour-cell metabolism. Unfortunately, such strategies have been disappointing clinically12. One explanation is that many tumour cells also show increased activity of the RhoGTPase and ROCK enzymes13,14, which promote actin-filament assembly, or overexpress oncogenes (which encode cancer-promoting proteins such as Ras and EGFR) that cause elevated actin–myosin tension13,14. The cells therefore probably form stress fibres regardless of ECM conditions. Indeed, when Park and colleagues grew lung cancer cells carrying oncogene mutations on soft substrates, they observed that the cells not only maintained prominent actin stress fibres, but also had reduced TRIM21 expression, elevated PFKP levels and high rates of glycolysis. Finally, the authors showed that the abnormally high glycolysis in their tumour cells could be normalized simply by increasing TRIM21 gene expression. This finding argues that compounds that stimulate protein degradation, augment ligase activity or reduce actin-fibre assembly should similarly normalize tumour glycolysis and hence could be new antitumour therapies. Perhaps the same goal could be accomplished by reducing tumour-cell tension or treating patients with ROCK inhibitors that have already been developed for clinical use — at least in those tumours in which TRIM21 or similar E3 ligases are not mutated (mutations in several E3 ligases have been implicated in either tumour suppression or tumour progression15). Indeed, ROCK and EGFR inhibition can reverse the proliferative and invasive traits of 3D in vitro tumour structures called spheroids, and prevent the progression of various cancer types in animal models in vivo13,14,16. </body>
<date id = '286'>12 February 2020</date>
<url id = '287'>https://nature.com/articles/d41586-020-00327-7</url>
<title id = '287'>How Nature reported an investigation into the origin of maize in 1920, and a paean to the work of the sea from 1870.</title>
<body id = '287'> It is well known that when America was discovered maize was widely cultivated by the aborigines, but the wild source of the plant has remained obscure. Various views concerning its origin have been entertained, one being the theory of Mr. Collins, based on breeding experiments and morphological comparisons, that maize arose as a hybrid between the Mexican teosinte (Euchlaena) and some unknown grass belonging to the Andropogoneae. Mr. Y. Kuwada … has studied the chromosomes of maize and its relatives, and brings cytological evidence in support of Mr. Collins’s hypothesis. Maize, as well as Euchlaena and Andropogon, is found to have ten pairs of chromosomes, but those of Euchlaena are longer than those of Andropogon, while in maize they are found to be of different lengths, a pair frequently being composed of a longer and a shorter chromosome. From this it is concluded that maize is hybrid in origin, the two types of chromosomes being traceable as in certain experimentally produced animal hybrids. From Nature 12 February 1920 The work done by the Sea is infinitely various, immeasurable in quantity and of inexpressible value to the inhabitants of the earth. It is the one ceaseless worker, never resting and ever accomplishing the tasks it has to perform. The land and the sea may appear to some to be for ever fixed and unalterable, and the map of the world represents to them the geography of the globe of 6,000, or 60,000 years ago, the geography of to-day, and the geography of 60,000 years hence. Still not only does Geology show by the testimony of the far-distant past the impossibility of this being so; but it has been given to man to see and record the constant rising and falling of the land, within the periods of history and even to measure the movement with sufficient accuracy and such certainty as to enable him to venture predicting, to some extent, on the probable geography of the future. The Earth is born of the Ocean. Continents and islands rise out of the sea, new, luxuriant and vigorous; and like ourselves they grow, mature and do their appointed work; then wane and seem to die, though they do not die. They sink beneath the waves, apparently for ever; but only to be regenerated, renewed, quickened into life and born again remodelled. And the sea — the invigorating and ever-toiling Mother — works this wonder. From Nature 10 February 1870 Latest on: Evolution Article 20 MAY 20 News & Views 20 MAY 20 News 19 MAY 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Ocean sciences News Feature 22 MAY 20 Review Article 01 APR 20 Comment 31 MAR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '287'>11 February 2020</date>
<url id = '288'>https://nature.com/articles/d41586-020-00213-2</url>
<title id = '288'>A massive international effort has yielded multifaceted studies of more than 2,600 tumours from 38 tissues, generating a wealth of insights into the genetic basis of cancer.</title>
<body id = '288'>Comprehensive genomic characterization of tumours became a major goal of cancer researchers as soon as the first human genome had been sequenced in 2001. Since then, advances in sequencing technology and analytical tools have allowed this research field to flourish. In six papers1–6 in this issue of Nature, the Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium presents the most comprehensive and ambitious meta-analysis of cancer genomes so far. Unlike previous efforts that focused largely on protein-coding regions of the cancer genome, PCAWG analyses whole genomes. Each article scrutinizes an important aspect of cancer genetics — together, their findings will be key to understanding the full genetic complexity of cancer. Before discussing the impact of these analyses, it is crucial to highlight the massive amount of data and the complex organizational framework that underpin the PCAWG endeavour. The project involved an interdisciplinary group of scientists from 4 continents, with 744 affiliations between them, who had to overcome major technical, legal and ethical challenges to carry out distributed analyses while protecting patient data. Researchers were divided into 16 working groups, each focused on distinct facets of cancer genomics — assessing the recurrence of mutations, for instance, or inferring tumour evolution. Altogether, the consortium performed integrative analyses of 38 tumour types. The group sequenced 2,658 whole-cancer genomes (Fig. 1), alongside matched samples of non-cancerous cells from the same individuals. These data were complemented by 1,188 transcriptomes — the sequences and abundances of RNA transcripts in a tumour. Figure 1 | A worldwide effort to tackle cancer. The Pan-Cancer Analysis of Whole Genomes (PCAWG) Consortium is a group of cancer researchers from four continents (blue). The group sequenced and analysed 2,658 whole-cancer genomes from 38 types of tumour. The huge amount of data involved in the effort required sophisticated cloud-computing approaches. Six papers1–6 from the PCAWG now describe different aspects of the analyses performed. (Nature publications remain neutral with regard to contested jurisdictional claims in published maps.) These efforts involved extensive quality control and coordinated data processing, as well as massive, systematic experimental validation of the computational pipelines used to detect mutations. Many computational algorithms and pipelines were used and compared in concert. This required hundreds of terabytes of data, spread across multiple data centres, and probably millions of processing hours — all facilitated by cloud computing. Notably, the PCAWG efforts provide a prime example of how cloud computing can make international collaboration possible and help to advance data-intensive fields.   The first of the current papers1 gives an overview of the breadth and depth of the PCAWG data set. The consortium reports that, on average, each cancer genome carries four or five driver mutations, which provide cancer cells with a selective advantage. Only 5% of tumours studied had no identified driver aberrations. By contrast, many cancers exhibited hallmarks of genomic catastrophes called chromoplexy (17.8% of tumours) and chromothripsis (22.3%), which result in major structural changes to the genome. The other five papers each delve into a different aspect of the data set in more detail. For instance, in the second paper, Rheinbay et al.2 set out to identify genetic drivers in non-coding DNA. This is an ambitious undertaking, because it is substantially more difficult to accurately detect mutations in non-coding regions than in coding regions, or to assess their recurrence. The authors used careful modelling to exclude artefacts and systematically identify non-coding driver mutations.   Their results call into question previously reported non-coding drivers, such as the long non-coding RNAs NEAT1 and MALAT1, but also reveal new ones. For example, the authors report a recurrent mutation in a non-coding region of the key tumour-suppressor gene TP53. They also found relatively frequent mutations in non-coding regions of the telomerase gene TERT that result in overexpression of the telomerase enzyme (which helps tumour cells to divide uncontrollably), mirroring the high (12%) prevalence of telomerase mutations found in a previous pan-cancer study of more-advanced (metastatic) tumours7. Although the study could not rule out the existence of other non-coding drivers, it decisively shows that this type of mutation is not common. In the third and fourth papers, Alexandrov et al.3 and Li et al.4 focus on genomic aberrations called signatures. Different processes, such as defective DNA-repair mechanisms or exposure to environmental mutagens, produce these characteristic patterns of DNA aberrations. Large genomic data sets are crucial if we are to refine known mutational signatures and discover new ones. Impressively, between them, Alexandrov et al. and Li et al. identify 97 signatures. This expansion on previous work encompasses not only conventional single-nucleotide signatures, but also signatures involving multi-nucleotide variants and small insertions or deletions of DNA.   Notably, Li and colleagues are among the first to uncover reproducible signatures involving structural variants (SVs) — rearrangements of large portions of the genome. The process was much more intricate than that for identifying mutational signatures because of the diversity and complexity of SVs. Through a series of mutation-subgrouping steps, the researchers identified 16 SV signatures, revealing, for example, a putative mechanistic link between two SVs, deletions and reciprocal inversions (the last of which involves a reversal of the orientation of a segment of DNA). They also gained insights into the roles of all 16 signatures in cancer. Mutations in certain DNA-repair genes were shown to associate with characteristic cancer signatures. For instance, the consortium found that mutations in the gene CDK12 associate with tandem stretches of duplicated DNA, and that truncated variants of the DNA-repair enzyme MBD4 co-occur with a distinct mutational signature involving DNA sequences called CpG sites. Altogether, these new signatures lay the foundation for understanding mechanisms of cancer development, and the role of mutagenic exposures in this process.   The idea that cancer develops through an evolutionary process was first presented in 19768. Since then, cancer evolution has been characterized in terms of random mutations and natural selection. A cancer cell harbouring a mutation that confers high fitness proliferates rapidly, becoming the most prominent cell clone in the population. This phenomenon, called a clonal sweep, occurs in recurring cycles during cancer growth. Cancer evolution is most effectively studied by sequencing multiple regions of a tumour over time, but it can also be reconstructed from a single biopsy — the approach taken by Gerstung et al.5 in the fifth paper. The authors introduce the concept of ‘molecular time’ to classify clonal and subclonal mutations. They reasoned that subclonal mutations, which are present in only a subset of a tumour’s cells, must have arisen late in the cancer’s evolution. They classify clonal mutations, which are present in all of a tumour’s cells, as early or late, depending on whether the mutations arose before or after the clone underwent copy-number gains (an increase in the number of copies of a gene or chromosomal region). The researchers aggregated evolutionary data from multiple tumours, allowing them to identify common mutational trajectories such as APC–KRAS–TP53 progression9, which describes the typical sequence in which mutations arise in colorectal cancer.   Gerstung et al. found that the driver mutations that most commonly occur in a given cancer also tend to occur the earliest. Similarly, if copy-number gains are highly recurrent in a particular cancer type, they tend to occur early. For example, a copy-number gain in part of chromosome 5 is common in clear cell kidney cancer, and tends to arise early in the disease’s evolution. Conversely, whole-genome duplication is a relatively late event in this cancer. Finally, the researchers found that mutational signatures change over time in at least 40% of tumours. These changes reflect a decreasing role for environmental exposures in disease progression and an increase in the frequency and severity of DNA-repair defects. Overall, the group’s findings suggest that driver mutations can occur years before cancer is diagnosed, which has implications for early detection and biomarker development. In the final paper, the PCAWG Transcriptome Core Group and their colleagues6 made use of the 1,188 PCAWG samples that had matched transcriptome data, to functionally link DNA and RNA alterations. The group found associations between hundreds of single-nucleotide DNA mutations and the expression of nearby genes. However, larger copy-number alterations were the main drivers of gene-expression changes in cancer cells. Mutations were also associated with changes in transcript structure, such as the formation of a new protein-coding region (an exon) within a non-coding region (an intron).   The authors also characterized the frequency of bridged fusions — a phenomenon in which two genes become fused owing to a third, intervening fragment of DNA. Finally, although 87 of the 1,188 samples analysed did not have a driver alteration at the DNA level, the group showed that every one of these had an RNA-level alteration. Together, these insights illustrate the power of integrated RNA- and DNA-sequencing analysis for cancer studies10. These six papers, together with companion papers being co-published elsewhere (see go.nature.com/3boajsm), represent a milestone in cancer and cloud genomics. By focusing on inferences, the PCAWG successfully expands on a decade of cancer sequencing studies that were rooted largely in observations. It is worth noting that, although inferential analyses offer a deeper look at cancer than do descriptive studies, their results are also associated with a higher degree of uncertainty. The broad availability and quality of the PCAWG data set will almost certainly spur a wave of biological insights and methodological developments. Integration with other functional genomic data sets, for example probing the 3D organization of the genome, will also undoubtedly provide further understanding of the causes and consequences of genetic aberrations. The biggest limitation of the current studies is the lack of clinical data concerning patient outcomes and treatments. Such data would allow researchers to identify the genetic changes that can predict clinical outcomes. Fortunately, a project called the International Cancer Genome Consortium–Accelerate Research in Genomic Oncology (ICGC–ARGO) is under way that will create such a resource for more than 100,000 people with cancer. Ultimately, the PCAWG brought together thousands of scientists, working together to achieve its aims. The long-term impact of these efforts will not be limited to the findings published today, but will also come from the collaborations that have formed and the knowledge exchanges that have taken place between members of this global consortium of researchers. </body>
<date id = '288'>05 February 2020</date>
<url id = '289'>https://nature.com/articles/d41586-020-00131-3</url>
<title id = '289'>Parkinson’s disease and multiple system atrophy involve the protein α-synuclein. Proof that aggregated α-synuclein adopts a different structure in each case suggests that its conformation underlies the distinct disorders.</title>
<body id = '289'>A snowflake begins life as a tiny crystal that acts as a seed on which water molecules aggregate, increasing the size of the snowflake as it descends to earth. Proteins can also act as seeds — for instance, in a class of age-related disorders called amyloid diseases, in which thousands of copies of a type of protein known as an amyloid adopt an abnormal structure and aggregate in harmful clumps. In Parkinson’s disease, aggregates of the amyloid protein α-synuclein accumulate in neurons. A rarer neurodegenerative disease, multiple system atrophy (MSA), involves α-synuclein aggregates in neuron-supporting cells called glia. It can be difficult to distinguish between the two disorders, given their overlapping symptoms, but they require different treatments. Writing in Nature, Shahnawaz et al.1 provide an explanation for this difference: like two dissimilar snowflakes composed of identical water molecules, α-synuclein aggregates form distinct 3D architectures in each disease.   In vitro and animal experiments have previously indicated that different aggregate structures of α-synuclein, called strains, yield different effects2. The various α-synuclein strains not only can have distinct cell-killing abilities and different seeding and propagation properties, but also can target different cell types and areas of the mammalian brain3,4. Shahnawaz et al. built on these previous findings using a technique called protein misfolding cyclic amplification (PMCA), which amplifies small amounts of α-synuclein aggregate, allowing thorough examination of minuscule samples. An amyloid-specific fluorescent dye is incorporated into the newly formed aggregates, enabling their analysis. Impressively, the authors amplified and analysed samples from the cerebrospinal fluid of more than 200 people who had either Parkinson’s disease or MSA, or who were healthy (Fig. 1). They found that samples taken from people with Parkinson’s disease displayed more fluorescence than those from people with MSA. Thus, PMCA could be used to discriminate between Parkinson’s disease and MSA. Figure 1 | Different structures for the α-synuclein protein. Two neurodegenerative disorders, Parkinson’s disease and multiple system atrophy (MSA), involve aggregates of α-synuclein, which are found in neurons and neuron-supporting glial cells, respectively. Shahnawaz et al.1 have demonstrated that α-synuclein adopts different structures in each disease, indicating that the structure of the protein might contribute to the distinct nature of each disorder. The group extracted tiny amounts of α-synuclein from cerebrospinal fluid (CSF) samples. Protein amplification and analyses revealed different structures for the two samples. These analyses were sufficient to discriminate between the diseases in around 95% of the 200 people studied. The different levels of fluorescence suggested that the amyloid dye interacted with each α-synuclein aggregate differently, and that distinct α-synuclein strains are involved in the two diseases. The authors confirmed this result by showing that the two strains could also be distinguished by using proteinase K digestion (an enzymatic treatment that breaks down strains that have different structures in different ways), and through other biophysical characterizations, including a microscopy approach called cryo-electron tomography. Shahnawaz and colleagues’ work has two major implications. First, it demonstrates that PMCA can be used as a diagnostic tool to discriminate between diseases involving α-synuclein. However, it should be noted that the samples analysed in this study were obtained from people who had already been diagnosed, and it remains unclear whether the approach could be used as a predictive tool to detect disease at earlier stages. Moreover, it is possible that PMCA is affected by the medication given to the participants who had Parkinson’s disease. These people typically receive the hormone dopamine (l-dopa), which has been shown to affect α-synuclein aggregation in vitro5. Second, the study adds to a growing body of evidence supporting the ‘one polymorph, one disease’ hypothesis6–8, which states that different structural forms (polymorphs) of the same aggregated protein can cause distinct pathologies and symptoms. What might lead a protein to adopt different structures? In vitro, distinct fold structures can result from distinct environmental conditions. For example, different α-synuclein polymorphs arise depending on whether the protein is kept in a phosphate-containing or phosphate-free buffer9. In vivo, α-synuclein is exposed to several environments. Indeed, the neurons that degenerate in Parkinson’s disease and the glia affected in MSA belong to different cell lineages, and have markedly different intracellular environments. In addition, α-synuclein can move between cells, exposing it to both intra- and extracellular environments2.   The idea of different polymorphs in disease dates back to studies of prion proteins6 in the 1990s. Much like amyloids, prions aggregate in harmful infectious clumps to cause neurodegenerative conditions such as Creutzfeldt–Jakob disease in humans and scrapie in sheep. Several strains of prion, each adopting a different polymorph, typically coexist in a given sample or organism7. The strains have different fitnesses in different environments, which governs their ability to replicate7 — a phenomenon known as the prion cloud10. A corollary of this idea is that if environmental conditions change, the relative abundance of each polymorph might change. This principle also governs the PMCA assay. Under given conditions, the fittest polymorphs should be amplified from a possible mix of pre-existing strains. Indeed, in Shahnawaz and colleagues’ experiments, a single distinct polymorph was amplified from Parkinson’s disease samples and another from MSA samples. By contrast, in another recent study that used PMCA, Strohäker and colleagues11 reported no significant differences between structures of α-synuclein derived from the brains of people who had Parkinson’s disease and those with from people with MSA. A possible explanation for this apparent discrepancy is that the two groups used different PMCA protocols. In addition, Strohäker et al. used a much smaller group of patients than did Shahnawaz and colleagues. In fact, analysis using nuclear magnetic resonance spectroscopy did indicate distinct structural features in a subset of Strohäker and colleagues’ samples. High-resolution cryo-electron microscopy has been used to demonstrate the existence of distinct disease-specific polymorphs of another neurodegeneration-associated protein, tau, at atomic resolution8. A similar approach using samples extracted under mild conditions might give us a clearer picture of the reality for α-synuclein. Taken together with similar observations for Alzheimer’s disease12, our understanding of the structural landscape of amyloid diseases is broadening. </body>
<date id = '289'>05 February 2020</date>
<url id = '290'>https://nature.com/articles/d41586-020-00244-9</url>
<title id = '290'>The structure of thyroglobulin, the enormous protein that acts as a precursor for thyroid hormones, has been determined, and its hormone-forming tyrosine amino-acid residues have been identified.</title>
<body id = '290'>The thyroid hormones thyroxine and triiodothyronine are small molecules that have a big biological impact. They regulate metabolism in almost all cells, and are essential for the development and maturation of the central nervous system, the musculoskeletal system and the lungs. They are also the only hormones that contain iodine and that are synthesized partly inside and partly outside cells. An enormous dimeric glycoprotein called thyroglobulin (each identical monomer of which has a mass of about 330,000 daltons) serves as the thyroid hormones’ precursor, scaffold and reservoir1. Writing in Nature, Coscia et al.2 report the first structure of full-length human thyroglobulin and identify its hormone-forming tyrosine amino-acid residues, thereby filling a crucial gap in our knowledge of the biosynthetic pathway of the thyroid hormones.   The thyroid gland is made up of spherical structures called follicles, which consist of a single layer of follicular cells surrounding a fluid known as the colloid, where thyroglobulin is stored. The complex biosynthesis of thyroid hormones1,3 takes place in the follicles. Thyroglobulin is synthesized in an intracellular organelle of the follicular cells, called the endoplasmic reticulum, where it forms a dimer before being secreted into the colloid. Iodide (I–) in the bloodstream around the follicles is actively taken up by the follicular cells through a cell-membrane protein, the Na+/ I– symporter4, and then transported into the colloid. Here, I– is oxidized to iodine by the thyroperoxidase (TPO) enzyme, using hydrogen peroxide produced by dual oxidase proteins, and then covalently incorporated into tyrosine residues in thyroglobulin in the colloid. This produces biosynthetic intermediates known as 3-monoiodotyrosine (MIT) and 3,5-diiodotyrosine (DIT) bound to thyroglobulin. MIT then reacts with DIT to form triiodothyronine, or two DITs react to produce thyroxine, still bound to thyroglobulin. When levels of thyroid hormones circulating in the blood decrease and levels of thyroid-stimulating hormone (TSH) rise, thyroglobulin is internalized into the follicular cells through a process called endocytosis. Thyroglobulin is then digested in organelles called lysosomes, producing free triiodothyronine and thyroxine, which are finally released into the bloodstream. The ratio of thyroxine to triiodothyronine in humans is about 80:201. MIT and DIT produced during the digestion process as a result of incomplete thyroid-hormone synthesis are metabolized in the follicular cells by an iodotyrosine dehydrogenase enzyme to produce I– and tyrosine, ensuring that any I– not incorporated into hormones is recycled. Coscia et al. set out to determine the structure of human thyroglobulin using cryo-electron microscopy (cryo-EM), to deepen our understanding of thyroid-hormone biogenesis. They purified thyroglobulin from cultured cells that had been engineered to secrete the protein at a high concentration. Using the cryo-EM data, the authors built an atomic model of the protein that contained 93% of its amino-acid residues, and defined five regions in the structure (Fig. 1): the amino-terminal domain (NTD), core, flap, arm and carboxy-terminal domain. The model reveals that the two monomers are intertwined, and that the NTD of each monomer interacts with all five regions of the other monomer. The interface between the monomers is immense (29,350 square ångströms), and each monomer has 60 disulfide bonds (structural motifs that stabilize the 3D structure of proteins). All of these disulfide bonds connect residues in monomers, as previously reported5, rather than between monomers. Figure 1 | Structure of human thyroglobulin. a, Coscia et al.2 report the structure of the dimeric thyroglobulin protein, which is the precursor for thyroid hormones. Each identical monomer contains five domains: the amino-terminal domain (NTD), core, flap, arm and the carboxy-terminal domain (CTD). In this illustration, the darker domains comprise one monomer and the pale domains are in the second monomer, behind the first. b, This diagram shows which amino-acid residues are found in each domain. Coscia and colleagues identified the four hormone-forming sites (A–D) that are conserved across species. Each site corresponds to the position of a tyrosine residue known as the acceptor; tyrosine residues that react with acceptors during hormone biosynthesis are called donors. Here, the labels for acceptors and donors indicate the number of the amino-acid residue, and the site to which it contributes (in parentheses). The acceptor and donor for site C are probably the same residue (tyrosine 2766) from each monomer. Coscia and colleagues identified the four hormonogenic (hormone-forming) sites (A–D) that are known to be conserved across species, from the sea lamprey6 to humans. Each site corresponds to the position of a tyrosine residue known as the acceptor; the tyrosine residues that react with acceptors during hormone biosynthesis are known as donors. At site A, the acceptor is tyrosine 24 (Tyr 24), and a donor (Tyr 149) had previously been discovered7. However, Coscia et al. find that a second residue (Tyr 234) also acts as a donor at site A. At the other sites, the acceptors were known8 but some of the donors were not. The authors report that Tyr 2573 is the acceptor at site B, and Tyr 2540 is the donor; and that at site D, the acceptor is Tyr 1310 and the donor is Tyr 108 of the other monomer. Strikingly, the acceptor and donor for site C are probably the same residue (Tyr 2766) from each monomer — but the resolution of this region of the protein structure is not high enough to be completely certain. When Coscia and co-workers replaced all eight hormonogenic tyrosine residues with a different residue, they could not detect any thyroxine production from the resulting mutant in their in vitro assay. The authors therefore conclude that only these residues are hormonogenic, out of 67 tyrosine residues in each monomer. However, it could be that the lack of hormone was due to other, unidentified sites ceasing to produce thyroxine as a result of conformational changes induced by the tyrosine substitutions.   So, do the eight identified tyrosine residues have anything in common that explains their hormonogenic activity? They are all at least partly exposed to the solvent around thyroglobulin, and the side chains of the donor–acceptor pairs formed by these residues face each other in an approximately antiparallel configuration. These residues are also all in highly mobile regions of the protein — presumably to enable the substantial bond rearrangements that need to take place to generate thyroxine. The authors went on to show that thyroxine can be produced in vitro from a bacterial protein (maltose-binding protein; MBP) that has nothing to do with thyroid-hormone production. They found that either a pair of tyrosine residues found naturally in MBP, or a pair that was specifically introduced to have the same geometric arrangement and flexibility as the hormonogenic residues in thyroglobulin, produced thyroxine in the presence of an I-oxidizing system and a peroxidase enzyme. Lactoperoxidase could be used instead of TPO, which is consistent with the previously reported observation that lactoperoxidase can promote the synthesis of thyroxine from thyroglobulin9. The observation that thyroxine can be produced using TPO and MPB indicates that the key requirement for generating thyroxine is the production of DIT, rather than the existence of a particular protein scaffold for the hormonogenic residues. For reasons that are unclear, Coscia et al. did not detect the generation of triiodothyronine in any of their in vitro experiments. An earlier study10 reported that triiodothyronine can be produced from thyroglobulin in vitro, and that the main site of hormonogenesis was Tyr 2766. It remains to be seen whether triiodothyronine was not observed in the current study because of the experimental conditions or because of the sensitivity of the assay used. More experiments are needed to understand not only normal triiodothyronine production, but also the mechanism that causes an increase in triiodothyronine biosynthesis in several situations: in Graves’ disease (an autoimmune disease that affects the thyroid); in I– deficiency; in people who have activating mutations of the TSH receptor; and when thyroid cells in culture are stimulated with sera from people with Graves’ disease1. In addition to shedding light on details of the biosynthesis of thyroid hormones, Coscia and colleagues’ determination of the 3D structure of thyroglobulin will probably also lead to a more thorough understanding of the effect of thyroglobulin mutations that cause congenital hypothyroidism — a deficiency of thyroid-hormone biosynthesis. It is a breakthrough as impressively big as the protein itself. </body>
<date id = '290'>05 February 2020</date>
<url id = '291'>https://nature.com/articles/d41586-020-00207-0</url>
<title id = '291'>The non-coding RNA Xist has been shown to enlist the SPEN protein to recruit a team of protein complexes — initiating the process that prevents transcription of one of the two X chromosomes found in female mammalian cells.</title>
<body id = '291'>Female mammals have two X chromosomes, whereas males have only one. A remarkable solution has therefore evolved to prevent a gross imbalance in gene expression occurring between the sexes: in every cell that has two X chromosomes, one entire X chromosome is ‘silenced’ to prevent RNA from being transcribed from it. This process is called X-chromosome inactivation (XCI) and initiates early in the development of female embryos. Once complete, XCI is essentially stable for life1 — thus, by extension, a human X chromosome can be propagated in the silenced state for more than 100 years.   XCI has become a paradigm for epigenetic processes — those in which DNA and associated proteins are modified to alter gene expression — and has been intensively studied for decades. For the past 25 years, much of this research has centred on a long non-coding RNA (lncRNA) called Xist, which is needed to orchestrate XCI. However, the details of Xist’s silencing mechanism have been elusive. Writing in Nature, Dossin et al.2 report a stunning series of experiments that reveal how Xist silences genes by partnering with a protein called SPEN. Xist is expressed exclusively from the X chromosome that will be inactivated, where it spreads locally and silences nearly every gene on the chromosome by associating with an array of proteins. For example, Xist engages the Polycomb protein complexes (which modify the histone proteins that package DNA into a condensed form called chromatin) to maintain gene silencing on the inactivated X chromosome3,4. Although this maintenance function is well documented, how Xist silences active genes in the first place has remained a mystery — in part because the majority of Xist’s protein partners were unknown. But in 2015, a series of studies5–9 revealed a comprehensive set of proteins involved in XCI. These screens all identified SPEN as a Xist-binding protein that is essential for XCI. SPEN belongs to an evolutionarily conserved family of RNA-binding proteins that have been implicated in transcriptional silencing and, curiously, RNA processing in both animals and plants10. To interrogate SPEN’s role in XCI, Dossin et al. first used a biological system known as an auxin-inducible degron to rapidly degrade SPEN in mouse embryonic stem cells. Consistent with a 2019 report11, the authors observed that Xist is almost completely unable to silence genes along the X chromosome in the absence of SPEN. In an important first, the authors demonstrated that SPEN is required for successful XCI in vivo in mice. They also found that SPEN was needed to dampen expression of ‘escapees’ — genes on the silenced X chromosome that partially evade XCI. By observing fluorescently labelled molecules in living cells, Dossin et al. found that SPEN is recruited to the X chromosome as soon as Xist expression begins at the onset of XCI. SPEN contains four RNA-binding domains (called RRMs) at its amino-terminal end and an evolutionarily conserved SPOC domain at its carboxy-terminal end. The authors found that, although RRMs 2–4 are required to bind Xist, the SPOC domain is the essential mediator of gene silencing. As suggested by previously reported experiments12, forcing an interaction between Xist and the SPOC domain alone was enough to restore XCI in cells that lack SPEN. It has been proposed7,13 that SPEN confers gene-silencing capabilities on Xist by recruiting and/or locally activating the enzyme HDAC3, which removes gene-activating acetyl groups from histones. However, HDAC3 accounts for only part of the gene silencing that occurs during the early stages of XCI13. To find other mechanisms by which SPEN might bring about silencing, Dossin et al. used a mass spectrometry technique to identify proteins that interact with the SPOC domain.   Confirming earlier work14, the authors found that SPEN’s SPOC domain interacts not only with HDAC3, but also with the associated co-repressor proteins NCOR1 and NCOR2 (also called SMRT), and with components of the nucleosome remodelling and deacetylase (NuRD) complex, all of which are epigenetic silencers. Moreover, the authors observed that the SPOC domain interacts with parts of the machinery used for transcription and splicing (the process by which newly made RNA transcripts are turned into messenger RNA), including RNA polymerase II, the enzyme that catalyses transcription. Dossin and colleagues identified interactions with components of the N6-methyladenosine (m6A) methyltransferase complex, several of which have been linked to XCI6,11,15. Accordingly, SPEN and its array of associated proteins might function like a molecular multi-tool to silence genes in various genomic contexts. Although much of SPEN’s silencing function might derive from its interactions with known epigenetic silencers, its association with transcription and RNA-processing machineries leaves open the possibility that SPEN can also silence genes through another, as-yet-undefined mechanism. Perhaps most strikingly, Dossin et al. adapted a technique called CUT&RUN to map the location of SPEN on an X chromosome that was being inactivated. This revealed that, shortly after Xist starts to be expressed, SPEN associates with active gene promoters and enhancers (DNA regions that initiate and increase the likelihood of transcription, respectively), but then disengages from these sites after it has silenced transcription. These discoveries imply that SPEN is part of a system that recruits silencing machinery specifically to transcriptionally active regulatory elements at the onset of XCI (Fig. 1). Whether this mechanism also requires chromatin modifications, RNA polymerase II, actively transcribed RNA or other factors should be addressed in the future. Another issue that should be investigated is why Xist isn’t silenced by SPEN, given that a large amount of SPEN accumulates over the Xist gene. Figure 1 | Mechanism of gene silencing by SPEN. The long non-coding RNA Xist and its protein cofactor, SPEN, suppress (silence) gene expression in one of the two X chromosomes found in female mammalian cells. This is an essential process that prevents a gross imbalance in gene expression between males and females. Dossin and colleagues’ experiments2 suggest that SPEN initiates this silencing mechanism by binding to active gene promoters (DNA sequences that initiate transcription) and enhancers (sequences that increase the likelihood of transcription). SPEN recognizes active promoters in part by interacting with constituents of the machinery used for gene transcription, including RNA polymerase II (Pol II, the enzyme that catalyses transcription). SPEN also recruits and/or locally activates the gene-inactivating protein HDAC3, and gene-silencing protein complexes such as the nucleosome remodelling and deacetylase (NuRD) complex. Once a gene has been silenced, SPEN disengages from its binding site, possibly displacing Pol II in the process. SPEN binds to a region of Xist RNA called Repeat A, which is required to initiate gene silencing5,8,16. Because deleting the Spen gene largely mirrors the effects of deleting Repeat A11, SPEN seems to be responsible for most of Repeat A’s silencing ability. However, Repeat A also binds to other proteins, including those that normally promote splicing, as well as to RBM15 and RBM15B, SPEN’s SPOC-domain-containing cousins5,15,17. Therefore, it is now crucial to determine how these proteins might compete or cooperate with SPEN to initiate gene silencing. Moreover, deletion of Repeat A drastically reduces levels of the Xist RNA itself18, and, in certain contexts, deletion of SPEN similarly reduces levels of Xist11. How Repeat A is required for the production of Xist, and how its role in Xist production relates to its ability to initiate silencing, are key questions for the future. For decades, Xist has served as a leading example of RNA’s role in regulating gene expression. Most notably, Xist was one of the first mammalian RNAs shown to be involved in Polycomb-mediated silencing3,4. It therefore seems appropriate that, by studying this RNA, Dossin et al. might have uncovered a new and fundamental aspect of gene regulation — the transient recruitment of SPEN to regulatory elements by RNAs, or even by proteins, which could be a general mechanism for silencing transcription throughout the mammalian genome. </body>
<date id = '291'>05 February 2020</date>
<url id = '292'>https://nature.com/articles/d41586-020-00212-3</url>
<title id = '292'>Particle colliders that use elementary particles called muons could outperform conventional colliders, while requiring much smaller facilities. Muon cooling, a milestone on the road to these muon colliders, has now been achieved.</title>
<body id = '292'>“SMASH! Colossal colliders are unlocking the secrets of the universe.” The cover story of the 16 April 1990 issue of Time magazine discussed giant particle accelerators, including the Superconducting Super Collider in Texas, which was ultimately judged to be too expensive for completion. Researchers at CERN, Europe’s particle-physics laboratory near Geneva, Switzerland, went on to construct the Large Hadron Collider (LHC) in an existing tunnel. The LHC and other accelerators have been responsible for many major discoveries, but these “colossal colliders” have become colossally costly. Innovative approaches will thus be required to reduce the expense of future colliders in the search for previously unseen particles and physics phenomena. In a paper in Nature, the Muon Ionization Cooling Experiment (MICE) collaboration1 reports results that bring scientists a step closer to realizing one of these innovative approaches: a muon collider.   Muons, like electrons, are elementary particles in the standard model of particle physics, but they have about 200 times the mass of electrons (go.nature.com/3twyjba). This fact has ramifications for the size, and therefore cost, of colliders, and for the energy that can be reached in their particle collisions (and thus their potential for discovery). Although the goal is to accelerate particles so that they collide at the highest possible energies, the particles actually lose energy through radiation when their trajectories are bent by accelerator magnets. Heavy particles such as protons and muons lose much less energy than do lightweight particles such as electrons. For this reason, the circular colliders that can reach the highest energies (for example, the LHC) use protons. However, protons are not elementary particles. They are made up of elementary particles called quarks, and because the collisions are between bound quarks, only about one-sixth to one-tenth of the energy from proton collisions is available to produce other particles2. By contrast, because muons are elementary particles, all of the energy from their collisions is available for particle production. Muon accelerators would have uses beyond those for particle colliders. For example, a ‘Higgs factory’ is a highly desirable facility that would produce huge numbers of elementary particles known as Higgs bosons and allow the properties of these particles to be precisely determined. A Higgs factory based on a conventional linear accelerator that collides electrons and positrons (the antiparticles of electrons) would have to be 10–20 kilometres long3. But one based on a circular muon collider would require a circumference of only about 0.3 km4. In another example, if muons could be stored in a racetrack configuration that has long, straight sections, the decay of the muons in these sections would produce intense neutrino beams. Such a facility, called a neutrino factory, would shed light on the mysteries of neutrinos and on physics beyond the standard model. Before a neutrino factory or a muon collider can exist, scientists must learn how to manipulate muon beams. Unlike electron beams, which are produced with almost laser-like quality, muon beams are generated through a complicated process resulting in a beam that is more reminiscent of the spray of pellets from a shotgun. This spray needs to be converted into a laser-like beam.   Such a conversion involves reducing the spread of the muons’ positions and velocities in the directions perpendicular to the beam. A temperature can be associated with this spread, and cooling the beam decreases the spread. Several cooling techniques are used at accelerators, but none is fast enough to cool muons, which are unstable and short-lived. Instead, a method called ionization cooling has been proposed for cooling muon beams5,6, although it has never been used. In this approach, muons travel through an accelerator, a portion of which contains a material of low atomic mass, and the spread of the muons’ positions and velocities is reduced as the particles ionize atomic electrons in the material. The MICE collaboration’s aim was to build and test a system for the ionization cooling of muons, to demonstrate this cooling for the first time and to validate simulation tools for the design of ionization-cooling systems. In the authors’ experiment, a proton beam from the ISIS accelerator at the Rutherford Appleton Laboratory near Didcot, UK, struck a target to produce secondary particles (Fig. 1). Some of these particles decayed into muons, which were directed into an experimental apparatus consisting of focusing magnets, beam instrumentation and a cooling section that contained an energy-absorbing medium made of lithium hydride or liquid hydrogen. Figure 1 | Production and ionization cooling of muons. The MICE collaboration1 carried out an experiment in which a beam of protons was directed at a target to generate secondary particles. Some of these particles decayed into elementary particles known as muons. The positions and velocities of the muons in the resulting beam had a wide spread (indicated by the dashed lines) in the directions perpendicular to the beam. Finally, the muons passed through an energy-absorbing medium made of lithium hydride or liquid hydrogen that reduced this spread by a process called ionization cooling. The process demonstrated by the authors could someday lead to a muon-based particle accelerator. Accelerator experiments usually measure the basic properties of a beam, such as its centre of mass, its spread in particle positions or its density profile. To demonstrate ionization cooling, the MICE collaboration took the unprecedented step of using the technology of collider detectors to measure both the input and output coordinates and velocities of every individual muon that passed through the experimental apparatus. As a result, the authors could unequivocally demonstrate that they had achieved ionization cooling of muons. Organizations worldwide are developing long-term strategies for exploring the high-energy frontier. Plans include designs for circular colliders up to 100 km in circumference and linear colliders up to 50 km long7. Although these approaches, which would use protons or electrons and positrons, have the least technical risk, they still have a substantial cost, as well as technical challenges, that affect their feasibility. Other plans include designs that would use innovative technologies such as those based on lasers and plasmas8. These approaches have made great progress in developing compact accelerator stages at low energy, but the combined use of such stages to reach high energies while retaining a high beam quality will require many years of research and development. Still other plans involve muon beams9. Thanks to the MICE collaboration, the first demonstration of ionization cooling of muons has been achieved. However, it must be noted that the amount of cooling was small. Although conceptual designs for muon colliders have been developed9, establishing the viability of a realistic muon-cooling system and of a muon collider will need much more work. It is too soon to say which, if any, of the proposed approaches will provide a technically and financially feasible path to the future energy frontier. But if physicists can learn how to cool and control muon beams, then it is hard to imagine that putting muons in a circular collider will not be the way forward. These particles offer clean collisions (unlike protons) and lose little energy when their trajectories are bent by accelerator magnets (unlike electrons). As a result, a muon collider could reach energies that match or surpass those of an electron or proton collider, but be substantially smaller. The MICE collaboration’s work is a milestone on the road to realistic muon-cooling systems that could someday lead to neutrino factories and muon colliders. </body>
<date id = '292'>05 February 2020</date>
<url id = '293'>https://nature.com/articles/d41586-020-00206-1</url>
<title id = '293'>Crystalline films of technologically useful oxide materials have been grown by a method based on surface-modified substrates. Unlike usual oxide films, these can be easily transferred to any material.</title>
<body id = '293'>Inorganic compounds that contain oxygen and at least two other elements are known as complex oxides. Crystalline films of these compounds have desirable properties such as superconductivity, magnetism and ferroelectricity (spontaneous electric polarization), and could be used in next-generation devices1–3 if they can be integrated with mature device technologies. Integration is typically achieved by growing films on compatible substrates using a method called epitaxy, but this approach works only for relatively limited material systems. In the past few years, free-standing membranes of certain oxides have been made4,5 by removing epitaxial films from substrates using a process dubbed chemical lift-off. Now, writing in Nature, Kum et al.6 report a versatile method for producing a wide variety of complex-oxide films that can be easily transferred to any material. The authors’ approach uses a technique known as remote epitaxy7–9, in which the epitaxial film and the substrate are separated by a few sheets of the two-dimensional material graphene (Fig. 1a). Potential-energy fields produced by atoms in the substrate can penetrate the graphene and transmit information about the substrate’s crystal lattice, enabling the epitaxial growth of high-quality films. The field penetrability is proportional to the strength of ionic bonds in the substrate material8. A film grown in this way can be easily removed (exfoliated) from the graphene because the two materials are coupled by only weak van der Waals forces (Fig. 1b). Remote epitaxy therefore combines outstanding epitaxial growth and exfoliation. Figure 1 | Growth and integration of complex-oxide films. a, Kum et al.6 report a versatile approach for making high-quality films of technologically useful compounds called complex oxides and transferring these films to other materials. As a demonstration, the authors grew a film of the complex oxide cobalt ferrite using a technique known as remote epitaxy, whereby the film and the underlying substrate are separated by a few sheets of the material graphene. b, They then exfoliated (removed) the film from the graphene to produce a free-standing membrane. c, Finally, the authors integrated the membrane with an electrode and a membrane of another complex oxide, lead magnesium niobate–lead titanate (PMN-PT), which had also been made using remote epitaxy (but replacing graphene with strontium ruthenate). Such integration is difficult to achieve using the conventional scheme for growing oxide films because cobalt ferrite and PMN-PT have different crystal structures. The fabrication of high-quality oxide films requires a well-regulated growth scheme and atomic-level control over the material interfaces and substrate surfaces1–3. In the past few decades, single-crystal oxide substrates of various crystal structures have become commercially available. These include substrates of strontium titanate, aluminium oxide and magnesium aluminate, which have perovskite, corundum and spinel crystal structures, respectively. For the epitaxial growth of a particular film, the substrate should be appropriately selected in terms of its crystal structure, lattice dimensions and coefficient of thermal expansion — a quantity that describes how the size of a material is affected by a change in temperature. Consequently, growth conditions, such as temperature, oxygen pressure and growth rate, need to be optimized to stabilize the desired crystalline phase and obtain high crystallinity.   In their remote-epitaxy work, Kum and colleagues carefully optimized the growth conditions of oxide films on graphene-coated substrates. In general, control over the degree of oxidation is crucial for making high-quality oxide films, so the background oxygen pressure should be well regulated during film growth. However, when the authors used a growth method called pulsed-laser deposition and supplied oxygen to their set-up at the required high temperature, they found that the graphene was etched from the substrate. To prevent this etching, they grew the initial part of the oxide film (a thickness of about 5–10 nanometres, compared with a final thickness of the order of 100 nm) in a vacuum. The crystallinity of this part was still high owing to oxidation of the film during the growth of the remaining part. Finally, the authors exfoliated the oxide film from the graphene to produce a free-standing oxide membrane. In other experiments, Kum et al. found that strontium ruthenate could be used instead of graphene to grow an oxide film by a process known as sputtering. The film could then be exfoliated from the strontium ruthenate by depositing a layer of nickel on top of the film. The nickel acts as a stressor — it provides enough strain energy to overcome the weak bonds between the film and the strontium ruthenate. Kum and co-workers demonstrated the transferral of oxide membranes to other materials for: strontium titanate, yttrium iron garnet and magnetic cobalt ferrite, produced by pulsed-laser deposition; lead magnesium niobate–lead titanate (PMN-PT), formed by sputtering; and ferroelectric barium titanate, made by a process called molecular-beam epitaxy. One example of a stacked structure produced by such transferral consists of a 300-nm-thick layer of cobalt ferrite, an electrode and a 500-nm-thick layer of PMN-PT (Fig. 1c).   The authors found that this structure displays high magnetostriction (coupling between magnetic and mechanical behaviour) and piezoelectricity (coupling between electric and mechanical behaviour), because it is free-standing rather than being clamped by a substrate. Cobalt ferrite, PMN-PT and yttrium iron garnet have different crystal structures, making it difficult to stack these materials by the usual growth scheme without such clamping. Kum et al. also stacked graphene and oxide membranes to examine the electrical coupling between these materials. The density of electric charge in graphene can be inferred from the positions of peaks in Raman spectra — spectra generated through the scattering of incident light. The authors found that these positions depend on the stacked structure, indicating that charge is transferred across graphene–membrane interfaces. These results suggest that stacks of other combinations of materials will offer ways to integrate the various functions of oxides with mature device technologies. The authors’ exfoliation technique enables complex-oxide films to be easily transferred from an epitaxial interface to any material. Because the thickness and stacking of films can be controlled, ultrathin membranes and stacks of various membranes could be possible. Such a simple way of transferring the functions of oxides might advance the field of oxide-based electronics through integration with emerging quantum material systems10. However, the availability of graphene-coated substrates could be a key issue for developing the method. This technique will probably be extended beyond the transferral of complex-oxide films. For example, it might provide an innovative strategy for engineering interfaces, by allowing 2D or 3D films and membranes to be integrated with each other through effects associated with multiple couplings between them. An understanding of the chemical or physical bonds at the interface between membranes in stacked structures is crucial and will reveal how such an interface differs from the epitaxial one. Finally, unusual material combinations (in which, for example, the size and orientation of crystal lattices of membranes are mismatched) could enable useful interface functions that are difficult to achieve or control at the epitaxial interface. </body>
<date id = '293'>05 February 2020</date>
<url id = '294'>https://nature.com/articles/d41586-020-00214-1</url>
<title id = '294'>How Nature reported the first attempt to fly across the whole of Africa in 1920, and the heat and perspiration produced by cows, in 1970.</title>
<body id = '294'> A cow is a cow is a one-bar fire and a desalination machine; according to a handout from the Ministry of Agriculture, twenty cows give off the equivalent of 20 kilowatts of heat an hour, which equals the output of 20 one-bar electric fires. In the same interval the twenty bovids perspire 10 pints of moisture between them. Why the ministry does its sums in scores of cows instead of units is not clear unless it hopes thereby to woo farmers gently to decimalization. From Nature 7 February 1970 With the assistance of the Air Ministry … Lord Northcliffe has been able to arrange, on behalf of the Times, for an attempted flight from Cairo to Cape Town, a distance of more than five thousand miles. This journey from one end of the continent of Africa to the other, and traversing country the nature of a large part of which is little known, is of particular interest to the scientific world in view of the fact that Dr. P. Chalmers Mitchell, secretary of the Zoological Society of London, is taking part in it as passenger and observer. The enterprise will thus not only test the practicability of the air route from Cairo to the Cape, but also doubtless lead to valuable scientific observations being made during the flight. The aeroplane left England on January 24 and arrived in Cairo on February 3. The machine is a Vickers-Vimy commercial aeroplane similar to those used for the flights across the Atlantic and to Australia, and it carries a crew of four in addition to the passenger. Dr. Chalmers Mitchell is carrying an autograph letter from the King to Lord Buxton, Governor-General of South Africa, and we hope that he will be able to deliver it in twelve days or so after a successful end to what is a pioneer effort in scientific exploration from the air. From Nature 5 February 1920 Latest on: Agriculture Comment 23 APR 20 Editorial 11 MAR 20 Correspondence 25 FEB 20 Geography Research Highlight 04 MAY 20 News & Views 21 JAN 20 News & Views 14 JAN 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '294'>04 February 2020</date>
<url id = '295'>https://nature.com/articles/d41586-020-00243-w</url>
<title id = '295'>Climate models published between 1970 and 2007 provided accurate forecasts of subsequently observed global surface warming. This finding shows the value of using global observations to vet climate models as the planet warms.</title>
<body id = '295'>Climate models are equations that describe climatically relevant processes and are solved on supercomputers. In addition to being invaluable tools for testing scientific hypotheses, these models have long provided societally important forecasts. The first climate models to numerically describe an evolving and interacting atmosphere, ocean and land surface on a grid covering the entire Earth date back to the 1970s (for example, refs 1–3). Since then, the planet’s surface has warmed, in large part because of increased emissions of greenhouse gases. Writing in Geophysical Research Letters, Hausfather et al.4 retrospectively assessed the forecasting skill of climate models published between 1970 and 2007. Their results show that the physics in these early models was accurate in predicting subsequently observed global surface warming. A key point emphasized by the authors is that the forecasting ability of climate models is limited by unknowable future climate drivers. Many major drivers, such as increased concentrations of carbon dioxide in the atmosphere caused by the burning of fossil fuels, result from human activities and decisions. Early climate modellers included estimates for future climate drivers in their forecasts. However, they could not know, for example, how the world would industrialize or the associated emissions of CO2 that would result.   Hausfather and colleagues developed a method for evaluating the forecasts of early climate models without penalizing the models for their inaccurate estimates of unknowable future climate drivers. The authors examined 17 projections of global mean surface temperature (GMST) from 14 models. Before applying their method, they found that 10 projections were consistent with observations. But when inaccuracies in the estimates of climate drivers were taken into account, the authors discovered that 14 projections agreed with the data. Of the three that did not, two predicted higher-than-observed surface warming and one predicted lower-than-observed warming. Developing credible climate models through an understanding of climatically relevant processes, observations and well-formulated equations is a considerable scientific and computational challenge. The equations that describe climate are complex and require substantial computing power to solve. As a result, climate models have always been run on the fastest supercomputers available. It is especially impressive that the earliest models assessed by Hausfather et al. produced accurate GMST forecasts, given the extremely limited computing power available then compared with that used today (Fig. 1). Figure 1 | A Univac 1108 computer, from 1972. Hausfather et al.4 demonstrate that climate models published over the past five decades accurately predicted subsequently observed changes in Earth’s global mean surface temperature. These models include ones reported in the 1970s that used supercomputers, such as the Univac 1108, that had extremely limited power relative to those used today.Credit: CSU Archives/Everett Collection/Alamy Although the authors’ findings show that climate models can accurately predict GMST, these forecasts are insufficient for understanding and preparing for the effects of ongoing climate change. For instance, regional climate change is especially subject to unpredictable climate variability, which greatly limits forecasting potential — even on decadal timescales when the climate drivers are known5. Moreover, on the basis of GMST forecasts alone, it is hard to predict, for example: to what extent sea level will rise; how ocean acidification caused by uptake of atmospheric CO2 will influence marine ecosystems; and the frequency and magnitude of future fires, droughts and floods. Scientists will have to continue to improve climate modelling and to increase their understanding of the effects of climate change, while keeping in mind the tension between the need for increased model resolution, greater representation of climatically relevant processes, and more simulations to characterize unpredictable climate variability. The successful forecasting of GMST by early climate models is impressive, but leaves much work to be done — as scientists, policymakers and stakeholders are all well aware.   Numerical models based on scientific equations describing the atmosphere are used daily to make decisions that save lives and money. As the climate continues to change owing largely to human activities, scientists need to use, improve and communicate the value of numerical models and the equations and knowledge that underlie them. Hausfather and colleagues’ work demonstrates that the physics in climate models has been providing accurate forecasts of GMST under increasing amounts of atmospheric CO2 for decades. Such predictions are useful for estimating the maximum amount of CO2 that can be released into the atmosphere over time to keep surface warming to a specified level. Crucially, the authors’ results also show that a major source of uncertainty in GMST forecasts comes from climate drivers. And, of these drivers, it is emissions of greenhouse gases from human activity that will largely determine future surface warming. The findings indicate the usefulness of climate-model predictions of GMST in response to increasing greenhouse-gas emissions, despite unknowable future climate drivers. But scientists must also continue to develop climate models in concert with everything else available to them, to plan for a changed climate that requires much more than forecasts of surface warming. </body>
<date id = '295'>03 February 2020</date>
<url id = '296'>https://nature.com/articles/d41586-020-00148-8</url>
<title id = '296'>Unlike many sugar-transporting proteins, a transporter in one species of malaria parasite can import several types of sugar equally effectively, aiding the parasite’s survival. The structure of this protein reveals the reason for its versatility.</title>
<body id = '296'>Most cases of malaria are caused by the protozoan parasite Plasmodium falciparum1. Given that there are more than 400,000 malaria-associated deaths annually, and that P. falciparum is constantly evolving to resist pharmacological therapies, opportunities for developing drugs that target this organism must be continuously explored. A protein called the P. falciparum hexose transporter 1 (PfHT1) has a proclivity for scavenging sugars from an infected host’s red blood cells to improve the parasite’s chances of survival in these cells, and is therefore a drug target. Writing in Nature, Qureshi et al.2 describe the 3D structure of PfHT1, and identify a mechanism that couples the docking of a sugar in the PfHT1 binding site to the process by which sugars are gated through the protein. This coupling facilitates the protein’s substrate promiscuity — that is, its ability to transport a wide range of sugar molecules effectively, a feature that gives the parasite a distinct survival advantage. Transporter proteins shuttle substrate molecules across the otherwise impermeable lipid bilayer of the cell membrane. The functional and dynamic properties of these membrane-embedded proteins are fundamentally related to their 3D structures, which are modulated at the atomic level over a broad range of timescales. Membrane transporters use the alternating-access mechanism for gating3, in which access to the substrate-binding site switches from one side of the membrane to the other (Fig. 1). Figure 1 | The alternating-access mechanism. Transporter proteins facilitate the passage of substrate molecules across cell membranes. Access to the substrate-binding site in the middle of transporters is controlled by two gates (red). a, In the outward open state, a pathway from the cell exterior allows substrates into the protein. b, In the outward occluded state, a substrate is trapped between the gates, but the outward-facing pathway is still present. c, In the fully occluded state with a bound substrate, no pathways are available. d, In the inward occluded state, a pathway to the cytoplasm has formed, but the gate remains closed. e, In the inward open state, substrates can exit to the cytoplasm. Qureshi et al.2 report the structure of PfHT1, a sugar transporter from the malaria parasite Plasmodium falciparum. They find that the binding of a sugar substrate to the structure shown in a is coupled to the gating mechanism, and that the transition from a to c occurs much faster than in other sugar transporters. This explains why PfHT1 transports a wide range of sugar molecules equally effectively, unlike other sugar transporters. The development of methods for determining the structures of membrane proteins in the past few years has produced near-complete pictures of the translocation mechanisms of several classes of transporter — that is, the global rearrangements that the proteins undergo during translocation cycles of substrate binding, transport and release have been visualized at atomic resolution. Intuitively, the substrate specificity of transporters has generally been found to depend on the amino-acid residues at the binding site. The structure of PfHT1 now implies that another mechanism affecting substrate specificity might be at play. Red blood cells infected by P. falciparum consume about 100 times more glucose than do non-infected cells4 because the parasite continuously metabolizes sugars from these cells to support its growth and replication. Because PfHT1 is responsible for transporting sugars from host cells, it has a crucial role in supporting this metabolism. It belongs to the well-studied major facilitator superfamily (MFS) of transporters, which promote the diffusion of substrates across the cellular membrane. It has the same overall 3D structure as the distantly related human GLUT transporters5. But whereas these specialize in the transport of either d-glucose or d-fructose, PfHT1 transports both of these sugars, and some others, with comparable efficiency.   Qureshi et al. resolved the 3D structure of PfHT1 in which d-glucose is captured in the sugar-binding site, and found that the protein was in a fully occluded conformation — that is, the transporter protein completely shielded the sugar from the aqueous environments on either side of the cell membrane. The structure therefore provides a snapshot of the substrate during a part of the translocation cycle that had not previously been visualized for an MFS transporter. Armed with their structure, the authors carried out extensive transport studies to try to work out why PfHT1 has less substrate selectivity than its human GLUT counterparts. They first demonstrated that the same set of amino-acid residues in PfHT1 is required to bind d-glucose and d-fructose. They then replaced residues in and around the sugar-binding site of PfHT1 by residues found in GLUT transporters, but none of these mutations conferred GLUT-like selectivity on the resulting proteins. They thus concluded that the unusual lack of selectivity of PfHT1 cannot be explained on the basis of the sugar-binding residues alone. So how can the substrate promiscuity of PfHT1 be explained? It has been known since the first structures of MFS transporters were reported6,7 in 2003 that bundles of α-helices in the proteins ‘rock’ around the central substrate-binding site, thereby establishing the alternating pathways for substrates through the protein: an outward-facing pathway, which allows substrates into the transporter from the cell exterior, and an inward-facing pathway that allows substrates to enter the cytoplasm (Fig. 1). By considering their structure of the fully occluded state of PfHT1 alongside structures of other sugar transporters captured at different stages in the translocation of d-glucose8–13, Qureshi et al. were able to describe a complete translocation cycle.   The authors found that, surprisingly, all of the sugar-binding residues maintain their orientations throughout the cycle. This implies that the switches from the outward-facing conformation of PfHT1 to the fully occluded state, and then to the inward-facing conformation, are not driven by structural rearrangements at the sugar-binding site. Instead, they are driven by a previously unknown mechanism. Qureshi and co-workers’ analysis of the gating mechanism of PfHT1 revealed interactions involving hydrophilic amino-acid residues in two transmembrane α-helices in the occluded state. By contrast, in human GLUT proteins, the equivalent residues are larger and more hydrophobic. Experiments in which the authors substituted these gating residues in PfHT1 with other residues demonstrated that they are crucial for sugar transport. Notably, the gating residues are about 15 ångströms away from the sugar-binding site — a large distance. This indicates that the binding of a sugar is coupled to remote conformational changes associated with gating of the transporter, a type of mechanism known as allosteric coupling. Thus, the ability of PfHT1, unlike its human counterparts, to transport many similar substrates results from its substrate-driven gating dynamics, which allows it to adopt the occluded conformation more easily and rapidly. The authors also carried out experiments to investigate how PfHT1 is inhibited by two small-molecule antimalarial drugs (C3361 and MMV009085). This allowed them to identify a hydrophobic pocket in the transporter that probably facilitates the binding of inhibitory drug molecules, and that might help to guide the design of new antimalarial compounds. However, the most exciting finding is the allosteric coupling between substrate binding and gating — it suggests that substrate recognition in transporters can be a consequence of the transporter’s conformational dynamics, rather than being the result of protein–substrate interactions, which underpin the conventional ‘lock and key’ model of how molecules interact with their biological targets. </body>
<date id = '296'>29 January 2020</date>
<url id = '297'>https://nature.com/articles/d41586-020-00165-7</url>
<title id = '297'>Healthy cells in smokers’ lungs have a high burden of mutations, similar to the mutational profile of lung cancer. Surprisingly, ex-smokers’ lungs have a large fraction of healthy cells with nearly normal profiles.</title>
<body id = '297'>According to the World Health Organization, there are 1.1 billion smokers worldwide and an estimated 1.8 million deaths from lung cancer annually. Lung cancer caused by smoking can take decades to arise, and smokers have up to a 30-fold higher risk of developing the disease than do non-smokers1. Carcinogenic components of tobacco smoke promote lung cancer by causing DNA damage that can lead to mutations through known mechanisms, but what the initial consequences of smoking are for healthy lung cells is poorly understood. Writing in Nature, Yoshida et al.2 report the mutational profiles of 632 healthy lung cells obtained from whole-genome sequencing of biopsied tissue from 16 individuals: children, adults, non-smokers, current smokers and ex-smokers. The authors analysed the frequency and properties of the mutations present, how they differed according to age and smoking status, and how these mutations related to those found in a type of lung cancer called squamous-cell carcinoma. The authors dissociated cells from lung tissue (Fig. 1) and isolated a type of epithelial cell called a basal cell (which can self-renew). Growing single cells into cellular colonies allowed the authors to determine the DNA sequence of the given original cell. A potential caveat of the study is that, although the authors obtained the genome sequences of hundreds of single cells, the number of individuals with each different smoking status was relatively small. The authors report that the number of single nucleotide (point) mutations increased with age — for each extra year of life, about 22 additional such mutations were found per cell. Figure 1 | Mutational burdens in normal human lung cells. Yoshida et al.2 analysed the pattern of mutations in healthy lung tissue in non-smokers, current smokers and ex-smokers. a, Using biopsied lung tissue, the authors determined whole-genome sequences corresponding to single cells. b, The cells of the non-smoking individuals had few mutations. By contrast, current smokers had a high proportion of cells with a large number of mutations (grey; darker colour indicates more mutations), and many of these mutations were of a type predominantly found in smokers. Compared with non-smokers, smokers also had greater variability in the mutational load between the different cells of a given individual. Surprisingly, the authors found that five out of six ex-smokers had a substantial fraction (20–50%) of cells that had low numbers of mutations and had hardly any smoking-associated mutational signatures. How these cells arise is a mystery — Yoshida et al. speculate that they are generated from a population of as-yet-unknown stem cells. However, being a former smoker added another 2,330, and being a current smoker added 5,300 point mutations per cell on average, confirming the mutational potency of smoking. Smokers’ genomes also had extensive examples of other types of alteration, such as insertion or deletion mutations. The number of mutations in different cells from the same individual could vary by tenfold in smokers, a much higher variability than was found in non-smokers. The stage of the cell cycle at which a cell is exposed to carcinogenic agents might affect how effectively DNA damage is repaired before DNA replication, which could offer an explanation for this high variability.   Yoshida and colleagues examined the mutations in individual cells using previously developed algorithms to focus on all the types of sequence alteration possible (for example, mutation of the DNA base adenine to cytosine, guanine or thymine) and also to assess the bases on either side of a mutated base. Such analysis identifies specific patterns (mutational signatures) that have been used before to characterize the genomes of tumour cells3. The authors report that the presence of certain mutational signatures increased with age and did not seem to be affected by smoking. These included a signature attributed to natural processes whereby the loss of an amino group in a modified cytosine (termed 5-methylcytosine) changes the base to a thymine. The most common mutational signature in all the samples was one that is rich in cytosine-to-thymine and thymine-to-cytosine mutations. The presence of this signature increased with age and was more common in people with a history of smoking. The underlying processes driving these mutations are unknown. The most common smoking-dependent signature consisted of guanine-to-thymine mutations, a signature that is characteristic of most smoking-associated lung cancers4–7. Lung cancers have some of the highest mutation frequencies of all tumour types8; however, it is thought that only a small number of tumour-promoting (driver) mutations need to occur in a single cell to kick off malignant growth. Given the high mutational burden and the specific smoking-associated mutational signatures found in smokers’ healthy epithelial cells, Yoshida and colleagues examined whether these mutations affected crucial genes that are relevant for cancer growth. Indeed, they found cells that had acquired mutations in genes, including TP53 and NOTCH1, that are driver mutations in squamous-cell carcinomas. These driver mutations were more common in the lung cells of smokers than in those of non-smokers. Some cells even had as many as three driver mutations. However, we do not know how many of these mutations (and in what combination) are required for human lung cancer to develop. Specific TP53 mutations were found in multiple cells from the same individual, suggesting that these mutations occur early, that cells with the mutation proliferate, or both — similar to what has been observed for sun-exposed healthy human skin9. The higher risk of lung cancer in ex-smokers compared with non-smokers is reflected in their high mutation burden and the signature of smoking-associated mutations in most of their lung cells (similar to the cellular profile of current smokers). Although ex-smokers have a high risk of developing lung cancer, their risk is reduced compared with that of current smokers, and this lowering depends on the length of time of smoking cessation1. Why this is the case has been hard to explain. However, perhaps the most surprising result of Yoshida and colleagues’ work might offer a clue: in 5 out of 6 ex-smokers, 20–50% of the cells had a low mutation burden that was similar to the profile of non-smokers of the same age range (Fig. 1).   These near-normal cells in ex-smokers had a low frequency of smoking-dependent mutational signatures. Moreover, compared with the ex-smokers’ highly mutated cells, these near-normal cells had longer versions of DNA structures called telomeres, which are found at the ends of chromosomes. Telomere length shortens with each cell division; thus, long telomeres suggest that these cells had not undergone many divisions. The authors speculate that these cells might have arisen comparatively recently from divisions of proposed previously dormant (quiescent) stem cells. However, whether such cells exist in human lungs is unknown. DNA damage can generate a mutation during DNA replication. Therefore, if a population of non-dividing stem cells exists in the human lung, even if exposed to carcinogenic agents, perhaps such cells might avoid incurring mutations if DNA damage is eventually repaired in the absence of division. But the lack of knowledge about these proposed long-lived stem cells and information about the longevity of the different cell types in the human lung make it difficult to explain what occurred in these ex-smokers’ cells with few mutations. Why do ex-smokers still have a substantial fraction of highly mutated cells that can proliferate, at least when grown in vitro? Any short-lived cells that were exposed to carcinogens during their proliferation should have vanished many years after the cessation of smoking. This raises the question of whether there are long-lived differentiated cells in the lung that carry a high mutational burden, and whether these cells can resume proliferation, perhaps because of the plasticity (the ability to change cellular identity) of lung cells10. A future challenge will be to understand the cell biology of the mechanisms underlying these observations. Perhaps one day it will be possible to develop ways to boost the population of lung cells with few mutations in ex-smokers. Yoshida and colleagues’ study has broadened our understanding of the effects of tobacco smoke on normal epithelial cells in the human lung. It has shed light on how the protective effect of smoking cessation plays out at the molecular level in human lung tissue and raises many interesting questions worthy of future investigation. </body>
<date id = '297'>29 January 2020</date>
<url id = '298'>https://nature.com/articles/d41586-020-00137-x</url>
<title id = '298'>The growth of a brain tumour can be affected by the activity of its neighbouring neurons. The finding that such tumours send signals that boost connections between these neurons reveals a pathway that drives cancer growth.</title>
<body id = '298'>A type of non-neuronal brain cell called a glial cell can give rise to a lethal cancer called glioblastoma1. Half of the cells in the human brain are glial cells, which normally act to support the function and communication of neurons2. Yet despite decades of research, there are no existing treatments for glioblastoma that substantially increase the survival time of people with such tumours. Writing in Nature, Yu et al.3 report their analysis of the effects on the brain of certain glioblastoma-associated mutations. These insights might open up new strategies for anticancer research.   DNA sequencing of cancers has identified many tumour-associated mutations. However, it is a challenge to determine which of these mutations have a causal role in tumour development and growth, and which have no effect. Moreover, some mutations can be context dependent, such that the same mutation might have differing effects depending on the type of tumour and its microenvironment. It is hard to predict whether different mutations (variants) of the same gene will lead to the same outcome in different tumour types. Yu and colleagues tackled these issues by studying the RTK–RAS–PI3K signalling pathway in glioblastoma. This pathway is altered in 90% of glioblastomas4,5, and mutations in it boost cell division and tumour growth. The authors focused on mutations in the gene encoding the enzyme PIK3CA — a pathway component that is often abnormal in human glioblastomas. The authors generated mouse models of glioblastoma, and mutated genes in the RTK–RAS–PI3K pathway using the gene-editing tool CRISPR–Cas9, which resulted in tumour growth. Yu et al. then engineered animals to express PIK3CA variants that are found in human glioblastomas (Fig. 1). This revealed that many of the tested variants made tumours more aggressive and rapidly lethal. This was the case both for known variants and for others that had not previously been associated with a role in glioblastoma. Figure 1 | Tumour modulation of neuronal connections. Yu et al.3 report their analysis of the brain cancer glioblastoma, which arises from the growth of non-neuronal (glial) cells. Using mouse models, the authors analysed mutations found in people who have glioblastoma. They report that a mutant form of the protein PIK3CA — the C420R variant — is associated with expression of genes in the cancer cell that can regulate the synaptic connections between neurons. One such gene encodes the protein glypican 3 (GPC3), which is secreted by cells and can boost synapse formation (synapses shown in pink). This leads to a rise in synaptic activity, which was associated with tumour progression. Enhanced neuronal activity might explain why seizures are associated with glioblastoma. Yu and colleagues studied whether alterations in the enzymatic activity of PIK3CA variants might explain how they accelerate glioblastoma progression. Surprisingly, the activity of PIK3CA was not always linked to an effect on tumour growth — some variants strongly increased enzyme activity, whereas others had a much milder effect. To investigate other mechanisms that might explain the effect on tumour growth, the authors performed RNA sequencing of tumour cells. Most PIK3CA variants had patterns of altered gene expression that were similar to each other in the animals’ tumours, but different from the pattern in healthy tissue. However, two variants (named C420R and H1047R) had distinctive patterns compared with each other and with the other variants. Tumours containing either of these two variants showed abnormally altered expression of hundreds of genes that regulate synapses (structures that connect neurons). C420R and H1047R were also each associated with alterations in different categories of synapse-regulating gene. Seizures are caused by a rise in neuronal activity and are often an early symptom of glioblastoma6. This effect might be driven by an abnormal increase in the connectivity of excitatory synapses (those that drive neuronal activity) and a decrease in the connectivity of inhibitory synapses (which decrease neuronal activity). Yu and colleagues report that mice with tumours that express C420R or H1047R variants had seizures, whereas a PIK3CA variant termed R88Q that does not alter synapse-related gene expression in tumours did not induce seizures. Compared with tumours expressing the R88Q variant, those expressing C420R or H1047R showed an increase in the number of excitatory synapses and a decrease in the number of inhibitory synapses in tissue surrounding the mouse tumours. This pattern might explain how certain PIK3CA variants drive neuronal excitability and seizures. When the authors engineered mouse brains to express increased amounts of the PIK3CA variants (without expressing other mutations that drive glioblastoma), they found that, in the absence of glioblastoma, H1047R drives seizures whereas C420R and R88Q do not. This indicates that H1047R acts ‘cell autonomously’ to regulate neuronal excitability — meaning that it functions within the cell itself rather than acting on a neighbouring cell. To determine how C420R might drive seizures, the authors expressed PIK3CA variants in a type of glial cell called an astrocyte that enhances the formation of synapses between neurons by releasing secreted molecules. The authors cultured astrocytes in vitro with neurons and assessed the astrocytes’ ability to induce neuronal synapse formation. Astrocytes that expressed C420R induced higher than normal synaptic formation between neurons, whereas astrocytes that expressed H1047R had no such effect. This suggests that C420R changes the properties of glioblastoma cells to make them induce the formation of synapses between the neurons that they contact.   Further analysis of RNA sequencing data by the authors revealed that tumour cells that expressed C420R showed increased expression of known synapse-regulating factors that are secreted by astrocytes. These included members of the glypican family (part of a group of sugar-containing proteins called proteoglycans), which can induce the formation of excitatory synapses in the brain7. The expression of one member of this family, glypican 3, was particularly highly upregulated in tumour cells that expressed C420R. When the authors engineered glioblastomas expressing C420R to lack glypican 3, this caused a decrease in seizures and a longer lifespan compared with mice that had C420R-expressing glioblastomas that expressed glypican 3. By contrast, animals with glioblastomas engineered to have higher expression of glypican 3 had more seizures and a decreased lifespan compared with animals with glioblastomas that did not overexpress glypican 3. This suggests that glypican 3 is necessary and sufficient in the context of glioblastoma to regulate seizures by controlling synapse formation in the neuronal tissue around the tumour. Yu and colleagues’ work supports growing evidence that tumour cells interact with their neighbouring healthy cells to alter brain function. Recent work has revealed that cells of the glioblastoma itself can form a synapse with surrounding neurons (in such synapses, the tumour has postsynaptic structures and the neuron forms presynaptic structures), and that blocking this synaptic input to the glioblastoma decreases tumour growth8–10. Yu et al. now reveal how a glioblastoma can remodel connections between its neighbouring neurons, and report that the underlying mechanism differs depending on the specific PIK3CA variant involved. It will be interesting to determine in future studies whether the same synapse-promoting signals are responsible for regulating the synapses that form between a tumour and its neighbouring neurons, and for regulating synapses that form between neurons surrounding the tumour. This is an intriguing matter, because glypicans regulate neuronal synapse formation through what are known as AMPA receptors, and the synapses that form between neurons and glioblastoma cells use AMPA receptors for signalling8,9. Abnormally high expression of glypican family members occurs in various types of tumour, including liver and pancreatic cancer11,12. In those cases, glypicans are thought to bind to and regulate the signalling of growth factors that promote tumour growth. Glypicans are clearly of interest in trying to understand many disorders, and efforts being made to block their function in other cancers might suggest approaches worth testing for use in glioblastoma treatments11,12. </body>
<date id = '298'>29 January 2020</date>
<url id = '299'>https://nature.com/articles/d41586-020-00149-7</url>
<title id = '299'>An optical study of cold solid hydrogen at extreme pressures indicates that electrons in the material are free to move like those in a metal. This suggests that the long-sought metallic phase of hydrogen might have been realized.</title>
<body id = '299'>Hydrogen is the most abundant element in the Universe. Its molecular-gas state is simple, but its solid state has proved to be complex. In 1935, it was predicted that solid hydrogen should behave like an electrical conductor at elevated pressures, owing to its molecules being separated into their atomic constituents1. This prediction heralded a race to prove experimentally that solid hydrogen displays such metallic behaviour under ultrahigh compression. However, although there have been many claims of proof (for example, refs 2–4), these studies have been challenged. Now, writing in Nature, Loubeyre et al.5 report that dense hydrogen shows a discontinuous and reversible change in optical reflectivity at extreme pressure and low temperature that can be attributed to a phase transition into a metallic state. It is common practice to use a device called a diamond anvil cell to achieve ultrahigh compression of a material and to study changes in the material’s physical properties at high density. A diamond anvil cell squeezes a sample, which is confined to a microscopic chamber in a thin metal foil, between two diamond anvils (Fig. 1a). The device operates on a deceptively simple physical concept: pressure is inversely proportional to the area of a surface over which a force is applied. In the present case, this simplicity comes with an inherent drawback: reaching extreme pressures inevitably implies working with tiny sample volumes. Figure 1 | Effect of increasing pressure on cold solid hydrogen. a, Loubeyre et al.5 have studied solid hydrogen at extreme pressure and low temperature using a device known as a diamond anvil cell. This device compresses a sample of the material, which is confined to a microscopic chamber in a thin metal foil, between two diamond anvils. At first when the pressure is applied, the sample is transparent to both infrared and visible light (GPa, gigapascals). b, When the pressure is raised to roughly 300 GPa, the dense hydrogen loses its transparency to visible light. c, Finally, when the pressure is above 425 GPa, the sample becomes reflective to both infrared and visible light, indicating a shift into the long-sought metallic state of hydrogen. Conventional techniques have been the bottleneck in applying extreme pressures to highly compressible materials such as hydrogen. Over the past few decades, research groups around the world have pushed the boundaries of pressure generation. They have also refined the tools and methods needed to accurately estimate pressures applied to a microscopic sample of compressed gas. Nevertheless, debate continues over the accuracy of reported pressures and the interpretation of results drawn from measurements of physical properties. Recognizing this long-standing problem, Loubeyre and colleagues’ research group developed an innovative approach that involves the precise sculpting of diamond-anvil surfaces using a stream of massive ions6 — a technique called focused ion-beam milling. A similar experimental development has also been reported7. The profiled anvils produce extreme pressures that can be reliably estimated, reaching more than 400 gigapascals (about 4 million times Earth’s atmospheric pressure). Moreover, the shape of the anvils helps to confine dense hydrogen samples that are suitable for optical measurements.   Under increasingly extreme pressures, dense hydrogen becomes more and more opaque to visible light. For pressures in excess of about 300 GPa, solid hydrogen becomes penetrable only by electromagnetic radiation of lower energy than visible light2–4,8, such as infrared radiation (Fig. 1b). Loubeyre et al. measured the optical transparency of solid hydrogen at pressures much higher than those reached previously, using the near-to-mid-infrared emission from a source of synchrotron radiation — electromagnetic radiation that is produced when charged particles are accelerated in a curved path. The authors found that a compressed sample of hydrogen blocks all light and exhibits an abrupt increase in optical reflectivity when the pressure is raised above 425 GPa (Fig. 1c). Moreover, they discovered that this transition is reversible. The authors attribute the change in optical reflectivity to a pressure-induced phase transition in which electrons in the sample become free to move like those in a metal. Hydrogen remains as a molecular solid up to the transition pressure; it possibly stays in this state above 425 GPa, but it is difficult to confirm this by spectroscopy because there is a reduced coupling between light and matter in these extreme conditions. It can certainly be argued that a definite proof for metallic hydrogen would come only from a measurement of the sample’s electrical conductivity at high pressure as a function of temperature. Solid hydrogen should exhibit a high level of electrical conduction that should then decrease as the sample temperature is raised. However, even with experimental techniques developed in the past few decades to study condensed matter in extreme conditions, electrical-transport measurements of hydrogen remain a huge challenge9,10.   Nevertheless, Loubeyre and co-workers’ findings should be considered as a close-to-definite proof of dense hydrogen reaching a metallic state in extreme-pressure conditions. Computational predictions of the pressure at which molecular hydrogen enters a metallic state still lack accuracy, because they require many different quantum-mechanical corrections that are difficult to address. However, the experimental value of 425 GPa agrees with calculations11 that predict a transition in hydrogen to a different solid phase at a similar pressure. Loubeyre and colleagues’ study has combined innovative techniques for ultrahigh-pressure generation with advanced experimental methods using synchrotron radiation. In doing so, it has raised expectations for the discovery of other remarkable properties of solid hydrogen at extreme density. For the time being, many questions remain. For instance, could electrical resistivity be measured across the metallic transition? Could superconductivity at a record-high temperature be achieved in hydrogen? And could the molecular order be disrupted under ultrahigh pressure and lead to an atomic phase in the solid state? Competition is still strong between different research groups seeking to answer these questions, and to further unveil and understand the characteristics of hydrogen at extreme density. More exciting findings are sure to come at every stage of the race. </body>
<date id = '299'>29 January 2020</date>
<url id = '300'>https://nature.com/articles/d41586-020-00147-9</url>
<title id = '300'>How Nature reported a controversy in 1970 over the harm caused by fallout from nuclear testing, and a 1920 call to end the trade in exotic bird plumage.</title>
<body id = '300'> If anyone doubted that pollution has now usurped the place of original sin in the public consciousness, he has only to read the diatribes of one of the more colourful of contemporary prophets, Professor Ernest Sternglass. Like Moses calling down the death of the Egyptian firstborn, Professor Sternglass is convinced beyond all reasonable doubt that the fall-out from nuclear testing has caused the death of sometimes 400,000 and sometimes two million children in the United States … People such as Professor Sternglass should perhaps ask themselves exactly what cause they are serving when they simplify a complex issue and present it in emotive terms … It is little wonder that if people of the undoubted ability and conviction of Professor Sternglass go around in girt-up loincloths prophesying fire and brimstone in advance of the basic facts being agreed upon, some of the bystanders will think men of science strange and dangerous. From Nature 31 January 1970 Prof. Duerden’s letter in Nature of January 15 might by its phrasing lead to the supposition that a few persons only are agitating for a novel Bill to prohibit the importation of plumage. The trade has been keenly opposed by all naturalists, not only in Great Britain, but also in the United States, Canada, Australia, and nearly every country in Europe for many years … Prof. Duerden has “grave doubts” whether the “ruthless destruction of birds” for trade can best be prevented by discouraging or prohibiting that trade. It is open to him to suggest a better way. The proposition that birds-of-paradise, lyre-birds, egrets, herons, trogons, orioles, terns, kingfishers, and all the rest of the feather-traders’ victims, from albatross to humming-bird, might be “farmed” after the manner of the flightless ostrich, and plucked or killed for the market “in conformity with the highest humane demands,” may be of interest to aviculturists; it has no practical bearing on the question of to-day. What science and humanity alike demand is immediate action to save the birds of the world from the ruthless and stupendous slaughter on which the trade now lives. From Nature 29 January 1920 Latest on: Environmental sciences News & Views 13 MAY 20 Research Highlight 12 MAY 20 Editorial 12 MAY 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '300'>28 January 2020</date>
<url id = '301'>https://nature.com/articles/d41586-020-00151-z</url>
<title id = '301'>Three methods for gene-expression profiling have now been combined to produce spatially defined single-cell maps of developing human organs from limited sample material, overcoming a major hurdle in studying human development.</title>
<body id = '301'>People are often curious about how their bodies work. So, it is no surprise that single-cell RNA sequencing (scRNA-seq) — which has the power to map all the cell types in the human body1–3 — has drawn great interest from scientists and funding agencies alike. But a major limitation of scRNA-seq is that it cannot provide information about where in the original tissue each cell was located. Writing in Cell, Asp et al.4 demonstrate a way of overcoming this hurdle by combining scRNA-seq with other sequencing methods that retain location information. They use this approach to create a spatially defined cell atlas of the developing human heart. ScRNA-seq involves dissociation of a tissue into hundreds or thousands of individual cells, each of which is analysed to determine its gene-expression profile. This profile indicates the proteins and pathways that are active in that cell — information that computational methods can then use to sort thousands of cells into different types or states at once. However, the tissue-dissociation protocol breaks the link between single cells and their original positions in the tissue. Without this information, interpretation of the data is incomplete. Asp et al. set out to bridge this gap by combining scRNA-seq with two approaches that produce spatially defined gene-expression maps, although at a lower resolution than scRNA-seq. The first is a technique called spatial transcriptomics5. Thin slices of tissue are placed on a specially prepared microscope slide that has been dotted with circular ‘patches’. Each patch contains many copies of a nucleic-acid probe that binds to messenger RNA in the tissue sample and carries a sequence called a barcode. Each patch has a different barcode, so that a specific label is attached to the mRNA in the area of tissue sitting on top of that patch. When the mRNA from each region is sequenced, the barcode acts as a record of the cells’ original locations in the tissue. A limitation of this method is that single-cell analysis is compromised, because gene sequences are pooled from approximately 30 cells in each patch. Nonetheless, a non-biased description of gene-expression profiles at discrete locations is obtained.   The second method is in situ sequencing (ISS), in which the expression of preselected genes is probed directly on a tissue slice on a microscope slide6,7. DNA probes are designed to bind to mRNA transcribed from genes of interest, with each probe carrying a unique barcode. When these probes are introduced into a tissue, they bind to their target mRNAs in the tissue’s cells and — after further processing steps — a fluorescent-imaging-based sequencing method is used to detect the barcodes while they remain in place in the tissue. This technique can provide information about the expression of 50–100 genes in individual cells at high spatial resolution. But the genes must be preselected, which requires some knowledge about which genes will be informative. Asp and colleagues ultimately used ISS to achieve cell-level mapping of heart development. But to determine which genes they should select for ISS, they needed to perform scRNA-seq and spatial transcriptomics (Fig. 1). The authors’ scRNA-seq analysis revealed genes correlating with cell type — genes expressed only in smooth-muscle cells, for instance. And their spatial transcriptomics analysis revealed groups of genes that broadly correlated with certain locations in the heart — genes specifically expressed in cells in a region called the outflow tract, for example. Figure 1 | A framework to map human development. Asp et al.4 combine three methods to produce single-cell maps of the developing human heart that retain spatial information. a, In single-cell RNA sequencing (scRNA-seq), a heart is dissociated into single cells and the RNA of each cell is sequenced to generate a gene-expression profile. A computer algorithm assigns a type to each cell, clustering together those that have similar gene-expression profiles. However, no information is retained about the original position of each cell in the tissue. b, In spatial transcriptomics, slices of heart tissue are placed on microscope slides and many small regions of about 30 cells are sequenced. This provides information about gene expression in specific locations, but not at the single-cell level. c, The authors used the results of these screens to choose a panel of 69 genes that mark specific cell types or locations in the heart. They used a third sequencing approach, in situ sequencing (ISS), to analyse expression of these genes at the single-cell level across slices of human heart tissue. An algorithm assigns each RNA molecule to a cell within the tissue slice, generating spatially resolved maps of cell type. The researchers then created a panel of 69 ISS probes — some corresponding to the location markers identified by spatial transcriptomics, some to the cell-type markers identified by scRNA-seq, and some to genes previously reported to be important for heart development. They combined the data produced by this ISS screen with their scRNA-seq data using a recently developed algorithm8 that assigns each RNA molecule detected by ISS to a specific cell nucleus, and each nucleus to a specific cell type. The final result was the first spatiotemporal cell atlas of the embryonic human heart between 6.5 and 7 weeks of development. Anybody can explore the atlas using a searchable online tool (see go.nature.com/3rj6dtf). Beyond a technical proof-of-principle, the authors also gained insights into heart development. For example, a previous study9 described several distinct, but transcriptionally similar, clusters of fibroblast-like cells — a structural cell type that is not fully understood, but which is known to participate in a pathological tissue-scarring process called fibrosis. Asp et al. demonstrated that these fibroblast subgroups are located in distinct parts of the heart, providing a clue as to how they might function differently.   As another example, Asp and colleagues described a previously unknown human equivalent of a subpopulation of cardiac muscle cells found in mice that expresses high levels of the gene Myoz210. Finally, the researchers performed a cellular analysis of a tissue called the atrioventricular mesenchyme, and identified the time point at which Schwann cells (neuron-associating cells that ensure proper electrical transmission) arise in this tissue. Such knowledge of human heart cells can be used to inform follow-up experiments aimed at defining those cells’ functions. However, limitations in Asp and colleagues’ study highlight that pre-existing knowledge is still required to fully appreciate single-cell data. For instance, some of their cell populations seem to be misidentified. One population is deemed by the authors to be cells that line capillary blood vessels (coronary endothelium). But on the basis of the cells’ marker-gene expression and location, we would suggest that they are instead endocardial cells that line the heart chamber. Furthermore, the authors’ approach could not be used to distinguish between all of the subtypes or substates in particular populations, such as the coronary endothelium, which is a mixed population that lines the arterial, venous and capillary blood vessels leading to and from the heart. However, the authors’ workflow is certain to be refined as advanced techniques are developed for directly profiling the spatial gene expression of tissues11,12. A major strength of Asp and colleagues’ study is that it provides a framework for maximizing the power of scRNA-seq. Anyone who begins experiments to understand human biology faces the challenge of limited sample availability, particularly when assessing embryonic development. The techniques presented here have the crucial advantage of providing a wealth of information from limited tissue. Asp and co-workers’ strategy is likely to benefit efforts such as the Human Cell Atlas and HuBMAP projects, which are both seeking to fully define the human body. </body>
<date id = '301'>27 January 2020</date>
<url id = '302'>https://nature.com/articles/d41586-020-00010-x</url>
<title id = '302'>HIV-1 can evade the immune system by hiding out in a dormant form. Two studies describe interventions that can effectively reactivate the latent virus in animals, potentially rendering it vulnerable to immune-mediated death.</title>
<body id = '302'>‘Shock and kill’ might sound like a military strategy, but in fact it describes the dominant model currently used in the search for a cure for HIV-1 infection. Although antiretroviral therapy (ART) is highly effective at limiting the extent of the infection, the virus can hide out in a ‘latent’ form in immune cells called CD4+ T cells, undergoing little or no transcription and thus remaining undetected by the immune system1,2. When ART is stopped, these viral-reservoir cells can rapidly fuel HIV rebound. The theory behind ‘shock and kill’ involves the use of drugs that reverse this latency and could increase viral gene expression (shock), rendering the viral-reservoir cells vulnerable to elimination (kill) by other cells of the immune system. Writing in Nature, two groups3,4 describe distinct interventions in animal models that cause what seem to be the most robust and reproducible disruptions of viral latency reported so far.   In the first study, Nixon et al.3 focus on a drug called AZD5582, which can activate the transcription factor NF-κB — a major instigator of HIV-1-gene expression. AZD5582 was originally developed to treat cancer, and activates the ‘non-canonical’ NF-κB pathway, which results in an atypical type of NF-κB-driven transcription that is slow but persistent. The authors tested AZD5582 in two animal models: ‘humanized’ mice (which carry human-derived liver, bone-marrow and thymus cells) that were infected with HIV; and rhesus macaques infected with the HIV-related simian immunodeficiency virus (SIV). Both groups of animals were already receiving ART. The authors demonstrated that AZD5582 treatment led to marked increases in the levels of viral RNA in CD4+ T cells in a range of tissues in both species, indicating that transcription of the virus had been activated. This was combined with a substantial rise in virus levels in the blood. AZD5582 is not optimized for use in humans; nonetheless, these results suggest that pharmacological activation of the non-canonical NF-κB pathway could be an attractive way to trigger HIV-1-gene expression as part of a shock-and-kill approach (Fig. 1). Figure 1 | Two approaches to reactivating dormant HIV-1. HIV-1 can integrate into the genome of CD4+ T cells in a latent form — it is not transcribed into messenger RNA and so is not detected by the body’s immune system. Two papers describe ‘shock’ treatments that can reactivate transcription of latent HIV in mice and the related virus SIV in monkeys. Nixon et al.3 used a drug called AZD5582 to activate the non-canonical NF-κB signalling pathway, which stimulates virus transcription. McBrien et al.4 used two interventions — a drug called N-803 to stimulate the protein IL-15, which promotes transcription, and an antibody treatment that depletes immune cells called CD8+ T cells, which seem to have a role in dampening HIV transcription. After these shock treatments have reactivated the virus, interventions that target and kill the virus-carrying CD4+ T cells should help to eliminate the latent viral reservoir. Such treatments remain to be designed. In the second study, McBrien et al.4 used an entirely different, though complementary, approach to disrupting viral latency. Again, the authors used both ART-treated humanized mice infected with HIV-1 and ART-treated, SIV-infected rhesus macaques. They combined two immunological interventions. The first involves antibody-mediated depletion of CD8+ T cells — immune cells previously shown to act in concert with ART to reduce levels of viral transcription5. The second, administered concurrently, involves treatment with a drug called N-803, which strongly activates the signalling molecule interleukin-15 (IL-15), and which has been previously shown6 to activate HIV-1 transcription in vitro. Like Nixon and colleagues, the researchers found that their treatment caused substantial increases in virus levels in the blood, and in viral RNA in cells from various tissues.   At first glance, the combined interventions used by McBrien and colleagues might seem contradictory, because IL-15 is one of the strongest activators of CD8+ T cells7,8. But the synergistic effects of these two interventions raise the provocative possibility that the best strategies for targeting viral-reservoir cells involve a mix of immune interventions — suppressing immune components that seem to have a role in stabilizing viral latency (such as CD8+ T cells) while activating others that can effectively disrupt latency (such as IL-15 signalling). How exactly CD8+ T-cell depletion interacts with IL-15 to reverse HIV-1 latency is unknown. Given the vast array of direct and indirect effects resulting from depletion of CD8+ T cells9, it will not be easy to define the precise molecular mechanisms underlying this synergy. But an understanding of this relationship might reveal downstream proteins that are jointly targeted by these interventions and that could therefore be used to optimize latency reversal in the clinic. In addition to the advances they make, the current studies showcase some of the conceptual and technical challenges intrinsically associated with pharmacological latency reversal. First, the latency-reversing agents (LRAs) evaluated (as well as all other LRAs described so far10) target factors that have crucial roles in modulating host-cell gene transcription, in addition to viral transcription. Their use therefore comes with an intrinsic risk of toxic off-target effects. The toxicity of the LRAs described by McBrien et al. and Nixon et al. seems to be acceptable in animal models, with most showing no clinical side effects. However, much more stringent safety standards must be met in human clinical trials. Mechanisms of viral latency might vary between individual viral-reservoir cells and are likely to be influenced by the position at which the HIV-1 genomes have integrated into the host-cell chromosomes11. It is therefore possible that only subsets of cells will respond to individual LRAs, which typically target one specific mechanism of viral latency. The actual proportion of viral-reservoir cells that responded to the interventions in the two current studies is uncertain, and would be difficult to determine experimentally12. Another uncertainty is how much of the increase in HIV-1 RNA is attributable to CD4+ T cells carrying HIV-1 that can replicate effectively13,14. This is of interest because most viral-reservoir cells harbour HIV-1 genomes that contain lethal sequence defects, probably as a result of errors introduced during reverse transcription of viral RNA, which produces the viral DNA that is integrated into the host genome. These defective viral genomes can often still be transcribed and respond to LRAs, but they cannot cause viral rebound when ART is stopped and so do not represent the main target for shock-and-kill interventions. In addition, it is unclear how disrupting latency might influence the evolutionary dynamics of the reservoir cells — whether, for instance, a shock treatment kills some subsets of CD4+ T cells that are highly susceptible to latency disruption, but confers a selective advantage on other subsets of non-susceptible, difficult-to-reactivate cells. Most importantly, neither of the interventions tested in the current studies led to a change in the expression of markers of viral-reservoir size. A decrease in these markers is the most informative and crucial endpoint parameter for shock-and-kill approaches. The absence of an effect on viral-reservoir size probably reflects the fact that the studies were mainly designed to investigate latency reversal, and lacked dedicated ‘kill’ interventions. Combining ‘shock’ interventions with ‘kill’ components is a key next step. In fact, that they provide a suitable model for evaluating ‘kill’ strategies in the setting of robust and efficient latency reversal might be one of the strengths of the current studies. Finally, the work of Nixon and colleagues and McBrien and colleagues should not distract from the fact that the shock-and-kill strategy currently remains largely a theoretical concept, not a therapeutic reality. Establishing evidence for its ability to reduce viral reservoirs and to deliver real benefits to patients will require much more work. </body>
<date id = '302'>22 January 2020</date>
<url id = '303'>https://nature.com/articles/d41586-020-00096-3</url>
<title id = '303'>The activity of calcium channels in the heart increases during what is called the fight-or-flight response. An investigation into the 50-year-old mystery of how this occurs has captured a previously overlooked suspect.</title>
<body id = '303'>In the Sherlock Holmes tale The Adventure of the Dancing Men, the detective runs a heart-pounding race to try to save his client’s life. The thumping of the sleuth’s heart — a literary example of the ‘fight-or-flight’ effect1 — reflects the changes that occur when the entry of calcium ions into the heart rises2. Writing in Nature, Liu et al.3 provide a solution to the long-standing riddle of how this occurs, through deductions worthy of Sherlock Holmes.   Some aspects of how calcium enters the heart during a fight-or-flight response are known. The process is mediated by the hormone adrenaline acting on β-adrenergic receptors — proteins that reside in the surface membrane of heart cells called cardiomyocytes. Receptor activation leads to an increase in the opening of what is called an L-type voltage-gated calcium channel. This occurs through a mechanism that involves the molecule cyclic AMP (cAMP)4,5 and an enzyme called protein kinase A (PKA) that requires cAMP for its function6. Similar types of PKA-mediated processes are found in other contexts. For example, some neurons use cAMP and PKA to enhance calcium entry through L-type calcium channels7. Exactly how the stimulation of β-adrenergic receptors modulates calcium-ion influx has been debated since the 1970s. Researchers have uncovered tantalizing clues to the identity of the target molecule that PKA modifies by the addition of a phosphate group (phosphorylation). However, proposals for specific candidate targets, phosphorylation sites8–10 and modulatory mechanisms have been repeatedly called into question by further tests, including some in painstakingly constructed mouse models11–13. Liu et-al. use a powerful technique called proximity proteomics14 to implicate a previously under-appreciated suspect15 and to establish its role. L-type voltage-gated channels provide a route by which calcium ions enter cardiomyocytes to help trigger a heartbeat. If channel opening is boosted, this results in a stronger and faster heartbeat. Previous investigations of how PKA might modulate channel opening focused mainly on amino-acid residues in the channel that occur in structural motifs possibly phosphorylated by PKA. But when Liu and colleagues performed a tour-de-force experiment in mice in which the channel was engineered so that all candidate phosphorylation sites were converted to an amino acid (alanine) that can’t be phosphorylated, and when this channel was studied alone, PKA-mediated enhancement of L-type channels nevertheless persisted. The authors therefore looked elsewhere for the elusive mediator of PKA’s ability to regulate the fight-or-flight effect.   Reasoning that some unknown factor must come into close proximity to the calcium channel during this regulatory process, the authors conducted a systematic search. Using proximity proteomics, Liu and colleagues engineered channel subunits to contain an enzyme that adds a tag called biotin to any protein within a radius of approximately 20 nanometres15. Tagged proteins were then identified by mass spectrometry. Hundreds of proteins in proximity to the calcium channel were analysed, and the authors found that the protein Rad was enriched in the channel microenvironment under resting conditions, but was noticeably depleted during stimulation of the β-adrenergic receptor. This dovetailed with an earlier clue — Rad is known to inhibit L-type voltage-gated calcium channels15, and, in mice, deletion of the gene that encodes Rad mimics the effect of β-adrenergic stimulation and eliminates further adrenaline-mediated enhancement of the activity of L-type channels16. Liu et al. investigated whether PKA could prevent Rad-mediated channel inhibition. The authors tested whether phosphorylation of amino-acid residues on Rad would enable it to move away from the vicinity of the calcium channel. They narrowed the candidate residues down to four serines (in some experiments, just two), which, if replaced by alanine, abolished PKA-mediated regulation of calcium entry. The calcium channel’s β-subunit was the prime suspect as the target of Rad inhibition. Ablation of the interaction between the calcium channel’s α1C-subunits and its β-subunits fully eliminates PKA-mediated modulation of channel activity17. Indeed, the authors’ measurements, using a technique called fluorescence resonance energy transfer, showed that the interaction between Rad and the calcium-channel β-subunit was inhibited by PKA phosphorylation of the key serines in Rad that the authors had identified. Further tightening the noose around Rad’s metaphorical neck, electrical recordings demonstrated that all of the biophysical fingerprints of modulation by β-adrenergic signalling — such as the activity of previously inactive calcium channels and a shift in the voltage dependence of their activation18 — were prevented by eliminating Rad phosphorylation. The results make a compelling case for the following scenario (Fig. 1). Adrenaline binds and activates the β-adrenergic receptor. This, in turn, results in the activation of an enzyme that produces cAMP, which activates PKA. PKA phosphorylates Rad and causes it to leave the vicinity of the calcium channel, thereby preventing it from inhibiting the channel. Figure 1 | Modulation of the cardiac calcium channel. In heart cells called cardiomyocytes, the activity of calcium-ion channels increases during what is called the fight-or-flight response. Activation of the enzyme protein kinase A (PKA) is required for this process, and, in mouse studies, Liu et al.3 reveal that its elusive target is the protein Rad. a, In the absence of a fight-or-flight response, the β-adrenergic receptor is not stimulated and PKA is inactive. Rad binds to a subunit of the calcium channel (beige; only the α1C- and β-channel subunits are shown) and calcium-ion (Ca2+) entry into cardiomyocytes is low. b, During the fight-or-flight response, the hormone adrenaline activates the β-adrenergic receptor. This leads to the production of cyclic AMP (cAMP) molecules, which activate PKA. Activated PKA adds a phosphate group (P) to Rad, causing Rad to dissociate from the channel and enabling channel activity to increase. This elevation of Ca2+ in the cytoplasm boosts the heartbeat. The study puts Rad and other members of this family of proteins front and centre as players in calcium-channel modulation. Is Rad the entire missing chapter in the story of PKA’s role in the heart, given Liu and colleagues’ compelling arguments that other potential PKA targets are unnecessary? Sceptics will want further in vivo evidence from a type of mouse model termed a knock-in — animals whose original Rad sequence is replaced either with a version in which Rad’s own PKA-phosphorylation sites are mutated or with a version in which the part of Rad needed for the interaction with the β-subunit is eliminated — to see whether any PKA-mediated modulation of the calcium channel still occurs. Hints of differences between channel regulation in the embryonic and adult heart13 also warrant further study. Might cardiac regulation by Rad be of clinical value? Heart failure in humans is associated with loss of regulation of calcium channels by β-adrenergic receptors. Rad levels fall during heart failure19, perhaps providing a temporary increase in the strength of heart contraction16. However, this would also reduce the heart’s ability to further increase its strength20, what is known as its functional reserve, which would be a severe price for a person’s heart to pay. There will undoubtedly be debate about how PKA modulation of calcium channels operates in neurons, such as in PKA-responsive CA1 pyramidal cells in which Rad is essentially absent. In those neurons, the mutation of a particular serine (serine 1928) to alanine in the L-type channel eliminates channel modulation and L-type channel-dependent strengthening of inter-neuronal (synaptic) connections7. Here, PKA might be phosphorylating the calcium channel, after all. Organ-specific pathways for regulation would make functional sense. Rad can completely inhibit calcium-channel activity, and so modifying such inhibition would give heart cells a wide range of regulatory capability18, suitable for a brief flight-or-fight response. Perhaps other cell types needing a more sustained but subtler boost to their calcium-channel activity might operate better without Rad-mediated regulation and rely instead on milder, more direct modulation of a subunit of the calcium channel. Liu and colleagues have set a high bar for future detective work on cellular signalling in the heart. Their work shows the power of a systematic round-up of suspects and relentless interrogation of their roles. </body>
<date id = '303'>22 January 2020</date>
<url id = '304'>https://nature.com/articles/d41586-020-00094-5</url>
<title id = '304'>Two-dimensional materials have potential uses in flexible electronics, biosensors and water purification. A method for producing air-stable 2D materials on an industrial scale, now reported, is a key step in bringing them to market.</title>
<body id = '304'>Modern materials science relies on a deep understanding of defects — interruptions to regular atomic arrangements in crystalline solids. Although ‘defects’ brings to mind imperfections and blemishes, they often make a material more useful than it otherwise would be. For example, metal impurities such as chromium and iron atoms in corundum (a crystalline form of aluminium oxide) are responsible for the colours of rubies and sapphires. Moreover, the addition of impurities to silicon has enabled the current era of computing and robotics. Writing in Nature, Du et al.1 report a method for producing a variety of technologically useful two-dimensional materials that contain deliberately introduced impurities, solving a fabrication problem for next-generation devices.   Transition-metal chalcogenides (TMCs) are emerging materials that hold great promise for their incorporation into a wide range of applications, from batteries and flexible electronics to biosensors and water-purification systems. They are composed of a transition metal such as molybdenum or tungsten and a chalcogen (an element in group 16 of the periodic table) such as sulfur, selenium or tellurium. The properties of TMC monolayers change greatly if the metallic element is altered. In particular, these structures can change from being normal metals to semiconductors, or even superconductors. In the past few years, many researchers2–4 have focused on making ultrathin electronics that have superior properties to those of existing silicon devices, by combining different TMC monolayers into a single object known as a heterostructure, using a technique called chemical-vapour deposition. Other researchers5 have produced functional devices using a single TMC in which different regions of the material have different properties, such as being metallic or semiconducting. However, although these techniques are good for fabricating prototype devices, they are not practical enough for real-world applications. The long-standing problem in incorporating TMC monolayers into a functional device has been the lack of a metallic-phase TMC monolayer that is stable in ambient conditions for more than a month6. Du and colleagues overcame this challenge, and made metallic-phase TMC monolayers that they show can exist in such conditions for about a year. The authors achieved this feat by introducing a technology based on a process known as doping.   Doping has shaped the digital revolution — the shift from analog to digital electronics that began in the second half of the twentieth century. The process involves changing the electrical conductivities of semiconductors such as silicon by adding impurities. Eighty years ago7, dopant atoms of boron and phosphorus were added to pure silicon to produce materials called p-type and n-type silicon, respectively; these form p–n junctions, the basis of computing. This doping technology continues to be useful today, and is found in our everyday electronics. Du and co-workers’ doping technology for 2D materials is also expected to have a long-term impact on the field. The authors produced TMC monolayers in three steps (Fig. 1). First, they prepared a crystal that contained two different transition metals (one of which provided impurity atoms for TMC doping), an element in group 13 or 14 of the periodic table, and carbon. Second, they heated the crystal at high temperatures (873–1,373 kelvin) for 4 hours in an environment that contained two gases. One of these was a chalcogen-containing gas that supplied chalcogen atoms for the TMC; the other gas was phosphorus, which provided further impurity atoms for TMC doping. Third, the authors used a process called liquid exfoliation to convert the resulting TMC crystal into TMC monolayers in the form of liquid inks. Figure 1 | Method for producing air-stable transition-metal chalcogenides (TMCs). Du et al.1 demonstrate a technology for making monolayers of materials called TMCs that they show can remain stable in ambient conditions for about a year. They first prepare a crystal that contains two different transition metals, an element in group 13 or 14 of the periodic table, and carbon. They then place the crystal in a container and heat it in a furnace for 4 hours, in an environment containing two gases. One of the gases contains a chalcogen (an element in group 16 of the periodic table) and the other is phosphorus gas produced by heating phosphorus powder in a separate container in the furnace. The result of this process is a TMC crystal. Finally, the authors use a process called liquid exfoliation to convert the crystal into TMC monolayers in the form of liquid inks. Du et al. used this three-step dual-doping technology to make, for example, metallic-phase TMC monolayers of tungsten disulfide that were doped with both yttrium and phosphorus atoms. They also produced undoped TMC monolayers by preparing layered crystals that contained one type of transition metal, rather than two, and removing the source of phosphorus gas. In total, the authors made six doped and seven undoped TMC monolayers, demonstrating the remarkable versatility of their approach for producing 2D materials. One major advantage of Du and colleagues’ method is that the final 2D materials are in the form of liquid inks. There is clearly a shift in this field towards making high-quality monolayer inks for commercialization8,9, rather than films produced by techniques such as epitaxial growth or chemical-vapour deposition. Such films require a process known as delamination to separate them from their growth substrates, which deteriorates the material’s quality and necessitates further processing10,11. By contrast, monolayer inks can be readily deposited on arbitrary substrates using techniques such as inkjet printing or spin coating, and so are easily integrated into 3D systems12,13. From a scientific standpoint, 2D materials need to be stable and usable in our immediate surroundings. Du and colleagues’ findings are promising for the field because they show that the presence of a low quantity (less than 1%) of impurity atoms can stabilize TMC monolayers. This result suggests that materials researchers should start to explore the use of chemical elements to stabilize 2D materials that would otherwise degrade in ambient conditions within hours, rather than using encapsulation layers, which complicate the monolayer systems. The next steps will be for theorists to predict suitable ‘impurity stabilizers’ for TMC monolayers, and for experimentalists to investigate the use of elements that are abundant on Earth. In the meantime, it should still be possible to build advanced machines for precise and reliable dual doping of TMCs, because only a low quantity of relatively rare yttrium and phosphorus is needed to stabilize TMC monolayers. Du and colleagues’ work demonstrates that, whatever new materials are discovered, it is crucial that we understand, manipulate and use their atomic-level defects. Every atom matters. </body>
<date id = '304'>22 January 2020</date>
<url id = '305'>https://nature.com/articles/d41586-020-00047-y</url>
<title id = '305'>A model has been devised that quantitatively describes how the shape of a river delta is affected by sediments, tides and waves. It reveals that the area of delta land is increasing globally, as a result of human activities upstream.</title>
<body id = '305'>Undisturbed river deltas are diverse ecosystems that encompass tidal wetlands and floodplains. Because of their rich soils and convenient positions for trade and transport, many deltas have also become hotspots of socio-economic development. The Nile delta, for example, with its iconic triangular shape, has been one such locus for more than 5,000 years. Not all deltas are triangular, however — their morphology can vary widely. Writing in Nature, Nienhuis et al.1 report a model that correlates the forces that shape deltas with delta morphology, and use it to analyse the shapes of some 11,000 coastal deltas. This global overview allows the authors to assess how delta morphology is affected by changes in sediment delivery caused by river damming and soil erosion.   The authors’ model estimates delta morphology on the basis of a quantitative characterization of three main drivers that shape deltas. These are: sediment delivered by the river; wave action that redistributes sediment along the coast; and sediment transported into or out of the delta by tidal flows. The relative influences of these drivers were used to determine two key morphological metrics; namely, the protrusion of the delta into the sea and the shape of the river channel. For example, Nienhuis et al. infer from the model that when the effects of sediment delivered by the river are greater than the effects of wave action, deltas protrude relatively far into the sea. Alternatively, the authors conclude that deltas widen towards the sea into a trumpet shape when tidal flows are important and sediment delivery is low. Nienhuis et al. validated their model by comparing the projected morphologies with those of real deltas, and provide robust statistics on the reliability of the results, which is a key strength of the study. Note that the authors’ definition of what constitutes a delta is broad (see the Methods section of the paper for the criteria used), which means that their model is truly global. However, the model’s ability to capture the general behaviour of all deltas comes at the expense of fine-grained accuracy — there will almost inevitably be errors in the morphologies projected for some individual deltas. Nevertheless, the model’s results are statistically valid at a global level. Nienhuis and colleagues used their model to estimate the effects of upstream human interventions on delta morphology during the period 1985–2015. They found that dam building led to decreases in sediment delivery, whereas accelerated soil erosion caused by deforestation increased sediment delivery. Of the approximately 11,000 deltas analysed, about 9% are significantly affected by reduced sediment delivery, producing a total land loss of 127 square kilometres per year, whereas about 14% received increased sediment, causing a total gain of 181 km2 yr–1 during the study period. The reason more deltas have experienced an increase in sediment delivery, rather than a decrease, is simply that the effects of massive deforestation have outpaced sediment trapping by dams.   Previously reported state-of-the-art studies2,3 of global coastal morphology involved the computationally intensive analysis of extremely large archives of satellite images, which have become available in the past few years. These studies also revealed a net increase in land surface area. Many of the land gains could be explained by large-scale phenomena, such as the disappearance of the Aral Sea in central Asia, and by extensive land-reclamation projects along the China coast. But beyond those special cases, it is also crucial to learn in greater detail where and why river deltas have gained or lost land across the globe. Nienhuis et al. fill in this key part of the puzzle. The new study also reveals notable regional patterns. For example, arctic river deltas have seen almost no change in morphology. Sediment delivery by rivers in North America has fallen overall, leading to large land losses — in the Mississippi delta, for example. And the largest land gains are in eastern South America and in south, southeast and east Asia, where soil erosion due to deforestation has caused a net growth in delta areas, despite the construction of sizeable dams in these regions. Large deltas, such as those of the Niger, Huang He and Mekong, have great socio-economic value. Such densely inhabited deltas typically experience many pressures in addition to changes in sediment delivery, such as stresses associated with groundwater pumping, sand mining, dyke construction and loss of biodiversity4–6. For these highly complex deltaic systems, local studies will be needed to assess the problems that adversely affect their morphology and to define specific solutions6. However, most of the deltas considered by Nienhuis and co-workers are much smaller. This could skew the picture painted by the overall numerical results, because large deltas have a much greater global impact than do small ones, but represent a tiny fraction of the total number of deltas analysed in the study. For example, the study calculates that the net land gain for all deltas was 54 km2 yr–1 during the period studied, which seems like good news. But this area is tiny compared with the 105,000 km2 covered by the Ganges delta alone (Fig. 1) — which, with its population of 170 million people, is subject to a multitude of stresses7. We should therefore not be complacent about the new findings. Figure 1 | The Ganges river delta.Credit: Alamy Nienhuis et al. did not include sea-level rise in their model, but sea levels rose by about 10 cm over the period studied (see go.nature.com/2tpjpxg). This will probably not have produced observable losses of delta land, given the large spatial variability of sea-level rises. Nevertheless, it would be interesting to see whether measurable losses did occur. The authors’ model provides a useful description of the background dynamics of changes in delta morphology against which the impact of rising seas can be measured once sea levels approach predicted increases of 60 cm8 or more9, as a result of global warming. Severe sea-level rise will undoubtedly cause coastline recession in deltas, as it has in the geological past10. Validated global models describing key parts of the Earth system are crucial in this time of unprecedented human-induced climate change. Deltas connect the terrestrial and maritime branches of the hydrological cycle and the associated sediment fluxes. As such, they encapsulate many key indicators of global change. By accounting for the baseline effects on deltas of human activities such as dam building and deforestation, Nienhuis and colleagues have provided a fundamental framework that will help assessments of the impacts of climate change for decades to come. </body>
<date id = '305'>22 January 2020</date>
<url id = '306'>https://nature.com/articles/d41586-019-03949-8</url>
<title id = '306'>Signalling from the sympathetic nervous system of mice when subjected to stress leads to the depletion of a stem-cell population in their hair follicles. This discovery sheds light on why stress turns hair prematurely grey.</title>
<body id = '306'>It has been said that Marie Antoinette’s hair went completely white on the night before her beheading. This story might be apocryphal, but rapid greying of the hair is now widely referred to as Marie Antoinette syndrome. It is often assumed to be caused by stress — a phenomenon perhaps best exemplified by photographs of heads of state before and after they held office. However, the relative contributions of ageing, genetic factors and stress to greying are not known — in part owing to a lack of mechanistic understanding of the process. Writing in Nature, Zhang et al.1 identify the mechanism governing premature greying in mice that have experienced stress. The average human scalp has 100,000 hair follicles, and a wide range of hair colours can be found across the human population. Hair colour is determined by cells called melanocytes, which produce different combinations of light-absorbing melanin pigments2. Melanocytes are derived from melanocyte stem cells (MeSCs), which are located in a part of the hair follicle called the bulge3. The normal hair cycle is divided into three stages: hair-follicle regeneration (anagen), degeneration (catagen) and rest (telogen). Melanocyte production begins early in the anagen phase (Fig. 1a). As people age, the pool of MeSCs is gradually depleted — and so pigmented hair becomes ‘salt and pepper’ coloured, and then turns to grey and finally to white after a complete loss of pigment in all hair follicles4. Figure 1 | Melanocyte stem cells and stress. Melanocyte stem cells (MeSCs) are located in the bulge of the hair follicle, which is innervated by neurons of the sympathetic nervous system that release the neurotransmitter molecule noradrenaline. The follicle cycles through three phases: regeneration (anagen), degeneration (catagen) and rest (telogen). a, Under normal conditions, MeSCs migrate away from the bulge (red arrows) and differentiate into melanocytes during anagen. Melanocytes synthesize pigments that add colour to the regenerating hair. During catagen and telogen, they begin to die and migrate out of the niche (not shown). However, plentiful MeSCs remain to replace the melanocytes in the next anagen phase. b, Zhang et al.1 show that stressful stimuli activate the sympathetic nervous system, increasing noradrenaline release in hair follicles. Noradrenaline causes complete conversion of MeSCs into melanocytes, which migrate out of the niche in catagen and telogen. The hair follicle is depleted of MeSCs that would have differentiated to replace these melanocytes. Without any pigment cells to colour the hair in the next anagen phase, it begins to look grey or white. Aside from ageing, there are several factors that bring about premature greying, including dietary deficiencies5, disorders such as alopecia areata or vitiligo6,7, and stress8,9. Zhang et al. set out to test the role of stress in the greying process in mice. They exposed the animals to three different stressors — pain, restraint and a model of psychological stress — during different phases of hair growth. Each stressor caused depletion of MeSCs from the bulge region, eventually leading to the development of patches of white hair. Prevailing theories posit that stress-induced greying involves hormones (such as corticosterone) or autoimmune reactions10. Zhang and colleagues examined these potential mechanisms, first by preventing corticosterone signalling and next by stressing animals that had compromised immune systems. In both cases, greying occurred after stress, indicating that neither corticosterone nor autoimmune reactions cause MeSC depletion. However, the authors found that MeSCs express β2-adrenergic receptors, which respond to noradrenaline — a neurotransmitter molecule involved in the ‘fight or flight’ response to stress. Loss of this receptor specifically in MeSCs completely blocked stress-induced greying. Adrenal glands are the main source of circulating noradrenaline. But, surprisingly, the researchers discovered that removing these glands did not prevent greying in response to stress in the mice.   Another source of noradrenaline is the sympathetic nervous system (SNS), which is highly active in response to stress, and which drives the fight-or-flight response. Zhang and colleagues showed that bulge regions are highly innervated by sympathetic neurons, and that ablating the SNS using a neurotoxin molecule, or blocking the release of noradrenaline from sympathetic neurons, prevented stress-induced greying. Next, the authors generated mice in which sympathetic neurons could be acutely activated, and found that overactivation of the SNS in these mice caused greying in the absence of stress. Together, these results indicate that noradrenaline released from active sympathetic neurons triggers MeSC depletion (Fig. 1b). Interestingly, Zhang et al. found that the propensity of an area to turn grey correlates with its level of sympathetic innervation. Exactly how does sympathetic activity cause depletion of MeSCs from hair follicles? Normally, these stem cells are maintained in a dormant state until hair regrowth is required. However, when the researchers tracked MeSCs labelled with a fluorescent protein, they discovered that MeSC proliferation and differentiation increase markedly under extreme stress or exposure to a high level of noradrenaline. This results in mass migration of melanocytes away from the bulge, and leaves no remaining stem cells. To further confirm this result, the researchers suppressed MeSC proliferation pharmacologically and genetically. When proliferation was dampened, the effects of stress on MeSC proliferation, differentiation and migration were blocked. Zhang and colleagues’ work raises several questions. For instance, is the mechanism underlying MeSC depletion in response to stress the same as that which causes greying during ageing? Future experiments modulating SNS activity over a longer period would determine whether age-related greying can be slowed or hastened. Perhaps, in the absence of sympathetic signals, MeSCs have the capacity for unlimited replenishment, pointing to a way to delay age-related greying.   Are other pools of stem cells similarly susceptible to stem-cell depletion in response to stress, if they or the cells that make up their niche express β2-adrenergic receptors? In support of this idea, haematopoietic stem and progenitor cells (HSPCs), which give rise to blood and immune lineages, reside in a bone-marrow niche that contains stromal cells, and stimulation of those cells by the SNS causes HSPCs to leave their niche11,12. Perhaps, like MeSCs, stress depletes HSPCs — which could partially explain why immune function is impaired in response to chronic stress13,14. Whether this type of relationship extends beyond MeSCs and HSPCs is an open question. It is fascinating to consider what possible evolutionary advantage might be conferred by stress-induced greying. Because grey hair is most often linked to age, it could be associated with experience, leadership and trust15. For example, adult male silverback mountain gorillas (Gorilla beringei beringei), which get grey hair on their backs after reaching full maturity, can go on to lead a gorilla troop16. Perhaps an animal that has endured enough stress to ‘earn’ grey hair has a higher place in the social order than would ordinarily be conferred by that individual’s age. Connecting the dots between stress, fight or flight, stem-cell depletion and premature greying opens up several avenues for future research. Beyond developing anti-greying therapies, Zhang and colleagues’ work promises to usher in a better understanding of how stress influences other stem-cell pools and their niches. </body>
<date id = '306'>22 January 2020</date>
<url id = '307'>https://nature.com/articles/d41586-020-00095-4</url>
<title id = '307'>How Nature reported an interview with Einstein from 1920, and fears that England was losing the race to reach the North Pole, from 1870.</title>
<body id = '307'> An interesting interview with Prof. Einstein appeared in the Daily Chronicle of January 15. A German by birth, Prof. Einstein went to Switzerland in his early youth, where he became naturalised. For some years he was professor of physics at the Federal Polytechnikum in Zürich, and for a short time also at the University of Prague. Shortly before the outbreak of war he was “called” to the University of Berlin, where he is still working, being at the same time director of the Kaiser Wilhelm Institute for Physical Research. Now little more than forty years of age, this eminent man of science conceived the outlines of the theory of relativity at the early age of eighteen, and presented his special theory to the scientific world at the age of twenty-seven. Prof. Einstein regards Prof. Lorentz (Leyden) as his “cooperator” in the special theory of relativity. He points out that, far from vitiating the results of Newton, the theory of relativity rather enhances the greatness of this genius. Though these new ideas will not overthrow the general conceptions of mankind, they will leave their impress on men’s thinking in the philosophical and allied sciences. From Nature 22 January 1920 England seems ready to resign the position she once held as chief of all the competitors in Arctic exploration. Our flag has been carried within 7½ degrees of the North Pole; our seamen have forced from the ice-bound straits which lie to the north of America the secret of the North-Western Passage; and from the days of Scoresby until those of Franklin we have been foremost in scientific researchers within the dreary Arctic wastes. But now the answer to all who would emulate the deeds of a Parry or a Ross, a Beecher or a Franklin, is the stereotyped cui bono. A business account of the probable gains of an Arctic journey must be rendered before England will send men or ships to the Polar seas. In the meantime, Swedish and German explorers are pushing their way boldly into the regions where England won her Arctic laurels — perhaps we ought rather to say, ice-wreaths. Already the most northerly spot reached by our seamen has been all but attained, and there is yet room for supposing that this very year the second German expedition may push its way to the Pole itself. From Nature 20 January 1870 Latest on: Geography Research Highlight 04 MAY 20 News & Views 04 FEB 20 News & Views 14 JAN 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Physics Article 21 MAY 20 News & Views 20 MAY 20 Article 20 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '307'>21 January 2020</date>
<url id = '308'>https://nature.com/articles/d41586-020-00093-6</url>
<title id = '308'>The reintroduction of ruminant herbivores to a national park in Mozambique has controlled the encroachment of a notoriously invasive plant species.</title>
<body id = '308'>Credit: Walter Stein/Getty The population of large animals in the Gorongosa National Park collapsed during the Mozambican civil war (1977–92), and led to encroachment of the invasive shrub Mimosa pigra. Writing in Nature Ecology & Evolution, Guyton et al.1 report that Gorongosa’s repopulation with large herbivores has reduced the abundance of mimosa to pre-war levels. By analysing faecal samples from Gorongosa’s five main ruminant herbivores, including waterbuck (Kobus ellipsiprymnus; pictured), the authors found that mimosa was the main component of the diets of these species in 2013–18. They also found that the shrub’s density and biomass were greater in fenced enclosures that excluded herbivores than in unfenced areas. The authors therefore conclude that the burgeoning populations of native large herbivores are consuming mimosa, and have thereby conferred resistance to its invasion in just ten years. The findings suggest that rewilding is a potentially useful strategy for reversing a common form of environmental degradation in Africa’s protected areas. </body>
<date id = '308'>16 January 2020</date>
<url id = '309'>https://nature.com/articles/d41586-020-00039-y</url>
<title id = '309'>Microorganisms related to lineages of the Asgard archaea group are thought to have evolved into complex eukaryotic cells. Now the first Asgard archaeal species to be grown in the laboratory reveals its metabolism and cell biology.</title>
<body id = '309'>Complex life forms including plants, animals and fungi are known as eukaryotes. These organisms are composed of cells that contain membrane-bound internal compartments such as nuclei and other organelles. Writing in Nature, Imachi et al.1 report that a type of microorganism called an Asgard archaeon, which might shed light on how early eukaryotic cells evolved, has finally been cultured in the laboratory. The achievement will enable detailed metabolic and cellular investigation of microbes that represent the closest Archaeal relative of eukaryotes cultured so far.   It is thought that eukaryotes arose when two types of single cell merged, with one engulfing the other. A cell from the domain archaea is proposed to have engulfed a bacterial cell of a type known as an alphaproteobacterium, and the engulfed bacterium evolved into eukaryotes’ energy-generating organelles — mitochondria. However, the nature of the ancestral cell that engulfed this bacterium is unclear. Genomic analyses have strengthened the idea that this cell traces back to archaea because many archaeal genes involved in central biological processes such as transcription, translation and DNA replication share a common ancestry with (are phylogenetically related to) the corresponding eukaryotic genes. Was the alphaproteobacterium engulfed by a bona fide archaeal cell, or by an archaeal cell that had already acquired some eukaryotic characteristics, such as a nucleus? No fossils have been found that could shed light on the early eukaryotic ancestors. However, investigation of archaeal lineages has offered a way forward. Since 2015, on the basis of genomic and phylogenetic analyses2, archaea of a newly discovered phylum termed Lokiarchaeota (after the Norse god Loki) have been proposed as the closest living relatives of the ancient archaeal host cells from which eukaryotes are thought to have evolved. Subsequent genomic research revealed yet more such lineages, for which other Norse gods have provided names (Thor, Odin, Heimdall and Hel)3,4, and which are now grouped together with Lokiarchaeota into what are collectively termed Asgard archaea (Fig. 1). Intriguingly, all of these lineages contain an unprecedentedly large number of genes that encode what are called eukaryotic signature proteins (ESPs), which are usually found only in eukaryotes2,3,5,6. Heimdallarchaeota currently represent the predicted closest Archaeal relative of eukaryotes on the basis of phylogenetic analysis and the ESP content of their genomes3,7. However, all members of the Asgard archaea were previously identified, and their metabolism predicted, solely by their DNA sequences, and thus their cellular features have remained unknown until now. Figure 1 | The evolution of eukaryotic cells. Imachi et al.1 report that they have cultured a microorganism, which they call ‘Prometheoarchaeum syntrophicum’, in the laboratory. The microbe belongs to a group known as Asgard archaea. This is the first time that an Asgard archaeon has been cultured, and has revealed previously unknown aspects of its cellular biology, including the presence of long protrusions. This development might shed light on how complex eukaryotic cells evolved. a, It is thought that an ancient Asgard archaeon interacted with a bacterium from the class Alphaproteobacteria, for example by exchanging metabolite molecules (grey circles). The mitochondrion, the energy-generating organelle of eukaryote cells, is thought to have evolved when such a bacterium was taken up in the archaeal cell. b, This simplified evolutionary tree includes branches of the lineages (Proteobacteria shown in red and Asgard archaea in blue) that might have contributed to the formation of eukaryotic cells. Dashed lines on the evolutionary trees represent lineages identified only by genomic analysis and not by organisms cultured in the laboratory. It is thought that eukaryotic cells evolved from a partnership between an alphaproteobacterium and a relative of a Heimdallarchaeote (neither of which is known). LUCA: the last universal common ancestor (the cell(s) from which bacteria and archaea evolved). Imachi and colleagues report that they have cultured in the laboratory an Asgard archaeon from the Lokiarchaeota phylum that they propose to call ‘Prometheoarchaeum syntrophicum’, which was obtained from deep-ocean sediments. The unusual shape and metabolism of Prometheoarchaeum prompt the authors to propose a new model for the emergence of the first eukaryotic cell. This event, predicted8 to have occurred between 2 billion and 1.8 billion years ago, is one of the key cellular transitions in evolutionary biology, and is also a major biological mystery. More than six years before Asgards were even identified, Imachi and colleagues had already started to generate enrichment cultures of microorganisms found in deep marine sediments9. Their original goal was to find organisms that could degrade methane, and the authors searched for such microbes at a site about 2.5 kilometres below the ocean surface off the coast of Japan. Imachi et al. set up a flow bioreactor device that mimicked the temperature (10 °C) and the low-oxygen and low-nutrient conditions at this underwater site. Within five years of starting this bioreactor work, a highly diverse consortium of active bacteria and archaea, including Lokiarchaeota, were obtained. Small subcultures were then used to gradually enrich for cultures in which archaeal cells were the dominant component, and Prometheoarchaeum was successfully enriched in this way after seven more years of work. These optimizations revealed that Prometheoarchaeum grows best in conditions that do not directly reflect its original habitat: at 20 °C and supplemented with amino acids, peptides and even baby-milk powder. The authors report that Prometheoarchaeum’s growth depends on the presence of other microbial partners that in turn rely on Prometheoarchaeum for their survival — a relationship called a syntrophy. The partners scavenge hydrogen released by Prometheoarchaeum, a metabolic product that was correctly predicted to be generated by Asgard archaea on the basis of genomic data5. The authors found that Prometheoarchaeum could be enriched to make up more than 80% of the cells in the culture, even though it grows extremely slowly, taking 2 to 4 weeks to replicate and divide. From preliminary studies using isotope analysis, the authors report that this organism can degrade externally supplied amino acids. However, that does not exclude the possibility that it also thrives on other nutrients in the growth medium. Prometheoarchaeum cells are relatively small (300–750 nanometres in diameter), have lipids characteristic of other archaea, and show no evidence for eukaryotic-like organelles. However, the organism forms intriguing structures on its cellular surface that include long and often branching protrusions.   On the basis of its cell shape and small size, and on evidence that Prometheoarchaeum produces and syntrophically transfers hydrogen and formate molecules to other organisms, the authors propose a new model for the emergence of eukaryotic cells — one involving three partners. In this model, a free-living bacterial ancestor that would give rise to mitochondria became entangled with, and was then engulfed by, an archaeal host cell that itself was in a syntrophic relationship with a bacterial partner. This model is consistent with earlier suggestions about the engulfment process in eukaryotic evolution10, and emphasizes the importance of membrane-mediated processes in the origin of eukaryotes11. However, extensive cellular protrusions are not found exclusively in this Asgard archaeon. It would therefore be of interest to investigate to what extent these protrusions differ from those of branched cellular extensions previously observed in other archaea such as Pyrodictium12 or Thermococcus species13. In addition, it will be interesting to determine whether the ESPs potentially involved in membrane remodelling are localized in these structures in Prometheoarchaeum. The syntrophic interactions that Imachi and colleagues propose in their model for the origin of mitochondria are based on the need for the host cell to adapt to oxygen use (as a consequence of rising oxygen levels on the ancient Earth). These ideas differ from the ‘reverse hydrogen flow’ model, which suggests instead that hydrogen produced by the archaeon is consumed directly by the bacterial mitochondrial ancestor, with no need to invoke a hypothetical third partner5. Considering that Prometheoarchaeum does not directly represent the archaeal ancestor of eukaryotes (nor does any other currently existing archaeon), other suggested metabolic exchanges between the archaeal host and bacterial mitochondrial ancestor, such as hydrogen consumption from the archaeal14,15 or the bacterial side5, remain plausible as initial drivers of a syntrophic relationship. In any case, the many models for the origin of eukaryotes5,11,14,15 highlight the importance of initial syntrophic associations5,14,15 and membrane-mediated processes10,11. Interestingly, albeit for different reasons, both syntrophy and membranes were crucial aspects in an engineered synthetic relationship in which an Escherichia coli bacterium was maintained inside a yeast cell for more than 120 days16. Imachi and colleagues’ success in culturing Prometheoarchaeum after efforts spanning more than a decade represents a huge breakthrough for microbiology. It sets the stage for the use of molecular and imaging techniques to further elucidate the metabolism of Prometheoarchaeum and the role of ESPs in archaeal cell biology. This, in turn, could guide the direction of future work investigating how eukaryotic cells emerged. </body>
<date id = '309'>15 January 2020</date>
<url id = '310'>https://nature.com/articles/d41586-019-03943-0</url>
<title id = '310'>Three studies reveal that the presence in tumours of two key immune components — B cells and tertiary lymphoid structures — is associated with favourable outcomes when individuals undergo immunotherapy.</title>
<body id = '310'>Current immunotherapies aim to reinvigorate immune cells called killer T cells to fight cancer, but only 20% of individuals with the disease see a lasting clinical benefit from this type of treatment1. Focusing on other immune cells in patients’ tumours might help us to improve these outcomes. Three studies in Nature, by Cabrita et al.2, Petitprez et al.3 and Helmink et al.4, now demonstrate that the presence of B cells in human tumours in compartments called tertiary lymphoid structures (TLS) is associated with a favourable response to immunotherapy. These complementary studies add to the immunotherapy toolbox by providing new ways of predicting prognosis. The presence of B cells in tumours has been considered to be a predictor of increased patient survival5,6, but there are reports of both anti- and pro-tumour roles for B cells7. These differing reports reflect the multiple roles that B cells can have in tumours. One component of the antitumour function of B cells is B-cell activation. This process involves the binding of tumour-derived proteins to the B-cell receptor protein on the cell surface and the subsequent processing of these tumour-derived proteins into smaller fragments called antigens. Further co-factors are also involved in activation. Activated B cells can release antibodies that tag tumour cells for attack by other cellular players of the immune system (a process known as antibody-dependent cell death)8, and can ‘educate’ T cells by presenting them with tumour antigens, enabling the T cells to target tumour cells effectively9. However, B cells in tumours can produce inhibitory factors that hinder the function of immune cells (Fig. 1). These might be signalling molecules that suppress the immune system7,10,11 or inhibitory molecules on the surfaces of B cells that limit the body’s ability to target and kill tumour cells. Figure 1 | Multifaceted B cells in the tumour microenvironment. B cells are thought to have multiple roles in suppressing or promoting the immune system’s ability to kill tumour cells, depending on whether they are located in immature or mature compartments called tertiary lymphoid structures (TLS), which also contain T cells. a, In poorly structured, immature TLS, one hypothesis is that B cells generate inhibitory factors. These might be molecules released from B cells that dampen the response of other immune cells, or molecules on the surfaces of B cells that hinder the targeting and destruction of tumour cells. Both of these inhibitory mechanisms might arise if B cells have less interaction with T cells and more interaction with the malignant tumour. Three studies2–4 now provide indirect evidence that immature TLS are associated with low activity of T cells in tumours. b, By contrast, B cells in well-structured, mature TLS can release antibodies that could target tumours, and B cells can present a tumour-derived protein called an antigen (yellow) to T cells in the tumour, activating the T cells. The studies suggest that the presence of B cells in mature TLS is correlated with increased T-cell activity, improving the immune system’s ability to target tumour cells, and increasing the likelihood that the tumour will respond to immunotherapy. TLS are aggregates of immune cells (mostly T and B cells) that arise in response to immunological stimuli. Mature TLS nurture B-cell development and function in an inner region of the structure called the germinal centre, whereas immature TLS do not contain proper germinal centres, and might not nurture full B-cell function. The presence of TLS in a tumour also correlates with increased patient survival in many cancer types12. The three current studies confirm this trend in the context of immunotherapy, demonstrating that infiltration of B cells into a tumour, along with the presence of TLS, is associated with an improved response to this type of treatment.   Cabrita et al. studied individuals who had a type of cancer called metastatic melanoma, and Petitprez et al. investigated people with sarcoma, a cancer of the bone. Both teams found that the presence of B cells in TLS in the tumour before treatment was associated with an increased chance that patients’ tumours would respond to immunotherapy. Helmink et al. corroborated these findings for metastatic melanoma, and reported the same pretreatment trend in renal cell carcinoma. These authors also demonstrated that, during treatment, TLS are more prevalent in people who have tumours that are responding to treatment than in those whose tumours are not. This timing is important — when present before treatment, TLS could be considered a predictor of patient response to immunotherapy, whereas the presence of TLS during treatment indicates that key combinations of immune cells are being manipulated to induce TLS formation. Identifying these cell combinations could help in establishing new and effective immune-based therapies. The three groups found that the B-cell and TLS signature was often more pronounced in responders than in non-responders. Furthermore, the signature was more prominent than typical T-cell signatures currently used for understanding immunotherapy outcomes. This suggests that B cells and TLS could have a key role in antitumour immunity.   In addition to these synergistic results, each study highlights a unique role for B cells or TLS in antitumour immunity. First, Cabrita et al. demonstrate that B cells in TLS synergize with killer T cells that could ultimately target tumour cells. Second, Petitprez et al. describe signatures characteristic of mature TLS in sarcoma. This implies that mature TLS can exist in tumour sites that are not normally thought to be infiltrated by immune cells, a phenomenon that has not previously been shown. Third, Helmink et al. find increased diversity of B-cell receptors in responders compared with non-responders. This indicates that pools of B cells in responders might have a greater ability to specifically recognize tumour antigens than do the B cells of non-responders. These papers are technologically savvy, use patient populations that are statistically robust and bring B cells and TLS to the forefront of antitumour immunity. However, there is much still to learn. First, more emphasis should be placed on understanding how TLS form in tumours. It is clear that these structures are variable, and can be immature or mature. What does this diversity mean for the function of B cells in TLS, and what causes the induction of one ‘flavour’ of TLS versus another? The contribution of environmental factors such as smoking or viral and bacterial infections should be considered, along with a person’s gender, age and tumour type.   Researchers should also ask whether mature TLS could be routinely induced to form in tumours, to maximize B-cell immunity. Addressing this issue will require investigation of B cells and TLS in individuals who have not yet undergone treatment, as well as proper modelling of the human tumour microenvironment. Current evidence indicates that B cells actually impede antitumour responses in most mouse models of cancer13–15. However, TLS formation is rare in these animals, and a lack of TLS might alter the fate and subsequent function of B cells. Indeed, more knowledge about B-cell function outside TLS is needed to provide a complete picture of B cells in the tumour microenvironment. There is still a need to define the full range of functions that B cells perform in tumours. In addition to their known roles in producing tumour-specific antibodies and presenting antigens8,9, B cells are likely to have other functions — for instance, inducing antibody-dependent cell death8. It will also be necessary to link these functions to specific B-cell types and to determine whether such cells are found inside or outside TLS. There are clear biomarkers for B-cell subsets, but linking these subsets to functions in human tumours would allow us to design treatments that optimize specific antitumour activities. Furthermore, this knowledge would help us to understand whether subsets of B cells perform separate tasks, or if there is crosstalk between subsets. For example, can the same B cell both produce a tumour-specific antibody and present antigens to T cells? Some of these studies can be done in human tumours, but in-depth mechanistic studies will require physiologically relevant models that contain naturally occurring TLS. With regard to clinical implications, the current studies suggest that therapeutics to enhance B-cell responses should be prioritized as a complement to T-cell-mediated immunotherapies. Researchers should now ask whether B cells could be engineered to target specific tumour antigens, similar to current efforts to engineer antigen-targeting T cells. More generally, could immunotherapies be improved by inducing B cells to form in TLS after a person has received T-cell-based immunotherapy? Overall, the current studies should act as a springboard for future mechanistic studies of B cells and TLS in cancer. Understanding how current therapies can be combined with approaches to harness B cells and TLS will be crucial for the development of effective B-cell-specific immunotherapies. </body>
<date id = '310'>15 January 2020</date>
<url id = '311'>https://nature.com/articles/d41586-020-00002-x</url>
<title id = '311'>The fundamental machine-learning task of classification can be difficult to achieve directly in ordinary computing hardware. Unconventional silicon-based electrical circuits can be evolved to accomplish this task.</title>
<body id = '311'>Artificial intelligence (AI) has allowed computers to solve problems that were previously thought to be beyond their capabilities, from defeating the best human opponents in complex games1 to automating the identification of diseases2. There is therefore great interest in developing specialized circuits that can complete AI calculations faster and with lower energy consumption than can current devices. Writing in Nature, Chen et al.3 demonstrate an unconventional electrical circuit in silicon that can be evolved in situ to carry out basic machine-learning operations.   Although computers excel at performing calculations that have well-defined answers, they have not been good at making guesses. For example, if you are thinking about selling your car, a computer is ideally suited for calculating the average price that similar cars have sold for, to help you determine your selling price. But by analysing the enormous digital data sets that are currently available, AI techniques such as machine learning can now teach computers to make sensible predictions. One of the most basic operations that machine-learning algorithms can carry out when provided with a large set of inputs (such as the age of a car and how many kilometres it has been driven) is classification into one of a set of categories, such as whether the car is in poor, fair or good condition and therefore whether you can expect to get the price you want for it. Using the structure of the human brain as inspiration, scientists and engineers have made substantial progress in developing specialized hardware to greatly reduce the amount of time and energy needed to perform tasks such as classification4. There are also many unconventional device concepts for machine learning that are still in the early stages of development but that could offer radical new capabilities. For example, researchers are exploring whether superconductor-based electrical circuits that work at only a few degrees above absolute zero, and that operate at gigahertz frequencies with high energy efficiency, could enable machine-learning applications that are currently infeasible using conventional approaches5. Chen and co-workers’ circuit is also inspired by the brain, and represents a major departure from typical electrical circuits. Normally, electrical current flows through circuits like water flowing in a river. If the river becomes so shallow that it is reduced to a set of small puddles, the water can no longer flow because the puddles are isolated from each other by barriers formed by the riverbed. Although the barriers between water puddles are too wide for a water molecule to hop from one puddle to the next, this is not the case for electrical charge in ‘puddles’ of charge separated by nanoscale distances. Tools such as scanning tunnelling microscopes use the high sensitivity of a hopping process called quantum-mechanical tunnelling to routinely image features as small as individual atoms on surfaces6. This tunnelling is also at the heart of quantum-computing technologies that have made impressive advances in the past decade7. Previous work by some of the current authors8 produced isolated charge puddles from a collection of gold nanoparticles that were randomly deposited on a silicon surface, with insulating molecules between them. These puddles were used to implement circuits that carried out conventional calculations, rather than machine learning, and are at the heart of Chen and colleagues’ circuit design (Fig. 1a). Information is input into such a circuit through electrical voltages that are applied using ordinary wires. The electric fields from these wires can alter whether or not hopping between neighbouring puddles can occur, and therefore can modify the hopping path for electrical charge through the circuit (Fig. 1b). The output of the circuit is determined by whether or not electrical current flows through another designated wire. Figure 1 | An unconventional circuit for machine learning. a, Chen et al.3 demonstrate an electrical circuit in which charge hops between ‘puddles’ of charge in a background material. The operation of the circuit is tuned by applying voltages to control wires. Inputs to the circuit are provided by voltages on input wires, and the circuit’s output is determined by whether or not charge flows through an output wire. b, The control wires modify the regions of the circuit in which charge hopping can occur, thereby modifying which hopping paths can exist. An example of the effect produced by changing the voltage for a single control wire is shown here. The authors use their circuit to carry out basic machine-learning operations. Because the distribution of charge puddles is random, it would seem impossible to predict how such a circuit would behave. However, the high sensitivity of tunnelling makes it possible to strongly modify the behaviour of the circuit using different control wires. This behaviour also cannot be easily predicted, but the previous work showed that configurations could be reliably found that carried out logic operations for two inputs, such as indicating whether at least one or both of the inputs were on (called an OR gate and an AND gate, respectively, in binary logic). Although these earlier circuits did not perform machine-learning operations, the researchers did use a machine-learning algorithm to determine the control parameters needed to make the circuits carry out different operations. This algorithm is inspired by biological evolution, starting from a set of random control parameters and using only the most promising outcomes to ‘breed’ new parameter sets for successive generations. The previous study was groundbreaking because it showed that a single circuit could be reprogrammed in situ to execute any two-input logic operation by simply changing the voltages applied to five control wires. In the present work, Chen et al. greatly expanded this basic idea by overcoming some of its key limitations and using the circuits to perform AI operations. First, the authors found a way to produce charge puddles directly in silicon by randomly implanting atoms that donate small amounts of charge to the silicon itself. This method makes the devices more broadly manufacturable than they previously were and potentially compatible with the current generation of electronics, which are also mainly based on silicon. Second, in this new material system, the maximum temperatures at which hopping dominates the electrical flow in these circuits, and therefore at which the desired operation is viable, is increased from barely above absolute zero to room temperature. To demonstrate the increased potential of these devices, Chen et al. evolved circuits that could classify all 16 possible sets of 4 binary inputs (0000, 0001, …, 1111, where 0 represents no input on a given wire and 1 represents an input on a given wire). This classification was possible even when the number of control voltages was reduced from five to three.   The authors then incorporated this in silico 4-input classifier into the more complex AI task of classifying a standard set of black and white images of handwritten digits, which were encoded as a 28 × 28 array of pixels, each with a value of either 0 (white) or 1 (black) (see Fig. 4 of the paper3). To do this, Chen et al. subdivided the original array into sets of 2 × 2 neighbouring pixels and fed the value of each of these 4 pixels into the 4-input classi-fier’s input wires. The authors then set the control wires to perform classification for each of the 16 possible sets of 4 inputs, and passed all 16 outputs for each set of 2 × 2 pixels to a machine-learning algorithm run on conventional hardware that identified the digit in the full image. Chen and colleagues’ hardware platform for classification is inherently scalable, and individual classifier devices can be run in parallel without any conflicts. In the current incarnation of the platform, the set-up used to perform the measurements limits the speed at which the classifiers can be operated and, in turn, the energy efficiency. The authors suggest alternative ways in which the measurements could be implemented to greatly improve the speed and energy efficiency of the circuits; demonstrating such improvement will be crucial if these designs are to move from the research laboratory to real-world applications. Because the random distribution of charge puddles in both the previous and current designs is difficult to model, the circuits’ high sensitivity to the control voltages is essential for evolving a set of control parameters after fabrication. Although the absence of the need to precisely position the puddles makes the circuits easier to fabricate, their performance might be further enhanced by having pre-defined, atomically precise arrangements of the individual impurity atoms that donate charge to the silicon9. Such enhancements could include reproducibility of control parameters for different devices, improved reliability of operation at higher temperatures and reduced energy consumption. </body>
<date id = '311'>15 January 2020</date>
<url id = '312'>https://nature.com/articles/d41586-019-03951-0</url>
<title id = '312'>Two threads of research in the quest for methods that predict the 3D structures of proteins from their amino-acid sequences have become fully intertwined. The result is a leap forward in the accuracy of predictions.</title>
<body id = '312'>Proteins perform or catalyse nearly all chemical and mechanical processes in cells. Synthesized as linear chains of amino-acid residues, most proteins spontaneously fold into one or a small number of favoured three-dimensional structures. The sequence of amino acids specifies a protein’s structure and range of motion, which in turn determine its function. Over decades, structural biologists have experimentally determined thousands of protein structures, but the difficulty of these studies has made the promise of a computational approach for predicting protein structure from sequence alluring. Writing in Nature, Senior et al.1 describe an algorithm, AlphaFold, that takes a leap forward in solving this classic problem by bringing to bear modern machine-learning techniques.   The diversity of protein structures precludes the possibility of obtaining simple folding rules, making structure prediction difficult. Protein folding is ultimately driven by quantum mechanics. Were it possible to compute the exact energy of protein molecules from quantum theory, and to do so for every possible conformation, then predicting a protein’s most energetically favoured structure would be easy. Unfortunately, a quantum treatment of proteins is computationally intractable (quantum computers might change this), and the total set of possible conformations that any protein can take is astronomical, prohibiting such a brute-force approach. This has not stopped scientists from attempting a direct attack on the problem. Physical chemists have devised tractable, but approximate, energy models for proteins2, and computer scientists have developed ways to explore protein conformations3. Much progress has been made on the first problem but the second has proved more recalcitrant. The set of shapes that a protein might take can be likened to a landscape: different locations in the landscape correspond to different shapes, with nearby locations having similar shapes. The height of a location corresponds to how energetically favourable the associated shape is, with the lowest point being the most favoured. Natural proteins evolved to have funnel-shaped landscapes that enable newly synthesized proteins, jostled by the thermal fluctuations of the cell, to cross the landscape and find their way to a favoured conformation in physiologically relevant timescales (milliseconds to minutes)4. Algorithms can search the landscape to find favoured conformations by following the landscape’s inclination, but the ruggedness of the terrain causes them to get stuck in troughs and valleys far from the lowest basin.   The course of the structure-prediction field changed nearly a decade ago with the publication of a series of seminal papers5–7 exploring the idea that the evolutionary record contains clues about how proteins fold. The idea is predicated on the following premise: if two amino-acid residues in a protein are close together in 3D space, then a mutation that replaces one of them with a different residue (for example, large for small) will probably induce, at a later time, a mutation that alters the other residue in a compensatory direction (in our example, swapping small for large). The set of co-evolving residues therefore encodes valuable spatial information, and can be found by analysing the sequences of evolutionarily related proteins. By transforming this co-evolutionary information into a matrix known as a binary contact map, which encodes which residues are proximal, the set of conformations that merit consideration by algorithmic searches can be restricted. This in turn makes it possible to accurately predict the most favourable protein conformation, especially for proteins for which many evolutionarily related sequences are known. The idea was not new8, but the rapid growth in available sequence data in the early 2010s, coupled with crucial algorithmic breakthroughs, meant that its time had finally come. Co-evolutionary analysis has been responsible for most progress in protein-structure prediction in the past few years, but it has not obviated the need for algorithms to search the energy landscapes of proteins: binary contact maps constrain the search space, but do not pin down a single 3D structure. Furthermore, the mathematics underpinning the conversion of co-evolutionary data into contact maps is restricted by the types of input used and the output generated. The initial injection of deep learning (a type of machine learning) into co-evolutionary analyses improved matters by incorporating richer inputs9. AlphaFold takes things a step further by changing the outputs.   In lieu of binary contact data, AlphaFold predicts the probabilities of residues being separated by different distances. Because probabilities and energies are interconvertible, AlphaFold predicts an energy landscape — one that overlaps in its lowest basin with the true landscape, but is much smoother. In fact, AlphaFold’s landscape is so smooth that it nearly eliminates the need for searching. This makes it possible to use a simple procedure to find the most favourable conformation, rather than the complex search algorithms employed by other methods. The idea that a complex search could be unnecessary for structure prediction is, in hindsight, unsurprising. Mathematically, the distances between points determine their relative locations. Predictions of distances can therefore predict structure. Moreover, relatively simple models of protein energy landscapes known as Gō potentials, in which experimentally determined distances between residues are favoured, can lead to protein-folding pathways that resemble ones experienced by real proteins10. This suggests that proteins fold more like simple origami than like an intricate knot — all parts can come together at once. My own work has shown that folding can be predicted implicitly using a deep-learning model without searching11, and minimal search procedures have also been embedded within another deep-learning model to predict protein structures12. What is notable about AlphaFold is that it predicts distances with sufficient accuracy to outperform state-of-the-art search methods (Fig. 1). Senior et al. used advances in deep learning to extract as much structural information as possible from protein sequences. The resulting algorithm outperformed all entrants at the most recent blind assessment of methods used to predict protein structures (the CASP13 event), generating the best structure for 25 out of 43 proteins, compared with 3 out of 43 for the next-best method. AlphaFold’s predictions had a median accuracy of 6.6 ångströms on this set of proteins — that is, for the middle-ranked protein in this set, the atoms in the proposed structures were on average 6.6 Å away from their actual positions. Figure 1 | Predictions of protein structures. Senior et al.1 report a machine-learning system called AlphaFold, which predicts the 3D structures of proteins from their amino-acid sequences. Template modelling (TM) scores measure how well a predicted structure matches the overall shape of the actual structure, on a scale from 0 to 1. TM scores for AlphaFold were better than those of other prediction systems for 25 out of 43 proteins in a blind test. Here, the TM scores for AlphaFold (red) are compared with those of other prediction systems (grey) in the blind test for six proteins whose 3D structures could be modelled only on the basis of their amino-acid sequences — no 3D structures of proteins that have similar amino-acid sequences were available to use as a starting point for modelling. AlphaFold made the most accurate predictions for five of these six proteins. (Adapted from Fig. 1b of ref. 1.) Challenges remain. AlphaFold is not yet accurate enough for most applications, such as working out the catalytic mechanisms of enzymes or how drugs bind to proteins (which both typically require 2–3 Å resolution). And although AlphaFold’s search procedure is much simpler than most modern methods, it can still be slow, taking tens to hundreds of hours to make a single prediction. For applications such as protein design, which require the structures of many different protein sequences to be modelled, the lack of speed is an impediment. Nevertheless, this is a watershed moment for the field. Given continued growth in the number of available protein sequences, it is possible that the coarse structures (about 4 Å resolution) of most proteins that consist of a single folded domain will become available in the next five years from structure predictions. Such broad availability of structural information might transform the life sciences, just as sequence information did in the preceding decades. This could mean that, combined with the rapid advances in protein–structure determination enabled by cryo-electron microscopy, we are entering a golden age of structural biology — one that makes possible a quantitative and mechanistic basis for the life sciences, broadly grounded in firm structural hypotheses. </body>
<date id = '312'>15 January 2020</date>
<url id = '313'>https://nature.com/articles/d41586-020-00038-z</url>
<title id = '313'>It has been difficult to make transparent materials that have extremely high piezoelectricity — a useful property related to the coupling of electric fields and mechanical strain. This hurdle has now been overcome.</title>
<body id = '313'>Piezoelectric materials show high electromechanical coupling, which means that they can generate large strains if an electric field is applied to them, and can transform external mechanical stimuli into electric charge or voltage1. They are widely used in electronic applications, including sensors, small motors and actuators — devices that convert electrical energy into movement. In addition, their high energy efficiency and ease of miniaturization are driving the development of new technologies, such as energy harvesters for the growing network of Internet-connected devices known as the Internet of Things, actuators for touch screens and microrobots. Writing in Nature, Qiu et al.2 report the preparation of high-performance piezoelectrics that have the long-desired property of near-perfect transparency to light. This breakthrough could lead to devices that combine excellent piezo-electricity with tunable optical properties.   Most high-performance piezoelectrics are ferroelectrics — materials that have a spontaneous electric polarization that can be reversed by the application of an external electric field. At the atomic level, ferro-electrics have a local polarization that is caused by the displacements of certain ions from their symmetric positions. Regions that have a uniform direction of polarization are referred to as ferroelectric domains and are separated by boundaries called domain walls. The material’s crystal structure determines the possible directions of polarization and, in turn, the types of domain wall. For example, a rhombo-hedral crystal structure enables 8 possible domain variants and 71°, 109° and 180° domain walls (where the angle refers to the difference in polarization direction between the domains separated by the wall). For these ferroelectrics to be used as piezo-electrics, they must first undergo a process known as poling, in which an external electric field is applied to the material to reorient unfavourably oriented domains and induce macroscopic polarization. Poled ferroelectrics show a large electromechanical response to an external electric field or to mechanical force, and this response is typically characterized by a quantity dubbed the piezoelectric co--efficient. The magnitude of this coefficient depends to a large extent on the domain configuration; the other major contribution comes from the crystal lattice. Some of the largest coefficients known today were reported3,4 for ferroelectric crystals based on lead magnesium niobate–lead titanate (PMN-PT). These crystals have piezoelectric coefficients above 1,500 picocoulombs per newton (pC N–1), which is about ten times higher than those of most other ferroelectrics. The poling process is conventionally carried out using direct-current (d.c.) electric fields. In rhombohedral PMN-PT crystals that are [001] oriented (a particular crystallographic orientation), d.c. poling results in the removal of 180° domain walls and the formation of a laminar (layered) domain structure consisting of 71° and 109° walls (Fig. 1a). When light propagates through such a structure, the difference between the refractive indices at each side of an encountered 71° wall induces scattering, resulting in the poled crystal having an overall opaque appearance. The 109° walls in this configuration, however, do not give rise to scattering. Figure 1 | Near-perfect light transmittance in a high-performance piezoelectric. a, Materials known as ferroelectrics contain regions of uniform electric polarization called domains (the different shades represent different orientations of polarization). These domains are separated by boundaries dubbed domain walls. When [001]-oriented rhombohedral ferroelectric crystals of lead magnesium niobate–lead titanate (PMN-PT) undergo a process called poling using direct-current (d.c.) electric fields, they contain both 71° and 109° domain walls (where the angle indicates the difference in polarization orientation between the domains separated by the wall). The 71° walls cause incident light to be scattered, such that the crystals are opaque. b, Qiu et al.2 demonstrate that, when the poling is carried out using alternating-current (a.c.) electric fields, the number of 71° domain walls is greatly reduced. The crystals show near-perfect light transmittance and ultrahigh piezoelectricity — a property associated with the coupling of electric fields and mechanical strain. Qiu and colleagues exploited this difference in light scattering between 71° and 109° domain walls. The authors simulated the evolution of domains in [001]-oriented rhombohedral PMN-PT crystals that were subjected to either d.c. or alternating-current (a.c.) electric fields. These simulations showed that the application of an a.c. poling field (a method reported only in the past few years5,6), instead of a d.c. one, greatly reduces the number of 71° domain walls (Fig. 1b). Qiu et al. attributed this effect to a process referred to as domain swinging, whereby the 71° walls alternate between two crystallographic planes and tend to merge, thereby decreasing their number. The 109° walls, by contrast, remain almost unaffected by the a.c.-poling process.   The authors then compared their simulation results with d.c.- and a.c.-poled [001]-oriented rhombohedral PMN-PT crystals that had been grown by a modified version of a widely used approach called the Bridgman method. They studied the domain structure of the crystals using three techniques (high-resolution X-ray diffraction, polarized-light microscopy and birefringence imaging spectroscopy), and confirmed the removal of 71° domain walls throughout the a.c.-poled samples. These samples exhibited near-perfect light transmittance, large birefringence (an effect in which a material’s refractive indices are different along different axes) and ultrahigh piezoelectric coefficients, exceeding 2,100 pC N–1. For comparison, ferro-electrics that have a similar level of transparency to these crystals, such as lithium niobate or polyvinylidine fluoride, typically have piezo-electric coefficients1 of less than 40 pC N–1. The report by Qiu et al. adds the optical component to high-performance piezo-electric crystals and therefore opens the door to the design of electro-optical-mechanical devices. Transparent actuators or motors could be used for touch screens in consumer electronics or for the development of invisible microrobots. Moreover, the crystals’ high electro-mechanical performance and transparency could be harnessed in an imaging technology known as photoacoustic imaging or in piezoelectric light guides. Although the authors demonstrated the applicability of their approach to several PMN-based crystals, it remains to be seen whether similar principles can be applied to other ferro-electric systems. Of particular interest are systems that remain polarized at high temperatures (above 100–150 °C), unlike PMN-based materials. Nevertheless, Qiu and colleagues’ discovery has arrived just in time to meet the growing demand for multitasking smart materials and hybrid devices. </body>
<date id = '313'>15 January 2020</date>
<url id = '314'>https://nature.com/articles/d41586-020-00040-5</url>
<title id = '314'>How Nature reported the discovery of ancient human bones in Australia in 1970, and how the First World War revolutionized the production of maps of France.</title>
<body id = '314'> The oldest human bones yet found in Australia have now been sufficiently studied for preliminary pronouncements on their significance. They were unearthed accidentally last year by a geologist working between Mildura and Ivanhoe in south-western New South Wales. Archaeologists then excavated the site. The bones are those of a young woman who had been cremated and buried. They were found in association with stone tools and the remains of fish, animals and eggs, evidently used for food. In Canberra last week, one of the archaeologists who took part in the excavation, Mr Rees Jones, … revealed that the finds were about twice the age of a skull discovered in Queensland which until now has been the oldest human relic known in Australia. The New South Wales skeleton is dated at between 25,000 and 32,000 years old. The later Queensland skull showed characteristics that are a blend of the modern Aborigine and of Java Man who flourished a quarter to half a million years ago. Of particular interest is the evidence the new find brings to the uncertain question of when human beings first reached Australia. From Nature 17 January 1970 The value of large-scale maps in war is the subject of an unsigned article in La Géographie … on the Service Géographique of the French Army. This Service was practically created by the war, when it was realised that the available maps of France were on too small a scale to be of use. Maps on scales of 1/80,000 and 1/200,000, although valuable for war in the open, were unsatisfactory for trench warfare. Large-scale plans were available only for the neighbourhood of Paris and certain fortified places. It was decided to make maps of the war area on a scale of 1/20,000, 1/10,000, and 1/5000 … Of these the smallest scale was for artillery use, the second for Staff work in general, and the largest scale, confined to front-line areas, for infantry use. Generally speaking, the 1/20,000 proved to be the most useful. It is hoped that this will be extended to the whole of France and be periodically revised. From Nature 15 January 1920 Latest on: Archaeology News 19 MAY 20 Article 11 MAY 20 Research Highlight 23 APR 20 Geography Research Highlight 04 MAY 20 News & Views 04 FEB 20 News & Views 21 JAN 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '314'>14 January 2020</date>
<url id = '315'>https://nature.com/articles/d41586-019-03918-1</url>
<title id = '315'>Semiconductors known as halide perovskites have remarkable optoelectronic properties, but their structural instability limits practical applications. A solution has been found that involves squeezing the compounds’ crystal lattices.</title>
<body id = '315'>The surprising discovery1 in 2009 that compounds known as halide perovskites can convert sunlight into electricity triggered a revolution in photovoltaics (solar cells), inspiring new cell designs that will enable solar energy to be harnessed efficiently and at low cost. However, tuning the properties of these crystals to enable practical applications has been a long-standing challenge. Writing in Nature, Chen et al.2 report that a solution to this problem has finally been found.   Discovering new materials and identifying appropriate applications can sometimes be achieved serendipitously, but often takes decades, and has historically required centuries or even millennia. The first halide perovskite was discovered in the 1890s3, but its potential remained untapped until a decade ago1, when it took photovoltaics by storm. As its name suggests, halogen atoms in the compound enable the formation of a cubic (or pseudo-cubic) array known as a perovskite structure. Harnessing the full potential of halide perovskites for technological applications has been difficult. A major obstacle is the tendency of one of the best-performing perovskite crystals, α-formamidinium lead iodide (HC(NH2)2PbI3, known as α-FAPbI3), to assume a hexagonal structure at room temperature (Fig. 1a) — the approximate temperature at which photovoltaic devices operate. This hexagonal structure cannot respond to most of the frequencies of light in solar radiation, and hence is not of interest for technological applications. It would therefore be helpful to stabilize the structure of α-FAPbI3. Figure 1 | The stabilization and strain engineering of a semiconductor. a, The semiconductor α-formamidinium lead iodide (HC(NH2)2PbI3, known as α-FAPbI3) belongs to the halide perovskite family of compounds, and has potential applications in solar cells. However, its cubic perovskite lattice is unstable, and converts to a hexagonal form that is unsuitable for practical applications. The octahedra represent subunits of the lattices: iodine atoms at the vertices surround a central lead atom; (HC(NH2)2)+ ions fill the gaps between octahedra, but are not shown. b, Chen et al.2 report that the structure of α-FAPbI3 can be stabilized by growing it on a stable halide perovskite (the substrate) that has an analogous structure, so that the atoms in the two lattices align. Because the lattice dimensions of the substrate are smaller than those of α-FAPbI3, the crystal lattice of the latter is squeezed (put under strain). This strain increases the mobility of electrical charge carriers in the material. Several strategies can be used to engineer a material’s properties. Two of the most effective are to alter the material’s composition or to use it at high or low temperatures, but the costs involved for commercial applications can be tremendous. Scientists have therefore developed a simple but extremely useful approach known as strain engineering, which has been used to tune the electronic properties of semiconductors. When a crystal is compressed or stretched, the resulting deformation is called strain. Strain is calculated by dividing the change of length of a deformed object by the object’s original length, and is expressed as a percentage of the original length. In semiconductors, strain can alter the mobility of charge carriers — a property that characterizes how fast a charge carrier, such as an electron, can travel in a crystal subjected to an electric field. By changing the charge mobility, the electronic properties of a semiconductor can be altered. Semiconductors in modern electronic devices are most commonly used as thin films, and the maximum sustainable strain of a film is less than a few per cent4. Nevertheless, strain modulation can be a highly effective tool for enhancing electronic properties. One such success story is that of lasers based on layered semiconductor structures known as quantum wells. By straining the quantum-well layers, the electrical current needed to power a laser can be reduced by as much as ten times, thus improving the energy efficiency of the devices5,6. Most commercial quantum-well lasers are therefore strained5. One of the most interesting but problematic features of halide perovskites is that only a few, including α-FAPbI3, have high charge mobility and absorb light strongly, but the technologically useful crystal structures of these few compounds are unstable — some can spontaneously transform into other phases in less than a second. In photovoltaic devices, fast-moving charge carriers and strong light absorption are both needed to convert solar energy into electricity with high efficiency. Unfortunately, structural stability, high charge mobility and the ability to absorb light strongly don’t seem to coexist in perovskite halides. Chen et al. have therefore used strain engineering to tackle this problem. They grew crystalline α-FAPbI3 from a solution so that it formed on another, more stable halide perovskite (the substrate). The FAPbI3 atoms in the growing crystal align with the cubic structure of the atoms in the substrate, thereby forming a pseudocubic structure themselves (Fig. 1b). The alignment of atoms in different materials is called epitaxy. This epitaxy locks α-FAPbI3 into the pseudocubic structure as a result of the strong chemical forces between it and the substrate, preventing its transformation into the undesirable hexagonal structure. The authors find that the pseudocubic structure remains stable for at least a year at room temperature. A compressive strain is imposed on the α-FAPbI3 film because the dimensions of the cubic array of the substrate are different from those of the natural atomic array of α-FAPbI3. Chen and colleagues were therefore able to control the strain of α-FAPbI3 from 0 to 2.4% compressive deformation by growing FAPbI3 on substrates that have different lattice dimensions. The authors found that this squeezing of the α-FAPbI3 crystal increases the mobility of positively charged quasiparticles called holes, which correspond to the absence of electrons in the crystal. The authors attribute this increased mobility to the modification of the electronic structure of the crystal under compressive strain: compression leads to faster oscillations of the holes’ wavefunctions, speeding up the movement of charge wavepackets (superpositions of wavefunctions) and thus producing higher charge mobility. Previous work on the strain engineering of halide perovskite films lacked strain control7 or involved straining methods that are harder to use8,9. By contrast, Chen and colleagues’ study provides an extremely accessible and practical avenue through which to explore and use the physical properties of strained halide perovskites. Questions remain about how the authors’ findings could be used in solar cells. Currently available halide perovskite photovoltaic devices do not contain a genuinely epitaxial substrate, and so new cell designs will be needed to make use of the reported discovery. But a range of halide perovskite compounds are available that have similar atomic arrays to α-FAPbI3, and which exhibit many different technologically important electronic properties. Chen and co-workers’ study therefore suggests that there is plenty of scope for designing and developing epitaxial quantum-well devices using these materials, by mimicking the way in which quantum-well devices were developed using semiconductors from the III–V family of materials. This might bring down the cost of manufacturing these devices. Finally, it will be interesting to see whether crystals of halide perovskites can be grown with sufficient atomic precision to make superlattices — periodic structures that contain multiple layers of two or more materials. The use of halide perovskites in superlattices could open up otherwise inaccessible electronic band structures, thereby allowing a rich array of physics to be explored, and emerging quantum-well devices to be further developed. </body>
<date id = '315'>08 January 2020</date>
<url id = '316'>https://nature.com/articles/d41586-019-03892-8</url>
<title id = '316'>A subpopulation of adaptive immune cells patrols the brain and cerebrospinal fluid in people who have Alzheimer’s disease. This discovery should broaden our understanding of how the immune system can influence neurodegeneration.</title>
<body id = '316'>For decades, research into Alzheimer’s disease has centred on neurons. Only in the past few years have scientists identified a role for immune cells in the progression of this neurodegenerative disorder1. Most research has focused on the nonspecific, innate branch of the immune system. But writing in Nature, Gate et al.2 report that an immune-cell subpopulation belonging to the adaptive immune system — which remembers and responds to specific foreign invaders — might also have a role in Alzheimer’s disease.   The authors isolated and analysed immune cells from the blood of healthy people and people who had Alzheimer’s disease or a precursor of the disease known as mild cognitive impairment (MCI). They discovered an immune-cell subpopulation called CD8+ T effector memory CD45RA+ (TEMRA) cells that was associated with MCI and Alzheimer’s disease. TEMRA cells have previously been linked to immunological memory, and they release inflammatory and cytotoxic (cell-death-promoting) molecules3. Analysis of a separate cohort of people who had Alzheimer’s disease revealed that an increased presence of TEMRA cells in the blood was associated with compromised cognitive performance. This finding could indicate that TEMRA cells contribute to neuronal dysfunction by secreting inflammatory and cytotoxic molecules in the brain (Fig. 1). Alternatively, a damaging mechanism that causes cognitive dysfunction might also elicit an inflammatory TEMRA-cell response in the blood. Figure 1 | TEMRA cells and age-related neurodegeneration. Gate et al.2 report that the presence of immune cells called CD8+ T effector memory CD45RA+ (TEMRA) cells in the brain is associated with Alzheimer’s disease. a, Evidence from a few people suggests that the cells are activated by binding between the T-cell receptor (TCR) and an antigen molecule (which can be from a host cell or a foreign invader). The cells then proliferate to produce an expanded pool of TEMRA-cell clones. The cells patrol the cerebrospinal fluid (CSF). b, TEMRA cells might promote neuronal damage indirectly, by releasing inflammatory and cytotoxic (death-promoting) molecules, or directly, by physically interacting with and severing neuronal processes, causing the formation of structures called spheroids that are associated with Alzheimer’s disease. Alternatively, they might have no role in disease progression (not shown). Gate et al. corroborated their findings in vitro, showing that stimulation with an inflammatory molecule caused immune cells from people who had MCI or Alzheimer’s disease to release more interferon-γ (a key pro-inflammatory protein) than did immune cells from people who did not have these conditions. This is consistent with another study4, which demonstrated that T cells derived from people who have Alzheimer’s disease become more active than do those from healthy people when exposed to β-amyloid, a protein associated with this disorder. The authors then asked whether the presence of TEMRA cells could be used to predict disease status. Indeed, a machine-learning algorithm could use measurements of TEMRA cells (together with information about other immune-cell populations) to distinguish between healthy people and those with MCI or Alzheimer’s disease with about 80% accuracy. Many immune processes alter during ageing and so are of limited use for predictive clinical testing, but age did not influence the level of TEMRA cells. This type of technique, once refined, might therefore be used alongside biomarkers of neuronal damage and degeneration for blood-based diagnostic tests, improving our ability to detect Alzheimer’s disease at an early stage. Next, Gate and colleagues analysed the brains of people who had died with Alzheimer’s disease. This revealed CD8+ T cells (which might be TEMRA cells) in the perivascular space around the brain’s blood vessels, and at sites of β-amyloid deposition, as previously reported for T cells in Alzheimer’s disease5,6. CD8+ T cells are known to physically contact and sever neuronal processes, causing structures called neuritic spheroids to form nearby — another hallmark of Alzheimer’s disease7. Thus, it is conceivable that TEMRA cells contribute to neuronal damage not only by secreting immune molecules, but also by directly damaging neuronal processes7. MCI and Alzheimer’s disease are associated with changes in the number and proportion of T cells in the cerebrospinal fluid (CSF) that surrounds the brain and spinal cord8,9. The investigators therefore asked whether TEMRA cells were found in the CSF and whether there was evidence of ‘clonal expansion’ of this cellular subpopulation. Naive T cells each have different T cell receptor (TCR) proteins, but when the receptor is stimulated by a particle called an antigen, the cell proliferates to form clones of itself. The presence of more than one cell with the same TCR therefore indicates clonal expansion — a sign that T cells have been activated previously. The authors sequenced TCRs from an independent cohort of people and identified several T-cell clones, including TEMRA cells, in people with Alzheimer’s disease. This is perhaps the first evidence that clonally expanded T cells invade the CSF in age-related neurodegenerative diseases.   Gate et al. then validated their result using gene-expression analysis. This revealed that the TEMRA-cell population was the predominantly expanded T-cell clone in each person who had Alzheimer’s disease. The population expressed various cytotoxic genes, and was enriched in the hippocampus — a brain region crucial for human memory. In line with this observation, hippocampal T-cell infiltration promotes cognitive decline in a mouse model of Alzheimer’s disease10. The authors also found evidence for TEMRA-cell clones and gene-expression changes in the CSF of people who had another neurodegenerative disorder, Parkinson’s disease, highlighting the possibility that different age-related neurodegenerative diseases share similar molecular underpinnings. Which antigens drive clonal expansion of TEMRA cells? By comparing TCR sequences from people who had MCI and Alzheimer’s disease, the investigators found evidence that clonally expanded TEMRA cells had been bound by two antigens produced by a virus of the herpes family, Epstein–Barr virus (EBV). However, it is important to note that a role for EBV infection in neurodegeneration has not yet been reported, and Gate et al. make no suggestion that EBV is involved in the development of Alzheimer’s disease. Gate and colleagues’ data involve only a few patients and should be interpreted with great care, particularly given that EBV infects about 95% of people during early life11. Previous work12 has shown a complex relationship between herpes viruses and Alzheimer’s disease in mice. One the one hand, β-amyloid fibres can entrap herpes viruses, extending survival in mouse models of Alzheimer’s disease. But on the other hand, virus infection strongly increases β-amyloid deposition in these animals. In addition, a study11 of 85 people who had Alzheimer’s disease found evidence of EBV DNA in the brains of only 6% of cases. All of these people carried the gene APOE4, which is associated with a high risk of Alzheimer’s disease and could explain why they developed the disorder. The same study did find that antibody responses against EBV increased during cognitive deterioration and progression of Alzheimer’s disease11. However, these responses are quite common in older people. Moreover, a recent meta-analysis found no correlation between herpes-virus infection and dementia13. Longitudinal studies involving many more people will be needed before solid conclusions can be drawn. It will be interesting to reconcile Gate and colleagues’ data with the finding14 that T cells can restrain cognitive deficits in mouse models of Alzheimer’s disease. Analysis of less-prominent T-cell clones in people with and without disease might reveal other, potentially harmful — or even protective — subclones. In addition, the current study will no doubt renew efforts to define the crosstalk between innate and adaptive immunity in general, as well as in neurodegeneration. Perhaps, in the future, these interactions could be harnessed for diagnostic purposes or to develop therapeutic interventions. </body>
<date id = '316'>08 January 2020</date>
<url id = '317'>https://nature.com/articles/d41586-019-03947-w</url>
<title id = '317'>How Nature reported the establishment of a wildlife sanctuary in the Pacific Ocean in 1920, and a wry analysis of whether procreation causes heart attacks.</title>
<body id = '317'> Sir,—Last night in the bath we were considering your distressing item about the incidence of fatal heart attacks during the act of procreation. On reflexion, we found that 0·6 per cent of one’s time is about one hour per week. Surely, Sir, this is a moderate—nay, a very moderate—estimate of the time the reasonable man would devote to this pursuit. Some indeed would regard it as a gross under-estimate. It follows that the act may not predispose to coronary infarction. On the contrary, your figures could be construed as showing a protective influence. Is it not time for the minister to consider setting up facilities for the study of this branch of preventive medicine? From Nature 10 January 1970 We note with much satisfaction that the Government of New Zealand has extended the absolute protection of seals in the area under its control for a period of three years, and that the Prime Minister of Tasmania has decided not to renew the lease of Macquarie Island to the company which so mercilessly exploited the wild life over which it had obtained control. Quite apart from the hideousness of the methods of slaughter, this protection has barely come in time to save these creatures from extermination. Although this danger has, time and again, been pointed out, the authorities allowed commercial interests to prevail. Yet the penguins, seals, sea-lions, and sea-elephants which contrived to maintain a hold on life in those inhospitable regions represent types of animal life which it was our bounden duty to preserve. A hope has been expressed that Macquarie Island may be set apart as an inviolable sanctuary for Antarctic life, and we trust not only that this will be done, but also that steps will be taken to guard against marauders who may be tempted to make occasional raids for the sake of the profits to be gained. To this end the island might be used for the purposes of a biological and meteorological station. Before it is too late we hope that the matter of protection for the whales in the Antarctic seas will also speedily find a place on the Statute-book. The subject has been long under consideration, but as yet nothing has been done on account of the opposition of commercial interests. From Nature 8 January 1920 Latest on: Cardiovascular biology News & Views 06 MAY 20 Article 25 MAR 20 News & Views 25 MAR 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '317'>07 January 2020</date>
<url id = '318'>https://nature.com/articles/d41586-019-03894-6</url>
<title id = '318'>Astronomical signals called fast radio bursts remain enigmatic, but a key discovery has now been made. A second repeating fast radio burst has been traced to its host galaxy, and its home bears little resemblance to that of the first.</title>
<body id = '318'>Millisecond-duration, extragalactic radio flashes called fast radio bursts1,2 (FRBs) present many puzzles that are strikingly similar to past mysteries concerning phenomena now known as quasars and γ-ray bursts. Like FRBs, these phenomena seemed to be uniformly distributed in the sky, but of almost impossible luminosity if cosmological in origin. Ultimately, it was revealed that quasars are associated with supermassive black holes, and γ-ray bursts with the formation of black holes after supernovae (stellar explosions) or with mergers of stellar remnants called neutron stars. Repeating FRBs provide the opportunity to identify the homes and possible progenitors of FRB sources. The first known repeater was localized to a star-forming region in a tiny, metal-poor galaxy3. Writing in Nature, Marcote et al.4 pinpoint the home of a second repeater and find it to be very different.   As an FRB travels through the ionized intergalactic medium, its radio waves interact with free electrons and are slowed. High-frequency waves are less affected than low-frequency ones, so the former arrive at an FRB detector slightly sooner than the latter. This time difference is proportional to the total number of electrons between the FRB source and the detector, and can therefore be used to estimate the enormous distance to the source. Early FRB detectors could not un--ambiguously identify an FRB host galaxy because their localization resolutions spanned hundreds or even thousands of potential host galaxies. But now, multi-element radio telescopes such as the Australian Square Kilometre Array Pathfinder (ASKAP) and the Deep Synoptic Array ten-antenna prototype (DSA-10) in California have high enough resolution that host galaxies can be easily identified. At the same time, a radio telescope known as the Canadian Hydrogen Intensity Mapping Experiment5 (CHIME) is revealing that many FRBs repeat. The individual bursts of a repeater tend to be clustered in time, which means that the probability of catching a burst is higher when a repeater is active. Because CHIME surveys the sky daily, it is in an ideal position to trigger other telescopes to carry out localizations. Marcote and colleagues caught four bursts of one of the repeaters discovered by CHIME and used these observations to determine the precise location of the repeater in a nearby galaxy (Fig. 1). This galaxy is about 200 times farther from Earth than is our nearest neighbour, the Andromeda galaxy. Figure 1 | Localization of a repeating fast radio burst. Fast radio bursts (FRBs) are bright, extragalactic radio flashes of unknown physical origin. Marcote et al.4 have determined the precise location of a source of repeating FRBs (indicated by the red circle). Unlike the previous localization of a repeating FRB3, this source was found in a fairly ordinary, massive spiral galaxy. The image is taken using optical light, and a higher-contrast zoom-in of the relevant region of the galaxy is included. (Image from ref. 4.) The first known repeater differed in at least three ways from many of the one-off FRBs. First, it showed peculiar changes in intensity as a function of frequency and time, akin to intense solar radio emissions called solar radio bursts. Second, it traversed a highly magnetized plasma that led to a twist in its polarization as a function of frequency. And third, it was emitted near a super-massive black hole. Although one-off FRBs are harder to study than are repeaters, they are known to sometimes have complicated, multi-component temporal profiles6, similar to the emissions from magnetars (highly magnetized neutron stars). Could it be that one-off FRBs and repeaters are like short- and long-duration γ-ray bursts — two different classes of phenomenon that just happen to have a similar emission time-scale and luminosity? Or is there a theory of FRBs that would link the two classes? Most FRB theories involve a compact astronomical object, and emissions that are associated with extremely powerful magnetic fields or rapidly spinning (and therefore energetic) neutron stars. Some scientists, desperate for an explanation, have even proposed alien transmissions7. It is possible to envisage a unifying model in which FRB sources have an energy reservoir (whether magnetic, kinetic or gravitational) that is plentiful when the source is produced near a star-forming region, causing frequent bursts. As this reservoir becomes depleted, the bursts are separated by increasingly long gaps, until they cease altogether. Such behaviour is similar to that of ‘glitches’ (sudden changes in spin frequency) in a type of neutron star called a pulsar. Young pulsars often glitch prolifically, but older pulsars can go for decades without exhibiting a glitch. Frequently repeating FRBs would be easier to find than those that have a lower repetition rate, but verifying that they are repeaters would become increasingly difficult as their repetition timescales approached the lifetime of FRB experiments (currently years). The first known repeater seemed to fit such a model. It could have been a young magnetar emitting from a relatively exotic location — near a supermassive black hole in the star-forming region of a small, metal-poor galaxy. Such an environment is very different from that of most young neutron stars in our own Galaxy. However, Marcote and colleagues found that the source of their repeater is not particularly magnetic, is nowhere near a supermassive black hole and is in a fairly normal galaxy. Even so, the source is close to a curious, V-shaped region of stars that only adds to the mystery of FRBorigins (Fig. 1). In the past year, ASKAP has localized several one-off FRBs8,9 and found them to inhabit a wide range of galaxy types. The beauty of the ASKAP results and those of Marcote et al. lies in the accuracy and reliability of the localizations. These features are not available to relatively compact telescopes10, such as DSA-10, which can only hint at the identity of the relevant galaxy (think of this as the FRB’s home city), rather than pinpointing the exact environment within it (the FRB’s home address). Ultimately, we will discover that, for each class of FRB, either these signals are produced near active sites of star formation or there is no particular correlation. If they originate near the sites of supernovae, the FRBs probably arise from young, highly magnetic neutron stars. If their origins are farther from such sites, they might be produced by time bombs of gravitational energy (such as merging neutron stars) and have diverse spatial distributions similar to those of short-duration γ-ray bursts, which are now known to be associated with neutron-star coalescence. Curiously, because the luminosity of a typical quasar is not dissimilar to the peak luminosity of an FRB, we cannot yet completely discount the involvement of supermassive black holes. But Marcote and colleagues’ results argue against that possibility, and the timescales for quasar variability are more likely to be days to months than milliseconds. Rates of FRB discovery and host-galaxy identification are likely to greatly increase as more detectors come online. Results from CHIME are widely anticipated, and facilities such as the MeerKAT radio telescope in South Africa and the Five-hundred-meter Aperture Spherical radio Telescope (FAST) in China will soon come online. Do not be surprised if there is more than one way to generate FRBs — without the need for aliens. </body>
<date id = '318'>06 January 2020</date>
<url id = '319'>https://nature.com/articles/d41586-019-03822-8</url>
<title id = '319'>Could artificial intelligence improve the accuracy of screening for breast cancer? A comparison of the diagnostic performance of expert physicians and computers suggests so, but the clinical implications are as yet uncertain.</title>
<body id = '319'>Screening is used to detect breast cancer early in women who have no obvious signs of the disease. This image-analysis task is challenging because cancer is often hidden or masked in mammograms by overlapping ‘dense’ breast tissue. The problem has stimulated efforts to develop computer-based artificial-intelligence (AI) systems to improve diagnostic performance. Writing in Nature, McKinney et al.1 report the development of an AI system that outperforms expert radiologists in accurately interpreting mammograms from screening programmes. The work is part of a wave of studies investigating the use of AI in a range of medical-imaging contexts2.   Despite some limitations, McKinney and colleagues’ study is impressive. Its strengths include the large scale of the data sets used for training and subsequently validating the AI algorithm. Mammograms for 25,856 women in the United Kingdom and 3,097 women in the United States were used to train the AI system. The system was then used to identify the presence of breast cancer in mammograms of women who were known to have had either biopsy-proven breast cancer or normal follow-up imaging results at least 365 days later. These outcomes are the widely accepted gold standard for confirming breast cancer status in people undergoing screening for the disease. The authors report that the AI system outperformed both the historical decisions made by the radiologists who initially assessed the mammograms, and the decisions of 6 expert radiologists who interpreted 500 randomly selected cases in a controlled study. McKinney and colleagues’ results suggest that AI might some day have a role in aiding the early detection of breast cancer, but the authors rightly note that clinical trials will be needed to further assess the utility of this tool in medical practice. The real world is more complicated and potentially more diverse than the type of controlled research environment reported in this study. For example, the study did not include all the different mammography technologies currently in use, and most images were obtained using a mammography system from a single manufacturer. The study included examples of two types of mammogram: tomosynthesis (also known as 3D mammography) and conventional digital (2D) mammography. It would be useful to know how the system performed individually for each technology. The demographics of the population studied by the authors is not well defined, apart from by age. The performance of AI algorithms can be highly dependent on the population used in the training sets. It is therefore important that a representative sample of the general population be used in the development of this technology, to ensure that the results are broadly applicable. Another reason to temper excitement about this and similar AI studies is the lessons learnt from computer-aided detection (CAD) of breast cancer. CAD, an earlier computer system aimed at improving mammography interpretation in the clinic, showed great promise in experimental testing, but fell short in real-world settings3. CAD marks mammograms to draw the interpreter’s attention to areas that might be abnormal. However, analysis of a large sample of clinical mammography interpretations from the US Breast Cancer Surveillance Consortium registry demonstrated that there was no improvement in diagnostic accuracy with CAD3. Moreover, that study revealed that the addition of CAD worsened sensitivity (the performance of radiologists in determining that cancer was present), thus increasing the likelihood of a false negative test. CAD did not result in a significant change in specificity (the performance of radiologists in determining that cancer was not present) and the likelihood of a false positive test3. It has been speculated that CAD was not as useful in the clinic as experimental data suggested it might be because radiologists ignored or misused its input owing to the high frequency of marks on the images that were not findings suggestive of cancer. This outcome was attributed by some to the limited processing power available for CAD, which meant that comparisons with previous imaging studies of the same person were not possible4. Thus, CAD might mark regions that were not changing over time and that could be easily dismissed by expert readers. Another factor that limited CAD is that it was developed using the performance of human-based diagnosis. It was trained using mammograms in which humans had found signs of cancer and others that were false negatives — cases in which humans could not see signs of cancer although the disease was indeed present4. Similar pitfalls could be encountered with AI-based decision aids, too.   A system by which AI finds abnormalities that humans miss will require radiologists to adapt to the use of these types of tool. Imagine a system in which an algorithm marks a dense breast area on a screening mammogram and the human radiologist cannot see anything that looks potentially malignant. With CAD, radiologists scrutinize the areas marked, and if they decide the mark is probably not cancer, they assign the mammogram as being negative for malignancy. However, if AI algorithms are to make a bigger difference than CAD in detecting cancers that are currently missed, an abnormality detected by the AI system, but not perceived as such by the radiologist, would probably require extra investigation. This might result in a rise in the number of people who receive callbacks for further evaluation. A clinical trial would show the effect of the AI system on the detection of cancer and the rate of false positive diagnoses, while also allowing the development of effective clinical practice in response to mammograms flagged as abnormal by AI but not by the radiologist. In addition, it would be essential to develop a mechanism for monitoring the performance of the AI system as it learns from cases it encounters, as occurs in machine-learning algorithms. Such performance metrics would need to be available to those using these tools, in case performance deteriorates over time. It is sobering to consider the sheer volume of data needed to develop and test AI algorithms for clinical tasks. Breast cancer screening is perhaps an ideal application for AI in medical imaging because large curated data sets suitable for algorithm training and testing are already available, and information for validating straightforward clinical end points is readily obtainable. Breast cancer screening programmes routinely measure their diagnostic performance — whether cancer is correctly detected (a true positive) or missed (a false negative). Some areas found on mammograms might be identified as abnormal but turn out on further testing not to be cancerous (false positives). For most women, screening identifies no abnormalities, and when there is still no evidence of cancer one year later, this is classified as a true negative. Most other medical tasks have more-complicated clinical outcomes, however, in which the clinician’s decision is not a binary one (between the presence or absence of cancer), and thus further signs and symptoms must also be considered. In addition, most diseases lack readily accessible, validated data sets in which the ‘truth’ is defined relatively easily. Obtaining validated data sets for more-complex clinical problems will require greater effort by readers and the development of tools that can interrogate electronic health records to identify and annotate cases representing specific diagnoses. To achieve the promise of AI in health care that is implied by McKinney and colleagues’ study, anonymized data in health records might thus have to be treated as precious resources of potential benefit to human health, in much the same way as public utilities such as drinking water are currently treated. Clearly, however, if such AI systems are to be developed and used widely, attention must be paid to patient privacy, and to how data are stored and used, by whom, and with what type of oversight. </body>
<date id = '319'>01 January 2020</date>
<url id = '320'>https://nature.com/articles/d41586-019-03893-7</url>
<title id = '320'>Observations of a distant cluster of galaxies suggest that star formation began there only 370 million years after the Big Bang. The results provide key details about where and when the first stars and galaxies emerged in the Universe.</title>
<body id = '320'>Shortly after the Big Bang, the Universe was completely dark. Stars and galaxies, which provide the Universe with light, had not yet formed, and the Universe consisted of a primordial soup of neutral hydrogen and helium atoms and invisible ‘dark matter’. During these cosmic dark ages, which lasted for several hundred million years, the first stars and galaxies emerged. Unfortunately, observations of this era are challenging because dark-age galaxies are exceptionally faint1. Writing in Nature, Willis et al.2 provide a glimpse of what happened during the dark ages by doing some galactic archaeology. By measuring the ages of stars in one of the most distant clusters of galaxies known, the authors located galaxies that formed stars in the dark ages, close to the earliest possible time that stars could emerge.   A galaxy cluster is a group of thousands of galaxies that orbit each other at speeds3 of about 1,000 kilometres per second. They are prevented from flying apart by the gravitational pull of the accompanying dark matter, which has the equivalent total mass of about one hundred trillion Suns4. Astronomers use these clusters as laboratories for many experiments in astrophysics, such as measuring the composition of the Universe, testing theories of gravity and determining how galaxies form. Willis et al. used one of the most distant clusters known to study when the most massive galaxies in the Universe began to produce stars. Although nearby clusters, such as the Coma cluster, are easier to observe than those farther away, we cannot measure their ages precisely because the galaxies are extremely old. It is difficult to differentiate between, for example, a galaxy that is 7 billion years old and one that is 13 billion years old5. Therefore, to obtain a precise date for when clusters first formed their stars, Willis and colleagues used NASA’s Hubble Space Telescope to look at one of the most distant clusters they could find. Because light travels at a finite speed, the most distant clusters we can see are also those in the earliest stages of the Universe that we can see. The light from the cluster examined by Willis et al. has been travelling for 10.4 billion years before it reaches Earth, which means that we are looking at a cluster as it was just 3.3 billion years after the Big Bang. Consequently, this cluster acts as a keyhole through which we can peer into the early Universe (Fig. 1). Figure 1 | Chronology of the Universe. After the Big Bang, the Universe consisted of a cosmic soup of radiation and matter. About 400,000 years later, it entered an era known as the cosmic dark ages in which it was devoid of light. The first stars and galaxies began to emerge a few hundred million years later, and gradually provided the Universe with light. Willis et al.2 report that star formation in a distant cluster of galaxies began roughly 370 million years after the Big Bang. The light that we see from this galaxy cluster was emitted when the Universe was about 3.3 billion years old. The cluster is likely to have become one of the largest structures in the present-day Universe, comparable in mass to the Coma cluster.Image credits: Willis and colleagues’ galaxy cluster: N. A. Hatch; Coma cluster: Russ Carroll, Rob Gendler, Bob Franke/Dan Zowada Memorial Observatory, Wayne State Univ. Willis and colleagues found that the cluster contains several galaxies that have similar red colours. The colour of a galaxy can be used to estimate its age because younger stars are bluer than their older, redder counterparts. As a result, galaxies that have red colours formed their stars a long time ago5. By comparing the colours of the cluster galaxies with those of models, the authors estimated that the stars of these galaxies started to emerge when the Universe was only 370 million years old. This epoch is when we expect the first stars to have formed in the cosmic dark ages6.   One particularly intriguing point is that Willis et al. identified at least 19 galaxies in the cluster that have similar colours, which means that the galaxies have similar ages. At the time when these galaxies formed their stars, they would have been well spread out, so it is a conundrum as to why they all began producing stars at approximately the same time. Were they influenced by their environment? Alternatively, did the star formation in one galaxy somehow trigger a chain reaction, leading to star formation in nearby gas clouds? We do not currently have the answer, but what is clear from the authors’ work is that these distant clusters are full of the oldest galaxies in the Universe. In my opinion, Willis and colleagues’ age estimates are the best ones possible, given the limited data that the authors have from the Hubble telescope. However, determining ages from the colours of galaxies is a relatively crude method that is subject to large uncertainties. For example, a young galaxy that contains a lot of astronomical dust can have the same colour as an old galaxy containing little dust. Therefore, although the authors’ results are tantalizing, they should be treated with caution until NASA’s James Webb Space Telescope (JWST) is launched in the next few years. The JWST will measure spectra of the light emitted by these galaxies. A comparison of the spectra with models will be a much more accurate way to determine the ages of the stars than using the colours of galaxies. Furthermore, because it is easier to measure the ages of earlier galaxies than those of more recent ones5, it makes sense to target galaxies in the progenitors of these galaxy clusters in the early Universe. Willis and colleagues’ results make a strong case for these distant clusters being some of the first targets that the JWST should observe. </body>
<date id = '320'>01 January 2020</date>
<url id = '321'>https://nature.com/articles/d41586-019-03597-y</url>
<title id = '321'>A widely used vaccine against tuberculosis has now been shown to provide almost complete protection when injected intravenously. This is a striking improvement over vaccination through the typical intradermal route.</title>
<body id = '321'>Tuberculosis is the deadliest human infection, killing 1.5 million people in 2018 alone (go.nature.com/2kbuiq). It is widely accepted that an effective vaccine against the bacterium responsible, Mycobacterium tuberculosis, would be the most practical way to control the disease. However, the pathogen is often able to resist the immune responses elicited by vaccination. This has raised the question of whether it is possible for a conventional vaccine to confer sterilizing immunity against TB — a gold-standard immune status for vaccines, under which disease is prevented and the pathogen completely eliminated, often before it can even establish a productive infection. In a paper in Nature, Darrah et al.1 provide a resounding answer to this question by showing that near-complete protection from TB infection can be conferred using a century-old vaccine, simply by changing its route of administration. The only currently licensed vaccine against TB is a live strain of the related pathogen Mycobacterium bovis, the virulence of which was attenuated in the laboratory between 1908 and 1921. The strain, known as bacille Calmette–Guérin (BCG), has been administered to more than one billion people (go.nature.com/2cxwew6) since then (Fig. 1). Figure 1 | Ampoules of the BCG vaccine against tuberculosis. This vaccine has been used for almost a century, typically given as an injection just under the skin. Darrah et al.1 now provide evidence in monkeys that the vaccine’s efficacy can be greatly improved using intravenous injection.Credit: Alfred Eisenstaedt/AP/Shutterstock The BCG vaccine is effective against some deadly early-childhood forms of TB. However, its ability to prevent the transmissible pulmonary form, which is the dominant form in adults, has been patchy2: it confers protection for some groups of people in some countries, but is generally insufficient to reduce the number of active TB cases in countries where the infection is endemic. Despite these limitations, BCG remains the only TB vaccine to confer protection in large-scale trials3. The mechanisms that determine its efficacy are a topic of much interest. BCG is typically given as an injection into the dermal tissue that lies just beneath the outer layer of the skin. This injection site is convenient and contains specialized cells that stimulate immune responses. However, vaccines that activate immune cells at the site of potential infection can be more effective at destroying invading pathogens. Thus, current immunological thinking suggests that vaccines administered directly into the lung or the upper airways would be better at preventing pulmonary infections, including influenza and TB. Darrah and colleagues therefore investigated whether a different route of BCG administration could improve protection against pulmonary TB.   Darrah et al. performed their analysis using rhesus macaques, because TB infection in these monkeys closely mirrors the human disease. They evaluated five vaccination strategies. Animals were given the BCG vaccine in one of the following ways: at the standard dose through the conventional intradermal (i.d.) route; at a higher-than-normal dose intradermally; by means of an aerosol to inoculate the lung; through a combination of the high dose i.d. and inoculation by aerosol; or through an intravenous (i.v.) injection. The authors exposed the macaques to M. tuberculosis six months after vaccination, and tracked disease progression to determine how the administration route and dose of the vaccine affected protection against the infection. Vaccinations given intradermally or by aerosol conferred, at best, modest protection from pulmonary TB. By contrast, i.v. vaccination afforded nearly complete protection from the disease. Strikingly, the researchers could not detect any trace of the pathogen in six out of ten animals that received the i.v. vaccination, indicating that the infection had been either prevented or cleared. Three of the other monkeys also showed high levels of protection. Thus, the route of BCG inoculation clearly affects immunity, and the i.v. route confers by far the strongest protection against TB. What makes i.v. BCG vaccination so effective? Clear immunological correlates of protection (characteristics indicative of immunity against a disease) proved difficult to identify in the current study, because only one of the ten animals that received i.v. BCG was not protected against the infection, making it hard to properly compare protected and unprotected animals. To gain an understanding of the potential underlying mechanism, Darrah and colleagues therefore compared the immune responses of animals vaccinated by the different routes. Compared with i.d. and aerosol vaccination, i.v. BCG led to a massive influx of immune cells called T cells into the lungs. The increased number of T cells was still apparent six months later, when the animals were exposed to M. tuberculosis. It is likely that this expansion occurs because i.v. injection leads to the delivery of a high dose of BCG to the lung — a hypothesis consistent with a recent study4 showing that direct intrabronchial inoculation of BCG can also protect against M. tuberculosis. The authors next showed that the T cells recognized protein fragments called antigens produced by BCG. Because BCG and M. tuberculosis are closely related bacteria, these T cells also recognize M. tuberculosis antigens. The T cells that were recruited to the lung were classified as differentiated ‘memory’ T cells on the basis of their gene-expression profiles, the proteins on their surfaces and their function. These T cells survive long after vaccination, and, because they recognize the antigens produced by M. tuberculosis, they can be rapidly activated on infection, producing many ‘effector’ T cells, which combat the invading pathogen.   Although this circumstantial evidence implicates T cells in immunity against M. tuberculosis, the surprising efficacy of i.v. BCG relative to the other vaccine routes (which also elicit T-cell responses) suggests that other mechanisms of immunity are also involved. As Darrah et al. propose, these might involve: antibody responses against M. tuberculosis; innate immune cells, which are activated indirectly by infection (and do not require specific recognition of M. tuberculosis antigens); or innate training, a process by which immune cells such as macrophages gain an enhanced ability to protect, often nonspecifically, against microbes. Darrah and co-workers’ findings raise the obvious possibility of controlling TB by giving people BCG by i.v. injection. In support of this idea, the intervention proved to be safe in the small cohort of rhesus macaques studied. But there is currently a drive to simplify vaccine deployment by eliminating the need for vaccines to be kept cold or for experts to administer them5 — both of which are crucial for i.v. injection. Whether or not i.v. BCG is developed for clinical use, research that builds on Darrah and colleagues’ work could lead to an improved understanding of what protection against TB looks like — that is, to define correlates of protection. In addition, future work must delineate the mechanisms that lead to sterilizing immunity after i.v. BCG. If successful, it might be possible to develop a vaccine designed to activate the same protective immune mechanisms as those triggered by i.v. BCG, but that could be administered in a way that is safe and adaptable to mass vaccination programmes. </body>
<date id = '321'>01 January 2020</date>
<url id = '322'>https://nature.com/articles/d41586-019-03866-w</url>
<title id = '322'>The reliance of infrared spectroscopy on light transmission limits the sensitivity of many analytical applications. An approach that depends on the emission of infrared radiation from molecules promises to solve this problem.</title>
<body id = '322'>Atoms in molecules oscillate when irradiated by infrared light. The particular light frequencies that drive these vibrations are absorbed by molecules, and depend on the molecules’ chemical structure and environment. The infrared absorption spectrum of a sample can therefore be used as a molecular fingerprint by which to characterize its chemical composition. This has made infrared spectroscopy a widespread analytical technique. However, infrared spectra are difficult to measure for low concentrations of analytes and for samples in water. Writing in Nature, Pupeza et al.1 present a concept for infrared spectroscopy that promises to alleviate these limitations.   Infrared light was discovered2 as a result of the problem it caused William Herschel while he was making astronomical observations of the Sun — it created a disturbing heating sensation in his eye that he wanted to filter out. Today, however, the benefits of infrared radiation for a multitude of analytical purposes are widely appreciated. Its applications range from the detection of molecules in outer space3,4, including that of water on Mars5, to deciphering the molecular mechanisms of proteins in living organisms6,7. In the everyday world, it is used in food analysis6,8 and in forensic police investigations6,9, for example. Much research is being done to bring infrared spectroscopy to the clinic, because the analysis of biological tissue and body fluids can be used to detect and diagnose disease6,7,10. One of the main obstacles to the infrared analysis of biological samples is the strong absorption of infrared radiation by water — a problem that limits the sample thickness to less than 10 micrometres for most purposes. This issue also makes it difficult to add aqueous solutions of reagents (such as acids or salts) to samples to manipulate the state of molecules in the sample. Such manipulations are desirable, for example, for studying the binding of small molecules to proteins, and are standard practice when using ultraviolet or visible spectroscopy. Furthermore, because infrared radiation is absorbed by water, samples must often be concentrated or dried.   Pupeza and colleagues report a solution to this problem. They irradiate samples with an ultrashort pulse (on the scale of femtoseconds; 1 fs is 10–15 seconds) of mid-infrared light. Specific frequencies of the light are absorbed by sample molecules, generating vibrations. These vibrations continue after the pulse has ended, and last until the vibrational energy is dissipated to the environment (which takes a few picoseconds; 1 ps is 10–12 s). Because the vibrating atoms carry partial electrical charges, their oscillations generate electromagnetic radiation, similar to the way in which oscillating electrons produce electromagnetic radiation in an antenna. The generated radiation has the same frequency as that of the molecular vibrations, and so carries information about all of the sample molecules — the authors therefore call it a global molecular fingerprint. It is measured using a second ultrashort pulse of light, this time in the near-infrared spectral range, through a method called electro-optic sampling11. The authors’ approach is conceptually different from conventional absorption measurements. In absorption spectroscopy, the signal is sensed only indirectly, from the light that does not interact with the sample (Fig. 1a). Weak absorption is therefore very difficult to detect, because it changes the intensity of the transmitted light only marginally. Theoretically, the detection of weak absorbers could be improved by increasing the intensity of the incident light, but commonly used infrared detectors become less sensitive at higher light intensities12, imposing a practical limit on the maximum light intensity that can be used. By contrast, Pupeza et al. detect the signal of interest — the radiation emitted from the vibrating molecules — directly (Fig. 1b). This is analogous to the difference between absorbance and fluorescence measurements in the visible spectral range: fluorescence measurements are the more sensitive because they detect a signal directly from the sample, and can even detect it from a single molecule. Figure 1 | A fresh approach for obtaining infrared spectra. a, In conventional infrared spectroscopy, molecules are irradiated with infrared light. They absorb certain frequencies of the light, which causes them to vibrate. The signals of interest are the absorption ‘troughs’ in the transmitted light spectrum, but these change the overall intensity of the transmitted light only marginally when the samples are highly diluted, limiting the sensitivity of this technique. b, Pupeza et al.1 irradiate analytical samples with ultrashort bursts of infrared light, again causing molecules in the sample to vibrate. These vibrations continue after the pulse has ended, and generate infrared radiation, shown here as a ‘tail’ that trails after the pulse. This tail is analysed to determine the infrared spectrum of the molecules. Because the experimental signal is emitted light and is detected directly, this method can be more sensitive than absorption infrared spectroscopy. Pupeza and colleagues demonstrate the high sensitivity of their approach in various ways. For example, they were able to detect 40-fold lower concentrations of a compound in solution, and to better distinguish between two similar compounds, than when using absorption spectroscopy. They also obtained spectra of biological samples that block nearly all of the incoming light (in one case, at least 99.999%). Thus, the new approach senses light where currently used methods see only darkness. This is an impressive achievement, and might alleviate both of the main problems of conventional infrared spectroscopy: sensitivity and strong infrared absorption by water. It will simplify sample preparation in many cases by removing the need for sample concentration or drying, and will open up new applications — particularly those involving aqueous biological samples. The authors suggest several ideas for taking the method further, such as by increasing the power of the laser used to irradiate the sample. It is to be hoped that such measures will further narrow the technological gap that at present prevents the method from achieving the ultimate goal of single-molecule sensitivity in bulk water. Other challenges will be to increase the spectral range of the measurements to include the shorter wavelengths at which prominent and diagnostically useful signals are found for proteins, lipids and nucleotides, and to develop a spectrometer suitable for commercialization at a competitive price. </body>
<date id = '322'>01 January 2020</date>
<url id = '323'>https://nature.com/articles/d41586-019-03867-9</url>
<title id = '323'>How Nature reported ambitions to modernize the Indian sugar industry in 1920 and a successful treatment for rattlesnake bites in 1870.</title>
<body id = '323'> The Government of India has appointed a Committee, consisting of European and Indian experts, to inquire into the conditions and prospects of the sugar industry. At present most of the sugar produced is locally consumed in the crude form of “jaggery,” but there seems little doubt that if capital and modern methods of manufacture could be introduced India might become one of the great sugar-producing areas in the world. The annual consumption of sugar in India, as elsewhere, has rapidly increased. India until recent years stood first of all the countries in the world in its area under sugarcane and its estimated yield of cane-sugar, and even now ranks second only to Cuba. From Nature 1 January 1920 Authentic cases of the successful treatment of snake bites are of some interest. Dr. Bell supplies two in his “New Tracks in North America.’’ On the Rio Grande, in October, 1867, two horses were bitten by the same rattlesnake, while grazing. A few hours afterwards the submaxillary, parotid, and all glands situated about the head and neck were greatly enlarged; from the nostrils and gums, a clear, mucous discharge ran down; the eyes were glairy, with the pupils greatly dilated, and the coat was rough and staring. To each animal Dr. Bell gave half-a-pint of whisky, with a little water, and half an ounce of ammonia, while the wounds were fomented with a strong infusion of tobacco, and afterwards poulticed with chopped tobacco leaves. Both horses recovered. One, although reduced in flesh, and thrown out of condition, was fit for work in a week, but the other only just escaped with his life, becoming a perfect skeleton, and only commencing to mend at the end of three months. Dr. Bell adds that a little weed, common throughout the Western States (called by Engelmann, Euphorbia lata, and by Torney, E. dilatata), is said to be a specific for the bite of the rattlesnake, but at the very time the plant was wanted it could not be found, although continually met with elsewhere, along the route, so that the experiment could not be tried. From Nature 6 January 1870 Latest on: Agriculture Comment 23 APR 20 Editorial 11 MAR 20 Correspondence 25 FEB 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Toxicology News & Views 06 NOV 19 News & Views 29 OCT 19 Outlook 11 SEP 19 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '323'>31 December 2019</date>
<url id = '324'>https://nature.com/articles/d41586-019-03728-5</url>
<title id = '324'>A chemosensory protein enriched in the legs of malaria-carrying mosquitoes gives them resistance to insecticides used to treat bed nets. This discovery points to the challenges of tackling malaria.</title>
<body id = '324'>The wide distribution of insecticide-treated mosquito nets across malaria-endemic regions has drastically reduced the incidence of the disease over the past 20 years, and so has saved millions of lives1. However, malaria-carrying mosquitoes have now developed strong resistance to the pyrethroid chemicals used in these long-lasting insecticidal nets (LLINs)2. An understanding of the mechanisms underlying resistance should reveal ways to make mosquitoes susceptible to insecticides once more. Writing in Nature, Ingham et al.3 demonstrate an unexpected way in which mosquitoes in Africa neutralize pyrethroids: they use a class of small proteins normally involved in chemical communication. Malaria parasites are transmitted to humans by female mosquitoes of the genus Anopheles, with Anopheles gambiae being a major carrier of the disease. The first identified mechanism of pyrethroid resistance in wild Anopheles populations was a phenomenon called knockdown resistance, which involves mutations in a voltage-gated sodium channel protein that reduce neuronal sensitivity to the insecticide4. Other mechanisms have also been identified, including enhanced metabolic activity of detoxifying enzymes such as cytochrome P450s (CYPs), which bind to and promote the breakdown of insecticides5 (Fig. 1). Figure 1 | Multiple lines of resistance. Malaria-carrying mosquitoes in West Africa have developed several mechanisms of resistance to insecticides called pyrethroids, which are used to treat bed nets2. Mutations in a voltage-gated sodium channel protein to which pyrethroids bind reduce the chemicals’ ability to recognize their target. Enhanced activity of metabolic enzymes such as the cytochrome P450s (CYPs) can break down the pyrethroids before they kill the insect. Thickening of the mosquito’s outer cuticular layer can slow insecticide penetration into the skin, thereby reducing the chemicals’ concentration. Ingham et al.3 now show that, in the insect’s legs, the chemosensory protein SAP2 binds to pyrethroids, sequestering them and so preventing them from functioning. The emergence of strong pyrethroid resistance in West African populations of A. gambiae6 prompted Ingham et al. to search for more mediators of resistance. The authors analysed the gene-expression profiles of insecticide-resistant A. gambiae populations from Burkina Faso and Côte d’Ivoire. To the authors’ surprise, they discovered higher-than-normal expression of genes that encode a family of chemosensory proteins, called sensory appendage proteins (SAPs). SAPs, like all chemosensory proteins, are found only in insects. They are small, soluble proteins that typically transmit chemical signals by transporting small hydrophobic molecules between cells. Ingham and colleagues found that reducing the levels of one of these proteins, SAP2, in pyrethroid-resistant A. gambiae significantly restored the mosquitoes’ susceptibility to pyrethroids. Conversely, overexpressing SAP2 in an otherwise susceptible A. gambiae colony boosted the mosquitoes’ resistance levels.   How does a chemosensory protein interfere with insecticide activity? Ingham et al. show that SAP2 binds to pyrethroids with high specificity, and that its expression is enhanced in the legs of mosquitoes. These data suggest that SAPs sequester pyrethroids that penetrate the mosquito’s hard exterior when it lands on a bed net, perhaps preventing the insecticide from exerting its toxic effect on the nervous system by promoting its breakdown. Finally, Ingham et al. analysed the genomes of West African Anopheles populations collected over time, making use of an existing database as well as sequences that they had gathered. They found that a ‘selective sweep’ had occurred near the genomic region that encodes SAP2 — a phenomenon in which one particular version of a genomic region becomes more prevalent in a population as a result of natural selection. The authors showed that the sweep had occurred over the time in which pyrethroid resistance sharply increased, maybe owing to the beneficial effects of one version of this genomic region on survival. Taken together, Ingham and colleagues’ data unveil chemosensory proteins as a key component of insecticide resistance in Anopheles mosquitoes. Chemosensory proteins represent a previously unknown class of resistance-causing factor, and so Ingham and co-workers’ finding points to a fresh opportunity to restore full susceptibility to pyrethroids in West African Anopheles populations. Resistance caused by CYP enzymes has been mitigated by adding compounds that interfere with CYPs to bed nets7; similarly, compounds that inhibit binding between SAP2 and insecticides could be generated and incorporated into next-generation LLINs. Moreover, the genomic region associated with SAP2 resistance can now be used as a molecular marker for tracking the spread of this resistance mechanism. In the future, it will be crucial to determine whether and how chemosensory proteins interact both functionally and spatially with other mechanisms of resistance, to inform the optimal design of resistance-management strategies. Although Ingham and co-workers’ study provides hope of reversing mosquitoes’ resistance to insecticides, it also highlights how skilled these insects are at escaping unwanted attention. Clearly, our understanding of insecticide resistance is far from complete, and we should expect other studies in different locations to identify yet more such mechanisms, acting at local or continental levels. Anopheles species have been populating Africa for more than 100 million years — considerably longer than humans and our ancestors8. Such an enduring connection with their natural habitat is a testament to the challenge that we face when targeting these insects.   The next generation of LLINs and indoor residual sprays (another method of delivering insecticides) is currently being deployed in Africa9. Simultaneously, new insecticide-based methods such as insect-attractive targeted sugar baits are being tested10. But, like previous interventions, these tools will probably undergo cycles of impactful but relatively short-lived success, followed by decreased effectiveness owing to the emergence of resistance. Beyond insecticides, other mosquito-control strategies will probably encounter similar resistance issues. These include methods that rely on mosquito killing, such as mosquitocidal drugs11, and genetic systems designed to suppress Anopheles populations (reviewed in ref. 12). It is possible that the combined use of multiple strategies will break mosquito endurance and lead to population collapse. However, work from my group recently showed that strong selective pressures imposed on Anopheles females could actually favour malaria transmission, for instance by triggering acceleration of parasite growth rates13. To avoid this issue, mosquito-targeting interventions could be integrated with approaches that block parasite development in the insects without causing them harm, thus reducing selective pressures. Furthermore, mathematical models suggest that our chances of achieving sustainable malaria control could be improved by incorporating antimalarials into LLINs or indoor residual sprays, to kill parasites and prevent their transmission even when mosquitoes become resistant to insecticides14. Similar results could be obtained by delivering antiparasitic agents through biological or genetic means12. Whatever the eventual solution, the road to malaria elimination remains long. Mosquitoes are sending clear signals that they will fight for their survival. </body>
<date id = '324'>25 December 2019</date>
<url id = '325'>https://nature.com/articles/d41586-019-03603-3</url>
<title id = '325'>A combination of laboratory experiments and mathematical and statistical analysis provides an affirmative answer to a decades-old question — can a predator and its prey coexist indefinitely?</title>
<body id = '325'>A key question in ecology is what allows species to persist over time — particularly when there are pairs of species in which one is an exploiter and the other its victim. A long-standing theory attempts to answer this question by explaining how relative numbers of predators and their prey can cycle continuously1. First, prey numbers would increase, giving the predator more food. The subsequent increase in predators would lead to a decline in prey. Predator numbers would then decline owing to a lack of food, restarting the cycle. However, it has proved unexpectedly challenging to demonstrate this type of persistent predator–prey cycle in simple controlled systems in the laboratory. Writing in Nature, Blasius et al.2 report just such a demonstration, succeeding where almost 90 years of experimental work has failed. Ecological theories of persistent predator–prey cycles are supported by the apparent existence of such cycles in nature, for instance between the lynx and hare in Canada3. However, it is hard to prove that these cycles endure in the wild, because observations over many decades would be needed. But if the theories provide a complete explanation of natural cycles, then it should be possible to demonstrate persistent cyclic behaviour in the laboratory, using species that have much shorter cycle times. The challenge posed by such a demonstration was exemplified in 1934 by the ecologist Georgii Gause4, who studied the dynamics of two unicellular organisms — the predator Didinium nasutum and its prey, Paramecium caudatum. Gause found that, on the one hand, if the predator was efficient, it ate up all the prey and then starved. On the other hand, if part of the environment helped to conceal the prey, the predator was less efficient — and so starved (Fig. 1a). Coexistence and long-term cycles could be achieved only through artificial means — namely, by adding prey at regular intervals. Figure 1 | A difficult balance to strike. Ecological theory predicts that the relative numbers of a predator and its prey should oscillate in persistent cycles, but demonstrating this in the laboratory has been hard. a, Classic experiments4 from the 1930s failed to achieve cycles, because predators were either too inefficient at eating prey (not shown) or too efficient, and ate them all. Either way, predator numbers dwindled. b, Blasius et al.2 have succeeded in achieving persistent predator–prey cycles in the laboratory. They found that cycles were mostly in phase, with predator cycles slightly lagging behind prey cycles. However, there were transient periods in which the two — for unknown reasons — fell out of sync. (Adapted from Fig. 2b of the paper.) In 1974, work with the same system showed that, by making the predator less efficient and by providing the prey with less food, the two populations could persist for longer5. Even so, coexistence could be maintained for just a few predator–prey cycles. Since then, some models that allow long-term cycle persistence have focused on space, for instance incorporating metapopulation dynamics6. In this phenomenon, subpopulations of a species migrate around a larger region. Although a subpopulation might become extinct in one area, the species persists across the region as a whole and can migrate back into that area. However, a better understanding of whether exploiter–victim cycles can persist locally without external input is still sorely needed.   Blasius and colleagues studied the aquatic invertebrate Brachionus calyciflorus and its prey, the green algal species Monoraphidium minutum. They found that, under simple and constant environmental conditions, the two species could coexist for more than a year — that is, over 50 predator–prey cycles. This result finally demonstrates that the long-standing theory of persistent cycles can be consistent with the reality of simple ecological systems. Next, the researchers carried out a rigorous statistical analysis of the cycle dynamics in their system. Specifically, they used wavelet analysis, which focuses on dynamics over short periods; the technique has become a standard way to study the presence of periodic behaviour in ecological time series7. The analysis revealed interesting dynamic phenomena. The oscillations in the relative numbers of each species showed a characteristic lag in phase, with predator numbers mostly changing in the wake of altered prey numbers. However, these oscillations would undergo sudden shifts, without any external driver. During these transient periods, both populations would oscillate out of sync with one another, before the in-phase cyclic dynamic resumed (Fig. 1b). The authors also performed experiments in which they introduced pulses of nutrients to the species’ environment. This mimics seasonal environmental changes experienced in many natural systems. The overall dynamics of the predator–prey cycle remained the same in these conditions, indicating that predator–prey interactions can govern cycle dynamics, even in a seasonally varying environment. Although Blasius and colleagues’ work answers one question, it raises others. First and foremost, why are the results so different from those of earlier experiments? It is not just a quirk of the species used, because the authors repeated the experiment using a different prey species and achieved the same result. One contributing factor could be that the researchers used a predator that has a complex life history involving transitions through several stages, from egg to adulthood. Another could be that persistence depends on the specific amplitude of oscillation in the cycle.   In support of these ideas, previous work involving a predator with a simpler life cycle demonstrated that populations of predators, prey or both went extinct if cycles reached an amplitude at which population numbers become low4. In addition, analysis of a different marine predator–prey combination8 showed that cyclic dynamics eventually gave way either to extinction or to persistence of the two populations in equilibrium. And mathematical theory1 suggests that the small-amplitude cycles observed by Blasius and colleagues would occur only under very specific conditions. Demonstrating persistent cycles in other systems is therefore a key challenge for the future. The life-history characteristics of the predator used by Blasius and colleagues could be a starting point in the search for systems that show similar behaviour. Another intriguing avenue for future study is to determine the relationship between transient dynamics and switches in system behaviour. Does a sudden change in the dynamics of a cycling system reflect changing environmental conditions, or is it an intrinsic phenomenon? The existence of unexpected, transient shifts in predator–prey dynamics limits the predictability of ecological systems9. In the real world, sudden shifts of this nature could spur an erroneous quest to identify drivers of the change, whereas the current study indicates that there may be none. The possibility of transient dynamics could also make it more difficult to predict the effect of ecological-management strategies. Blasius et al. have provided one of the clearest demonstrations so far of transient dynamics in the laboratory, where it is absolutely certain that no external influences caused the change. It will be important to bring together this work (and another convincing experimental demonstration10) with theoretical models that are currently being developed9. Doing so should provide more insights into the role of transient dynamics in natural systems. </body>
<date id = '325'>18 December 2019</date>
<url id = '326'>https://nature.com/articles/d41586-019-03811-x</url>
<title id = '326'>The brain fluctuates between different internal states, each of which drives particular behaviours. Brain-wide imaging reveals the internal states that help zebrafish larvae to choose between exploring and hunting.</title>
<body id = '326'>Internal brain states modulate how we perceive, feel about and respond to the stimuli around us. The neurons and networks that encode these various states remain mysterious, owing to the brain’s enormous complexity. As a result, researchers have only a vague sense of how animals, including humans, use recent experience and context to select appropriate behaviours. Writing in Nature, Marques et al.1 perform a first-of-its-kind exploration of brain-wide activity in zebrafish larvae as the animals encounter a classic state-driven behavioural choice: whether to explore their environment or to exploit resources in the local area. By monitoring individual neurons across the brain as these decisions play out, the authors open a window onto the neural underpinnings of internal states and how they can influence behaviour.   Advanced microscopy is revolutionizing the ways in which we observe the functioning brain. Historically, neural activity has been monitored either using brain-wide methods that cannot resolve individual neurons, or with electrophysiology, which records individual neurons or small localized populations. Neither approach can address activity across vast assemblies of individual neurons — which is exactly what our brains are. Genetically encoded calcium indicators (GECIs) offer a possible solution to this problem. These proteins provide a pulse of fluorescence when a neuron is active. When expressed throughout the brain and combined with the right microscope, they produce a blinking light field that can reveal the activity of thousands of individual neurons simultaneously. Zebrafish larvae are small and transparent, making it possible to image GECIs across their entire brains at cellular resolution2,3. However, microscopy techniques have typically required that the animals are stationary, either paralysed or with their heads embedded in agarose, hampering our ability to analyse how sensory processing relates to natural behaviour. Brain-wide GECI imaging during free-swimming behaviour became possible when the group behind the current study introduced a tracking fluorescence microscope4. This machine constantly monitors and predicts the movements of a free-swimming larva, and moves the imaging platform to compensate for the animal’s movement, keeping the brain in the microscope’s field of view. Meanwhile, it uses brief pulses of light to record fluorescence signals from GECIs in the brain. After spatial alignment of the images across the experiment, the approach produces brain-wide, cell-level data on neural activity, complemented by behavioural information on the animal’s position, speed and movements. In the current study, Marques and colleagues used this platform to explore the neural underpinnings of two larval behaviours that involve free swimming: exploration (efficient swimming over distance) and exploitation (localized predation). Exploration and exploitation place opposing pulls on diverse animals5,6 — the former offers the possibility of resource discovery, but at a metabolic cost, and the latter is an effective approach for predatory feeding, but only if prey are present. These behavioural modes are influenced by external inputs and hunger level, but also alternate spontaneously, suggesting that internal brain states have a role in which behaviours are chosen. The authors recorded videos of free-swimming larvae, and categorized each of the animals’ movements to generate thorough descriptions of behaviours during exploration and exploitation phases. They showed that movements such as forward swimming and routine turns tend to occur at a similar time and indicate that the larva is in an exploratory phase. Other movements, including targeting turns and predatory strikes typical of hunting, identify exploitative phases. Marques et al. next performed GECI imaging to analyse which brain networks encode the corresponding internal states, and what changes occur during the transitions between phases of behaviour. They extracted activity data from tens of thousands of individual neurons per animal, seeking neurons with activity that correlated with specific swimming manoeuvres, with exploration or exploitation phases, or with transitions between exploration and exploitation. This analysis revealed three notable categories of neuron (Fig. 1). Figure 1 | Mapping exploration and exploitation. a, Using whole-brain imaging of neuronal activity in free-swimming zebrafish larvae, Marques et al.1 have identified distinct categories of neuron that are active during phases of larval behaviour involving exploration (blue larva) and exploitation (hunting; red). As this cyclical graph shows, a network of neurons (blue) is steadily active throughout exploration. This activity is replaced by a peak in a ‘trigger network’ of neurons (yellow), which then drives the activity of exploitation-network neurons (red). Exploitation-network activity tails off over time, reaching a baseline as the exploration network becomes active once more and the cycle begins again. b, The exploration network is distributed throughout pre-motor regions of the zebrafish hindbrain. The exploitation network is found mainly in the dorsal raphe. The trigger network is also found in this region, and in the habenulae. First, there are neurons dispersed across the brain that are active during exploration. Because they are broadly distributed and their activity does not show obvious peaks, they are probably involved in executing exploratory manoeuvres, rather than encoding or triggering the state itself. Second, there is a neuronal population that is present in high numbers in a structure called the dorsal raphe. These neurons are powerfully active at the beginning of an exploitation phase. Their activity gradually weakens over time, settling to a baseline during the phase transition back to exploration. Third, there is a ‘trigger network’ of neurons, located in multiple localized regions across the brain. The activity of these neurons peaks sharply when the larva is transitioning from exploration to exploitation. The strength of the peak from the trigger network predicts the strength of the subsequent activation in the dorsal raphe’s exploitation-network neurons. This, in turn, predicts the duration of the ensuing exploitation phase.   How does this arrangement help to deliver appropriate behaviours at appropriate times? The expectation is that exploration and exploitation phases are influenced both by hunger and by the local availability of prey. A hungry larva should stay where it is and exploit prey when they are present, and should look for prey if they are absent locally. In agreement with this theory, the authors found that exposing larvae to light after a period of dark — revealing the presence of local prey to the larvae, whose predation is largely visual — caused increased trigger-network activity, activation of exploitation neurons in the dorsal raphe, and predation. By contrast, however, they found only a weak trend towards exploitation in hungry animals presented with prey. These mixed results provide a strong incentive to study the combinations of environmental stimuli and internal cues that spur trigger-network activity, and therefore feeding. In summary, Marques et al. have delivered a plausible and satisfying scheme for how exploration and exploitation behaviours are controlled by internal brain states. Rather than being a conclusion, this is a departure point for further exploration of the underlying networks. Although whole-brain GECI imaging is a powerful tool, it provides little information on neuronal connectivity, and reveals only correlations between neurons’ activity rather than the causal relationships among them. Going forward, ablations or light-based optogenetic manipulations of targeted neurons could help to reveal exactly how activity patterns or network-level properties drive transitions between exploration and exploitation states. Furthermore, mapping out neurons’ functional properties against their physical architecture (as described in a recent atlas of cell morphologies in zebrafish7) or connectivity (as inferred from brain-wide electron-microscopy data8) would provide an anatomical framework for the flow of information through the system. Such functional and anatomical information will allow researchers both to test Marques and colleagues’ model of internal states, and to identify the fine details of how exploration, exploitation and trigger networks interact to control behaviour. </body>
<date id = '326'>18 December 2019</date>
<url id = '327'>https://nature.com/articles/d41586-019-03831-7</url>
<title id = '327'>A material that has electrically conducting surfaces has been found to show, when cooled, a type of magnetic ordering that reduces conduction at the surfaces. Such remarkable behaviour could have practical applications.</title>
<body id = '327'>An ordered arrangement of magnetic moments in a material normally prevents the formation of another kind of electronic order associated with an exotic state of matter known as a topological insulator. But Otrokov et al.1 and Rienks et al.2 report in Nature that manganese bismuth telluride integrates these two types of electronic behaviour. The complex layered structure of alternating magnetic and topologically non-trivial regions in this material leads to an intriguing and potentially technologically useful interplay between magnetic and topological order. One of the earliest descriptions of electronic order in solids was of ferromagnetism, the existence of which was reported in natural minerals in Greece and China more than 2,000 years ago. In a simple ferromagnet, microscopic magnetic moments, arising predominantly from the spin (intrinsic angular momentum) of a material’s electrons, align in the same direction, leading to an overall macroscopic magnetic moment. The concept of antiferromagnetism, in which spins align in alternating directions and the average magnetic moment is zero, was developed only in the 1930s. These two kinds of magnetic order are viewed theoretically as breaking time-reversal symmetry: when the direction of time is reversed, the pattern of spins is changed. Time reversal acts by reversing velocities (a bit like running a movie backwards) as well as by reversing the direction of angular momenta, including spins. However, unbroken time-reversal symmetry is required to produce topological insulators, which have been a tremendously active area of study3,4. Topological insulators have an electrically insulating interior, but a conducting surface. The property that sets them apart from ordinary insulators is that their surfaces cannot be made to have a simple insulating state as long as time-reversal symmetry is unbroken. This makes topological insulators ideal platforms for generating excitations known as Majorana zero modes, which are the basis for a topological approach to quantum computing5. The two current papers demonstrate that crystalline manganese bismuth telluride manages to combine these seemingly incompatible magnetic and topological orders. When cooled, the material becomes magnetic, and yet displays a kind of topological insulating behaviour. Unlike an ordinary topological insulator, for which every surface is conducting, such an antiferromagnetic topological insulator has surfaces that are conducting or insulating, depending on how the specific surface cuts through the alternating pattern of spins6 (Fig. 1). Figure 1 | A new spin on topological insulators. a, In an ordinary topological insulator, the atoms are unmagnetized (beige), the interior is electrically insulating and all of the surfaces are conducting. b, When random magnetic impurities (red) are introduced, all of the surfaces become insulating. Rienks et al.2 present evidence that magnetic manganese atoms sit in random locations when added to bismuth selenide, but separate into layers when added to bismuth telluride (not shown). c, If the electron spins (intrinsic angular momenta) of atoms form a pattern that alternates between layers (illustrated by the red and blue atoms), an antiferromagnetic topological insulator forms. This has both conducting and insulating surfaces; whether a particular surface is conducting or insulating depends on its orientation relative to the magnetic structure. Here, the top and bottom surfaces are insulating, and the others are conducting. Otrokov et al.1 present evidence for such an antiferromagnetic topological insulator. Manganese bismuth telluride can be viewed as a stack of magnetic manganese telluride layers that are separated by layers of a benchmark topological insulator, bismuth telluride7. Indeed, a thin-film version of this material was produced earlier this year by alternately growing single layers of manganese telluride and bismuth telluride8. The current work shows that crystalline manganese bismuth telluride integrates two of the most actively studied kinds of order: topological insulating behaviour and single-layer magnetism, as seen in monolayer ferromagnets, such as chromium triiodide9. The key feature of the antiferromagnetism in manganese bismuth telluride is that it can retain a modified version of time-reversal symmetry. In an antiferromagnet, the flipping of spins associated with time reversal changes the pattern of alternating spins. However, the combined transformation of first flipping the spins and then shifting the position of the pattern by a unit cell (the smallest repeating unit of a crystal lattice) can leave the pattern unchanged. Consider adding such antiferromagnetism to a topological insulator. The conducting surfaces of the topological insulator are susceptible to externally applied or intrinsic magnetic fields, because such fields break time-reversal symmetry. Depending on how an antiferromagnetic pattern ends at a surface, the surface can have alternating spins akin to those in a 2D antiferromagnet, uniform spins similar to those in a 2D ferromagnet or a more complicated spin configuration. Consequently, what happens to the surface conduction of the material depends on how these surfaces cut the magnetic order (Fig. 1).   Otrokov et al. carried out numerical simulations in conjunction with various experimental probes of manganese bismuth telluride. On the basis of the results, they argue that the material has both an antiferromagnetic order and a band inversion (whereby the usual ordering of electron energy bands is flipped), which is a signal of a topological insulator. The authors used a method known as X-ray magnetic circular dichroism to confirm the magnetic ordering experimentally. In addition, these authors used a technique called angle-resolved photoemission spectroscopy to study the electronic structure at the material’s surface. They observed that a structure in the surface electron bands, known as a Dirac cone, is modified near and below the temperature at which the material becomes antiferromagnetic. This Dirac cone is the distinguishing feature of the surface of a topological insulator, and its disappearance is a sign that the surface has been converted from conducting to insulating. Rienks et al. carried out detailed studies of the atomic and electronic structure of bismuth telluride to which varying amounts of manganese had been added through a process called doping. They found that a small amount of doping (about 6% manganese) turns the topological insulator into a ferromagnet (as opposed to the antiferromagnet seen by Otrokov et al.). Rienks and colleagues also observed a tendency of doped bismuth telluride to form septuple layers, similar to those in crystalline manganese bismuth telluride, but separated by standard, quintuple layers of bismuth telluride. In addition, they studied manganese-doped bismuth selenide, which might have been expected to behave in the same way as its telluride counterpart, given that these compounds are similar topological insulators.   Rienks et al. found that manganese added to bismuth selenide does not have as strong a tendency to form layers as it does in bismuth telluride, and they showed that this structural difference has consequences for the magnetic structure and surface electrons. The telluride compound has a gap in energy between surface electron bands and has a magnetization that is perpendicular to the plane of the surface. By contrast, the selenide compound does not have this gap and has in-plane magnetization. The energy gap in the telluride compound disappears when the temperature is raised above the magnetic transition temperature. This connection between the energy gap and magnetism has been challenging to observe in other materials and samples. A notable requirement for an antiferromagnetic topological insulator is that whether a surface has gaps (insulating) or does not (conducting) depends on the surface termination of the 3D crystal. This difference could be seen by producing other surfaces, or by looking at steps on a surface using scanning tunnelling microscopy. The combination of magnetic and topological behaviour observed in the current papers could lead to new spin-based electronic devices, because the topological aspects might, for example, improve how materials transport spin currents. There is an overlap in potential applications with a topological phenomenon called the quantum anomalous Hall effect (QAHE), which is generated using magnetic impurities in thin films of topological insulators10,11. Steps on the surface of an antiferromagnetic topological insulator produce the same perfectly conducting edge channels as in the QAHE. The fact that manganese bismuth telluride is intrinsically magnetic, rather than having its magnetism result from randomly located impurities, as in current QAHE materials, could be advantageous. Key questions remain about how the magnetism varies between different samples of this material. In particular, it seems that both the magnetic transition temperature and the nature of the magnetic ordering between planes could vary, and that an applied magnetic field might be used to switch this ordering. At a more fundamental level, antiferromagnetic topological insulators are predicted to support, without applied electric or magnetic fields, a quantized electromagnetic response (one that comes in discrete units), known as axion electrodynamics6. The current papers show how the synthesis and theory of crystals that have various symmetries combine to reveal important types of electronic order. </body>
<date id = '327'>18 December 2019</date>
<url id = '328'>https://nature.com/articles/d41586-019-03832-6</url>
<title id = '328'>How Nature reported claims of a dinosaur living in the Congo region in 1919, and efforts to encourage women to take up medicine in 1869.</title>
<body id = '328'> The newspapers have lately published a big-game hunter’s report that a gigantic dinosaurian reptile related to the extinct Brontosaurus and Diplodocus has been seen living in the Congo region of Africa. Palaeontologists, however, receive the story with incredulity, and are decidedly of opinion that it must be founded on mistaken observations. The Dinosauria and all their gigantic reptilian contemporaries, whether on land, in the sea, or in the air, disappeared from every part of the world at the end of the Cretaceous period. If any had survived, some fragments of them would ere this have been found in the Tertiary formations which record the progress of life between that period and the present day. It is no contrary argument to quote Sir Harry Johnston’s discovery of the okapi in the Congo forest, for this is merely a kind of ancestral giraffe which is known by fossils to have existed so far north as Greece so recently as the beginning of the Pliocene Tertiary. The okapi is a congruous African animal, but a dinosaur would be an anachronism. From Nature 18 December 1919 [T]he University of Edinburgh has just made arrangements to enable ladies who wish to do so to study medicine. Those who avail themselves of the opportunity are taught in separate classes from those of the other medical students, each Professor at the University holding one class for men and another for women. Five lady students have already presented themselves for the medical matriculation examination. In London there is a “Female Medical Society,” under the presidency of the Earl of Shaftesbury. The objects of the Female Medical Society are—“1. To promote the employment of properly educated women in the practice of midwifery, and the treatment of the diseases of women and children. 2. To provide educated women with proper facilities for learning the theory and practice of midwifery, and the necessary branches of medical science.” To carry out these objects, the Society established a “Ladies’ Medical College” five years ago, and eighty-two ladies have during that period availed themselves of the facilities it offered; most of them have since started in business, and are succeeding admirably. From Nature 16 December 1869 Latest on: Education Correspondence 05 MAY 20 Correspondence 05 MAY 20 Career Column 01 MAY 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Palaeontology Research Highlight 06 MAY 20 Article 29 APR 20 Obituary 22 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '328'>17 December 2019</date>
<url id = '329'>https://nature.com/articles/d41586-019-03833-5</url>
<title id = '329'>The slipperiness of ice is poorly understood at a microscopic level. Experiments that probe how the surface of ice melts and flows in response to wear help to explain the exceptionally low friction that underpins winter sports.</title>
<body id = '329'>It is widely thought that ice skating is enabled by the formation of a layer of water on the surface of ice, which lubricates the skate. Writing in Physical Review X, Canale et al.1 report that, although there is indeed a layer of something, it is not simply water — the lubricating layer has surprising flow properties that are very different from those of bulk water, and are intermediate between those of liquid water and ice. The idea that a thin film of meltwater wets the surface of ice has been accepted since the nineteenth century2. This layer should be present beneath skates or other objects sliding on ice. Most of the debate about this topic, therefore, deals with the origin of the layer — is it the result of surface melting, heating associated with friction generated by the skate, or pressure-induced melting? Pressure melting has largely been discounted, because this process is impossible for ice below about –20 °C, whereas skating is possible at such low temperatures. Surface melting and frictional heating also seem unlikely explanations because these phenomena are not specific to solid water ice, yet ice is almost the only material that it is possible to skate on. Also unknown are the mechanical properties of the meltwater layer. Whether a liquid lubricates or not is mostly determined by its resistance to being squeezed out from the gap between the two rubbing surfaces — thick grease is usually needed for good lubrication. Water is not a good lubricant, because its low viscosity means that it is easily squeezed out of gaps. The idea that a layer of water is sufficient to lubricate a skate on ice is therefore strange. It doesn’t even make intuitive sense, given that it is impossible to skate on a road or a kitchen floor with a layer of water on it. Canale et al. report clever experiments that investigate the microscopic mechanisms responsible for the slipperiness of ice (Fig. 1). The authors measured the coefficient of friction (a measure of the friction produced at a surface) of ice and the properties of the lubrication layer simultaneously, using an experimental set-up that resembles a large tuning fork. The fork was made to vibrate, so that a millimetre-scale glass bead attached to one of its prongs oscillated across an ice surface. The bead thus functioned as a tiny ice skate, gliding for distances of the order of tens of micrometres across the same region of ice. Figure 1 | Probing the lubricating layer of ice. Canale et al.1 investigated the thin layer of melt water that lubricates the slipping of objects on ice. The authors attached a millimetre-scale glass bead to a prong of a macroscopic aluminium tuning fork. The fork was made to vibrate, causing the bead to oscillate over the ice surface. An accelerometer on the fork measured the amplitude of the bead’s oscillations parallel and perpendicular to the surface. From these measurements, the friction between the bead and the ice, and the flow properties (rheology) of the lubrication layer, could be calculated. The observed viscosity and elasticity of the layer suggest that it consists of a mixture of water and ice particles. An accelerometer attached to the same prong of the fork as the bead measured the amplitude of the bead’s oscillations parallel to the surface, and compared them with the amplitude of the driving force. Canale et al. used the difference in amplitude to calculate the friction force between the bead and the ice. Simultaneously, tiny oscillations of the bead (restricted to just a few tens of nanometres, so as not to influence the friction measurements) perpendicular to the ice surface allowed the authors to probe the properties of the lubrication layer. The authors used a similar procedure to the one used to measure parallel movements to measure the forces acting on the bead perpendicular to the surface. The coefficient of friction between glass and ice could then be calculated from the ratio of the two measured forces.   Canale and colleagues’ procedure not only measured the amplitude change of the perpendicular movements, but also the time taken for the driving force to produce these oscillations of the bead (the phase shift). This allowed the authors to measure the mechanical impedance of the lubricating layer on the ice — a measure of its resistance to forces applied by the bead. These measurements, in turn, correlate with the flow properties (rheology) of the lubrication layer. Taken together, the authors’ measurements allowed the friction and rheology of the lubrication layer to be probed simultaneously. The experiments show that the lubrication layer has both viscous and elastic responses to an object sliding on top of it. This is not only different from the behaviour of bulk water, but also to that of ice. The results, therefore, reveal that skating requires more than just a layer of water. So what is happening? Sophisticated spectroscopy techniques have previously been used to study the surface of ice at different temperatures3, and revealed that melting occurs discontinuously through successive bilayers of ice molecules — the first bilayer melts at about 70 oC, and the second one at 20 oC. Friction measurements over that same temperature range have shown, however, that friction at the surface doesn’t change as the bilayers melt4. It therefore seems that the slipperiness of ice does not just depend on its surface properties during melting, but also on its interaction with the skate.   Canale and co-workers’ experiments support that idea. The authors suggest that repeated sliding over the same spot generates a mixture of ice and water, which displays both elastic behaviour (from the ice) and viscous behaviour (from the water) in response to a load. The resulting layer of material would be more difficult to squeeze out of gaps than ordinary water. This could — at least in part — explain the layer’s excellent lubrication properties. The new study also provides the beginning of an explanation of why skating is so specific to ice. Lubricated friction and concomitant wear often lead to the formation of a layer of different material, known as a third body, between the two rubbing objects. Because this process relies on the specific ways in which the surfaces of rubbing objects wear away, and often even on friction-induced chemistry, the formation of a third body is largely system-specific. In other words, few materials can form a viscoelastic, liquid–solid third body in response to friction and wear, such as that observed in Canale and colleagues’ experiments. Interesting questions remain. For example, does the observed layer form rapidly enough on pristine ice to lubricate on contact? And how does the presence of the layer explain that the optimal ice temperature for skating4,5 is about –7 oC? (The temperature at which all speed-skating rinks are kept.) Beyond ice skating, detailed measurements of the properties of thin lubrication films are likely to be useful, because many such films do not show the same properties as the bulk lubricant6, and because the origin and effects of the third body in lubrication remain a largely open question. These are important issues because lubrication is often the only way to reduce friction, which is responsible for an estimated 20% of the world’s energy consumption7. </body>
<date id = '329'>16 December 2019</date>
<url id = '330'>https://nature.com/articles/d41586-019-03834-4</url>
<title id = '330'>Highlights from News & Views published this year.</title>
<body id = '330'>Credit: Sylvain Cherkaoui/Reuters Fish are a source of micronutrients that help to prevent nutrient-deficiency diseases. For 43 countries, Hicks et al. mapped the relationship between the fish-derived nutrients available from fisheries’ catches and the prevalence of such diseases. Their data demonstrate that catches in some developing countries should be enough to meet the key micronutrient needs of their populations. However, in many developing tropical countries, a substantial proportion of local fish catches are either exported or processed locally to generate fishmeal that is then exported and used to feed farmed fish. Many of the local fisheries (pictured), which had traditionally supplied the regional markets, now instead supply fishmeal plants. This does not reduce the pressure on wild fish. Moreover, it deprives people on low incomes of previously affordable, nutritious local fish. Original research: Nature 574, 95–98 (2019). Huntington’s disease is caused by an abnormally long stretch of glutamine amino-acid residues in the huntingtin (HTT) protein. Cells degrade the mutant huntingtin (mHTT) through autophagy — a clearance mechanism that involves engulfment of proteins by a vesicle called the autophagosome. Li et al. hypothesized that compounds that bind to both the mutant polyglutamine tract and the protein LC3B, which resides in the autophagosome, would lead to engulfment and enhanced clearance of mHTT. The authors conducted small-molecule screens to identify candidate compounds, and used wild-type HTT in a counter-screen to rule out compounds that bind to the normal version of the protein. They found encouraging evidence that four compounds could produce functional improvements in models of Huntington’s disease across three species. This therapeutic strategy might also be useful for other diseases involving expanded polyglutamine tracts. Original research: Nature 575, 203–209 (2019). Credit: NASA/JPL In 1989, the NASA spacecraft Voyager 2 detected six moons of Neptune that are interior to the orbit of the planet’s largest moon, Triton. Showalter et al. report the discovery of a seventh inner moon, Hippocamp. Originally designated as S/2004 N 1 and Neptune XIV, this moon was found in images taken by NASA’s Hubble Space Telescope in 2004–05 and 2009, and then confirmed in further images captured in 2016. Hippocamp is only 34 kilometres wide, which makes it diminutive compared with its larger siblings, and it orbits Neptune (pictured) just inside the orbit of Proteus — the planet’s second-largest moon. The discovery of Hippocamp is intriguing because of the moon’s relationship to Proteus and the role that both objects might have had in the history of Neptune’s inner system. Original research: Nature 566, 350–353 (2019). Materials known as superconductors transmit electrical energy with 100% efficiency. They have a wide range of applications, such as magnetic resonance imaging in hospitals. However, these applications have been hampered, largely by the fact that the superconducting state exists only at temperatures well below room temperature (295 kelvin). Drozdov et al. report several key results that confirm that, when compressed to pressures of more than one million times Earth’s atmospheric pressure, lanthanum hydride compounds, which are rich in hydrogen, become superconducting at 250 K. In the next few years, experiments will probably focus on searching for superconductivity in other pressurized hydrogen-rich materials. Given that only a small fraction of possible hydrogen-rich systems have been subjected to experiments at these tremendous pressures, it seems more likely than ever that the dream of room-temperature superconductivity might be realized in the near future. Original research: Nature 569, 528–531 (2019). Tremendous progress has been made in developing gene-editing tools. But a seemingly fundamental limit to the efficiency and precision of gene editing had been reached, owing to the tools’ reliance on complex and competing cellular processes. Anzalone et al. now describe ‘search-and-replace’ genome editing, which enables the genome to be altered precisely. In their process, the ‘search’ part of an RNA guide directs a Cas9 protein to a specific sequence in a DNA target, where it cuts one of the two DNA strands. A reverse transcriptase enzyme then produces DNA complementary to the sequence in the ‘replace’ part of the guide, and installs it at one of the cut ends, where it takes the place of the original DNA sequence. DNA repair then produces a fully edited duplex. Imperfect edits are almost entirely avoided. Original research: Nature 576, 149–157 (2019). Credit: Lauren Andrews/NASA Sediments beneath glaciers and ice sheets harbour carbon reserves that, under certain conditions, can be converted to methane, a potent greenhouse gas. Lamarche-Gagnon et al. present direct measurements of dissolved methane in water discharged from a land-terminating glacier of the Greenland Ice Sheet (pictured) during the summer. The water was supersaturated with methane, and the amount of methane released to the atmosphere rivals that from other terrestrial rivers. Subglacial sediments can therefore act as a local source of methane, corroborating the results of other studies. Lamarche-Gagnon et al. go further by demonstrating that the continuous flux of methane from the Greenland subglacial environment varies with the efficiency of subglacial meltwater drainage. The study provides an example of how our planet’s icy domains can interact with the surrounding Earth system in unexpected and potentially important ways. Original research: Nature 565, 73–77 (2019). The DNA of eukaryotic organisms (such as animals, plants and fungi) is stored in two cellular compartments: in the nucleus and in organelles called mitochondria. A healthy individual’s mitochondrial DNA (mtDNA) molecules are mostly identical. However, in people with diseases caused by mtDNA mutations, normal and mutant mtDNA molecules typically coexist in a single cell — a situation termed heteroplasmy. Mitochondrial DNA was thought to derive exclusively from maternal egg cells, with no paternal contribution, but Luo et al. challenge this dogma, identifying three families with mtDNA heteroplasmy caused by biparental mitochondrial inheritance. Previous work has shown that mitophagy, the process by which cells ‘eat’ their own mitochondria, has a role in the selective elimination of paternal mitochondria. These rare instances of paternal mtDNA transmission might therefore be attributed to defective mitochondrial turnover. Original research: Proc. Natl Acad. Sci. USA 115, 13039–13044 (2018). Credit: Péter Fankhauser/ANYbotics AG Young animals gallop across fields, climb trees and immediately find their feet with enviable grace after they fall. And like our primate cousins, humans can deploy opposable thumbs and fine motor skills to complete tasks such as effortlessly peeling a clementine or feeling for the correct key in a dark hallway. Although walking and grasping are easy for many living things, robots have been notoriously poor at gaited locomotion and manual dexterity. Even a robot that performs beautifully in simulation will stumble and fall after a few encounters with seemingly minor physical obstacles. Writing in Science Robotics, Hwangbo et al. report that a data-driven approach to designing robotic software can improve the locomotion skills of robots. They demonstrate their method using the ANYmal robot (pictured) — a medium-dog-sized quadrupedal system. Original research: Sci. Robot. 4, eaau5872 (2019). A copper-catalysed reaction, known as the CuAAC reaction, is the poster child for click chemistry. A reaction is defined as click chemistry if it is (among other things) operationally simple, high-yielding, applicable to a broad range of compounds, yet exceptionally selective — the chemical groups that undergo the reaction must react only with each other. CuAAC reactions are used in many disciplines, but their applications would be even broader if structurally complex azide compounds (which contain N3 groups) were more widely available to use as reactants. Meng et al. report that fluorosulfuryl azide (FSO2N3) reacts with almost any primary amine (compounds that contain NH2 groups) to achieve a nearly 100% yield of the corresponding azide, and used their reagent to make a library of 1,224 azides. Their reaction meets the speed, breadth and efficiency criteria for click chemistry. Moreover, the prepared azide solutions can be used directly in CuAAC reactions. Original research: Nature 574, 86–89 (2019). We asked readers to vote for a News & Views article to be included as part of our round-up of the year. This is the one they chose. Détroit et al. report the remarkable discovery of a human relative that will no doubt ignite plenty of scientific debate. This newly identified species was found in the Philippines and named Homo luzonensis. Rapidly changing knowledge about hominin evolution in Asia is forcing the re-examination of ideas about early hominin dispersals from Africa to Eurasia. Homo luzonensis provides yet more evidence that hints that Homo erectus might not have been the only globe-trotting early hominin. The interesting mix of features observed in H. luzonensis raises questions about the species’ ancestry and its relationships with other human relatives. One thing can be said for certain — our picture of hominin evolution in Asia just got even messier, more complicated and a whole lot more interesting. Original research: Nature 568, 181–186 (2019). </body>
<date id = '330'>13 December 2019</date>
<url id = '331'>https://nature.com/articles/d41586-019-03729-4</url>
<title id = '331'>Experiments show that quantum fluctuations can allow heat to be transported between two objects separated by a vacuum gap. This effect could be harnessed to exploit and control heat transfer in nanoscale devices.</title>
<body id = '331'>Acoustic waves and electromagnetic waves can transport heat between objects through their respective energy carriers: phonons and photons. At or near room temperature, the heat transfer between objects separated by a material medium occurs at a much higher rate when facilitated by phonons than by photons. However, phonons are generally thought to be ineffective at transporting heat between objects separated by a vacuum gap, because these energy carriers are vibrations in an atomic lattice and thus would require a material medium to propagate. Writing in Nature, Fong et al.1 report experimental evidence that phonons can travel across a vacuum gap and therefore induce heat transfer between vacuum-separated objects because of the effect of quantum fluctuations.   In simple terms, quantum fluctuations can be understood as being the source of an electromagnetic signal that a perfectly sensitive detector would detect in a vacuum, even when this vacuum is shielded from all possible internal and external sources of electromagnetic waves, such as charges and currents2. The fluctuations are a consequence of a law in quantum mechanics known as Heisenberg’s uncertainty principle3, which states that certain pairs of physical quantities cannot be determined at the same time with absolute precision. The presence of quantum fluctuations subtly influences surrounding matter, leading to several observable effects. One of these effects, relevant to Fong and colleagues’ work, is the Casimir force4 — the force that two neutral atoms separated by a vacuum gap exert on each other. The Casimir force results when quantum fluctuations induce fluctuating charge densities in these atoms; the charge densities then interact through their electric fields. The force that sticks a gecko’s foot to a wall is an example of a macroscopic manifestation of the Casimir force. It arises from the combined interactions between fluctuating charge densities in all the atomic constituents of the two objects. To understand how the Casimir force can induce phonon transfer between vacuum-separated objects, consider an object that is maintained at a particular temperature by being kept in contact with a heat source (Fig. 1). Thermal agitation of the object’s atoms, which can be thought of as being interconnected by elastic springs, gives rise to phonons. In the presence of these phonons, the surface of the object undulates over time. When a second object is brought close to the first one, it is subjected to a time-varying Casimir force owing to its interaction with the undulations of the first object’s surface. The second object’s surface is thus subjected to tugging that then gives rise to phonons in the object’s interior. Phonons are therefore transmitted from the first object to the second one. Figure 1 | Phonon transmission across a vacuum. Fong et al.1 show that phonons — vibrations in an atomic lattice — can be transported between objects that are separated by a vacuum gap. To understand how this process occurs, consider an object at a fixed temperature T1. Thermal agitation of the object’s atoms produces phonons that propagate as acoustic waves and cause the object’s surface to exhibit time-varying undulations (the amplitudes of the undulations shown are exaggerated for clarity). A second object, at a fixed temperature T2 < T1, is brought close to the first object, with a vacuum gap between the objects. The undulations of the first object’s surface exert a time-varying ‘Casimir’ force (caused by quantum fluctuations) on the second object’s surface, which gives rise to phonons in the second object. Because phonons are heat carriers, heat is transferred from the first object to the second one. Because phonons are heat carriers, when they are transported from one object to another across a vacuum gap, as a result of the Casimir force, they induce heat transfer if the second object is maintained at a lower temperature than that of the first one. This phenomenon of heat transport facilitated by the Casimir force has been predicted previously using theoretical models5–7. Fong et al. have now measured such a heat-transfer mode experimentally. The authors used a technique called optical interferometry to observe the thermal agitation of atoms (Brownian motion) at the surface of a membrane. This membrane was kept in contact with a heat source held at a constant temperature. Measurements of thermal agitation can be related to, and therefore used as a gauge for, the temperature of the atoms at the membrane’s surface. Moreover, the difference in this temperature with and without Casimir interaction with another, closely juxtaposed membrane is directly proportional to the resulting heat transfer between the two interacting membranes. The authors used these features to estimate the amount of heat transmitted between the membranes for vacuum gaps of different sizes. They found that their measurements accurately conform to theoretical estimates of such heat transport.   Fong and colleagues’ work provides conclusive evidence that the Casimir force can induce heat transfer. However, the use of this method to transport heat between two objects is limited, because the Casimir force decreases rapidly in strength as the space between the objects is increased. It is only when the gap between two objects is of the order of a few nanometres that the Casimir force is strong enough for this heat-transfer mode to dominate over competing modes, such as photon tunnelling8. The authors discovered a way to amplify the Casimir mode of heat transfer so that it remains dominant even when the gap between the membranes is in the range of hundreds of nanometres. The membranes were carefully designed in such a way that their dimensions and the temperatures at which they were maintained allowed them to vibrate with their maximum possible displacements — in other words, at their natural frequencies. Thus, applications that are devised to exploit this heat-transfer mode to dissipate heat (such as in a hard-disk drive, where the distance between the writing head and the storage disk is a few nanometres) would require such careful design to ensure that the mode is amplified. Achieving this would be a challenge for the future. </body>
<date id = '331'>11 December 2019</date>
<url id = '332'>https://nature.com/articles/d41586-019-03731-w</url>
<title id = '332'>Enhancing antitumour immune responses has revolutionized cancer treatment, yet some hurdles impede this approach. The discovery of a way to boost the lifespan and function of antitumour immune cells removes a key obstacle.</title>
<body id = '332'>Immune cells called cytotoxic CD8 T cells can directly kill tumours and are key weapons mobilized in many immunotherapy approaches used in the clinic. However, the cells’ activity can be thwarted by the ability of tumours to create harsh microenvironments, recruit immunoregulatory cells and induce inhibitory signals that hamper T-cell function, accumulation and tumour infiltration. Writing in Nature, Wei et al.1 report that depletion of the protein REGNASE-1 extends the survival of antitumour CD8 T cells and enhances their function, enabling the cells to fight cancer more effectively.   The development of anticancer clinical strategies that use immune cells has profoundly improved the treatment of certain malignancies. The delivery of T cells that can specifically target tumours is used in an approach called adoptive T-cell therapy (ACT), which relies on T cells that have been taken from a person’s blood or tumour. These cells are stimulated in the laboratory to cause them to divide and to increase the number of antitumour T cells, and, in some cases, they are modified to enhance their ability to eliminate cancer cells2. For example, T cells can be engineered to express a receptor, called a chimaeric antigen receptor (CAR-T), that specifically targets tumours, and such cells are remarkably successful at treating leukaemia. Despite the immense potential of this strategy, the use of ACT is currently limited because the modified T cells that are transferred back to a person with cancer can be short-lived, and are often unable to overcome a tumour’s ability to hinder their function. When naive T cells recognize a disease-causing agent or a tumour cell, they proliferate to form short-lived tumour-killing (that is, cytotoxic) CD8 T cells (also known as effector cells) that kill these infected or malignant cells (Fig. 1). If the infection or the tumour cells are eliminated, most of these CD8 T cells die, but a small population remains in the form of long-lived memory T cells, which are self-renewing and can generate cytotoxic CD8 T cells if the same infection or malignancy is encountered again3. Figure 1 | Boosting T cells to enable a sustained antitumour response. Immunotherapy uses immune cells called T cells to target cancer in the clinic; however, these tumour-killing cytotoxic CD8 T cells are short-lived. Moreover, they can be inhibited by the tumour microenvironment and lose their functional activity because of chronic stimulation if the tumour is not eliminated. Wei et al.1 studied the factors affecting T-cell responses against tumours in mouse models. A T-cell response begins when a naive T cell becomes activated by recognizing its target protein (not shown). Activated T cells can give rise to memory-like T cells that can self renew or generate cytotoxic CD8 T cells. The authors report that if T cells are engineered to lack the protein REGNASE-1, they are reprogrammed to form high numbers of memory-like T cells and have enhanced antitumour activity compared to the case of REGNASE-1-expressing T cells. Therefore, if T cells are REGNASE-1 deficient, they are better equipped to sustain high numbers of cytotoxic CD8 T cells in tumours and produce a prolonged antitumour response. Loss of REGNASE-1 provides the joint benefit of generating T cells that have more tumour-killing activity and memory-like qualities. Whether these reprogrammed memory-like T cells directly contribute to tumour-killing activity is unknown. By contrast, T cells that express REGNASE-1 produce a short-lived antitumour response that fails to overcome tumour inhibition of the immune response. However, if the infection or tumour cannot be eliminated, the cytotoxic T cells progressively lose their function (a process termed exhaustion). The ideal population of T cells for use in ACT would infiltrate tumours and accumulate in substantial numbers while retaining cytotoxic function and the capacity for self-renewal2. Yet the differentiation of T cells into cytotoxic CD8 T cells impairs successful retention of the potential to form long-lived memory cells. This raises the question of whether a strategy can be found to induce both of these beneficial traits in T cells used for ACT. It has been speculated that, in the unforgiving tumour microenvironment, CD8 T cells would need to have a robust metabolism to sustain the nutritional and energetic requirements needed for survival and to retain their antitumour activity4. Wei et al. used the CRISPR–Cas9 gene-editing technology to disrupt more than 3,000 genes associated with metabolism in T cells, to test their functions in a mouse model of antitumour ACT. The authors identified more than 200 genes that have a striking ability to affect the persistence and function of the CD8 T cells transferred into tumour-bearing mice. The disruption of many genes had a negative effect on the ability of the cells to persist and thus accumulate in tumours, but the disruption of four genes resulted in a much higher than normal number of T cells infiltrating the tumours. At the top of this list is the gene that encodes the enzyme REGNASE-1. Its deletion in CD8 T cells caused 2,000 times more of these cells to accumulate in tumours than did CD8 T cells that expressed REGNASE-1. This enzyme binds to and degrades RNA, and influences immune responses5–7, but its role in the antitumour function of CD8 T cells had not been explored. CD8 T cells that lacked REGNASE-1 were better than wild-type CD8 T cells at fighting two types of tumour in mice: an aggressive skin cancer called melanoma and a blood cancer termed acute lymphocytic leukaemia. The REGNASE-1-deficient CD8 T cells proliferated at a similar rate to the wild-type cells, but did not die as rapidly, allowing them to accumulate. To better understand how REGNASE-1 deficiency resulted in this increased persistence of T cells, the authors analysed gene-expression profiles of REGNASE-1-deficient and wild-type cells. REGNASE-1 deficiency was linked with an increase in a molecular signature characteristic of memory T cells, suggesting the presence of a larger than normal population of long-lived memory-like cells that can give rise to cytotoxic CD8 T cells. REGNASE-1-deficient CD8 T cells showed striking increases in mitochondrial function (mitochondria are organelles that provide a crucial source of cellular energy), including the ability to produce energy and consume oxygen. This is notable because this capacity is often compromised in tumour-fighting T cells8. These combined effects of REGNASE-1 deficiency enabled CD8 T cells and CAR-T cells used for ACT to accumulate and remain active over time in the cancers targeted in the mouse models.   To further understand this mechanism, Wei and colleagues used CRISPR–Cas9 to disrupt approximately 20,000 genes in REGNASE-1-deficient CD8 T cells, to pinpoint key downstream genes that mediate the REGNASE-1-dependent cellular reprogramming. The inactivation of the transcription factor BATF, a key regulator of the differentiation of CD8 T cells9, abolished the long lifespan of T cells lacking REGNASE-1 and their high expression of genes associated with mitochondria. The authors found that the combined depletion of REGNASE-1 with that of either of the proteins PTPN2 or SOCS1 had a synergistic effect that increased the persistence, accumulation and antitumour activity of T cells compared with the properties of T cells that were deficient only in REGNASE-1. Wei and colleagues report that REGNASE-1-deficient CD8 T cells had a higher expression of cytotoxic proteins than did wild-type CD8 T cells in both the memory-like and the cytotoxic CD8 T cells in tumours. Wild-type cells with memory-like properties typically do not kill tumour cells directly10. It is not clear whether REGNASE-1-deficient memory-like T cells function solely to self-renew and produce the cytotoxic CD8-T-cell population, or whether they can also directly mediate tumour-cell killing, given that their expression of cytotoxic molecules is higher than that of wild-type CD8 T cells. If they do have a role in tumour-cell killing, how do these REGNASE-1-deficient memory-like CD8 T cells manage to both do this and maintain the population of cytotoxic CD8 T cells? Intriguingly, the authors show that, for increased persistence, the REGNASE-1-deficient CD8 T cells need to encounter the tumour protein that they recognize. This might explain why the accumulation of antitumour T cells was more pronounced in mouse tumours than in the animals’ spleens, which are rich in T cells but are located away from the sites of exposure to the tumour proteins. It remains to be investigated whether other cues in the tumour microenvironment contribute to boosting the persistence of REGNASE-1-deficient CD8 T cells. To this end, it might be informative to assess the metabolic profile of CD8 T cells in the tumour that have combinatorial depletions of REGNASE-1, PTPN2, SOCS1 and BATF. This could provide insights into the effect of these proteins on the reprogramming of CD8-T-cell metabolism and to what extent this is important for the cells’ differentiation and antitumour function. Also, finding the relevant metabolites in this context might provide clues to how these CD8 T cells can be influenced by a nutritionally depleted tumour microenvironment. Wei and colleagues’ study reveals promising leads that might result in advances in ACT-based immunotherapies. It will be worth testing whether engineering CD8 T cells to delete or express low levels of the gene encoding REGNASE-1 would be feasible as part of the manufacturing process for CAR-T cells. Finally, given that the inhibition of PTPN2 in tumour cells sensitizes them to immunotherapy11, this study offers a strong incentive to investigate the use of combinatorial approaches, including REGNASE-1 and PTPN2 inhibitors, as a way to reprogram CD8 T cells to improve current therapies. </body>
<date id = '332'>11 December 2019</date>
<url id = '333'>https://nature.com/articles/d41586-019-03670-6</url>
<title id = '333'>Immune cells called cytotoxic T cells can recognize and destroy cancer cells. The finding that stem-cell-like T cells exist in tumours, at niche sites that support these cells, could aid efforts to boost anticancer immune responses.</title>
<body id = '333'>Certain anticancer treatments have been revolutionized by the ability to harness a person’s own immune cells for therapeutic purposes1. Such immunotherapy can result in lasting anticancer responses in people with advanced-stage blood cancers or solid tumours. But not everyone responds. For a variety of cancers, the presence of cytotoxic T cells — immune cells that can kill cancer cells — in a tumour correlates with, but does not predict, an anticancer response and survival. And it is unclear why robust tumour infiltration by T cells occurs in some people, but not in others. Writing in Nature, Jansen et al.2 reveal a previously unknown source of tumour-infiltrating T cells.   Because tumour cells can proliferate continuously, tumour-targeting T cells must have a similar ability to persist and divide until the last remaining tumour cell is eradicated. In people undergoing immunotherapy, a greater longevity of antitumour T cells correlates with a better therapeutic outcome3. Therefore, for effective immunotherapy, it is crucial to understand the factors that influence T-cell persistence and infiltration of tumours. Some clues already exist4 about these factors, such as the presence of long versions of structures called telomeres, found at the ends of chromosomes, and high expression levels of the protein CD27 in T cells. In addition to these factors, another clue came from the identification of a subset of stem-cell-like T cells called memory T cells, which can provide long-lasting immune responses5,6, and which express high levels of TCF7 (previously known as TCF-1). This protein is important for maintaining a stem-cell-like state in T cells that also express the protein CD8 (known as CD8 T cells)7,8. Such stem-cell-like cells can self-renew and give rise to different types of T cell, including a type of CD8 T cell called a cytotoxic CD8 T cell. The presence of stem-cell-like T cells in people who have cancer was reported previously5; however, the anatomical location of these cells had not been elucidated. Jansen and colleagues now show that human kidney tumours contain stem-cell-like T cells that reside in the tumour in niches that support them (Fig. 1). Figure 1 | Stem-cell-like T cells reside in niches in tumours. Jansen et al.2 report the discovery of stem-cell-like T cells that inhabit kidney tumours. a, The authors compared the profile of tumour samples obtained from people who had undergone tumour-removal surgery. In non-responders, whose cancer progressed more rapidly after surgery, there was a low level of tumour infiltration by cancer-killing (cytotoxic) T cells of the immune system. These tumours contain antigen-presenting cells (APCs) and structures called lymph vessels. b, By contrast, people who had longer progression-free survival after surgery had a high level of tumour infiltration by cytotoxic T cells. The authors report that the tumours of these responders had sites, called niches, containing stem-cell-like T cells that could give rise to cytotoxic T cells. These niches were associated with APCs and lymph vessels. There were more lymph vessels in these responding tumours than in the non-responding tumours. The authors investigated how the level of tumour-infiltrating cytotoxic CD8 T cells varied. They analysed samples of human kidney tumours obtained from people who had undergone tumour-removal surgery, and noted a wide variation in the level of T-cell infiltration between the samples. In people who had tumours in which CD8 T cells accounted for fewer than 2.2% of cells in the sample, the cancer continued to grow, indicating that the surgery and the person’s immune response to the residual cancer cells were insufficient to halt disease progression. By contrast, above this threshold of 2.2% infiltration, cancer growth after surgery was four times slower. Jansen and colleagues then studied the composition and type of T cell in the tumour samples using a technique called flow cytometry, and identified two distinct sets of T cell. One set consisted of cytotoxic CD8 T cells that express high levels of cancer-killing molecules but that also express ‘immune-checkpoint’ molecules. Expression of checkpoint molecules can drive cytotoxic T cells to enter a dysfunctional state known as exhaustion, which can occur in the tumour microenvironment after prolonged exposure to cancer cells recognized by the T cells. The other set consisted of stem-cell-like T cells that Jansen et al. demonstrate give rise to cytotoxic CD8 T cells that help to promote an effective antitumour immune response. Stem-cell-like T cells were present only at very low levels in tumours with low levels of T-cell infiltration, whereas tumours with high levels of T-cell infiltration had high levels of the stem-cell-like T cells. To gain further insight, the authors assessed cellular gene-expression profiles, and analysed epigenetic modifications — types of modification to DNA and its associated proteins that can affect gene expression. They found that, compared with the exhausted cytotoxic CD8 T cells, the stem-cell-like T cells express distinctive immune-signalling molecules called chemokines that are correlated with better patient survival, along with higher levels of key co-stimulatory molecules (which are essential for T-cell differentiation into cytotoxic T cells). Previous analyses9,10 of T cells revealed a pattern of progressive steps in epigenetic modification as stem-cell-like T cells give rise to cytotoxic CD8 T cells and then eventually become exhausted.   The epigenetic-modification profile of T cells in tumours can be profoundly influenced by factors in the tumour microenvironment, which can affect the ability of T cells to function as stem cells11,12. For example, the concentration of potassium ions in a tumour modulates epigenetic modifications that influence whether T cells are in the stem-cell-like state that is needed for them to give rise to cytotoxic CD8 T cells11,12. The effect of the tumour microenvironment on the development of cancer-targeting T cells is unclear, and should be a subject for future studies. Jansen and colleagues noted that the higher than normal expression of chemokines and chemokine-binding receptors in the stem-cell-like T cells is similar to that seen in cells in the microenvironment of lymph vessels — structures through which immune cells move and which support T-cell activation and survival. The authors’ analyses demonstrate that stem-cell-like T cells are located in niches in tumours near lymph vessels (Fig. 1), and are confined to dense zones of antigen-presenting cells, which can prime T cells to target tumours. The discovery of these niches by Jansen and colleagues now reveal how stem-cell-like T cells can be maintained in tumours in a functional state capable of generating cytotoxic T cells. The authors observed a correlation between the presence of protein markers of stem-cell-like T-cell niches and longer, progression-free survival of the people assessed in the study. By contrast, other common ways of assessing an immune response in tumours, such as the expression of the immune-checkpoint protein PD-L1, did not reveal a correlation with progression-free cancer survival. Previous research13 identified stem-cell-like T cells that express rising levels of immune-checkpoint molecules as they progress towards forming cytotoxic CD8 T cells that eventually become exhausted14. In one example13, approaches to block the immune-checkpoint protein PD-1 caused a burst of proliferation in stem-cell-like T cells that express the TCF7 protein. Similarly, in a skin cancer called melanoma, people whose CD8 T cells express TCF7 have a better clinical outcome if they receive immunotherapy to block immune-checkpoint proteins15. These results suggest that people whose tumours cannot be removed by surgery might benefit from therapy that blocks immune-checkpoint molecules, if their tumours contain stem-cell-like T cells. Jansen and colleagues’ work raises questions about how the stem-cell niches are generated and maintained, and whether tumours might act on them to evade destruction by the immune system. The discovery that resident stem-cell-like T cells exist in specialized niches in tumours suggests that clinical leveraging of such cells to increase the immune infiltration of tumours, together with immunotherapy to boost exhausted T cells, might unleash T-cell responses to aid the success of anticancer treatment. </body>
<date id = '333'>11 December 2019</date>
<url id = '334'>https://nature.com/articles/d41586-019-03733-8</url>
<title id = '334'>How Nature reported a boost for research into human intestinal protozoa in 1919, and a European celebration of conservation in 1969.</title>
<body id = '334'> Conferences, competitions, special postage stamps, film festivals, a youth parliament and Viking beacons will be some of the features of European Conservation Year, to be launched in the United Kingdom in the traditional way with a dinner at the Guildhall, London, on December 16. Twenty-five countries are involved in this concerted effort to communicate the need to stop spreading fear and gloom and to do something about pressures on the environment. The highlight of the year seems likely to be the gathering of top people, including Mr Anthony Crosland, the Secretary of State for Regional Government and Planning, at the European Conservation Conference in Strasbourg next February. Later in the year there will be a large international congress in London organized by the World Wildlife Fund. The Netherlands will have a conservation fleet and Germany a State Railways exhibition train to carry the good news from place to place. Italy is to have a film festival with a prize of a million lire for the best film on conservation, and four Scandinavian countries will light Viking warning pyres every ten miles along two thousand miles of coast; it is to be hoped that they do no harm to the environment. The youth parliament will be in Stockholm, where the Prime Minister will join the young delegates. Special stamps are to be issued by at least fifteen of the participating countries, and most of them will be organizing competitions for schools. In Britain the Nature Conservancy and the Shell organization are to run a competition for secondary school pupils, who will be encouraged to study local problems and suggest conservation projects. The winners, after competing in public for the final honours, will be taken on a tour of Europe at the expense of Shell. From Nature 13 December 1969 In 1915 the return to this country of large bodies of troops from the Eastern war area—many afflicted with dysentery—rendered it necessary to examine the stools of a very large number of patients in order to decide whether those returning from Gallipoli and Egypt were suffering, as was supposed (but shown to be erroneously so), from “amoebic dysentery.” A large number of trained workers were required for this purpose. Their training was undertaken, at first, by Dr. C. M. Wenyon, but when his services were required elsewhere at the end of 1915, Prof. Dobell took charge of the work and for four years has devoted himself uninterruptedly to the practical study of the intestinal protozoa of man. A large part of his time has been occupied with the routine work of diagnosis, with teaching that routine to others, and with the investigation of methods of treating amoebic dysentery. But, as he says, he has had great opportunities for studying the human intestinal protozoa from the zoological point of view, and probably no zoologist has ever before had such an immense amount of this special material at his disposal. From Nature 11 December 1919 Latest on: History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Microbiology News 25 MAY 20 News 22 MAY 20 Career Column 22 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '334'>10 December 2019</date>
<url id = '335'>https://nature.com/articles/d41586-019-03730-x</url>
<title id = '335'>A double membrane protects certain bacteria from antibiotics, but compounds have now been generated that can overcome this obstacle, seemingly by targeting a crucial protein in the outer membrane.</title>
<body id = '335'>Antibiotic resistance is a growing global public-health problem1. One group of bacteria, called Gram-negative bacteria, is particularly difficult to treat, because the cells are shielded by a double-membrane envelope, which constitutes a formidable barrier to antibiotics2. When antibiotics do breach the membranes, these bacteria often use efflux pumps to remove the drugs3,4. Three papers (two in Nature5,6 and one in the Proceedings of the National Academy of Sciences7) now describe antibiotics that overcome these obstacles by targeting, directly or indirectly, a protein integral to the outer membrane. The outer membrane of Gram-negative bacteria contains lipopolysaccharide (LPS) molecules in its outer leaflet, with outer-membrane proteins (OMPs)8 spanning the entire outer membrane. OMPs are folded into the membrane by a protein complex called the β-barrel assembly machine (BAM), the central component of which, BamA, is an OMP itself (Fig. 1). Because BamA is exposed to the extracellular space, it could be an Achilles heel in the bacterial shield — inhibitors that access BamA would not need to penetrate the cell. Indeed, a proof-of-concept study9 has shown that this approach inhibits OMP folding and compromises membrane integrity, albeit by an unknown mechanism. Figure 1 | Overcoming a double-membrane barrier. Gram-negative bacteria are protected by inner and outer membranes. The outer membrane contains lipopolysaccharide (LPS) molecules in the outer layer and integral outer-membrane proteins (OMPs). These proteins are synthesized in the cell’s cytoplasm and transported to the space between the membranes by the translocation machinery (dark blue). From here, they are captured, inserted and folded into the outer membrane by the BAM protein complex (red arrows). BamA is the central component of BAM and is accessible from the bacterial surface. Three studies5–7 describe new antibiotics that seem to target BamA, preventing the normal OMP folding that is required for bacterial survival. The three current studies took different approaches to develop antibiotics against Gram-negative bacteria. In the first, Imai et al.5 turned to Gram-negative bacteria that live symbiotically in the gut of nematode worms and can secrete antibiotics to fend off competing bacteria — including other Gram-negative species. A screen of the secretions from 22 of these symbionts revealed a Gram-negative-targeting antibiotic, which the authors named darobactin. Darobactin displayed antibiotic activity against multiple Gram-negative bacteria, both in vitro and in infected mice, including against several drug-resistant human pathogens such as polymyxin-resistant Pseudomonas aeruginosa and β-lactam-resistant Klebsiella pneumoniae and Escherichia coli. Darobactin was not toxic to human cells at the concentrations at which it was an effective antibiotic.   Next, Imai et al. asked what bacterial molecule darobactin targets. The group identified three strains of E. coli that were resistant to darobactin and showed that each harboured mutations in the bamA gene. The mutations all changed amino-acid residues in the same region of BamA’s protein structure, suggesting a putative binding site for darobactin that would be accessible from the extracellular space. The authors provided evidence that darobactin and BamA bind to each other directly, using a technique called isothermal titration calorimetry, which measures the heat changes associated with physical interactions between molecules. The results of nuclear magnetic resonance (NMR) spectroscopy experiments were also consistent with direct binding, and suggested that the antibiotic stabilizes the protein in a potentially inactive conformation. The researchers next showed that darobactin inhibits the ability of an isolated BAM complex to perform its OMP-folding function in vitro, consistent with direct BamA targeting. However, only one of the resistant BamA mutants showed reduced inhibition by darobactin in this assay. A test of whether darobactin–BamA binding is impaired in the bamA mutants could be used in the future to confirm BamA as the molecular target.   In the second of the current studies, Luther et al.6 focused on analogues of an existing antibiotic, murepavadin10, which targets a surface-exposed protein called LptD that is involved in assembling LPSs in the outer membrane8. Murepavadin displays potent but narrow antibiotic activity against P. aeruginosa10. The authors therefore screened for murepavadin analogues that had antibiotic activity against other Gram-negative species. Luther and colleagues chemically linked the compounds identified through this screen to a portion of another antibiotic, polymyxin B, that binds to LPS directly11. Intact polymyxins efficiently disrupt bacterial membranes and kill cells, but are rather toxic to humans12. The researchers hoped that linking just the LPS-binding portion of polymyxin B could increase the membrane targeting of their murepavadin analogues. Indeed, their strategy produced several chimaeras that had potent activity, both in vitro and in mice infected with K. pneumoniae, P. aeruginosa, E. coli and other Gram-negative bacteria, including drug-resistant strains. Notably, the chimaeras showed low toxicity in mice. It might be expected that the chimaeras would target LptD, but when Luther and colleagues tested for interacting partners, they found evidence of BamA targeting. The authors analysed strains of K. pneumoniae that showed resistance to the chimaeras. They found that resistant strains carried mutations in several genes, including bamA and genes responsible for LPS modification. Reintroduction of the wild-type bamA gene into the resistant strains led to increased sensitivity to the chimaera, indicating that BamA has a role in the antibiotic’s mechanism of action. Direct chimaera–BamA binding was confirmed with in vitro assays in which the authors fluorescently labelled the chimaeras and monitored changes in fluorescence that indicate binding to a large protein such as BamA. As with darobactin, NMR experiments suggested that chimaera binding stabilizes BamA in a potentially inactive conformation, consistent with direct BamA targeting. However, when the bacteria were treated directly with the chimaeras, both the outer and inner membranes were rapidly permeabilized; this suggests that the compounds might act directly on the membrane. The results raise the possibility that the chimaeras act in a similar way to polymyxins, with binding to BamA strengthening their membrane targeting. In the third study, Hart et al.7 identified a compound, MRL-494, that had similar antibiotic potency against both wild-type E. coli and a mutant defective in outer-membrane integrity and efflux mechanisms, suggesting that this antibiotic might not need to penetrate the cell to exert its activity. In vitro, MRL-494 exhibited moderate potency against Gram-negative pathogens, including K. pneumoniae and P. aeruginosa. The efficacy of MRL-494 in animal models remains to be tested. The authors showed that treatment of E. coli with the compound resulted in decreased abundance of OMPs in the outer membrane, indicating BamA as a possible target. In support of this possibility, Hart et al. identified a bamA mutation that confers resistance to MRL-494 in E. coli. They showed that, whereas MRL-494 inhibited normal folding of a model OMP in E. coli cells expressing wild-type bamA, it had less effect on the resistant cells. The researchers found that MRL-494 stabilizes BamA against heat-induced protein aggregation in cells, suggesting an interaction between the two. However, MRL-494 stabilizes the resistant bamA mutant to a similar extent. Furthermore, MRL-494 displays similar potency against Gram-positive bacteria, which lack BamA. Therefore, in Gram-negative bacteria, MRL-494 might inhibit BamA directly or might target the outer membrane and affect BamA function indirectly. Together, these studies describe new antibiotics that are active against difficult-to-treat Gram-negative bacteria. Given the compounds’ size and chemistry, they are likely to act at the cell surface, bypassing the need to breach the permeability barrier. Imai et al. provided compelling evidence that BamA is the target of darobactin, including a putative binding site, to be confirmed by demonstrating reduced binding to resistant mutants. The chimaeric compounds both seem to bind BamA and LPS. But, as is also the case for MRL-494, further experiments will be required to determine whether their activity is caused by direct effects on BamA. Future research to identify specific BamA binding sites for any of the compounds, and to examine the mechanism by which antibiotic binding impairs BamA activity, would provide a platform for further antibiotic development. Such research might also shed light on how BAM mediates the insertion and folding of OMPs, which is poorly understood. Darobactin and MRL-494 are initial lead compounds, and medicinal-chemistry efforts could yield more-potent and effective analogues. Preclinical studies aimed at determining their toxicity in animal models will also be important. Luther and colleagues’ chimaeras are at a more advanced stage of development, because, as the authors show, they have potent in vivo activity as well as favourable toxicity, pharmacokinetics and pharmacodynamics in animal models. The future looks promising for this newly discovered class of antibiotic. </body>
<date id = '335'>09 December 2019</date>
<url id = '336'>https://nature.com/articles/d41586-019-03734-7</url>
<title id = '336'>In 1994, an unconventional form of superconductivity was detected in strontium ruthenate. The discovery has shed light on the mechanism of unconventional superconductivity at high temperatures.</title>
<body id = '336'>Superconductivity is an effect in which a material’s electrical resistance vanishes and any magnetic field is expelled below a transition temperature. Despite the remarkable phenomenology, this behaviour is actually quite common: almost half the elements in the periodic table are superconductors1, albeit at temperatures near or below the extremely low one at which helium gas liquefies (about 4 kelvin). Since Nobel-prizewinning work in the late 1950s, we have had a successful theory2 of superconductivity in these conventional systems. Electrons bind into ‘Cooper pairs’ that have isotropic (direction-independent) properties through an interaction with vibrations of surrounding ions. Over the past 40 years, researchers have looked for unconventional superconductors that involve different pairing interactions, such as magnetic ones. In 1994, Maeno et al.3 reported one of the clearest examples of unconventional superconductivity, in strontium ruthenate near 1 K.   Understanding unconventional superconductors requires identifying both the pairing interaction and the order parameter — a quantity that reflects the interaction and the macroscopic, typically anisotropic, properties of the unconventional superconductivity. The most substantial development in this area of study was the discovery of superconductivity in layered copper-oxide compounds (known as cuprates) in the mid-to-late 1980s. The phenomenon was detected4 at the unprecedentedly high temperature (for that time) of 30 K, which led to a worldwide effort to understand the mechanism of cuprate superconductivity. The cuprates are now thought to have a highly anisotropic order parameter, and to have Cooper pairs made of electrons that have anti-aligned spins (intrinsic angular momenta). Such spins form non-magnetic states that have even parity, which means that the wavefunction of the state does not change sign if the signs of the spatial coordinates are flipped. Cuprate superconductivity has been proposed5 to arise from an interaction of electrons with antiferromagnetic spin fluctuations (antiferromagnetism is a form of magnetism in which spins are anti-aligned with their neighbours). However, no theory has yet gained general acceptance. One method that has been used to try to understand these compounds is to search for superconductivity in materials that are related in some way to the cuprates. In this way, it might be possible to identify the structural, electronic or magnetic features that are essential for the materials’ high transition temperatures. In particular, the cuprate discovery led to a huge effort to investigate compounds that contain transition metals other than copper. It was against this backdrop that Maeno and colleagues found superconductivity in strontium ruthenate, at about 1 K. This was decidedly not high-temperature superconductivity. But the work caused tremendous excitement because it described the detection of superconductivity in another layered transition-metal oxide — and in a material that has the same crystal structure as the original superconducting cuprate, lanthanum barium copper oxide4 (Fig. 1). Almost immediately, it was realized that there were both similarities and differences between the cuprates and strontium ruthenate. Figure 1 | Crystal structures of two superconductors. In 1986, lanthanum barium copper oxide was found4 to superconduct (transport electricity without resistance) at the relatively high temperature of 30 kelvin. Eight years later, Maeno et al.3 reported the discovery of superconductivity in strontium ruthenate at about 1 K. Although these two materials have the same crystal structures at high temperatures, their superconductivity mechanisms are likely to be markedly different. One main difference is that pure compounds of the cuprates (such as lanthanum copper oxide) are antiferromagnetic insulators and require the substitution of atoms (such as barium for lanthanum) to conduct electricity. By contrast, pure strontium ruthenate is strongly metallic. A striking aspect of the superconducting cuprates is that their metallic state at temperatures above the transition temperature seems to be even more unconventional than their superconducting state. The metallic state is thought to be the result of strong interactions between electrons. A radically new theory of ‘strange metals’ might be needed to understand the high-temperature metallic state and thereby also the superconducting state that forms from it6. In strontium ruthenate, electron interactions are also strong, but they do not change the fundamental character of the metallic state. This aspect, and the fact that related materials in the larger ruthenate family exhibit ferromagnetism (a form of magnetism in which spins are aligned with their neighbours), led to the proposal7 in 1995 that superconductivity in strontium ruthenate could be an analogue of the superfluid A phase in helium-3. In this phase, the compound exists as a superfluid (a zero-viscosity liquid) made from odd-parity Cooper pairs of neutral helium-3 atoms that have aligned spins8. The proposal gained much support, both for the compelling science that suggests it and for the beautiful idea that there could be an odd-parity superconductor driven by ferromagnetism in the same way that the cuprates might be even-parity superconductors driven by antiferromagnetism. Of course, the “great tragedy of Science [is] the slaying of a beautiful hypothesis” by experimental facts9. Experiments always have the final say.   The exciting science, the ability to grow large, extremely pure crystals and an exceedingly collaborative research community pushed superconducting strontium ruthenate forward as a highly active topic of investigation. Moreover, there was the abiding sense that it should be possible to unambiguously determine the nature of the material’s unconventional order parameter, because its high-temperature metallic state — unlike that of the cuprates — seemed to obey the conventional theory of metals. This determination is an ongoing saga, with field-changing results coming even this year. Notable early work showed evidence for unconventional odd-parity pairing of electrons in nuclear magnetic resonance (NMR) spectroscopy10, and for spontaneous generation of magnetism11,12 consistent with the proposal outlined above. In the past five years, sophisticated measurements of strontium ruthenate have failed to show an odd-parity superconducting transition splitting into two under mechanical strain, as had been predicted13. These measurements, along with a reinvestigation using NMR spectroscopy14, have given compelling evidence that the superconductivity is likely to be even parity. But this even-parity state is inconsistent with the experiments that showed the presence of spontaneous magnetism. Therefore, the nature of unconventional superconductivity in strontium ruthenate must be considered unresolved. This problem, together with that of the cuprates, has pushed theory, experiment and materials synthesis forward in directions that would have been unimaginable when superconductivity in these compounds was discovered. And as is so often the case, many of the ideas that scientists have grappled with in the context of a hard problem have turned out to be incredibly influential in areas well beyond their original scope. In this particular case, important cross-fertilizing connections can be made with topological insulators (bulk electrical insulators that have conducting surfaces) and quantum computation15. The research community is still hard at work on the mystery of strontium ruthenate. Experiments always have the final say. </body>
<date id = '336'>09 December 2019</date>
<url id = '337'>https://nature.com/articles/d41586-019-03732-9</url>
<title id = '337'>Plaques are lipid-rich structures in the blood-vessel wall that can cause heart attacks or strokes if they rupture. It now seems that blood-cell fragments called platelets alter the function of immune cells in ways that accelerate plaque formation.</title>
<body id = '337'>Heart attacks or strokes might seem to be sudden events, but they are the consequences of a condition called atherosclerosis, which can be decades in the making. Atherosclerosis involves the accumulation of lipids and immune cells into structures called plaques in the blood-vessel wall. If these plaques become unstable they can rupture, blocking blood flow and so depriving tissues such as the heart and brain of oxygen, respectively triggering a heart attack or a stroke. Identifying precisely how plaques grow at cellular and molecular scales is therefore crucial for understanding and so treating atherosclerosis. Writing in Science Translational Medicine, Barrett et al.1 enrich our thinking about how atherosclerosis evolves, providing evidence that platelets in the blood promote the formation of bigger, more dangerous plaques by shaping the function of immune cells. Monocytes are a class of short-lived immune cell crucial to host defence. They survey their environment, patrolling the vasculature and frequently migrating in and out of the blood to scout for injuries or infections. This movement is aided by endothelial cells, which demarcate the border between blood and tissue, and which produce a panoply of monocyte-attracting chemical messengers, enabling monocyte surveillance of and migration across the blood-vessel wall. Platelets — blood-cell fragments best known for making blood clots — likewise help monocytes to infiltrate the vessel wall by adhering to the cells to form monocyte–platelet aggregates. Precisely how such aggregates promote migration is not clear, but it is known that platelets can deliver a variety of mediators to which monocytes can respond2. Because of their role in monitoring the vasculature, monocytes are key to the development of atherosclerosis. Voracious eaters, they ingest lipids that accrue in plaques, before morphing into larger, less agile macrophages. As this transformation occurs, the cells can wreak inflammatory havoc, contributing to a feed-forward loop that generates bigger, rupture-prone plaques3. But because monocytes are crucial for host defence, eliminating them entirely is not therapeutically viable. Identifying and blocking factors involved in monocyte recruitment to plaques might, however, be an alternative strategy.   Barrett and colleagues investigated interactions between platelets, monocytes and their descendent macrophages in mice that have abnormally high levels of cholesterol — a risk factor for atherosclerosis. They observed that platelets adhere to monocytes in blood more readily when mice have high cholesterol levels, bolstering the idea that monocyte–platelet aggregates augment monocyte recruitment to growing plaques. In parallel, the authors performed single-cell RNA sequencing of immune cells retrieved from plaques, and found an increase in platelet-specific factor Pf4 on macrophages, suggesting that platelet adherence persisted beyond monocyte recruitment. Platelets, it seemed, were also aggregating with macrophages. This liaison spells trouble. The group used specific antibodies to deplete platelets in a genetically engineered strain of mouse susceptible to atherosclerosis, and compared the macrophages of these animals with those of counterparts that had not received platelet-depleting antibodies. Single-cell RNA sequencing revealed that exposure to platelets triggers increased production and release of plaque-enhancing inflammatory molecules by macrophages. Interleukin-1β is one such mediator — and, indeed, therapeutic blockade of this protein in humans attenuates cardiovascular disease4. Next, Barrett et al. provided further evidence that the presence of platelets accelerates plaque growth. In addition to inducing inflammatory-molecule production, platelets impaired macrophages’ capacity to ingest dying cells through efferocytosis, increasing the number of undigested dying cells in plaques — a phenomenon that increases the likelihood of plaque rupture. Thus, platelets promote atherosclerosis by fostering monocyte recruitment to plaques and by reprogramming macrophage function (Fig. 1). Figure 1 | How platelets might promote plaques. Atherosclerosis is a condition in which immune cells and lipids aggregate in structures called plaques in the blood-vessel wall. Barrett et al.1 provide evidence for a model in which blood-cell fragments called platelets promote plaque build-up in mice that have high cholesterol levels. In this model, platelets adhere to immune cells called monocytes, and promote (through an unknown mechanism) both production of inflammatory signalling molecules such as interleukin-1β (IL-1β) and movement to plaques. In plaques, monocytes ingest lipids and transform into macrophage cells. Platelets promote expression of SOCS3, a protein that induces macrophages to adopt inflammatory characteristics. The cells then secrete high levels of IL-1β and other inflammatory factors, and have a low capacity to ingest dying cells through efferocytosis. Together, these factors promote plaque growth. The authors next investigated the factors that govern the switch in macrophage function. Two transcription factors, suppressor of cytokine signalling 1 (SOCS1) and SOCS3, are known to influence macrophage behaviour5. Specifically, a low ratio of SOCS1 to SOCS3 triggers gene-expression patterns that lead to inflammatory characteristics, whereas a high ratio prompts tissue-repairing traits. The team found that macrophages taken from plaques in platelet-depleted mice had a higher SOCS1:SOCS3 ratio than did macrophages from untreated animals, indicating that platelets somehow alter this pathway in macrophages to trigger inflammatory characteristics. Finally, Barrett et al. asked whether their findings might be applicable to humans. They found that, in a group of women, the platelet count — and expression of genes that encode SOCS3 and interleukin-1β — was higher in those who had had a heart attack than in those who had not had one. Moreover, the authors report an inverse relationship between the SOCS1:SOCS3 ratio and markers of platelet activation in people with peripheral-artery disease. Thus, this mechanism is potentially relevant to human disease.   Barrett and colleagues’ results are intriguing. Platelet blood-clotting ability serves an essential function in wound healing, but it can be detrimental in the wrong context. Blood-thinning, anti-clot drugs, such as clopidogrel or aspirin, have well-documented therapeutic effects in preventing blood clots. The current study suggests that blocking platelets might have collateral antiatherosclerotic benefits, which aligns with previous work6. Of course, many questions remain. For instance, it is unclear precisely how platelets foster monocyte recruitment and how they reprogram macrophages. There are clues to be found in other work, given that platelets are a source of various immune mediators2,7. Barrett et al. suggest that platelets stimulate macrophages by releasing the protein S100A9, which triggers the inflammatory TLR signalling pathway in macrophages, but this possibility requires further exploration. Another question is whether distinct types of platelet have evolved for specialized cell communication. In support of this idea, research8 suggests that large platelet-producing cells called megakaryocytes, which reside in different locations, have differing functions. Finally, it will be important to know whether all macrophages are equally affected by platelet instruction, or whether the partnership is specific to certain stages of development, anatomical locations or times. The authors caution against drawing sweeping conclusions, and, indeed, there are caveats to the study that should be considered. For instance, it would be useful to reduce platelet levels by approaches other than the anti-CD42b antibody used here. This antibody is expected to deplete platelets only transiently, and it might have collateral, unforeseen effects. In addition, it would be valuable to visualize the aggregates in vivo, perhaps using electron microscopy, to obtain a clear picture of what a macrophage–platelet aggregate really looks like. Finally, future work will need to determine whether this phenomenon occurs broadly in other situations involving monocyte recruitment and consequent macrophage activity, for instance in infected or injured tissue. Nevertheless, the study builds on a long line of work implicating platelets, monocytes and macrophages as key contributors to atherosclerosis. The conceptual power of exploring how immune and blood-clotting pathways intersect, the insights into monocyte and macrophage function, and the corroborating human data, are all worthy of further exploration. </body>
<date id = '337'>09 December 2019</date>
<url id = '338'>https://nature.com/articles/d41586-019-03702-1</url>
<title id = '338'>Scattering between electrons in the material graphene can cause these particles to flow like a viscous liquid. Such flow, which has previously been detected using measurements of electrical resistance, has now been visualized.</title>
<body id = '338'>Water in a river shows a variety of flow patterns and whirls. Any obstacle in the river, such as a bridge pillar or simply a rough bank, will lead to a distinctive flow pattern. It has been comparatively less obvious how electrons flow in a solid. But in a paper in Nature, Sulpizio et al.1 report an experiment in which the flow pattern of electrons in an electrical conductor is imaged.   The electrical resistance of a metal is caused by electrons being scattered from impurities in the material’s atomic lattice or from lattice vibrations called phonons. However, it is not affected by electron–electron scattering. When two electrons scatter off each other, their individual momenta are changed by the scattering event. But the total momentum of the two electrons is conserved, as is the total momentum of a sea of electrons in a metal. Therefore, simply measuring the resistance of a metal will not unveil the effects of electron–electron scattering. To nail down these effects, materials need to be tuned to a regime in which electron–electron scattering is dominant and the electrons flow like a viscous liquid2,3. At low temperatures, electron–electron (as well as electron–phonon) scattering is suppressed and electron–impurity scattering dominates. Conversely, at high temperatures, electron–phonon scattering takes over. For graphene (a single layer of carbon atoms arranged in a honeycomb lattice), there is an intermediate temperature range4 (50–250 kelvin) for which the rate of electron–electron scattering is the highest among all scattering rates (Fig. 1). However, even in this case, the material’s resistance will not be modified by electron–electron scattering because of momentum conservation. Figure 1 | Electron interactions in graphene. The material graphene consists of a single layer of carbon atoms arranged in a hexagonal lattice. Electrons flowing through graphene can be scattered from impurities (such as foreign atoms in the lattice), from other electrons and from lattice vibrations known as phonons. At low temperatures, electron–impurity scattering dominates. By contrast, at high temperatures, electron–phonon scattering takes over. Sulpizio et al.1 report observations of graphene at intermediate temperatures for which the rate of electron–electron scattering is the largest among all scattering rates. One way to investigate the viscous-flow regime has been to measure a local resistance, known as vicinity resistance4, on an extremely small scale. The value of this quantity changes sign in the case of viscous flow. Another option has been to observe an effect called superballistic resistance5 for electrons flowing through a narrow opening in a material. Here, the resistance is reduced below the value expected for a ballistic system, in which there is effectively no scattering. Such pioneering experiments were crucial for demonstrating that viscous electron flow can be important in electron transport. However, they provide only indirect evidence for the existence of such flow and do not give insights into the spatial arrangements of flow patterns. Electrons passing through a sample of a conducting material are driven by an electric field. As a result, there is a voltage gradient along the direction of current flow. Unfortunately, this local voltage gradient is independent of the flow regime. But when a weak magnetic field is applied to the sample, another voltage, known as a Hall voltage, is produced perpendicular to the direction of current flow. The spatial profile of the Hall voltage does provide information about the flow characteristics. Sulpizio and colleagues use a sensitive electric-field sensor that enables local probing of this Hall voltage. The sensor is an innovative technology developed by this research group6. It consists of an electronic device called a single-electron transistor, the conductance of which depends sensitively on its electrostatic environment. In the present work, the sensor is made from ultraclean carbon nanotubes. Individual electrons are confined within these nanotubes by electrodes. Such an arrangement provides the required sensitivity for detecting weak electric fields or voltage gradients, such as those associated with the Hall voltage. The spatial resolution of the sensor is limited by its size and the distance of the sensor to the object to be probed.   Changing the temperature and the number of charge carriers per given area in the sample induces different flow regimes, which lead to different Hall-voltage profiles. Sulpizio et al. use this property to image local electric fields in a uniform layer of graphene, and investigate the transition between the regime in which electron–electron scattering dominates and those in which electron–phonon or electron–impurity scattering takes over. The authors demonstrate experimentally how electron–electron scattering alters the Hall-voltage profile of a uniform conductor. Viscous flow in liquids leads to turbulence and whirls, depending on the viscosity of the liquid and on obstacles to the flow. However, the observation of such features in electron transport is beyond the scope of the present work and could require different experimental tools, such as sensitive magnetic-field sensors, or samples that have complex geometries. What do Sulpizio and colleagues’ results mean for our understanding of electron transport in conductors? In the viscous regime, the flow of electrons is described by a universal hydrodynamic concept known as Poiseuille flow. The authors’ imaging of electronic Poiseuille flow is a breakthrough in the study of electron transport as well as a demonstration of a sophisticated imaging technique that combines high spatial resolution with extreme sensitivity. We now know that electron flow can be diffusive, ballistic or viscous, and that there are experimental tools for differentiating between these regimes. For solid-state systems in general, electron–electron interactions are relevant for phenomena as diverse as ferromagnetism (the familiar type of magnetism found in iron bar magnets) and the fractional quantum Hall effect (whereby electrons in a strong magnetic field act together to behave like particles that have a fractional electric charge). The authors’ technique could also be used to investigate, on a local scale, the superconductivity that was discovered last year in a twisted bilayer of graphene7. The potential to extract local information about strongly interacting systems of electrons will have far-reaching consequences for this field. Further applications of the technique could enable local probing of electric fields as they arise in complex quantum circuits — which might one day lead to a quantum computer. </body>
<date id = '338'>04 December 2019</date>
<url id = '339'>https://nature.com/articles/d41586-019-03665-3</url>
<title id = '339'>NASA’s Parker Solar Probe is currently making a series of close encounters with the Sun. Initial observations from the spacecraft have improved our understanding of both the Sun and its environment.</title>
<body id = '339'>Although the Sun is quite near to us compared with other stars, it has always kept intriguing and fundamental scientific secrets from us. For instance, we still don’t know how the solar corona — the Sun’s outermost atmosphere — maintains temperatures in excess of one million kelvin, whereas the visible surface has temperatures of just below 6,000 K1. The corona produces the solar wind, an outflow of plasma particles (free ions and electrons) that expands into the space between the planets. In 2018, NASA launched the Parker Solar Probe2 (PSP) with the aim of identifying the mechanisms behind the heating of the corona and the acceleration of the solar wind. Four papers in Nature3–6 report the first results from the PSP.   The measurements from the PSP were taken when the spacecraft was as close as 24 million kilometres to the Sun (for comparison, the average distance between Mercury and the Sun is about 58 million kilometres). They show that the solar wind near the Sun is much more structured and dynamic than it is at Earth (Fig. 1). Bale et al.3 present measurements of the direction and strength of the Sun’s magnetic field, which is dragged out into space by the solar wind. The authors find rapid reversals in the direction of the field that last for only minutes. Although some similar magnetic structures have been seen before7, the large amplitude and the high occurrence rate of these reversals are surprising. In fact, the nature of these structures remains unknown. Figure 1 | The near-Sun environment. The Sun’s outermost atmosphere generates an outflow of plasma particles (ions and electrons) called the solar wind. ‘Strahl’ electrons and energetic particles in the wind stream along the Sun’s magnetic-field lines. Four papers3–6 report observations from the Parker Solar Probe (PSP), which is currently in orbit around the Sun. The PSP data suggest that the field lines contain S-shaped bends and that the Sun releases blobs of plasma that form part of the young solar wind. The ultraviolet-light image of the Sun was taken by NASA’s Solar Dynamics Observatory on the day that the PSP made its first close encounter with the Sun. Bale and colleagues also report that the PSP’s sensors detected fluctuations in the local electric and magnetic fields in the solar wind that are larger than those detected near Earth. These fluctuations can be generated by turbulence in the solar wind or by plasma instabilities that are driven by ions or electrons. The presence of such fluctuations suggests that plasma instabilities have a much larger effect on the dynamics and energetics of the solar wind than previously expected.   Kasper et al.4 present observations of the Sun’s plasma ions and electrons. They find that the reversals in the Sun’s magnetic field are often associated with localized enhancements in the radial component of the plasma velocity (the velocity in the direction away from the Sun’s centre). The authors use the extremely clear signal of the solar wind’s strahl — a collimated and fast beam of electrons that stream along the magnetic field — to study the field’s geometry and configuration. This method leads Kasper and colleagues to interpret the magnetic-field reversals as travelling S-shaped bends in the field lines coming from the Sun. These authors also report a surprisingly large azimuthal component of the plasma velocity (the velocity perpendicular to the radial direction). This component results from the force with which the Sun’s rotation slingshots plasma out of the corona when the plasma is released from the coronal magnetic field — much like a spinning hammer-thrower slingshots the hammer when releasing it from their hands. However, the reason for the large observed value of the azimuthal velocity is currently unclear.   McComas et al.5 study detections of energetic ions and electrons, some of which are observed more often in the region just outside the corona than they are near Earth. These particles are accelerated by flares (eruptions of radiation) in the corona or by shock waves associated with coronal mass ejections (eruptions of plasma), which travel through interplanetary space. The authors identify particles corresponding to both types of source region. Because energetic particles travel along the Sun’s magnetic field, the difference in the time at which fast and slow particles arrive at the PSP can be used to estimate the path length of their trajectory along the field. McComas and colleagues find that this path length is longer than expected, which suggests that the magnetic field has a more complicated geometry than assumed. This finding could be accounted for by the S-shaped magnetic-field reversals.   The imaging instrument on board the PSP makes remote observations of light scattered by electrons and dust near the Sun. Howard et al.6 report that the intensity of the dust-scattered light decreases with distance from the Sun in almost the same way as it does when observed from Earth. However, the authors find some preliminary evidence for the existence of a hypothesized dust-free zone8 near the Sun that has not been detected before. The detailed images from the PSP also show spatial variations in the solar wind that are consistent with variations in the Sun’s magnetic field on its surface, and reveal small blobs of plasma that are ejected from the Sun and form part of the young solar wind. These four papers show that, by going into an unexplored region of the Solar System, the PSP has already made great discoveries. In the near future, it will be important to combine all the available sources of information to develop a deeper understanding of the physics of the Sun and the solar wind. For instance, researchers should combine the measurements of the electric and magnetic fields with detailed observations of the plasma particles to determine how fields and plasma interact and drive instabilities9. They must also study the large azimuthal flow velocity further to confirm whether it is a persistent feature or just a one-time exception during these initial PSP measurements. The use of magnetic-field models will enable scientists to learn more about the path of energetic particles between the Sun and the PSP, and, in turn, about space weather — the effects of the Sun and the solar wind on Earth and human technology. These energetic-particle studies must also be linked with remote observations of the Sun’s surface and the corona. Examining the potential presence of the dust-free zone near the Sun must be another short-term goal, but might have to wait for closer approaches of the PSP to the Sun in the future. It is expected that PSP data will guide our understanding of the Sun and the solar wind for many years. New models and theories will be motivated by the spacecraft’s discoveries, and this knowledge will be transferable to other stars and astrophysical plasmas throughout the Universe. After all, the Sun is the only star that we can study up close using spacecraft. The orbit of the PSP will bring the spacecraft even closer to the Sun in the coming years, to just over 6 million kilometres from the surface2. During this time, the Sun will transition into a more active phase of its 11-year cycle, so we can expect even more-exciting results soon. In 2020, the European Space Agency will launch the Solar Orbiter mission10. Although this spacecraft will not go quite as close to the Sun as will the PSP, its more extensive suite of scientific instruments will be used in combination with the PSP to reveal key information about the Sun. For example, Solar Orbiter will measure the elemental composition and charge states of ions and will take photographs of the Sun in different wavelengths of light. These joint measurements will certainly close some of the remaining gaps in our knowledge of the Sun and the solar wind. For now, however, the Sun has proved again that it still holds more secrets for us to discover. </body>
<date id = '339'>04 December 2019</date>
<url id = '340'>https://nature.com/articles/d41586-019-03701-2</url>
<title id = '340'>Conventional alloys have undesirably coarse-grained microstructures when used in 3D printing. A designer alloy overcomes this problem, potentially opening the way to the widespread adoption of 3D metal printing.</title>
<body id = '340'>There are many potential benefits to using additive manufacturing — also known as 3D printing — for making metal parts, rather than conventional manufacturing processes. For example, additive manufacturing is highly customizable, it can produce complex structures and it can be used for the economical production of low numbers of metal components. But to achieve the strict specifications needed for some applications, the microscopic structure of printed metal objects must be controlled. Writing in Nature, Zhang et al.1 describe titanium–copper alloys that produce practically useful microscopic structures during additive manufacturing, removing the need for subsequent treatment. The resulting materials exhibit promising combinations of mechanical properties, comparable to those of the ubiquitous structural alloy Ti-6Al-4V, produced using conventional and additive manufacturing processes. In metal additive manufacturing, an alloy (in the form of powders or wires) is deposited in a layer and then melted by a rapidly moving heat source to form a solid mass; successive layers are built up to produce a 3D part. The process typically produces large temperature gradients, high solidification rates and repeated cycles of heating and cooling. A common characteristic of 3D-printed metals is coarse columnar grains that grow along specific directions of the crystal lattice that are favourably oriented with the heat flow (Fig. 1a). Figure 1 | Grain structure in printed metals. a, When conventional metal alloys are used for 3D printing, large columnar grains tend to form, as shown here for the structural alloy Ti-6Al-4V. This causes the printed alloy to have undesirable anisotropic (direction-dependent) properties. b, Zhang et al.1 report that titanium–copper alloys produced by 3D printing contain fine grains that have similar dimensions in all directions. The alloy shown here was produced using the same conditions as in a. (Images from ref. 1.) Coarse columnar grains are usually undesirable because they can cause the printed material to have direction-dependent (anisotropic) mechanical properties and make it susceptible to tearing or cracking during solidification2–4. However, columnar solidification can undergo a transition to equiaxed solidification — in which the grains produced have similar dimensions in all directions — by changing the processing conditions used for additive manufacturing2. Alloys with equiaxed grains have desirably uniform properties, and so methods for producing them are of great technological value4. Models and experiments have been used to study the columnar-to-equiaxed transition (CET) in nickel-based alloys that have been melted using an electron beam2,3. The number of nuclei (tiny crystals that ‘seed’ the growth of the solid phase) in the liquid metal, and the processing conditions used during electron-beam additive manufacturing, were found to have a larger influence on grain structure than did the composition of the alloy3. This suggests that the CET can be controlled through process design and by promoting nucleus formation in alloy melts. Additives called inoculants, which cause nuclei to form in the melt, have been incorporated into metal-alloy powders used in additive manufacturing, to increase the density of nuclei and thereby promote the formation of equiaxed grains4. However, suitable inoculants for titanium alloys remain elusive.   Zhang et al. now show that fine equiaxed grains, on average less than 10 micrometres in diameter, can be produced in titanium–copper alloys during additive manufacturing, without adding inoculants (Fig. 1b). The authors propose that nucleation and CET are promoted in these alloys by the formation of a large zone of supercooled liquid — melted alloy that is fully liquid, despite its being below the temperature at which the alloy should start to solidify. The final product consists of two solid phases that contain different amounts of titanium and copper, forming a microstructure that includes nanoscale plates (lamellae). The mechanical properties of the printed material compare favourably with those of Ti-6Al-4V, and of cast (and heat-treated) titanium–copper alloys. The authors suggest that equiaxed grains are produced during solidification of the melt, and that further microstructural refinement might then occur during the cyclical temperature changes associated with the 3D-printing process. However, it is difficult to tell unambiguously whether the solidification step is the genesis of the fine grains, because the microstructures produced at high temperatures during solidification will be replaced by features that develop during subsequent solid-state phase transitions. Another plausible scenario is that columnar grains form during solidification, and that equiaxed grains are produced and refined during solid-state thermal cycling. Such grain refinement has been reported in steels5. When steels that have a two-phase lamellar microstructure at low temperatures are heated above a critical temperature, new grains of a third phase (austenite) nucleate and grow. The two low-temperature phases then re-form on cooling5. Repeated nucleation and growth of the various phases can therefore occur under suitable conditions during thermal cycling, leading to significant grain refinement.   Alloys such as Ti-6Al-4V typically do not undergo grain refinement during thermal cycling6, because no new grains of the high-temperature phase nucleate. However, it is unclear whether new grains of high-temperature phase can nucleate and grow in Ti-6Al-4V during thermal cycling typical of additive manufacturing7, which might conceivably refine grains. Zhang and colleagues’ titanium–copper alloys have high- and low-temperature phases analogous to those of steels. Clarifying the role of nucleation and growth of these phases in grain refinement during thermal cycling should be a topic of future research. A deeper understanding of solidification and solid-state phase transitions is clearly needed to guide the design of future alloys for additive manufacturing and to control their microstructures — although the nucleation stage is hard to study experimentally. It is also imperative that we have a better understanding of how the rapidly changing conditions during additive manufacturing influence microstructure development. In situ characterization of phase transitions and dynamic phenomena, for example using imaging and diffraction techniques in experiments that simulate the conditions of additive manufacturing8,9, might help to unveil some of the complexity of the processes involved. Such efforts are timely, and are necessary to produce optimized alloys that will lead to the widespread adoption of additive manufacturing for the production of high-performance structural parts, for which reliably high-quality microstructures and mechanical properties are of the utmost importance. </body>
<date id = '340'>04 December 2019</date>
<url id = '341'>https://nature.com/articles/d41586-019-03704-z</url>
<title id = '341'>Knowing how dietary fibre nourishes gut microorganisms might suggest ways to boost health-promoting bacteria. A method developed to pinpoint bacteria that consume particular types of dietary fibre could advance such efforts.</title>
<body id = '341'>Certain gut microorganisms can boost human health, but it is unclear how diet could be harnessed to easily manipulate the composition of gut microbes to boost the levels of desired bacteria. Writing in Cell, Patnode et al.1 present a useful approach for assessing interactions between human gut microbes and the dietary fibre that sustains their existence. Dietary fibre is promoted as part of a healthy diet worldwide. Many people, however, do not achieve their recommended fibre intake because they consume insufficient fruit, vegetables and cereals. Inadequate fibre intake is associated with common conditions including obesity, diabetes and cancer2. Yet understanding the mechanisms that link fibre-rich food to good health is challenging. Dietary fibre encompasses a wide range of complex molecules, most of which are present in plant cells; among them are carbohydrate molecules called glycans, which are resistant to digestion by human enzymes. As a consequence, some ingested fibre is excreted unchanged in faeces, whereas most is metabolized by gut microbes. These microbes have a diverse and extremely complex metabolic capacity. Bacteria that express different enzymes for metabolizing fibre can survive and grow using a range of foods. Some bacterial species might compete with each other for the same food source, which could lower the abundance of species that compete less successfully. How might gut microbes be manipulated through human dietary intervention? For example, the concept of using prebiotics — compounds that affect gut microbes, thereby benefiting the human host — has been proposed. One such idea is to use particular fibre sources that provide food for the desired gut microbes3,4. However, determining whether dietary fibre can promote health in this way requires a sophisticated understanding of the interactions that occur when the complex community of gut microbes encounters a source of fibre.   Previous work5 had indicated that transferring the gut microbes of human twins who have contrasting body masses (obese and lean) into mice induced a corresponding difference in the animals’ body masses. However, when some of the obese mice were housed with the lean mice, they had less adipose fat than did obese animals that were not co-housed with lean mice — and this weight-loss effect correlated with the transfer of Bacteroides bacterial species from the lean mice to the obese mice5. High consumption of fibre-rich plant foods was required for this adipose-fat reduction to occur5. However, the types of fibre responsible for this effect, and how these interact with specific gut microorganisms, was unknown. Patnode and colleagues now reveal how particular types of glycan can drive competition between different Bacteroides species resident in the human gut. Patnode et al. studied mice that lacked their normal microbes, and instead harboured 15 strains of gut-dwelling bacteria from a lean human who had an obese twin. The authors fed the mice different combinations of fibre sources as part of their diet. Analysing faecal samples enabled the researchers to track how the diets affected the relative abundance of each bacterial species in the animals’ gut. This approach pinpointed, for example, a dose–response effect of pea fibre on the relative abundance of Bacteroides thetaiotaomicron in the bacterial population, as well as a pronounced effect of certain types of barley fibre (β-glucan and bran) on the relative abundance of Bacteroides ovatus. These results reveal the specificity of the effects that different forms of dietary fibre can have on bacterial populations. To identify the genes required for a specific bacterium of interest to metabolize fibre, the authors gave mice bacterial strains that were engineered to contain mutations at random sites across their genome, and fed the animals different kinds of dietary fibre. By analysing the proteins in mouse faecal samples, the authors identified a set of bacterial proteins that allow certain microbes to grow successfully in particular feeding regimes. For example, when mice received dietary fibre from fruit peelings (citrus pectin) that are rich in a type of molecule called methylated homogalacturonan, this led to a rise in the expression of proteins that degrade such molecules in the bacterium Bacteroides cellulosilyticus. And when mice received pea fibre, which is rich in a polymer molecule called arabinan (which contains the sugar arabinose), the expression of proteins involved in arabinan degradation rose in the bacterium B. thetaiotaomicron. Perhaps the most original part of this research is the development of artificial ‘food particles’ consisting of glycan-coated magnetic beads (Fig. 1) that can be administered orally to mice and recovered by applying a magnetic field. Patnode et al. used this strategy to investigate how bacterial species respond to different food sources by assessing the extent of glycan degradation in the recovered beads. When mice that had been colonized only with B. cellulosilyticus or Bacteroides vulgatus were given food particles coated with pea fibre, the levels of arabinose in the recovered beads were lower than the original levels, demonstrating that both of these bacterial species had metabolized this molecule in vivo. Figure 1 | Investigating how human gut-dwelling bacteria metabolize dietary fibre. a, Patnode et al.1 gave mice that lacked their natural gut microbes a set of 15 bacterial strains that dwell in the human gut, including the species Bacteroides cellulosilyticus, Bacteroides ovatus and Bacteroides vulgatus. The authors developed a method for tracking fibre digestion. They generated magnetic beads coated with a fibre of interest, and fed these beads (termed food particles) to the animals. Applying a magnetic field enabled the recovery of food particles and assessment of the extent of fibre degradation. The animals received food particles that included some coated with pea fibre that is rich in the molecule arabinan, and some coated with the molecule arabinoxylan. B. vulgatus and B. cellulosilyticus competed to degrade the arabinan, B. cellulosilyticus degraded arabinoxylan, and B. ovatus degraded other molecules (not shown). b, When the experiment was repeated without B. cellulosilyticus, B. ovatus demonstrated metabolic flexibility, by switching to degrade arabinoxylan. B. ovatus degraded less arabinoxylan than did B. cellulosilyticus. In a parallel experiment, mice were colonized either with all 15 bacterial strains from the lean twin, or with 14 of the strains (B. cellulosilyticus excluded), before being given food particles containing pea fibre (Fig. 1). The level of degradation of arabinose in the arabinan-rich pea-fibre beads was then compared, and was found to be the same in both cases. This suggests that some change occurs in the bacterial community, in the absence of B. cellulosilyticus, that enables arabinose from pea fibre to be degraded as much as it would be if all 15 bacterial strains were present. The story might be different for other forms of dietary fibre. Along with the food particles coated with pea fibre, the animals received some coated with molecules of arabinoxylan (a polymer of the sugars arabinose and xylose). However, in the case of arabinoxylan, the bacterial strains were less able to process this molecule when B. cellulosilyticus was absent than when it was present, and the arabinoxylan-metabolizing activity was attributed to B. ovatus. In the absence of B. cellulosilyticus, B. ovatus undergoes a metabolic shift that boosts its ability to use arabinoxylan. When both B. ovatus and B. cellulosilyticus were absent from the bacterial populations, arabinoxylan-coated beads retained their original levels of arabinose, revealing that none of the remaining 13 bacterial strains took advantage of arabinoxylan availability.   This study reveals the flexibility and adaptability of gut microbes in response to their nutritional environment. It provides a useful focus on specific forms of dietary fibre and bacterial species known to be linked to diet-associated resistance to a rise in adipose tissue5. This ‘simplification’ of the context suggests a way forward in understanding the key genes and proteins of Bacteroides that are crucial for the degradation of dietary fibre, and that might affect the abundance of particular gut bacteria. The findings also reveal how B. cellulosilyticus can have a dominant role in its interactions with certain bacteria with which it can compete for the same food source. The work also uncovers hidden metabolic flexibility, such as the ability of B. ovatus to adapt its metabolic strategy. When assessing this study, it is worth bearing in mind that Bacteroides is not the only type of bacterium that commonly uses dietary fibre for food, and that the fibre-containing foods tested by the authors are not the major sources of dietary fibre in a typical human diet. Moreover, the abundance of Bacteroides varies enormously between people6, and the hypothesis that key Bacteroides species might affect the success of dieting efforts to control obesity requires further investigation. Although it concentrates on Bacteroides only, Patnode and colleagues’ work represents useful progress towards developing personalized nutrition strategies for tailoring gut microbes in the future. The study also complements other research7–9 that explores how bacteria in the human gut might contribute to the body’s response to a particular diet. Thanks to Patnode et al., we have fresh insights into how specific types of bacterium use and compete for dietary fibre. Future research will undoubtedly continue to refine the link between fibre-rich food and health, by taking into account the role of the gut microbial community. </body>
<date id = '341'>04 December 2019</date>
<url id = '342'>https://nature.com/articles/d41586-019-03703-0</url>
<title id = '342'>How Nature reported the evolution of the dragon in 1919, and plants that move spasmodically in 1869.</title>
<body id = '342'> The Evolution of the Dragon. By Prof. G. Elliot Smith — The dragon may be regarded as the most venerable symbol employed in ornamental art, and it has been the inspiration of much of the world’s great literature in every age and clime. The dragon-myth also represents the earliest doctrine or systematic theory of astronomy and meteorology. The study of dragon-lore thus leads us back to some of the most primitive workings of the human mind, and embraces many subjects which at first sight seem to have little connection with the end in view. Prof. Elliot Smith’s work … indeed, alludes to almost every aspect of primitive thought and myth, and the author discusses questions which vary from the origin of embalming to the worship of the cow, the elixir of life … and the reasons for wearing clothes … Prof. Elliot Smith maintains that the dragon was originally a beneficent creature, the personification of water … The substratum of its anatomy usually consists of a serpent or a crocodile, with the scales of a fish for covering, the feet and wings … of an eagle or hawk, and the fore-limbs … of a lion. All the parts are symbols of the various attributes and uses of water in Nature. With various slight additions and modifications, this composite wonder-beast ranges from Western Europe to the Far East of Asia, and thence across the Pacific to America. From Nature 4 December 1919 M. Lecocq, of Clermont Ferrand, records in the Belgique Horticole some singular spasmodic movements in the leaves of Colocasia esculenta. These motions bear no resemblance to those produced in the Sensitive plant by the warmth of the hand, but occur spontaneously independently of the action of the wind or of any external cause, at irregular intervals, and at different periods of the day and night. M. Lecocq describes the movement as a kind of trembling or quivering affecting the whole plant, sufficiently powerful to tinkle little bells attached to the branches, and on one occasion even to shake the pot in which the plant was contained, and to resist a pressure of the hand, the number of the pulsations varying from 100 to 120 per minute. He states that the Colocasia is destitute of the stomata with which the leaves of plants are generally provided, especially on their under-surface, and attributes the phenomenon to the incessant pulsations of the imprisoned sap. From Nature 2 December 1869 Latest on: Culture Obituary 23 MAY 20 Technology Feature 08 MAY 20 World View 14 APR 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 Plant sciences Article 29 APR 20 Article 22 APR 20 Article 08 APR 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '342'>03 December 2019</date>
<url id = '343'>https://nature.com/articles/d41586-019-03587-0</url>
<title id = '343'>Drug resistance in malaria parasites is mediated by mutations in a transporter protein. The transporter’s structure reveals the molecular basis of how key mutations bring about resistance to different drugs.</title>
<body id = '343'>About half a million people, most of them children living in Africa, are killed each year by malaria1. Management of malaria, particularly that caused by the highly virulent protozoan parasite Plasmodium falciparum, is challenged by the emergence of resistance to antimalarial drugs2. Writing in Nature, Kim et al.3 report the structure and molecular properties of a key protein that facilitates resistance, the P. falciparum chloroquine-resistance transporter (PfCRT). The structure reveals the consequences of finely tuned mutations of the amino-acid residues that line a crucial central cavity in PfCRT. These mutated residues allow resistant parasites to transport certain antimalarial drugs away from their site of action — and the effect of the mutations is different for closely related drugs.   Malaria parasites spend part of their life cycle inside human red blood cells. There, they use a specialized membrane-bound compartment known as the digestive vacuole to degrade the protein haemoglobin, thereby generating amino-acid building blocks for growth4. Haemoglobin digestion also produces a toxic side product called haem, which is exploited by antimalarials from the quinoline family of drugs (which includes chloroquine and piperaquine). These drugs bind to the released haem in the digestive vacuole and prevent the compound’s detoxification by the parasite — in effect, poisoning the parasite with its own metabolic debris4. Chloroquine’s affordability, safety and efficacy made it the drug of choice for combating malaria, until widespread resistance developed in the 1980s. Piperaquine is a structurally related compound that retains activity against chloroquine-resistant parasites and is currently used in combination with another antimalarial drug, dihydroartemisinin. Unfortunately, resistance to piperaquine is now widespread in parts of southeast Asia5. Paradoxically, piperaquine-resistant parasites are often more sensitive to chloroquine than are piperaquine-sensitive parasites6. Drug resistance can arise because of mutations either in the drug’s target or in biological machinery that transports the drug to or from the target. A landmark report7 in 2000 identified PfCRT as the main mediator of chloroquine resistance in P. falciparum. Resistance emerged independently at different locations around the world, but is always associated with a particular mutation in the transporter — the substitution of a lysine amino-acid residue by a threonine residue (a K76T mutation). K76T combines with other geographically specific PfCRT mutations to mediate resistance and improve the fitness of the mutant parasites6–8. PfCRT is a member of the superfamily of proteins known as drug/metabolite transporters9 and is located in the membrane of the parasite’s digestive vacuole. Structural and biochemical studies of related proteins suggest that resistance to chloroquine arises as a result of the transporter passing chloroquine out of the digestive vacuole, thus removing it from its site of action8. Interestingly, piperaquine resistance emerges when parasites harbouring the K76T mutation acquire further mutations6.   Efforts to design new resistance-busting quinoline antimalarials have been stymied by a lack of information about the structures of mutated PfCRT. But in the past few years, advances in a technique called cryo-electron microscopy10 (cryo-EM) have revolutionized structural biology by enabling the direct imaging of membrane-embedded proteins, such as PfCRT, that are not amenable to study using X-ray crystallography. Kim et al. used single-particle cryo-EM to determine the structure of PfCRT in South American 7G8 parasites, which harbour mutations that confer high-level chloroquine resistance. The authors first had to work out a protocol for efficiently expressing, purifying and reconstituting PfCRT into a membrane-like environment, to maintain a native conformation of the protein. PfCRT is relatively small (49 kilodaltons) compared with proteins that are typically studied using cryo-EM, and so Kim et al. prepared an antibody fragment (known as an antigen-binding fragment, or Fab) that binds to PfCRT, thereby forming a complex that has sufficient mass and stability to allow cryo-EM-based structural elucidation. This approach yielded a structure with 3.2-ångström resolution. PfCRT was revealed to have ten transmembrane domains and a negatively charged central cavity. The cavity opens on to the digestive vacuole, but closes about halfway through the membrane. The Fab-bound cavity has an opening 25 Å in diameter, which is large enough to contain chloroquine (the maximum dimension of which is about 14 Å) and piperaquine (with a maximum dimension of about 21 Å). The structure shows that wild-type PfCRT has a positively charged lysine residue (K76) positioned in the cavity. This residue is thought to repel both chloroquine (which has two positive charges) and piperaquine (which has four positive charges), thereby trapping them in the digestive vacuole (Fig. 1a). Figure 1 | Mutations affect the transport of antimalarial drugs through the PfCRT protein. a, The drugs chloroquine and piperaquine target a membrane-bound organelle in the malaria parasite, called the digestive vacuole. Plasmodium falciparum chloroquine-resistance transporter (PfCRT) resides in the membrane of the digestive vacuole of the most virulent species of malaria parasite. A positively charged lysine amino-acid residue (K76) in wild-type PfCRT is thought to repel the positively charged drugs, preventing transport out of the vacuole (arrow) — which would lead to drug resistance. b, Kim et al.3 report the structure of PfCRT that contains a mutation known as K76T. The authors find that K76T alters the charge distribution of the lining of the PfCRT cavity. Both chloroquine and piperaquine bind to the mutated cavity, but only chloroquine passes through it (possibly because it has a lower positive charge than piperaquine), enabling resistance to chloroquine, but not to piperaquine. c, Further mutations in PfCRT (such as the C350R mutation) cause piperaquine to bind more weakly to the cavity than in b. This potentially underpins the drug’s ability to pass through PfCRT and leads to piperaquine resistance. In this scenario, the transport of chloroquine out of the digestive vacuole is less efficient than the transport of piperaquine. Interestingly, Kim and colleagues’ biochemical experiments show that PfCRT from 7G8 parasites can bind both chloroquine and piperaquine, but transport only chloroquine; thus, parasites with the K76T mutation are resistant to chloroquine, but not piperaquine (Fig. 1b). By contrast, when the authors introduced further mutations to 7G8 that have been observed in South American11 and southeast Asian6 parasites in the past five years, piperaquine efflux increased — an effect that was associated with decreased sensitivity to piperaquine, but increased sensitivity to chloroquine (Fig. 1c). Kim et al. used their cryo-EM structure to carry out molecular modelling and electrostatic analysis of PfCRT, to help explain why mutations can have opposite effects on the sensitivity of malaria parasites to chloroquine and piperaquine. The modelling suggests that mutations associated with piperaquine resistance can reduce the negative charge or alter the conformation of the PfCRT central cavity. This might prevent piperaquine from binding too tightly to the cavity and thus increase its transport out of the digestive vacuole. The authors propose that the distribution of surface charges in the cavity can be fine-tuned so that the initial binding of a drug to PfCRT, and its subsequent release for transport, is different for different drugs, thereby producing distinct effects on drug sensitivity. Taken together, Kim and colleagues’ findings show that P. falciparum is engaged in an ongoing balancing act, generating mutations that block the action of different drugs while maintaining optimal fitness of the parasite. A limitation of the new structure is that PfCRT is locked in a conformation in which it is open to the digestive vacuole, as a result of Fab binding in its central cavity. Further studies will be required to work out how drug binding couples to the conformational rearrangements that permit drug transport. On a practical level, it is particularly important to understand the mechanisms of resistance that are likely to arise in Africa, where more than 90% of cases of malaria caused by P. falciparum occur1. Compared with other parts of the world, there are fewer PfCRT mutations in Africa, where resistance to chloroquine is decreasing in many countries and where the combination of piperaquine and dihydroartemisinin remains highly effective12. We can look forward to further studies of PfCRT, including visualization of the drug-bound and open-to-cytoplasm structural conformations, which will further explain the effects of resistance mutations and might help to identify drugs that circumvent resistance. For now, we can appreciate the insights gained from Kim and colleagues’ beautiful marriage of structure, biochemistry, genetics and parasitology, and particularly from the first atomic-resolution structure of PfCRT — the fine-tuned, resistance-mediating machine of malaria parasites. </body>
<date id = '343'>27 November 2019</date>
<url id = '344'>https://nature.com/articles/d41586-019-03563-8</url>
<title id = '344'>A molecular catalyst dispersed on carbon nanotubes has been found to catalyse the electrochemical conversion of carbon dioxide to methanol — a liquid fuel and industrially useful bulk chemical.</title>
<body id = '344'>Molecular catalysts that mediate reactions with carbon dioxide often promote chemical reductions that form either carbon monoxide or formic acid (HCO2H), but lack the activity and selectivity to reduce these compounds further to make other useful products, such as methanol, ethanol or methane. Writing in Nature, Wu et al.1 report that a molecular catalyst immobilized on carbon nanotubes can promote the electrochemical conversion of CO2 to methanol in water. The result holds promise for advancing the search for catalysts that make highly reduced products from CO2. Such products can then be used as fuels and as feedstock chemicals for industrial processes.   The heavy use of fossil fuels has led to excessive emissions of CO2 into the atmosphere, and poses imminent threats to our climate system. Renewable energy sources, such as solar and wind power, are green and sustainable alternatives to fossil fuels for powering our society. Unfortunately, their intermittent nature limits their widespread use. Methods for storing the energy from these sources are therefore needed, to even out the supply. Electrically powered methods for transforming CO2 into fuels and other CO2-derived chemicals are a promising strategy for tackling some of these energy issues, with the added bonus that they might help to mitigate atmospheric CO2 levels2. But the development of such methods is by no means easy, because CO2 is a stable and relatively unreactive molecule. Catalysts are therefore essential to activate CO2 and drive its conversion into desired products. Among the various classes of catalytic material, molecular catalysts that consist of ligand molecules bound to metal ions have certain advantages: they follow well-characterized reaction pathways, and have chemically modifiable structures that allow their activity to be tuned quite precisely. Molecular catalysts that promote the electrochemical conversion of CO2 to carbon monoxide or, in fewer cases, to formic acid (or its formate salt) have been known for decades3. Attempts to reduce these compounds further to make methanol, ethanol, methane or ethylene (CH2=CH2) have been unsuccessful — or, at best, have provided these products in small quantities with low selectivity4,5 (that is, as a small component of a mixture with other products). Copper-based materials have previously been the most successful catalysts for such reactions6. Wu et al. now reveal that, when a complex called cobalt phthalocyanine is dispersed on carbon nanotubes, it has appreciable catalytic activity and selectivity for the electrochemical reduction of CO2 to methanol (Fig. 1). More specifically, the cobalt phthalocyanine complex must be physically adsorbed to the surface of the carbon nanotubes as individual molecules. The key finding is that this mixed catalyst system not only activates CO2 to produce carbon monoxide, but also, surprisingly, promotes further reduction to methanol when high voltages are applied in the electrochemical cell. Figure 1 | Electrochemical production of methanol from carbon dioxide. Wu et al.1 report that a cobalt phthalocyanine catalyst immobilized on carbon nanotubes can electrochemically reduce carbon dioxide in water. The reaction first produces carbon monoxide, which is reduced further to methanol (CH3OH), an important liquid fuel and bulk chemical. The conversion of CO2 to methanol using molecular catalysts has previously been ineffective. The point of attachment of three of the amino (NH2) groups to the benzene rings in the catalyst is not known. Co, cobalt. The researchers found that optimization of the catalytic system was difficult, because many extrinsic factors affected the activity of the molecular catalyst. These included the method used to immobilize the catalyst on the support; the specific carbon support chosen; the ratio of the concentration of the catalyst to that of the support; and the voltage used for the electrochemical reduction. The product selectivity of CO2 reductions catalysed by cobalt phthalocyanine can be strongly affected by even a subtle variation in any of these factors7. However, the optimized catalyst system has significantly improved activity and selectivity compared with previous molecular-catalyst systems. Still, it is not as good as the state-of-the-art, solid-state metallic catalysts that have been reported for methanol production8,9.   A long-standing issue associated with molecular catalysts in general is their long-term stability. Wu et al. found that their cobalt phthalocyanine system lost its catalytic activity over the course of five hours, and they identified the deactivation process as degradation of the phthalocyanine ligand. When they modified the ligand by appending amino (NH2) substituents to it, they found that their system’s stability was enhanced — it lasted for more than 12 hours, with only a slight loss of overall activity and selectivity. The reason for the stabilizing effect is not known. Note, however, that the catalyst would need to last for thousands of hours if the reduction process were to be implemented in an industrial setting. The findings reveal that molecular catalysts have great prospects for use in CO2 transformations. Future research could focus on further improving the activity, selectivity and stability of the molecular catalyst–carbon nanotube hybrid system through judicious chemical manipulations of the catalyst and the support, and of the interactions between them. Detailed mechanistic insight into the catalytic conversion of CO2 to carbon monoxide, and further to methanol, might be gained using computational modelling and ‘operando’ characterization techniques, which monitor the consumption of reactants and the build-up of products during catalysis. Such efforts would lay the foundations not only for improving the performance of existing systems, but also for discovering new catalysts involving metal complexes, or structurally similar catalysts consisting of single metal atoms dispersed in carbon materials10. Concerns have been raised that the generally moderate activity, selectivity and stability of molecular catalysts for CO2 reactions will prevent them from being used on an industrial scale. Moreover, the transport of CO2 in electrochemical cells that have been used in proof-of-concept experiments is limited by the low solubility of this gas in water11. However, the adoption of flow technology in which a large quantity of gaseous CO2 is fed directly to catalysts can greatly improve the outcome of CO2 transformations12. With continued efforts to improve catalyst performance and the design of electrochemical cells, the industrial production of methanol from CO2 could well be within reach. </body>
<date id = '344'>27 November 2019</date>
<url id = '345'>https://nature.com/articles/d41586-019-03602-4</url>
<title id = '345'>The finding that a thin sheet of fibrous tissue under the skin contains a prefabricated, movable cellular sealant that can heal deep wounds might have implications for the treatment of scars and ulcers.</title>
<body id = '345'>Skin consists of an outer epidermal layer (the epidermis) and an inner dermal layer (the dermis). If you pinch your skin, you can lift it because these two cellular layers move freely above a membranous sheet called the fascia, which contains cells and extracellular-matrix material. This gelatinous tissue creates a frictionless interface between the skin and the more rigid structures beneath it, such as muscle and bone. However, it now seems that the fascia has roles beyond providing a non-stick surface. Writing in Nature, Correa-Gallegos et al.1 report that the fascia contains a movable sealant that patches up deep injuries to enable rapid wound repair.   The scar tissue of a healing skin wound contains fibroblast cells, which make and modify extracellular-matrix proteins. These fibroblasts can be identified by their expression of a protein called Engrailed-1, and are termed Engrailed-positive fibroblasts (EPFs). The idea that the fascia might be a repository of cellular components involved in wound healing and scar formation came from a previous study2, which reported that EPFs reside not only in the skin, as expected, but also in the fascia. To investigate wound healing in mice, Correa-Gallegos and colleagues grafted fascia that contained cells engineered to express green fluorescent protein onto skin cells expressing red fluorescent protein. The authors then wounded this dual-coloured ‘fluorescent sandwich’ and transplanted it into a healthy mouse. Comparison of the percentages of green and red cells revealed that 80% of cells in the healing wound came from the fascia. Furthermore, the vast majority of many cell types found in the healing injury originated from the fascia, including contractile fibroblasts (or myofibroblasts), blood-vessel cells, macrophages of the immune system and nerve cells. To confirm that their observations were not due to any peculiarities of this artificial grafted structure, the authors injected a dye into the fascia of mice, and then gave the mice a deep wound that penetrated the animals’ skin and fascia. The authors mapped the dye-labelled cells that populated the healing wound and the surrounding scar tissue. More than half of the cells in the healed wound were labelled with the dye, confirming that the fascia is a major source of scar-forming tissue after deep injury. Deep wounds lead to scars that are larger and harder to heal than those arising from superficial wounds that do not penetrate the fascia3. The authors used two-photon microscopy to analyse deep skin wounds in mice engineered4 to express fluorescent proteins, which can be used to trace scar-forming EPFs. They found that a cellular plug in the fascia, consisting of extracellular matrix, macrophages, blood vessels and nerves, moved upwards into the damaged skin to form a scar. This healing process did not require cell division, indicating that the plug was prefabricated. Importantly, the authors found that key proteins that have been reported to define the types of fibroblast found in scars5 are expressed at higher levels on fascial than on dermal fibroblasts, consistent with a model in which fascial EPFs are a major source of fibroblasts in healing deep wounds (Fig. 1). Figure 1 | The healing of deep skin wounds. The skin consists of an outer layer called the epidermis and an inner layer, the dermis. Superficial wounds no deeper than skin level can be repaired by cells called Engrailed-positive fibroblasts (EPFs) in the dermis, which make extracellular-matrix material. Working with mice, Correa-Gallegos et al.1 investigated the healing of deep wounds that penetrated below the skin into a layer known as the fascia. The fascia contains EPFs, extracellular matrix, blood vessels, nerves and immune cells called macrophages. The authors report that a prefabricated plug of material from the fascia moves upwards, steered by fascial EPFs, to seal the wound. (Image based on Fig. 6 of ref. 1.) Given that fibroblasts regulate the extracellular matrix, the authors used microscopy to visualize physical features of fibres of the protein collagen, which is a component of the extracellular matrix. Collagen in the fascia was more coiled and immature than were the stretched and interwoven collagen fibres in the dermis. Furthermore, when a fluorescent dye was used to tag collagen in an injured animal, this revealed that the extracellular matrix of the fascia moved upwards like a pliable gel into the damaged tissue, to plug and then repair the wound. By contrast, dermal collagen remained immobile. The authors then tested whether EPFs from the fascia drive the movement of the prefabricated plug. They inserted non-adhesive membranes in mice to separate the fascia from the dermis, which resulted in delayed repair and non-healing wounds that remained open. Animals in which these membranes were not inserted did not show these effects. The removal of fascial EPFs by a genetic approach also resulted in the plug not entering wounds and in poor healing. These findings indicate that fascial EPFs do indeed steer the plug that seals deep wounds.   Although this study has potential relevance for human disease, most of the work was carried out in an artificial mouse model. Moreover, mice have a type of muscle called the panniculus carnosus, which lies between the fascia and the skin and is used to twitch the skin6. However, humans lack this twitching ability and have only a small remnant of this muscle. Therefore, the authors needed to determine whether scar formation occurs in a similar manner in humans and mice despite such differences. The team analysed fascial fibroblasts in human skin and investigated a type of human raised scar called a keloid, which grows bigger than the original injury and can be profoundly itchy, inflamed and painful7. Many of the proteins that characterize the mouse fascia were also highly expressed in human fascia and keloid scars. This similarity suggests that the same processes are involved in wound healing and scar formation in both species. However, it is not yet clear whether these findings in mice reveal general principles that are relevant to human skin disease. The authors’ findings provide satisfying potential explanations for some unsolved clinical conundrums. Nerves, blood vessels and macrophages in the prefabricated plug are dragged into the mouse wound; if the same phenomenon occurs in humans, this could explain why keloids itch and are painful. Keloid formation is more common at sites of thicker fasciae (such as the chest, back and thighs) than at sites where the fascia is thinner (for example, the feet), which is consistent with a model in which the fascia drives keloid formation. Could these discoveries about the skin shed light on other clinically relevant fibrotic diseases (conditions associated with the accumulation of extracellular matrix) that affect organs in which the fascia is not present, such as the lungs and liver? Perhaps the mechanisms uncovered in mice might have relevance for the processes underlying skin damage in the leg ulcers that can develop in people who have diabetes. In any case, it is clear that advances made in understanding the biology of the fascia might reveal new targets for treating scarring diseases of the skin. </body>
<date id = '345'>27 November 2019</date>
<url id = '346'>https://nature.com/articles/d41586-019-03607-z</url>
<title id = '346'>Scientists have engineered semiconducting nanocrystals called quantum dots that lack toxic heavy metals and are highly efficient light emitters. These nanostructures might be used in displays, solar cells and light-emitting diodes.</title>
<body id = '346'>Tiny semiconductor crystals dubbed quantum dots (QDs) are one of the biggest nanotechnology success stories so far. Since their first synthesis1,2 in the 1980s, QDs have featured in a wide range of optoelectronic devices, and QDs suspended in solution have been used in many in vivo and in vitro imaging, labelling and sensing techniques. However, two technical problems need to be resolved before their potential can be fully realized. First, QDs based on cadmium must be replaced by ones that are highly efficient light emitters and that do not contain such toxic heavy metals. And second, QD phosphors (substances that exhibit luminescence) in televisions must be replaced by QD light-emitting diodes (LEDs), to reduce power consumption. In a paper in Nature, Won et al.3 report QDs that address both issues. The absorption spectra of nanocrystal QDs depend on their size. This property was discovered independently for QDs in glass1 and in aqueous solution2 and was first described quantitatively4,5 in the early 1980s. For practical applications, such a feature should be converted into size-dependent photoluminescence. In this process, an electron in the valence energy band of a QD absorbs a photon and moves to the conduction energy band, leaving behind a hole (electron vacancy). The photoexcited electron and hole then recombine (merge), releasing a photon (Fig. 1a). Figure 1 | Efficient light emission from quantum dots. a, Semiconducting nanocrystals known as quantum dots (QDs) can produce light through photoluminescence. In this process, there is a negatively charged electron in the conduction energy band of the QD and a positively charged hole (electron vacancy) in the valence energy band. The electron moves to the valence band and merges with the hole, releasing a photon. b, However, a process called non-radiative Auger recombination can instead occur. In this process, the energy generated by the electron–hole merger is transferred to another charge carrier, and a photon is not emitted. c, Won et al.3 demonstrate QDs that are highly efficient light emitters, because Auger recombination is suppressed. The QDs consist of a uniform indium phosphide core, a thick inner shell of zinc selenide and a thin outer shell of zinc sulfide. The colour of the light produced by the core depends on its size. Photoluminescence was achieved initially by coating QD surfaces with organic molecules6 and later by using QDs that comprise a semiconductor core surrounded by a shell of a semiconductor that has a large bandgap7 — the energy difference between the valence and conduction bands. In the latter case, the offset in energy between the bands of the shell and those of the semiconductor core prevents electrons and holes from the core escaping to the external surface and enables intrinsic photoluminescence. In cadmium selenide QDs, the size-dependent absorption and photoluminescence spectra cover the entire range of visible wavelengths from red to deep green8. For QDs grown using current techniques, the photoluminescence quantum yield (the number of photons emitted by the QD divided by the number of photons absorbed) can be quite high. However, this high quantum yield is still not good enough for some applications. For instance, a quantum yield of less than 100% is associated with blinking9 — a phenomenon observed in single-QD experiments, in which the photoluminescence intensity varies under constant illumination. This blinking is linked to random processes in which QDs become charged and are subsequently neutralized.   An electron–hole pair that is photoexcited in a neutral QD can recombine only by emitting a photon — in other words, by photoluminescence. However, photoexcitation in a charged QD triggers another recombination process, which is known as non-radiative Auger recombination. In this process, the energy of the photoexcited electron–hole pair is transferred to another electron or hole and a photon is not emitted (Fig. 1b). For commonly used QDs that comprise a core surrounded by a thin shell, the rate of Auger recombination is usually much higher than that of photoluminescence. As a result, the former process completely quenches the latter process in most charged QDs. To achieve a photoluminescence quantum yield that is close to 100%, Auger recombination needs to be suppressed. One approach is to prevent the optically produced electrons and holes from escaping to the QD surface, to avoid charging of the QD. This can be realized, for example, by using QDs that have a thick shell9. In the case of QD-LEDs, QD neutrality can be controlled by ensuring that electron and hole conductivities are similar. A complementary approach is to engineer the QDs to have a soft confinement potential — a potential-energy profile that gently increases at the QD surface and so reduces the rate of Auger recombination10. This potential can be produced by forming an alloy of the core and shell materials at the core–shell interface of QDs, or by using multi-shell QDs in which each subsequent shell has a larger bandgap than the preceding one. Successful efforts to engineer such QDs were reported last year11. However, despite the outstanding optical properties of the cadmium selenide-based QD structures that were attained in that work, the photoluminescence quantum yield did not reach 100%.   Won and colleagues have developed an innovative method for synthesizing heavy-metal-free QDs that consist of a uniform indium phosphide core, a thick inner shell of zinc selenide and a thin outer shell of zinc sulfide (Fig. 1c). The technique involves two consecutive steps for the growth of the core: the addition of hydrofluoric acid to etch off the oxidized core surface during the growth of the initial zinc selenide shell, and high-temperature zinc selenide growth at 340 °C. The resulting QDs have a highly symmetrical spherical shape, which is one of the conditions for realizing a soft confinement potential. Any cavity or sharp corner on the surface or at the core–shell interface would enhance the rate of non-radiative Auger recombination. Charged or deep-level defects would also lead to such enhancement. The authors found that the thick zinc selenide shell suppresses Auger recombination, suggesting that the interface is of extremely high quality and that there are no crystal defects called stacking faults in the zinc selenide shell. The intrinsic photoluminescence quantum yield of these QDs is 100%. Won et al. used their QDs to make LED devices in which electrons and holes are injected into the QDs instead of being photoexcited. To maintain QD neutrality during such injection, and to improve the transport of electrons and holes in the LEDs, the authors replaced long-chain ligand molecules at the QD surface with short-chain ones. The QD-LEDs achieve an external quantum efficiency (the number of photons that leave the LED divided by the number of charges injected into it) of 21.4%, which is the theoretical maximum. The improved injection and transport of charges reduces accumulated electrical resistance during operation,lowers power consumption and increases the lifetime of the LED device. This work shows that the detailed understanding of the physical properties of QDs that has accumulated over more than 30 years should now allow us to engineer QDs for multiple and diverse applications. These could include televisions and displays, LEDs and solar cells. </body>
<date id = '346'>27 November 2019</date>
<url id = '347'>https://nature.com/articles/d41586-019-03605-1</url>
<title id = '347'>The configuration of middle-ear bones in an ancient fossil suggests that specializations suited to eating plants might have influenced how the jaw joint evolved to form the mammal’s ear.</title>
<body id = '347'>The presence of three delicate bones in the middle ear that are completely separated from the lower jaw can be used to distinguish existing mammals from other vertebrates. This arrangement evolved independently at least three times in mammals, so it is not found in all mammalian fossils. Writing in Nature, Wang et al.1 describe a newly discovered fossil that reveals how these different middle ears evolved into distinct configurations.   The authors named this previously unknown species Jeholbaatar kielanae. It was about the size of a vole, and scampered around China about 120 million years ago. It belonged to the longest-lived mammalian lineage, the multituberculates. These typically small-bodied mammals persisted from about 160 million to 34 million years ago, and diverse members of this lineage became common throughout the Northern Hemisphere2. Multituberculates might have been so successful because they chewed differently from other mammals. Instead of slicing food into pieces using a vertical biting motion like a cat does, or grinding their food by moving their lower jaw (the mandible) horizontally and sideways like a cow, multituberculates sliced and ground food by drawing their mandible horizontally but backwards. This innovation, ‘palinal motion’, required specializations of the teeth, jaw joint and musculature. It contributed to the unmatched longevity of the multituberculate lineage, and it facilitated group diversification by enabling multituberculates to use plants as a food source at a time in prehistory when other mammals mainly ate insects or small vertebrates. Wang and colleagues argue that the adaptation of this chewing approach also drove the evolution of an unusual type of ear. In each independent instance, mammalian middle ears evolved from an ancestral jaw joint. In every case, the articular bone at the back of the mandible and the quadrate bone (which became the incus bone of the middle ear) that it made contact with on the skull retained their connection. These bones shifted slightly internally to form a middle ear together with a bone called the stapes, which was present in mammalian ancestors. Other bones then formed the jaw joint that mammals have today. In transitional stages of this evolutionary process, the connection between the middle ear and the mandible was still present at a middle-ear bone called the malleus, although the extent of this connection was reduced compared with the connection in the ancestral state3. Both the jaw and the ear had to function at all stages of the transition. If multituberculates had adopted palinal chewing before the separation of the middle-ear bones from the jaw, how would this arrangement have worked? The tiny but exquisitely preserved middle ear of Jeholbaatar (Fig. 1) is completely separated from the jaw, but it provides the beginning of an answer to this question. Figure 1 | The evolution of mammalian middle ears. Wang et al.1 report the discovery of a fossil of a previously unknown mammalian species, Jeholbaatar kielanae. Its middle ear is similar to that of an extinct animal called Arboroharamiya. a, This similarity might indicate that Jeholbaatar and Arboroharamiya should be grouped close together on a mammalian family tree, and suggests that the ‘palinal’ chewing motion used by Jeholbaatar and Arboroharamiya has a single origin in a shared ancestor. Also shown in this tree are playtpuses (Ornithorynchus) and opossums (Didelphis), mammals that don’t use palinal chewing and that have middle-ear configurations that are distinct from each other and from Jeholbaatar and Arboroharamiya. b, However, there is some debate about whether Arboroharamiya were mammals. If not, as in this tree, then the similar middle ears of Jeholbaatar and Arboroharamiya evolved independently. c, The configurations of the left middle-ear bones of these four creatures are presented as viewed directly from above, with the animal’s front to the right. The different configurations of the incus, malleus and surangular bones might reflect the evolution of jaw specializations before bones separated from the jaw to form the ear. (Images based on ref. 1 and not shown to scale.) It has long been suspected that, in mammalian ancestors, the articular bone and the prearticular bone of the ancestral jaw fused to form the malleus. Fossil discoveries have suggested that a third bone, the surangular, also fused with the articular, at least in some lineages3,4. In Jeholbaatar, the surangular is present as a separate bone distinguishable along the lateral side of the malleus. The only other animal in which a separate surangular has been described in the ear also shares a second odd trait with Jeholbaatar4: the position of the incus in the middle ear. The incus lies flat on top of the malleus in Jeholbaatar, in contrast to its position in humans and opossums (Didelphis), in which it is positioned posteriorly, behind the malleus. This contact between the incus and the malleus in Jeholbaatar, horizontal and parallel to the plane in which the teeth would have met, is what we would expect to see if palinal chewing had evolved before the middle ear was separate from the jaw4.   During transitional evolutionary stages, when the malleus was connected to the mandible, palinal jaw movement would have constrained the plane in which the malleus and incus could have been in contact; had the incus been in the more familiar posterior position found in most mammals today, it would have acted as a stop on backward jaw motion. Once palinal motion for chewing was established, increasing the distance the lower jaw moved forwards and backwards on the jaw joint would have made chewing more efficient. Any remaining tether to the ear would have limited the distance that the lower jaw could travel in a single chew, so selection pressure for a fully separate ear and jaw would have been strong, and full separation could have evolved rapidly. The other animal known to have a surangular in the ear is Arboroharamiya, a member of an ancient group known as euharamiyidans with a palinal element to its chewing and an earlier origin than that of multituberculates4,5. Arboroharamiya, like Jeholbaatar, has its incus positioned above the malleus4,6. The relationship between euharamiyidans and multituberculates on the evolutionary tree is a matter of lively debate, with some studies, including that of Wang and colleagues, showing them to be closely related within mammals3,4,7, whereas others place euharamiyidans on a lineage that branched off before the common ancestor of living mammals evolved8,9. If the latter scenario is the case, then euharamiyidans would represent a fourth instance of the independent evolution of a fully detached middle ear. The question of whether the similarities between the ears of Jeholbaatar and Arboroharamiya reflect a close relationship on the evolutionary tree or independent (convergent) evolution driven by similar chewing adaptations is further complicated by another consideration: the incus of living platypuses (Ornithorhynchus) and echidnas, or spiny anteaters (Tachyglossus), also lies above the malleus. These mammals belong to a group called monotremes, whose middle ear evolved independently of that of other mammals. Monotremes do not use a palinal chewing motion, and the teeth of fossil monotremes do not suggest that such a motion occurred in early members of that lineage10. They might have this arrangement of their incus and malleus for reasons that are entirely different from those explaining the arrangement of these bones in multituberculates or euharamiyidans. Monotremes do not retain a recognizable surangular. If the similarities in the middle ears of Jeholbaatar and Arboroharamiya reflect the functional similarity in the way the animals chewed, the unfused surangular in Jeholbaatar and Arboroharamiya might simply reflect the rapidity with which the transition to detachment of the middle ear from the jaw occurred, spurred on by the increased efficiency in food processing that this complete separation would have provided. </body>
<date id = '347'>27 November 2019</date>
<url id = '348'>https://nature.com/articles/d41586-019-03606-0</url>
<title id = '348'>How Nature reported a fungus responsible for a potato-tuber disease in 1919 and a look-back on the history of astronomy in 1869.</title>
<body id = '348'> In the Kew Bulletin … M. N. Owen gives an account of one of the minor diseases of potato-tubers, which has never been thoroughly investigated. It is known as skin-spot, the tubers becoming dotted with small dark spots during storage. It is found to be due to a minute species of mould-fungus hitherto undescribed (Oospora pustulans). The author describes in detail the structure and development of the fungus as determined from artificial cultures. The disease is confined to the surface layers of the tubers, and, besides disfigurement, may cause serious injury by weakening or destruction of the eyes. From Nature 27 November 1919 It is a well-known remark of the historian of science that our progress in astronomy has been made in exact accordance with certain laws which regulate the advancement of knowledge. Neither the march of the sun by day, nor that of the moon by night, is more rigidly surrounded and circumscribed by law than the march of that intellect which has successfully interpreted celestial motions. We had first of all an observing age. Thousands of years ago in the plains of the East we had astronomers who, albeit with imperfect instruments, lacked neither zeal nor intelligence in their nightly study of the stars. Many of their theoretical ideas were untenable, nay, even absurd, but yet they served to bind together into a formal law the mass of observations which their nightly industry collected. And so step by step our knowledge of celestial motions progressed, until it culminated in the discoveries of Copernicus and Kepler; and we were presented at last with a bird’s-eye view of the solar system, taken, as it were, from without, in which that which appears to be, finally gave place to that which is. From Nature 25 November 1869 Latest on: Agriculture Comment 23 APR 20 Editorial 11 MAR 20 Correspondence 25 FEB 20 Astronomy and astrophysics News & Views 20 MAY 20 Article 20 MAY 20 News Q&A 19 MAY 20 History Obituary 23 MAY 20 Obituary 15 MAY 20 Obituary 12 MAY 20 An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday. Advanced search  ISSN 1476-4687 (online) © 2020 Springer Nature Limited </body>
<date id = '348'>26 November 2019</date>
<url id = '349'>https://nature.com/articles/d41586-019-03492-6</url>
<title id = '349'>A sophisticated imaging pipeline has been developed to track neurons in early-stage zebrafish embryos over time and space. It reveals how newborn neurons come together to build a spinal cord capable of locomotion.</title>
<body id = '349'>Where a person comes from and what they do are often considered key parts of their identity. Similarly, neurons can be categorized by both their developmental history and their role in the nervous system. But, just as knowing someone’s job title does not necessarily tell you what part they play in a team at work, knowing what role a neuron has does not mean that we understand how it comes together with other diverse neuron types to form circuits — for instance, to permit movement. Writing in Cell, Wan et al.1 describe an imaging protocol that will help researchers determine how neural circuits form. They use their method to comprehensively chart motor-circuit assembly and emerging function in the spinal cord of zebrafish. In vertebrate embryos, the first neuronal circuits to respond to sensory information and orchestrate movement are found in the spine2. These motor circuits are assembled from dozens of molecularly specialized types of neuron. Nonetheless, this is a relatively simple set-up, making it a useful system for studying how neuronal circuits come together to produce behaviour — in this case, muscles contracting in distinct patterns. Wan et al. set out to study the formation of these early motor circuits in zebrafish embryos (Fig. 1). This research group has long been at the forefront of in vivo microscopy, pioneering light-sheet microscopy techniques that can illuminate all of the individual cells that make up developing organisms such as zebrafish without harming them. Zebrafish are well suited to such studies because they are small, transparent and develop rapidly. Figure 1 | Tracking the building blocks of a circuit. Wan et al.1 have developed an imaging and computational pipeline to track neurons of the zebrafish spinal cord, from their ‘birth’ 6 hours after embryo fertilization until they begin to show the coordinated activity of a motor circuit at 22 hours. The authors traced newly born sister cells (derived from the same immediate ancestor, indicated by different shades of the same colour). By 17 hours, the cells have migrated to their mature positions and adopted molecular characteristics of either motor neurons (star-shaped cell) or interneurons (circular cell). By 22 hours, the cells become wired into coordinated circuits (inset). Motor neurons are the first to become active, and the authors showed that they then imprint their activity onto other neurons (dotted arrows), leading these neurons to adopt the same activity pattern. The researchers imaged zebrafish from 6 hours after embryo fertilization, when spinal neurons first arise from their progenitors, to 22 hours after fertilization, when the patterns of neuronal activity that trigger tail movements begin. The imaging process generated vast libraries of images that Wan and colleagues processed to extract information about the location of individual cells over time. In addition, the authors optimized their microscope design to allow them to measure emergent patterns of functional activity from individual cells. The result was a data set that enabled the group to track the organization and function of every cell in the zebrafish spinal cord throughout early development.   Motor neurons and interneurons are key neuron types in spinal motor circuits. The former are responsible for triggering muscle-fibre contraction and the latter coordinate signalling within and between circuits3 (for example, to ensure alternating left–right movements during swimming). Motor neurons have often been thought of as passive cells controlled by upstream interneuron inputs, whereas interneurons had been thought to be the driving force behind the assembly and function of spinal motor circuits4. But over the past few years, evidence has emerged that both developing5 and mature6 motor neurons can control their connections to interneurons, and even control interneuron activity. In zebrafish, motor neurons are the first spinal neurons to display spontaneous activity patterns7. As a circuit develops, neurons often first become active on their own, and then coordinate their activity with that of other neurons. Wan et al. therefore asked whether this activity originates in the motor neurons themselves, or reflects interneuron control. The authors found that select motor neurons seem to impose their own activity on neighbouring motor neurons and interneurons, producing pairs of cells that have the same activity patterns. Thus, the earliest patterns of collective activity are initiated by motor neurons. This finding adds to the emerging picture of motor neurons as a fundamental driver of spinal-cord development. Consistent with previous findings, the authors also confirmed that interneurons coordinate the global patterns of activity necessary at later developmental stages for tail movement. One theory of neural development states that cells that have a shared ancestry are destined to have common connectivity, and to perform similar roles in a circuit8. Evidence for such determinism remains contentious, reflecting the challenge of tracing related neurons as they migrate9. But Wan and colleagues were able to investigate this issue, thanks to their ability to comprehensively track cells over time.   The authors examined the activity of sister neurons — those that shared an immediate ancestor. In line with ideas of determinism, sister neurons that ended up in close proximity to one another were more likely than unrelated neurons to be co-active. But, intriguingly, most sibling pairs did not remain close to one another. Indeed, sister neurons were just as likely to migrate to opposite sides of the spinal cord, where they would participate in different phases of movement. Thus, ancestry can explain only a small part of functional organization. That said, Wan and colleagues’ study is limited to the earliest part of development, well before zebrafish hatch and swim freely. It will be interesting to re-evaluate questions of ancestral determinism over longer periods of time. Another limitation of the authors’ technique is that their cutting-edge microscope is best suited to small model organisms. It would be interesting to analyse whether their findings also apply to more-complex organisms. However, current microscopes cannot be used for such purposes. Notably, the group that performed the study (and the Janelia Research Campus in Ashburn, Virginia, at which it works) is committed to providing access to the microscope used in the current work. In addition, the authors’ data and analysis pipelines are available to download. Thus, other researchers can further assess the relationship between the developmental history and function outlined in the current study. Advances in the transcriptional profiling of single cells have revealed remarkable variability among neurons10, making circuit development ever-more fascinating but incredibly challenging to fully understand. Until we have a greater understanding of the molecular logic that enables neurons to form motor circuits, our ability to prevent, diagnose and treat disorders of movement will remain limited. The apparatus and analysis pipeline developed by Wan et al. present a technically demanding but demonstrably fruitful path towards better grasping how a neuron’s birth shapes its future role in a circuit. </body>
<date id = '349'>20 November 2019</date>
<url id = '350'>https://nature.com/articles/d41586-019-03443-1</url>
<title id = '350'>Two studies in flies reveal the mechanism by which the brain’s directional system learns to align information about self-orientation with environmental landmarks — a process crucial for accurate navigation.</title>
<body id = '350'>As everyone knows, a good sense of direction is needed to successfully navigate the world. In mammals, this ‘sense’ involves neurons called head-direction cells. Each such cell becomes most active when the animal faces a particular direction relative to landmarks in its environment. Together, the cells’ activity indicates which direction the animal is facing in at any given moment. In 2015, it emerged that fruit flies, which are much easier than mammals to study experimentally, have strikingly similar cells, called heading neurons1. Writing in Nature, Fisher et al.2 and Kim et al.3 now build on this discovery to tackle a decades-old problem: how does this type of neuron respond to the locations of landmarks in a manner that is stable enough to be reliable, but flexible enough to allow adaptation to new environments? To give an example of the problem, imagine emerging from a subway station onto a crowded street. If you are a regular visitor, a glance around is all you need to be on your way. However, if you have never been to this station before, you might need a moment to orient yourself. You take note of surrounding street signs, shops and monuments. Before long, you have your bearings and can set off in the right direction.   This example highlights two challenges for the brain’s directional system. First, it must stably indicate direction in familiar environments: returning to the same station should call the same orientation to mind. Second, it must have the flexibility to learn new configurations of landmarks, even when similar landmarks have been seen before — the particular configuration of street signs at the new station must be learnt, even though you may have seen similar street signs in other places. The neural mechanisms that underlie these abilities in flies are a beautiful example of form following function. The insects’ heading neurons (also known as E–PG, or compass, neurons) are arranged in a ring (Fig. 1) that corresponds to the 360° of possible directions in which the fly can face1, sometimes called heading angles. Because of inhibition between neurons, only one heading angle can be indicated at one time, providing the fly with an unambiguous signal. Of note, rather than always aligning their activity to a cardinal direction such as north, heading neurons realign their activity arbitrarily when the fly enters a new environment. The heading neurons receive input from visual ring neurons, which are activated by visual cues at particular orientations relative to the fly, and from internal cues about self-motion. Figure 1 | Neurons in the central complex of the fruit-fly brain, tagged with fluorescent proteins. The central complex includes a ring-like structure called the ellipsoid body that contains heading neurons. These cells correspond to all the possible directions in which the fly can face, providing the insect with a compass-like signal that it uses to navigate. Two studies2,3 have revealed how flies orient themselves in familiar environments and adapt to new ones, thanks to signalling to heading neurons from visual ring neurons, which originate in the eyes (not shown).Credit: Tanya Wolff Fisher et al. set out to test whether and how the connections between visual ring neurons and heading neurons change with experience, using a range of experimental techniques (many of which are possible only in fruit flies). They implemented a virtual-reality (VR) system in which the fly walked on a floating ball. An array of lights around the fly flashed on and off in concert with the animal’s movements4, providing visual cues to enable the fly to orient itself. The authors then measured inputs from visual ring neurons to heading neurons as the flies explored this virtual environment. They also used genetic techniques to inhibit the activity of visual ring neurons. These experiments revealed that individual heading neurons are inhibited by visual ring neurons that are activated by visual cues at specific angles relative to the fly. Because of the specificity of this pairing, the visual input reinforces the directional preference of the heading neurons. This work solves the problem of how the brain can transform visual input into a stable directional signal in a familiar environment — the first of the challenges in our subway scenario. Next, Fisher et al. tested how heading neurons can adapt when their environment changes. They presented flies with two identical visual cues, separated by 180° — an ambiguous environment in which a half turn produces the same visual cue as a full turn. The flies’ heading neurons, which can represent only one heading angle at a time, flipped between being preferentially activated by two opposing heading orientations. After the flies were returned to the one-cue world, the relationship between visual input and the activity of the heading network as a whole sometimes changed by 180°. The strength of visual inputs to heading neurons also changed, but only in neurons that were active during the two-cue period. This finding shows that new associations can form between visual ring neurons and heading neurons in new environments. However, simple visual changes are not enough. Instead, there must be a coordinated activation of the upstream visual ring neuron and downstream heading neuron. This leads to a decrease in the strength of the inhibitory synaptic connection between them, so that the heading neuron becomes less sensitive to inhibition by the visual ring neuron — a phenomenon known as associative plasticity.   In a complementary experiment, Kim et al. presented flies with VR scenes derived from natural images, moving a step closer to naturalistic conditions. They then stimulated heading neurons in arbitrary orientations relative to the visual cues the fly was receiving, thereby altering neurons’ preferred heading directions. After this stimulation period, the offsets between heading-neuron activity and visual input remained intact, demonstrating the capacity of the system to learn new visual–heading associations. Even partial views of a scene, when paired with stimulation, caused global changes in the activity of the heading-neuron network. This reveals a useful property of the network for our subway set-up: it enables you to orient yourself at a new station without having to survey all 360° of the scene. But the system’s flexibility could have a downside — if synapses can change, can they also be erased? Kim et al. asked whether the heading network can ‘remember’ multiple scenes. First, they found that presenting flies with different scenes elicited different heading-neuron direction preferences, which varied from fly to fly. But, crucially, these preferences were stable for a given scene for each fly, even when the scene was presented as part of a ‘slide show’ of multiple different scenes. This shows that the fly’s heading network can store and retrieve memories of scenes. The authors conclude their paper by developing theories that predict what types of scene can be simultaneously stored and what kinds of rule allow scenes to be learnt without existing memories being erased. Together, these studies rigorously establish the ability of the fly’s heading network to learn through associative plasticity. Future work should explore the memory capacity of the system. A key question is whether flies and other insects use memories of complex scenes for navigation, or rely more heavily on celestial cues such as the Sun5. Other types of sensory input, such as light polarization, also probably have a role in anchoring insect heading representations, and need to be taken into account. In addition, molecular and cellular work will be needed to uncover the synaptic-plasticity rules at work in the system and to determine whether they match Kim and colleagues’ theoretical prediction. Finally, this work generates hypotheses that should be tested in other species, because many properties of the fruit fly’s heading neurons are similar to those of mammalian head-direction cells. So, although it might not have mastered the subway, the fruit fly has deepened our understanding of the neural mechanisms that underlie our sense of direction. A rich landscape of further research awaits. </body>
<date id = '350'>20 November 2019</date>
