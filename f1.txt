<url>https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/</url>
<title>Implementing Web Scraping in Python with BeautifulSoup</title>
<body>
There are mainly two ways to extract data from a website:

Use the API of the website (if it exists). For example, Facebook has the Facebook Graph API which allows retrieval of data posted on Facebook.
Access the HTML of the webpage and extract useful information/data from it. This technique is called web scraping or web harvesting or web data extraction.

This article discusses the steps involved in web scraping using the implementation of a Web Scraping framework of Python called Beautiful Soup.
Steps involved in web scraping:

Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use a third-party HTTP library for python-requests.
Once we have accessed the HTML content, we are left with the task of parsing the data. Since most of the HTML data is nested, we cannot extract data simply through string processing. One needs a parser which can create a nested/tree structure of the HTML data. There are many HTML parser libraries available but the most advanced one is html5lib.
Now, all we need to do is navigating and searching the parse tree that we created, i.e. tree traversal. For this task, we will be using another third-party python library, Beautiful Soup. It is a Python library for pulling data out of HTML and XML files.

Step 1: Installing the required third-party libraries

Easiest way to install external libraries in python is to use pip. pip is a package management system used to install and manage software packages written in Python.
All you need to do is:

pip install requests
pip install html5lib
pip install bs4

Another way is to download them manually from these links:



Step 2: Accessing the HTML content from webpage
Let us try to understand this piece of code.

First of all import the requests library.
Then, specify the URL of the webpage you want to scrape.
Send a HTTP request to the specified URL and save the response from server in a response object called r.

Now, as print r.content to get the raw HTML content of the webpage. It is of ‘string’ type.

Step 3: Parsing the HTML content
A really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib, lxml, html.parser, etc. So  BeautifulSoup object and specify the parser library can be created at the same time.
In the example above,
soup = BeautifulSoup(r.content, 'html5lib')
We create a BeautifulSoup object by passing two arguments:

r.content : It is the raw HTML content.
html5lib : Specifying the HTML parser we want to use.

Now soup.prettify() is printed, it gives the visual representation of the parse tree created from the raw HTML content.
Step 4: Searching and navigating through the parse tree
Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. In our example, we are scraping a webpage consisting of some quotes. So, we would like to create a program to save those quotes (and all relevant information about them).











import requests 
from bs4 import BeautifulSoup 
import csv 
   
r = requests.get(URL) 
   
soup = BeautifulSoup(r.content, 'html5lib') 
   
quotes=[]  
   
table = soup.find('div', attrs = {'class':'container'}) 
   
for row in table.findAll('div', 
                         attrs = {'class':'col-6 col-lg-3 text-center margin-30px-bottom sm-margin-30px-top'}): 
    quote = {} 
    quote['theme'] = row.h5.text 
    quote['url'] = row.a['href'] 
    quote['img'] = row.img['src'] 
    quote['lines'] = row.img['alt'].split(" #")[0] 
    quote['author'] = row.img['alt'].split(" #")[1] 
    quotes.append(quote) 
   
filename = 'inspirational_quotes.csv'
with open(filename, 'w', newline='') as f: 
    w = csv.DictWriter(f,['theme','url','img','lines','author']) 
    w.writeheader() 
    for quote in quotes: 
        w.writerow(quote) 









Before moving on, we recommend you to go through the HTML content of the webpage which we printed using soup.prettify() method and try to find a pattern or a way to navigate to the quotes.

It is noticed that all the quotes are inside a div container whose id is container. So, we find that div element (termed as table in above code) using find() method :
table = soup.find('div', attrs = {'class':'container'})
The first argument is the HTML tag you want to search and second argument is a dictionary type element to specify the additional attributes associated with that tag. find() method returns the first matching element. You can try to print table.prettify() to get a sense of what this piece of code does.
Now, in the table element, one can notice that each quote is inside a div container whose class is quote. So, we iterate through each div container whose class is quote.
Here, we use findAll() method which is similar to find method in terms of arguments but it returns a list of all matching elements. Each quote is now iterated using a variable called row.
Here is one sample row HTML content for better understanding:

Now consider this piece of code:
for row in table.find_all_next('div', attrs = {'class': 'col-6 col-lg-3 text-center margin-30px-bottom sm-margin-30px-top'}):
    quote = {}
    quote['theme'] = row.h5.text
    quote['url'] = row.a['href']
    quote['img'] = row.img['src']
    quote['lines'] = row.img['alt'].split(" #")[0]
    quote['author'] = row.img['alt'].split(" #")[1]
    quotes.append(quote)
We create a dictionary to save all information about a quote. The nested structure can be accessed using dot notation. To access the text inside an HTML element, we use .text :

quote['theme'] = row.h5.text
We can add, remove, modify and access a tag’s attributes. This is done by treating the tag as a dictionary:
quote['url'] = row.a['href']
Lastly, all the quotes are appended to the list called quotes.
Finally, we would like to save all our data in some CSV file.
filename = 'inspirational_quotes.csv'
with open(filename, 'w', newline='') as f:
    w = csv.DictWriter(f,['theme','url','img','lines','author'])
    w.writeheader()
    for quote in quotes:
        w.writerow(quote)
Here we create a CSV file called inspirational_quotes.csv and save all the quotes in it for any further use.

So, this was a simple example of how to create a web scraper in Python.  From here, you can try to scrap any other website of your choice. In case of any queries, post them below in comments section.
Note : Web Scraping is considered as illegal in many cases.  It may also cause your IP to be blocked permanently by a website.  
This blog is contributed by Nikhil Kumar. If you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.
Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.


</body><url>https://www.geeksforgeeks.org/</url>
<title>A computer science portal for geeks</title>
<body>

			Featured Article		



As the placement season is back so are we to help you ace the interview. We have selected some most commonly asked and must do… Read More »




			Featured Article		



Amazon… “Hire and Develop the Best”. One of the Big Four Tech Companies founder Jeff Bezos is now the richest person on this planet. Jeff… Read More »




			Featured Article		



Topic : Google Facebook Microsoft Adobe Oracle Amazon D E Shaw MAQ Software Directi Yahoo Accolite Walmart Labs Samsung Paytm Ola Cabs Flipkart SAP Labs… Read More »






I’m sure you all use voice assistants like Alexa, Siri, etc. Suppose you ask Alexa what is the weather today? Alexa will handle your request… Read More »






Data science is an extremely important field in current times! So much so that data scientist is now called the “Sexiest Job of the 21st… Read More »






Given an array arr[] of length N, the task is to find the minimum number of prizes required such that if two elements are adjacent,… Read More »






Dynamic Programming is one way which can be used as an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls… Read More »






This is a very simple app suitable for beginners to learn the concepts. The following pre-requisites is good enough before starting. Android fundamentals for beginners… Read More »






Given an undirected graph of N vertices and M edges, the task is to assign directions to the given M Edges such that the graph… Read More »






Given a Doubly linked list and Circular singly linked list containing N nodes, the task is to remove all the nodes from each list which… Read More »






Given a number N, the task is to find the minimum value K such that the sum of cubes of the first K natural number… Read More »






Given a number N, the task is to check if N is a Myriagon Number or not. If the number N is an Myriagon Number… Read More »






Given a number N, the task is to check if N is a Decagonal Number or not. If the number N is an Decagonal Number… Read More »






Given a number N, the task is to check if N is a Octadecagon Number or not. If the number N is an Octadecagon Number… Read More »






Given a tree of N nodes, the task is to find the node having maximum depth starting from the root node, taking the root node… Read More »






Given a number N, the task is to check if N is a Octagonal Number or not. If the number N is an Octagonal Number… Read More »






Given an integer N, the task is to check if N is a Heptagonal Number or not. If the number N is an Heptagonal Number… Read More »






Given a string str consisting of only lowercase English alphabets, the task is to find the substring of smallest length which contains all the vowels.… Read More »






Given a number N, the task is to check if N is a Pentadecagon Number or not. If the number N is an Pentadecagon Number… Read More »


</body><url>https://www.geeksforgeeks.org/category/algorithm/</url>
<title>Algorithms Archives - GeeksforGeeks</title>
<body>



Given an array arr[] of length N, the task is to find the minimum number of prizes required such that if two elements are adjacent,… Read More »






Dynamic Programming is one way which can be used as an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls… Read More »






Given an undirected graph of N vertices and M edges, the task is to assign directions to the given M Edges such that the graph… Read More »






Given a Doubly linked list and Circular singly linked list containing N nodes, the task is to remove all the nodes from each list which… Read More »






Given a number N, the task is to find the minimum value K such that the sum of cubes of the first K natural number… Read More »






Given a number N, the task is to check if N is a Myriagon Number or not. If the number N is an Myriagon Number… Read More »






Given a number N, the task is to check if N is a Decagonal Number or not. If the number N is an Decagonal Number… Read More »






Given a number N, the task is to check if N is a Octadecagon Number or not. If the number N is an Octadecagon Number… Read More »






Given a tree of N nodes, the task is to find the node having maximum depth starting from the root node, taking the root node… Read More »






Given a number N, the task is to check if N is a Octagonal Number or not. If the number N is an Octagonal Number… Read More »






Given an integer N, the task is to check if N is a Heptagonal Number or not. If the number N is an Heptagonal Number… Read More »






Given a number N, the task is to check if N is a Pentadecagon Number or not. If the number N is an Pentadecagon Number… Read More »






Given an array arr[] consisting of N positive numbers and Q queries of the form [L, R], the task is to find the number of… Read More »






Given a string S of length N, and an array A, consisting of lowercase letters. Also given a positive integer K. the task is to… Read More »






Two players player1 and player2 are playing a game on a given number sequence S where player1 starts first and both of them play optimally.… Read More »


</body><url>https://www.geeksforgeeks.org/category/algorithm/analysis/</url>
<title>Analysis Archives - GeeksforGeeks</title>
<body>



Given a tree of N nodes, the task is to find the node having maximum depth starting from the root node, taking the root node… Read More »






Given an array arr[] consisting of N positive numbers and Q queries of the form [L, R], the task is to find the number of… Read More »






Two players player1 and player2 are playing a game on a given number sequence S where player1 starts first and both of them play optimally.… Read More »






Given an array arr containing N values describing the priority of N jobs. The task is to form sets of quadruplets (W, X, Y, Z)… Read More »






Given two integers L and R, the task to find the number of Double Prime numbers in the range. A number N is called double… Read More »






Parenthesis Theorem is used in DFS of graph. It states that the descendants in a depth-first-search tree have an interesting property. If v is a… Read More »






Given an array A containing N integers, the task is to count the number of elements which form a cycle in the array, based on… Read More »






Given a lower alphabetic string “S” of size N, and an integer K; the task is to find the count of characters that will remain… Read More »






Given an array arr[] of N integers, the task is to perform the following two queries: query(L, R): Print the number of Even Parity numbers… Read More »






Given an integer N, the task is to find an array of length N that contains same count of odd and even elements with an… Read More »






Given an array arr[] of N integers and the number K, the task is to find the last occurrence of K in arr[]. If the… Read More »






Prerequiste: NP-Completeness NP Problem: The NP problems set of problems whose solutions are hard to find but easy to verify and are solved by Non-Deterministic… Read More »






Design a Data Structure that can support the following operations in O(1) Time Complexity. insert(x): Inserts x in the data structure. Returns True if x… Read More »






Given a Tree, the task is to find the farthest node from each node to another node in this tree. Example: Input: Given Adjacency List… Read More »






Given a number N. The task is to find the expected number of times a coin must be flipped to get N heads consecutively. Example:… Read More »


</body><url>https://www.geeksforgeeks.org/analysis-of-algorithms-set-1-asymptotic-analysis/</url>
<title>Analysis of Algorithms | Set 1 (Asymptotic Analysis)</title>
<body>
Why performance analysis?
There are many important things that should be taken care of, like user friendliness, modularity, security, maintainability, etc. Why to worry about performance? 
The answer to this is simple, we can have all the above things only if we have performance. So performance is like currency through which we can buy all the above things. Another reason for studying performance is – speed is fun!
To summarize, performance == scale. Imagine a text editor that can load 1000 pages, but can spell check 1 page per minute OR an image editor that takes 1 hour to rotate your image 90 degrees left OR … you get it. If a software feature can not cope with the scale of tasks users need to perform – it is as good as dead. 

Given two algorithms for a task, how do we find out which one is better?
One naive way of doing this is – implement both the algorithms and run the two programs on your computer for different inputs and see which one takes less time. There are many problems with this approach for analysis of algorithms.
1) It might be possible that for some inputs, first algorithm performs better than the second. And for some inputs second performs better.
2) It might also be possible that for some inputs, first algorithm perform better on one machine and the second works better on other machine for some other inputs.
Asymptotic Analysis is the big idea that handles above issues in analyzing algorithms. In Asymptotic Analysis, we evaluate the performance of an algorithm in terms of input size (we don’t measure the actual running time). We calculate, how the time (or space) taken by an algorithm increases with the input size.
For example, let us consider the search problem (searching a given item) in a sorted array. One way to search is Linear Search (order of growth is linear) and the other way is Binary Search (order of growth is logarithmic). To understand how Asymptotic Analysis solves the above mentioned problems in analyzing algorithms, let us say we run the Linear Search on a fast computer A and Binary Search on a slow computer B and we pick the constant values for the two computers so that it tells us exactly how long it takes for the given machine to perform the search in seconds. Let’s say the constant for A is 0.2 and the constant for B is 1000 which means that A is 5000 times more powerful than B. For small values of input array size n, the fast computer may take less time. But, after a certain value of input array size, the Binary Search will definitely start taking less time compared to the Linear Search even though the Binary Search is being run on a slow machine. The reason is the order of growth of Binary Search with respect to input size is logarithmic while the order of growth of Linear Search is linear. So the machine dependent constants can always be ignored after a certain value of input size.
Here are some running times for this example:
Linear Search running time in seconds on A: 0.2 * n
Binary Search running time in seconds on B: 1000*log(n)


------------------------------------------------
|n      | Running time on A | Running time on B |
-------------------------------------------------
|10     | 2 sec             | ~ 1 h             |
-------------------------------------------------
|100    | 20 sec            | ~ 1.8 h           |
-------------------------------------------------
|10^6   | ~ 55.5 h          | ~ 5.5 h           |
-------------------------------------------------
|10^9   | ~ 6.3 years       | ~ 8.3 h           |
-------------------------------------------------


Does Asymptotic Analysis always work?
Asymptotic Analysis is not perfect, but that’s the best way available for analyzing algorithms. For example, say there are two sorting algorithms that take 1000nLogn and 2nLogn time respectively on a machine. Both of these algorithms are asymptotically same (order of growth is nLogn). So, With Asymptotic Analysis, we can’t judge which one is better as we ignore constants in Asymptotic Analysis.
Also, in Asymptotic analysis, we always talk about input sizes larger than a constant value. It might be possible that those large inputs are never given to your software and an algorithm which is asymptotically slower, always performs better for your particular situation. So, you may end up choosing an algorithm that is Asymptotically slower but faster for your software.

GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 

</body><url>https://www.geeksforgeeks.org/analysis-of-algorithms-set-2-asymptotic-analysis/</url>
<title>Analysis of Algorithms | Set 2 (Worst, Average and Best Cases)</title>
<body>
In the previous post, we discussed how Asymptotic analysis overcomes the problems of naive way of analyzing algorithms. In this post, we will take an example of Linear Search and analyze it using Asymptotic analysis.
We can have three cases to analyze an algorithm:
1) Worst Case
2) Average Case
3) Best Case
Let us consider the following implementation of Linear Search.

C++











#include <bits/stdc++.h> 
using namespace std; 
  



int search(int arr[], int n, int x) 
{ 
    int i; 
    for (i=0; i<n; i++) 
    { 
    if (arr[i] == x) 
        return i; 
    } 
    return -1; 
} 
  

int main() 
{ 
    int arr[] = {1, 10, 30, 15}; 
    int x = 30; 
    int n = sizeof(arr)/sizeof(arr[0]); 
    cout << x << " is present at index " 
              << search(arr, n, x); 
  
    getchar(); 
    return 0; 
} 
  










C










#include <stdio.h> 
  



int search(int arr[], int n, int x) 
{ 
    int i; 
    for (i=0; i<n; i++) 
    { 
       if (arr[i] == x) 
         return i; 
    } 
    return -1; 
} 
  

int main() 
{ 
    int arr[] = {1, 10, 30, 15}; 
    int x = 30; 
    int n = sizeof(arr)/sizeof(arr[0]); 
    printf("%d is present at index %d", x, search(arr, n, x)); 
  
    getchar(); 
    return 0; 
} 








Java










  
public class GFG { 
  


    static int search(int arr[], int n, int x) { 
        int i; 
        for (i = 0; i < n; i++) { 
            if (arr[i] == x) { 
                return i; 
            } 
        } 
        return -1; 
    } 
  
    
    public static void main(String[] args) { 
        int arr[] = {1, 10, 30, 15}; 
        int x = 30; 
        int n = arr.length; 
        System.out.printf("%d is present at index %d", x, search(arr, n, x)); 
  
    } 
} 
  
  









Python3










  


def search(arr, n, x): 
    for i in range(n): 
        if arr[i] == x: 
            return i 
    return -1
  

arr = [1, 10, 30, 15] 
x = 30
n = len(arr) 
print(x, "is present at index", 
             search(arr, n, x)) 
  










C#










using System; 
public class GFG { 
   


    static int search(int []arr, int n, int x) { 
        int i; 
        for (i = 0; i < n; i++) { 
            if (arr[i] == x) { 
                return i; 
            } 
        } 
        return -1; 
    } 
   
    
    public static void Main() { 
        int []arr = {1, 10, 30, 15}; 
        int x = 30; 
        int n = arr.Length; 
        Console.WriteLine(x+" is present at index "+search(arr, n, x)); 
   
    } 
} 
   
   









PHP









<?php 

  



function search($arr, $n, $x) 
{ 
    for ($i = 0; $i < $n; $i++) 
    { 
    if ($arr[$i] == $x) 
        return $i; 
    } 
    return -1; 
} 
  

$arr = array(1, 10, 30, 15); 
$x = 30; 
$n = sizeof($arr); 
echo $x . " is present at index ".  
             search($arr, $n, $x); 
  











Output:

30 is present at index 2

Worst Case Analysis (Usually Done)
In the worst case analysis, we calculate upper bound on running time of an algorithm. We must know the case that causes maximum number of operations to be executed. For Linear Search, the worst case happens when the element to be searched (x in the above code) is not present in the array. When x is not present, the search() functions compares it with all the elements of arr[] one by one. Therefore, the worst case time complexity of linear search would be  Θ(n).
Average Case Analysis (Sometimes done) 
In average case analysis, we take all possible inputs and calculate computing time for all of the inputs. Sum all the calculated values and divide the sum by total number of inputs. We must know (or predict) distribution of cases. For the linear search problem, let us assume that all cases are uniformly distributed (including the case of x not being present in array). So we sum all the cases and divide the sum by (n+1). Following is the value of average case time complexity.
Average Case Time = 

                  =  

                  = Θ(n) 

Best Case Analysis (Bogus) 
In the best case analysis, we calculate lower bound on running time of an algorithm. We must know the case that causes minimum number of operations to be executed. In the linear search problem, the best case occurs when x is present at the first location. The number of operations in the best case is constant (not dependent on n). So time complexity in the best case would be Θ(1)
Most of the times, we do worst case analysis to analyze algorithms. In the worst analysis, we guarantee an upper bound on the running time of an algorithm which is good information.
The average case analysis is not easy to do in most of the practical cases and it is rarely done. In the average case analysis, we must know (or predict) the mathematical distribution of all possible inputs.
The Best Case analysis is bogus. Guaranteeing a lower bound on an algorithm doesn’t provide any information as in the worst case, an algorithm may take years to run.
For some algorithms, all the cases are asymptotically same, i.e., there are no worst and best cases. For example, Merge Sort. Merge Sort does Θ(nLogn) operations in all cases. Most of the other sorting algorithms have worst and best cases. For example, in the typical implementation of Quick Sort (where pivot is chosen as a corner element), the worst occurs when the input array is already sorted and the best occur when the pivot elements always divide array in two halves. For insertion sort, the worst case occurs when the array is reverse sorted and the best case occurs when the array is sorted in the same order as output.

GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 


</body><url>https://www.geeksforgeeks.org/analysis-of-algorithms-set-3asymptotic-notations/</url>
<title>Analysis of Algorithms | Set 3 (Asymptotic Notations)</title>
<body>
We have discussed Asymptotic Analysis, and Worst, Average and Best Cases of Algorithms. The main idea of asymptotic analysis is to have a measure of efficiency of algorithms that doesn’t depend on machine specific constants, and doesn’t require algorithms to be implemented and time taken by programs to be compared. Asymptotic notations are mathematical tools to represent time complexity of algorithms for asymptotic analysis. The following 3 asymptotic notations are mostly used to represent time complexity of algorithms.
1) Θ Notation: The theta notation bounds a functions from above and below, so it defines exact asymptotic behavior.
A simple way to get Theta notation of an expression is to drop low order terms and ignore leading constants. For example, consider the following expression.
3n3 + 6n2 + 6000 = Θ(n3)
Dropping lower order terms is always fine because there will always be a n0 after which Θ(n3) has higher values than Θn2) irrespective of the constants involved.
For a given function g(n), we denote Θ(g(n)) is following set of functions.
Θ(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such 
                 that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0}
The above definition means, if f(n) is theta of g(n), then the value f(n) is always between c1*g(n) and c2*g(n) for large values of n (n >= n0). The definition of theta also requires that f(n) must be non-negative for values of n greater than n0.
2) Big O Notation: The Big O notation defines an upper bound of an algorithm, it bounds a function only from above. For example, consider the case of Insertion Sort. It takes linear time in best case and quadratic time in worst case. We can safely say that the time complexity of Insertion sort is O(n^2). Note that O(n^2) also covers linear time.
If we use Θ notation to represent time complexity of Insertion sort, we have to use two statements for best and worst cases:
1. The worst case time complexity of Insertion Sort is Θ(n^2).
2. The best case time complexity of Insertion Sort is Θ(n).
The Big O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm.
O(g(n)) = { f(n): there exist positive constants c and 
                  n0 such that 0 <= f(n) <= c*g(n) for 
                  all n >= n0}
3) Ω Notation: Just as Big O notation provides an asymptotic upper bound on a function, Ω notation provides an asymptotic lower bound.
Ω Notation can be useful when we have lower bound on time complexity of an algorithm. As discussed in the previous post, the best case performance of an algorithm is generally not useful, the Omega notation is the least used notation among all three.
For a given function g(n), we denote by Ω(g(n)) the set of functions.
Ω (g(n)) = {f(n): there exist positive constants c and
                  n0 such that 0 <= c*g(n) <= f(n) for
                  all n >= n0}.
Let us consider the same Insertion sort example here. The time complexity of Insertion Sort can be written as Ω(n), but it is not a very useful information about insertion sort, as we are generally interested in worst case and sometimes in average case.

Properties of Asymptotic Notations :
As we have gone through the definition of this three notations let’s now discuss some important properties of those notations.

General Properties : 
If f(n) is O(g(n)) then a*f(n) is also O(g(n)) ; where a is a constant.
Example: f(n) = 2n²+5 is O(n²)
  then 7*f(n) = 7(2n²+5)
              = 14n²+35 is also O(n²)
Similarly this property satisfies for both Θ and Ω notation.
We can say
If f(n) is Θ(g(n)) then a*f(n) is also Θ(g(n)) ; where a is a constant.
If f(n) is Ω (g(n)) then a*f(n) is also Ω (g(n)) ; where a is a constant. 
Reflexive Properties : 
If f(n) is given then f(n) is O(f(n)).
Example: f(n) = n² ; O(n²) i.e O(f(n))
Similarly this property satisfies for both Θ and Ω  notation.
We can say
If f(n) is given then f(n) is  Θ(f(n)).
If f(n) is given then f(n) is  Ω (f(n)). 
Transitive Properties : 
If f(n) is O(g(n)) and g(n) is O(h(n))  then f(n) = O(h(n))  .
Example: if f(n) = n ,  g(n) = n² and h(n)=n³
       n is O(n²) and n² is O(n³)
       then n is O(n³)
Similarly this property satisfies for both Θ and Ω notation.
We can say
If f(n) is Θ(g(n)) and g(n) is Θ(h(n))  then f(n) = Θ(h(n))  .
If f(n) is Ω (g(n)) and g(n) is Ω (h(n))  then f(n) = Ω (h(n)) 
Symmetric Properties : 
If f(n) is  Θ(g(n))  then g(n) is Θ(f(n)) .
Example: f(n) = n² and g(n) = n²
    then f(n) = Θ(n²) and g(n) = Θ(n²) 
This property only satisfies for Θ notation. 
Transpose Symmetric Properties : 
If f(n) is O(g(n)) then g(n) is Ω (f(n)).
Example: f(n) = n , g(n) = n²
    then n is O(n²) and n² is Ω (n)
This property only satisfies for O and Ω   notations.
Some More Properties : 

 If f(n) = O(g(n)) and f(n) = Ω(g(n)) then f(n) = Θ(g(n))
 If f(n) = O(g(n))  and d(n)=O(e(n))
   then f(n) + d(n) = O( max( g(n), e(n) ))
   Example: f(n) = n i.e O(n)
      d(n) = n² i.e O(n²)
       then f(n) + d(n) = n + n² i.e O(n²)
 If f(n)=O(g(n))  and d(n)=O(e(n))
     then f(n) * d(n) = O(  g(n) * e(n) )
   Example: f(n) = n i.e O(n)
      d(n) = n² i.e O(n²)
        then f(n) * d(n) = n * n² = n³  i.e O(n³)



Exercise:
Which of the following statements is/are valid?
1. Time Complexity of QuickSort is Θ(n^2)
2. Time Complexity of QuickSort is O(n^2)
3. For any two functions f(n) and g(n), we have f(n) = Θ(g(n)) if and only if f(n) = O(g(n)) and f(n) = Ω(g(n)).
4.  Time complexity of all computer algorithms can be written as Ω(1)

Important Links : 
References:
Lec 1 | MIT (Introduction to Algorithms)
This article is contributed by Abhay Rathi. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.


GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 

</body><url>https://www.geeksforgeeks.org/analysis-of-algorithems-little-o-and-little-omega-notations/</url>
<title>Analysis of algorithms | little o and little omega notations</title>
<body>
The main idea of asymptotic analysis is to have a measure of efficiency of algorithms that doesn’t depend on machine specific constants, mainly because this analysis doesn’t require algorithms to be implemented and time taken by programs to be compared. We have already discussed Three main asymptotic notations. The following 2 more asymptotic notations are used to represent time complexity of algorithms.
 Little ο asymptotic notation
Big-Ο is used as a tight upper-bound on the growth of an algorithm’s effort (this effort is described by the function f(n)), even though, as written, it can also be a loose upper-bound. “Little-ο” (ο()) notation is used to describe an upper-bound that cannot be tight. 
Definition : Let f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is ο(g(n)) (or f(n) Ε ο(g(n))) if for any real constant c > 0, there exists an integer constant n0 ≥ 1 such that 0 ≤ f(n) < c*g(n).

Thus, little o() means loose upper-bound of f(n). Little o is a rough estimate of the maximum order of growth whereas Big-Ο may be the actual order of growth.

In mathematical relation,
  f(n) = o(g(n)) means
    lim  f(n)/g(n) = 0
    n→∞
Examples:
Is 7n + 8 ∈ o(n2)?
In order for that to be true, for any c, we have to be able to find an n0 that makes
f(n) < c * g(n) asymptotically true.
lets took some example,
If c = 100,we check the inequality is clearly true. If c = 1/100 , we’ll have to use
a little more imagination, but we’ll be able to find an n0. (Try n0 = 1000.) From
these examples, the conjecture appears to be correct.
then check limits,
 lim  f(n)/g(n) = lim  (7n + 8)/(n2) = lim  7/2n = 0 (l’hospital)
 n→∞              n→∞                 n→∞
hence 7n + 8 ∈ o(n2)
 Little ω asymptotic notation
Definition :  Let f(n) and g(n) be functions that map positive integers to positive real numbers. We say that f(n) is ω(g(n)) (or f(n) ∈ ω(g(n))) if for any real constant c > 0, there exists an integer constant n0 ≥ 1 such that f(n) > c * g(n) ≥ 0 for every integer n ≥ n0.
f(n) has a higher growth rate than g(n) so main difference between Big Omega (Ω) and little omega (ω) lies in their definitions.In the case of Big Omega f(n)=Ω(g(n)) and the bound is 0<=cg(n)<=f(n), but in case of little omega, it is true for 0<=c*g(n)<f(n).

The relationship between Big Omega (Ω) and Little Omega (ω) is similar to that of Big-Ο and Little o except that now we are looking at the lower bounds. Little Omega (ω) is a rough estimate of the order of the growth whereas Big Omega (Ω) may represent exact order of growth. We use ω notation to denote a lower bound that is not asymptotically tight.
And, f(n) ∈ ω(g(n)) if and only if g(n) ∈ ο((f(n)).

In mathematical relation,
if f(n) ∈ ω(g(n)) then,
 lim  f(n)/g(n) = ∞
         n→∞
Example:
Prove that 4n + 6 ∈ ω(1);
the little omega(ο) running time can be proven by applying limit formula given below.
if lim  f(n)/g(n) = ∞ then functions f(n) is ω(g(n))
   n→∞
here,we have functions f(n)=4n+6 and g(n)=1
lim   (4n+6)/(1) =  ∞
n→∞
and,also for any c we can get n0 for this inequality 0 <= c*g(n) < f(n), 0 <= c*1 < 4n+6
Hence proved.
This article is contributed by Kadam Patel. If you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.
Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 

</body><url>https://www.geeksforgeeks.org/lower-and-upper-bound-theory/</url>
<title>Lower and Upper Bound Theory</title>
<body>
The Lower and Upper Bound Theory provides a way to find the lowest complexity algorithm to solve a problem. Before understanding the theory, first lets have a brief look on what actually Lower and Upper bounds are. 

Lower Bound –
Let L(n) be the running time of an algorithm A(say), then g(n) is the Lower Bound of A if there exist two constants C and N such that L(n) >= C*g(n) for n > N. Lower bound of an algorithm is shown by the asymptotic notation called Big Omega (or just Omega). 

Upper Bound –
Let U(n) be the running time of an algorithm A(say), then g(n) is the Upper Bound of A if there exist two constants C and N such that U(n) <= C*g(n) for n > N. Upper bound of an algorithm is shown by the asymptotic notation called Big Oh(O) (or just Oh).


1. Lower Bound Theory:
According to the lower bound theory, for a lower bound L(n) of an algorithm, it is not possible to have any other algorithm (for a common problem) whose time complexity is less than L(n) for random input. Also every algorithm must take at least L(n) time in worst case. Note that L(n) here is the minimum of all the possible algorithm, of maximum complexity.    
The Lower Bound is a very important for any algorithm. Once we calculated it, then we can compare it with the actual complexity of the algorithm and if their order are same then we can declare our algorithm as optimal. So in this section we will be discussing about techniques for finding the lower bound of an algorithm. 
Note that our main motive is to get an optimal algorithm, which is the one having its Upper Bound Same as its Lower Bound (U(n)=L(n)). Merge Sort is a common example of an optimal algorithm.
Trivial Lower Bound –
It is the easiest method to find the lower bound. The Lower bounds which can be easily observed on the basis of the number of input taken and the number of output produces are called Trivial Lower Bound. 
Example: Multiplication of n x n matrix, where,
Input: For 2 matrix we will have 2n2 inputs
Output: 1 matrix of order n x n, i.e.,  n2 outputs 
In the above example its easily predictable that the lower bound is O(n2). 
Computational Model –
The method is for all those algorithms that are comparison based. For example in sorting we have to compare the elements of the list among themselves and then sort them accordingly. Similar is the case with searching and thus we can implement the same in this case. Now we will look at some examples to understand its usage.
Ordered Searching –
It is a type of searching in which the list is already sorted.
Example-1: Linear search
Explanation –
In linear search we compare the key with first element if it does not match we compare with second element and so on till we check against the nth element. Else we will end up with a failure. 
Example-2: Binary search
Explanation –
In binary search, we check the middle element against the key, if it is greater we search the first half else we check the second half and repeat the same process.
The diagram below there is an illustration of binary search in an array consisting of 4 elements 

Calculating the lower bound: The max no of comparisons are n. Let there be k levels in the tree. 

No. of nodes will be 2k-1
The upper bound of no of nodes in any comparison based search of an element in list of size n will be n as there are maximum of n comparisons in worst case scenario 2k-1
Each level will take 1 comparison thus no. of comparisons k≥|log2n|

Thus the lower bound of any comparison based search from a list of n elements cannot be less than log(n). Therefore we can say that Binary Search is optimal as its complexity is Θ(log n).
Sorting –
The diagram below is an example of tree formed in sorting combinations with 3 elements.

Example – For n elements, finding lower bound using computation model.
Explanation –
For n elements we have a total on n! combinations (leaf nodes). (Refer the diagram the total combinations are 3! or 6) also it is clear that the tree formed is a binary tree. Each level in the diagram indicates a comparison. Let there be k levels => 2k is the total number of leaf nodes in a full binary tree thus in this case we have n!≤2k.
As the k in the above example is the no of comparisons thus by computational model lower bond = k. 
Now we can say that,
n!≤2T(n)
Thus, 
T(n)>|log n!| 
=> n!<=nn
Thus,
log n!<=log nn
Taking ceiling function on both sides, we get
|-log nn-|>=|-log n!-|
Thus complexity becomes Θ(lognn) or Θ(nlogn) 
Using Lower bond theory to solve algebraic problem:

Straight Line Program –
The type of programs build without any loops or control structures is called Straight Line Program. For example,










Sum(a, b) 
{ 
    
    c:= a+b; 
    return c; 
} 










Algebraic Problem –
Problems related to algebra like solving equations inequalities etc., comes under algebraic problems. For example, solving equation ax2+bx+c with simple programming.









Algo_Sol(a, b, c, x) 
{  
    
    v:=a*x;  
  
    
    v:=v+b; 
  
    
    v:=v*x; 
  
    
    ans:=v+c; 
    return ans; 
}     









Complexity for solving here is 4 (excluding the returning). 
The above example shows us a simple way to solve an equation for 2 degree polynomial i.e., 4 thus for nth degree polynomial we will have complexity of O(n2). 
Let us demonstrate via an algorithm.    
Example: anxn+an-1xn-1+an-2xn-2+…+a1x+a0 is a polynomial of degree n.









pow(x, n)  
  {  
    p := 1;  
    
    
    for i:=1 to n  
        p := p*x;  
  
    return p;  
  }  
  
polynomial(A, x, n)  
  {  
     int p, v:=0;  
     for i := 0 to n  
    
         
         v := v + A[i]*pow(x, i);  
     return v;  
  }  









Loop within a loop => complexity = O(n2);
Now to find an optimal algorithm we need to find the lower bound here (as per lower bound theory). As per Lower Bound Theory, The optimal algorithm to solve the above problem is the one having complexity O(n). Lets prove this theorem using lower bounds.
Theorem: To prove that optimal algo of solving a n degree polynomial is O(n)
Proof: The best solution for reducing the algo is to make this problem less complex by dividing the polynomial into several straight line problems.
=> anxn+an-1xn-1+an-2xn-2+...+a1x+a0 
can be written as, 
((..(anx+an-1)x+..+a2)x+a1)x+a0
Now, algorithm will be as,
v=0
v=v+an
v=v*x
v=v+an-1
v=v*x
...
v=v+a1
v=v*x
v=v+a0 









polynomial(A, x, n)  
     { 
      int p, v=0;  
  
      
      for i = n to 0  
             v = (v + A[i])*x; 
  
         return v; 
      } 









Clearly, the complexity of this code is O(n). This way of solving such equations is called Horner’s method. Here is were lower bound theory works and give the optimum algorithm’s complexity as O(n).


2. Upper Bound Theory:
According to the upper bound theory, for an upper bound U(n) of an algorithm, we can always solve the problem in at most U(n) time.Time taken by a known algorithm to solve a problem with worse case input gives us the upper bound.
GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 
If you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.Please Improve this article if you find anything incorrect by clicking on  the "Improve Article" button below.

</body><url>https://www.geeksforgeeks.org/analysis-of-algorithms-set-4-analysis-of-loops/</url>
<title>Analysis of Algorithms | Set 4 (Analysis of Loops)</title>
<body>
We have discussed Asymptotic Analysis,  Worst, Average and Best Cases  and Asymptotic Notations in previous posts. In this post, analysis of iterative programs with simple examples is discussed.
1) O(1): Time complexity of a function (or set of statements) is considered as O(1) if it doesn’t contain loop, recursion and call to any other non-constant time function.
   // set of non-recursive and non-loop statements
For example swap() function has O(1) time complexity.
A loop or recursion that runs a constant number of times is also considered as O(1). For example the following loop is O(1).
   // Here c is a constant   
   for (int i = 1; i <= c; i++) {  
        // some O(1) expressions
   }
2) O(n): Time Complexity of a loop is considered as O(n) if the loop variables is incremented / decremented by a constant amount. For example following functions have O(n) time complexity.
   // Here c is a positive integer constant   
   for (int i = 1; i <= n; i += c) {  
        // some O(1) expressions
   }

   for (int i = n; i > 0; i -= c) {
        // some O(1) expressions
   }
3) O(nc): Time complexity of nested loops is equal to the number of times the innermost statement is executed. For example the following sample loops have O(n2) time complexity
  
   for (int i = 1; i <=n; i += c) {
       for (int j = 1; j <=n; j += c) {
          // some O(1) expressions
       }
   }

   for (int i = n; i > 0; i -= c) {
       for (int j = i+1; j <=n; j += c) {
          // some O(1) expressions
   }
For example Selection sort and Insertion Sort have O(n2) time complexity.
4) O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount.
   for (int i = 1; i <=n; i *= c) {
       // some O(1) expressions
   }
   for (int i = n; i > 0; i /= c) {
       // some O(1) expressions
   }
For example Binary Search(refer iterative implementation) has O(Logn) time complexity. Let us see mathematically how it is O(Log n). The series that we get in first loop is 1, c, c2, c3, … ck.  If we put k equals to Logcn, we get cLogcn which is n.
5) O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables is reduced / increased exponentially by a constant amount.
   // Here c is a constant greater than 1   
   for (int i = 2; i <=n; i = pow(i, c)) { 
       // some O(1) expressions
   }
   //Here fun is sqrt or cuberoot or any other constant root
   for (int i = n; i > 1; i = fun(i)) { 
       // some O(1) expressions
   }
See this for mathematical details.
How to combine time complexities of consecutive loops?
When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops.
   for (int i = 1; i <=m; i += c) {  
        // some O(1) expressions
   }
   for (int i = 1; i <=n; i += c) {
        // some O(1) expressions
   }
   Time complexity of above code is O(m) + O(n) which is O(m+n)
   If m == n, the time complexity becomes O(2n) which is O(n).   

How to calculate time complexity when there are many if, else statements inside loops?
As discussed here, worst case time complexity is the most useful among best, average and worst. Therefore we need to consider worst case. We evaluate the situation when values in if-else conditions cause maximum number of statements to be executed.
For example consider the linear search function where we consider the case when element is present at the end or not present at all.
When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if else and other complex control statements.
How to calculate time complexity of recursive functions?
Time complexity of a recursive function can be written as a mathematical recurrence relation. To calculate time complexity, we must know how to solve recurrences. We will soon be discussing recurrence solving techniques as a separate post.
Quiz on Analysis of Algorithms
Next – Analysis of Algorithm | Set 4 (Solving Recurrences)
Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 

</body><url>https://www.geeksforgeeks.org/analysis-algorithm-set-4-master-method-solving-recurrences/</url>
<title>Analysis of Algorithm | Set 4 (Solving Recurrences)</title>
<body>

In the previous post, we discussed analysis of loops. Many algorithms are recursive in nature. When we analyze them, we get a recurrence relation for time complexity. We get running time on an input of size n as a function of n and the running time on inputs of smaller sizes. For example in Merge Sort, to sort a given array, we divide it in two halves and recursively repeat the process for the two halves. Finally we merge the results. Time complexity of Merge Sort can be written as T(n) = 2T(n/2) + cn. There are many other algorithms like Binary Search, Tower of Hanoi, etc.
There are mainly three ways for solving recurrences.
1) Substitution Method: We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect.

For example consider the recurrence T(n) = 2T(n/2) + n

We guess the solution as T(n) = O(nLogn). Now we use induction
to prove our guess.

We need to prove that T(n) <= cnLogn. We can assume that it is true
for values smaller than n.

T(n) = 2T(n/2) + n
    <= 2cn/2Log(n/2) + n
    =  cnLogn - cnLog2 + n
    =  cnLogn - cn + n
    <= cnLogn
2) Recurrence Tree Method: In this method, we draw a recurrence tree and calculate the time taken by every level of tree. Finally, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. The pattern is typically a arithmetic or geometric series.
For example consider the recurrence relation 
T(n) = T(n/4) + T(n/2) + cn2

           cn2
         /      \
     T(n/4)     T(n/2)

If we further break down the expression T(n/4) and T(n/2), 
we get following recursion tree.

                cn2
           /           \      
       c(n2)/16      c(n2)/4
      /      \          /     \
  T(n/16)     T(n/8)  T(n/8)    T(n/4) 
Breaking down further gives us following
                 cn2
            /            \      
       c(n2)/16          c(n2)/4
       /      \            /      \
c(n2)/256   c(n2)/64  c(n2)/64    c(n2)/16
 /    \      /    \    /    \       /    \  

To know the value of T(n), we need to calculate sum of tree 
nodes level by level. If we sum the above tree level by level, 
we get the following series
T(n)  = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + ....
The above series is geometrical progression with ratio 5/16.

To get an upper bound, we can sum the infinite series. 
We get the sum as (n2)/(1 - 5/16) which is O(n2)
3) Master Method:
Master Method is a direct way to get the solution. The master method works only for following type of recurrences or for recurrences that can be transformed to following type.
T(n) = aT(n/b) + f(n) where a >= 1 and b > 1
There are following three cases:
1. If f(n) = Θ(nc) where c < Logba then T(n) = Θ(nLogba)
2. If f(n) = Θ(nc) where c = Logba then T(n) = Θ(ncLog n)
3.If f(n) = Θ(nc) where c > Logba then T(n) = Θ(f(n))
How does this work?
Master method is mainly derived from recurrence tree method. If we draw recurrence tree of T(n) = aT(n/b) + f(n), we can see that the work done at root is f(n) and work done at all leaves is Θ(nc) where c is Logba. And the height of recurrence tree is Logbn

In recurrence tree method, we calculate total work done. If the work done at leaves is polynomially more, then leaves are the dominant part, and our result becomes the work done at leaves (Case 1). If work done at leaves and root is asymptotically same, then our result becomes height multiplied by work done at any level (Case 2). If work done at root is asymptotically more, then our result becomes work done at root (Case 3).
Examples of some standard algorithms whose time complexity can be evaluated using Master Method 
Merge Sort: T(n) = 2T(n/2) + Θ(n). It falls in case 2 as c is 1 and Logba] is also 1. So the solution is Θ(n Logn)
Binary Search: T(n) = T(n/2) + Θ(1). It also falls in case 2 as c is 0 and Logba is also 0. So the solution is Θ(Logn)
Notes: 
1) It is not necessary that a recurrence of the form T(n) = aT(n/b) + f(n) can be solved using Master Theorem. The given three cases have some gaps between them. For example, the recurrence T(n) = 2T(n/2) + n/Logn cannot be solved using master method.
2) Case 2 can be extended for f(n) = Θ(ncLogkn)
If f(n) = Θ(ncLogkn) for some constant k >= 0 and c = Logba, then T(n) = Θ(ncLogk+1n)
Practice Problems and Solutions on Master Theorem.
Next – Analysis of Algorithm | Set 5 (Amortized Analysis Introduction)
References:
http://en.wikipedia.org/wiki/Master_theorem
MIT Video Lecture on Asymptotic Notation | Recurrences | Substitution, Master Method
Introduction to Algorithms 3rd Edition by Clifford Stein, Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest
Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
GeeksforGeeks has prepared a complete interview preparation course with premium videos, theory, practice problems, TA support and many more features. Please refer Placement 100 for details 

</body>